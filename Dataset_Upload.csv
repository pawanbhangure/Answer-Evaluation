Id,TITLE,ABSTRACT,Computer Science,Physics,Mathematics,Statistics,Quantitative Biology,Quantitative Finance
1,Reconstructing Subject-Specific Effect Maps,"  Predictive models allow subject-specific inference when analyzing disease
related alterations in neuroimaging data. Given a subject's data, inference can
be made at two levels: global, i.e. identifiying condition presence for the
subject, and local, i.e. detecting condition effect on each individual
measurement extracted from the subject's data. While global inference is widely
used, local inference, which can be used to form subject-specific effect maps,
is rarely used because existing models often yield noisy detections composed of
dispersed isolated islands. In this article, we propose a reconstruction
method, named RSM, to improve subject-specific detections of predictive
modeling approaches and in particular, binary classifiers. RSM specifically
aims to reduce noise due to sampling error associated with using a finite
sample of examples to train classifiers. The proposed method is a wrapper-type
algorithm that can be used with different binary classifiers in a diagnostic
manner, i.e. without information on condition presence. Reconstruction is posed
as a Maximum-A-Posteriori problem with a prior model whose parameters are
estimated from training data in a classifier-specific fashion. Experimental
evaluation is performed on synthetically generated data and data from the
Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on
synthetic data demonstrate that using RSM yields higher detection accuracy
compared to using models directly or with bootstrap averaging. Analyses on the
ADNI dataset show that RSM can also improve correlation between
subject-specific detections in cortical thickness data and non-imaging markers
of Alzheimer's Disease (AD), such as the Mini Mental State Examination Score
and Cerebrospinal Fluid amyloid-$\beta$ levels. Further reliability studies on
the longitudinal ADNI dataset show improvement on detection reliability when
RSM is used.
",1,0,0,0,0,0
2,Rotation Invariance Neural Network,"  Rotation invariance and translation invariance have great values in image
recognition tasks. In this paper, we bring a new architecture in convolutional
neural network (CNN) named cyclic convolutional layer to achieve rotation
invariance in 2-D symbol recognition. We can also get the position and
orientation of the 2-D symbol by the network to achieve detection purpose for
multiple non-overlap target. Last but not least, this architecture can achieve
one-shot learning in some cases using those invariance.
",1,0,0,0,0,0
3,Spherical polyharmonics and Poisson kernels for polyharmonic functions,"  We introduce and develop the notion of spherical polyharmonics, which are a
natural generalisation of spherical harmonics. In particular we study the
theory of zonal polyharmonics, which allows us, analogously to zonal harmonics,
to construct Poisson kernels for polyharmonic functions on the union of rotated
balls. We find the representation of Poisson kernels and zonal polyharmonics in
terms of the Gegenbauer polynomials. We show the connection between the
classical Poisson kernel for harmonic functions on the ball, Poisson kernels
for polyharmonic functions on the union of rotated balls, and the Cauchy-Hua
kernel for holomorphic functions on the Lie ball.
",0,0,1,0,0,0
4,A finite element approximation for the stochastic Maxwell--Landau--Lifshitz--Gilbert system,"  The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the
Maxwell equations (the so called stochastic MLLG system) describes the creation
of domain walls and vortices (fundamental objects for the novel nanostructured
magnetic memories). We first reformulate the stochastic LLG equation into an
equation with time-differentiable solutions. We then propose a convergent
$\theta$-linear scheme to approximate the solutions of the reformulated system.
As a consequence, we prove convergence of the approximate solutions, with no or
minor conditions on time and space steps (depending on the value of $\theta$).
Hence, we prove the existence of weak martingale solutions of the stochastic
MLLG system. Numerical results are presented to show applicability of the
method.
",0,0,1,0,0,0
5,Comparative study of Discrete Wavelet Transforms and Wavelet Tensor Train decomposition to feature extraction of FTIR data of medicinal plants,"  Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species
were used to explore the influence of preprocessing and feature extraction on
efficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and
Discrete Wavelet Transforms (DWT) were compared as feature extraction
techniques for FTIR data of medicinal plants. Various combinations of signal
processing steps showed different behavior when applied to classification and
clustering tasks. Best results for WTT and DWT found through grid search were
similar, significantly improving quality of clustering as well as
classification accuracy for tuned logistic regression in comparison to original
spectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a
more versatile and easier to use as a data processing tool in various signal
processing applications.
",1,0,0,1,0,0
6,On maximizing the fundamental frequency of the complement of an obstacle,"  Let $\Omega \subset \mathbb{R}^n$ be a bounded domain satisfying a
Hayman-type asymmetry condition, and let $ D $ be an arbitrary bounded domain
referred to as ""obstacle"". We are interested in the behaviour of the first
Dirichlet eigenvalue $ \lambda_1(\Omega \setminus (x+D)) $. First, we prove an
upper bound on $ \lambda_1(\Omega \setminus (x+D)) $ in terms of the distance
of the set $ x+D $ to the set of maximum points $ x_0 $ of the first Dirichlet
ground state $ \phi_{\lambda_1} > 0 $ of $ \Omega $. In short, a direct
corollary is that if \begin{equation} \mu_\Omega := \max_{x}\lambda_1(\Omega
\setminus (x+D)) \end{equation} is large enough in terms of $ \lambda_1(\Omega)
$, then all maximizer sets $ x+D $ of $ \mu_\Omega $ are close to each maximum
point $ x_0 $ of $ \phi_{\lambda_1} $.
Second, we discuss the distribution of $ \phi_{\lambda_1(\Omega)} $ and the
possibility to inscribe wavelength balls at a given point in $ \Omega $.
Finally, we specify our observations to convex obstacles $ D $ and show that
if $ \mu_\Omega $ is sufficiently large with respect to $ \lambda_1(\Omega) $,
then all maximizers $ x+D $ of $ \mu_\Omega $ contain all maximum points $ x_0
$ of $ \phi_{\lambda_1(\Omega)} $.
",0,0,1,0,0,0
7,On the rotation period and shape of the hyperbolic asteroid 1I/`Oumuamua (2017) U1 from its lightcurve,"  We observed the newly discovered hyperbolic minor planet 1I/`Oumuamua (2017
U1) on 2017 October 30 with Lowell Observatory's 4.3-m Discovery Channel
Telescope. From these observations, we derived a partial lightcurve with
peak-to-trough amplitude of at least 1.2 mag. This lightcurve segment rules out
rotation periods less than 3 hr and suggests that the period is at least 5 hr.
On the assumption that the variability is due to a changing cross section, the
axial ratio is at least 3:1. We saw no evidence for a coma or tail in either
individual images or in a stacked image having an equivalent exposure time of
9000 s.
",0,1,0,0,0,0
8,Adverse effects of polymer coating on heat transport at solid-liquid interface,"  The ability of metallic nanoparticles to supply heat to a liquid environment
under exposure to an external optical field has attracted growing interest for
biomedical applications. Controlling the thermal transport properties at a
solid-liquid interface then appears to be particularly relevant. In this work,
we address the thermal transport between water and a gold surface coated by a
polymer layer. Using molecular dynamics simulations, we demonstrate that
increasing the polymer density displaces the domain resisting to the heat flow,
while it doesn't affect the final amount of thermal energy released in the
liquid. This unexpected behavior results from a trade-off established by the
increasing polymer density which couples more efficiently with the solid but
initiates a counterbalancing resistance with the liquid.
",0,1,0,0,0,0
9,"SPH calculations of Mars-scale collisions: the role of the Equation of State, material rheologies, and numerical effects","  We model large-scale ($\approx$2000km) impacts on a Mars-like planet using a
Smoothed Particle Hydrodynamics code. The effects of material strength and of
using different Equations of State on the post-impact material and temperature
distributions are investigated. The properties of the ejected material in terms
of escaping and disc mass are analysed as well. We also study potential
numerical effects in the context of density discontinuities and rigid body
rotation. We find that in the large-scale collision regime considered here
(with impact velocities of 4km/s), the effect of material strength is
substantial for the post-impact distribution of the temperature and the
impactor material, while the influence of the Equation of State is more subtle
and present only at very high temperatures.
",0,1,0,0,0,0
10,$\mathcal{R}_{0}$ fails to predict the outbreak potential in the presence of natural-boosting immunity,"  Time varying susceptibility of host at individual level due to waning and
boosting immunity is known to induce rich long-term behavior of disease
transmission dynamics. Meanwhile, the impact of the time varying heterogeneity
of host susceptibility on the shot-term behavior of epidemics is not
well-studied, even though the large amount of the available epidemiological
data are the short-term epidemics. Here we constructed a parsimonious
mathematical model describing the short-term transmission dynamics taking into
account natural-boosting immunity by reinfection, and obtained the explicit
solution for our model. We found that our system show ""the delayed epidemic"",
the epidemic takes off after negative slope of the epidemic curve at the
initial phase of epidemic, in addition to the common classification in the
standard SIR model, i.e., ""no epidemic"" as $\mathcal{R}_{0}\leq1$ or normal
epidemic as $\mathcal{R}_{0}>1$. Employing the explicit solution we derived the
condition for each classification.
",0,0,0,0,1,0
11,A global sensitivity analysis and reduced order models for hydraulically-fractured horizontal wells,"  We present a systematic global sensitivity analysis using the Sobol method
which can be utilized to rank the variables that affect two quantity of
interests -- pore pressure depletion and stress change -- around a
hydraulically-fractured horizontal well based on their degree of importance.
These variables include rock properties and stimulation design variables. A
fully-coupled poroelastic hydraulic fracture model is used to account for pore
pressure and stress changes due to production. To ease the computational cost
of a simulator, we also provide reduced order models (ROMs), which can be used
to replace the complex numerical model with a rather simple analytical model,
for calculating the pore pressure and stresses at different locations around
hydraulic fractures. The main findings of this research are: (i) mobility,
production pressure, and fracture half-length are the main contributors to the
changes in the quantities of interest. The percentage of the contribution of
each parameter depends on the location with respect to pre-existing hydraulic
fractures and the quantity of interest. (ii) As the time progresses, the effect
of mobility decreases and the effect of production pressure increases. (iii)
These two variables are also dominant for horizontal stresses at large
distances from hydraulic fractures. (iv) At zones close to hydraulic fracture
tips or inside the spacing area, other parameters such as fracture spacing and
half-length are the dominant factors that affect the minimum horizontal stress.
The results of this study will provide useful guidelines for the stimulation
design of legacy wells and secondary operations such as refracturing and infill
drilling.
",1,0,0,0,0,0
12,Role-separating ordering in social dilemmas controlled by topological frustration,"  ""Three is a crowd"" is an old proverb that applies as much to social
interactions, as it does to frustrated configurations in statistical physics
models. Accordingly, social relations within a triangle deserve special
attention. With this motivation, we explore the impact of topological
frustration on the evolutionary dynamics of the snowdrift game on a triangular
lattice. This topology provides an irreconcilable frustration, which prevents
anti-coordination of competing strategies that would be needed for an optimal
outcome of the game. By using different strategy updating protocols, we observe
complex spatial patterns in dependence on payoff values that are reminiscent to
a honeycomb-like organization, which helps to minimize the negative consequence
of the topological frustration. We relate the emergence of these patterns to
the microscopic dynamics of the evolutionary process, both by means of
mean-field approximations and Monte Carlo simulations. For comparison, we also
consider the same evolutionary dynamics on the square lattice, where of course
the topological frustration is absent. However, with the deletion of diagonal
links of the triangular lattice, we can gradually bridge the gap to the square
lattice. Interestingly, in this case the level of cooperation in the system is
a direct indicator of the level of topological frustration, thus providing a
method to determine frustration levels in an arbitrary interaction network.
",0,1,0,0,0,0
13,Dynamics of exciton magnetic polarons in CdMnSe/CdMgSe quantum wells: the effect of self-localization,"  We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se
diluted-magnetic-semiconductor quantum wells using time-resolved
photoluminescence (PL). The magnetic field and temperature dependencies of this
dynamics allow us to separate the non-magnetic and magnetic contributions to
the exciton localization. We deduce the EMP energy of 14 meV, which is in
agreement with time-integrated measurements based on selective excitation and
the magnetic field dependence of the PL circular polarization degree. The
polaron formation time of 500 ps is significantly longer than the corresponding
values reported earlier. We propose that this behavior is related to strong
self-localization of the EMP, accompanied with a squeezing of the heavy-hole
envelope wavefunction. This conclusion is also supported by the decrease of the
exciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and
temperature.
",0,1,0,0,0,0
14,On Varieties of Ordered Automata,"  The classical Eilenberg correspondence, based on the concept of the syntactic
monoid, relates varieties of regular languages with pseudovarieties of finite
monoids. Various modifications of this correspondence appeared, with more
general classes of regular languages on one hand and classes of more complex
algebraic structures on the other hand. For example, classes of languages need
not be closed under complementation or all preimages under homomorphisms, while
monoids can be equipped with a compatible order or they can have a
distinguished set of generators. Such generalized varieties and pseudovarieties
also have natural counterparts formed by classes of finite (ordered) automata.
In this paper the previous approaches are combined. The notion of positive
$\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final
states are specified) is introduced and their correspondence with positive
$\mathcal C$-varieties of languages is proved.
",1,0,0,0,0,0
15,Direct Evidence of Spontaneous Abrikosov Vortex State in Ferromagnetic Superconductor EuFe$_2$(As$_{1-x}$P$_x$)$_2$ with $x=0.21$,"  Using low-temperature Magnetic Force Microscopy (MFM) we provide direct
experimental evidence for spontaneous vortex phase (SVP) formation in
EuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting
$T^{\rm 0}_{\rm SC}=23.6$~K and ferromagnetic $T_{\rm FM}\sim17.7$~K transition
temperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the
vicinity of $T_{\rm FM}$. Also, upon cooling cycle near $T_{\rm FM}$ we observe
the first-order transition from the short period domain structure, which
appears in the Meissner state, into the long period domain structure with
spontaneous vortices. It is the first experimental observation of this scenario
in the ferromagnetic superconductors. Low-temperature phase is characterized by
much larger domains in V-AV state and peculiar branched striped structures at
the surface, which are typical for uniaxial ferromagnets with perpendicular
magnetic anisotropy (PMA). The domain wall parameters at various temperatures
are estimated.
",0,1,0,0,0,0
16,A rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ with 432 symmetries,"  The recent discovery that the exponent of matrix multiplication is determined
by the rank of the symmetrized matrix multiplication tensor has invigorated
interest in better understanding symmetrized matrix multiplication. I present
an explicit rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ and
describe its symmetry group.
",0,0,1,0,0,0
17,The PdBI Arcsecond Whirlpool Survey (PAWS). The Role of Spiral Arms in Cloud and Star Formation,"  The process that leads to the formation of the bright star forming sites
observed along prominent spiral arms remains elusive. We present results of a
multi-wavelength study of a spiral arm segment in the nearby grand-design
spiral galaxy M51 that belongs to a spiral density wave and exhibits nine gas
spurs. The combined observations of the(ionized, atomic, molecular, dusty)
interstellar medium (ISM) with star formation tracers (HII regions, young
<10Myr stellar clusters) suggest (1) no variation in giant molecular cloud
(GMC) properties between arm and gas spurs, (2) gas spurs and extinction
feathers arising from the same structure with a close spatial relation between
gas spurs and ongoing/recent star formation (despite higher gas surface
densities in the spiral arm), (3) no trend in star formation age either along
the arm or along a spur, (4) evidence for strong star formation feedback in gas
spurs: (5) tentative evidence for star formation triggered by stellar feedback
for one spur, and (6) GMC associations (GMAs) being no special entities but the
result of blending of gas arm/spur cross-sections in lower resolution
observations. We conclude that there is no evidence for a coherent star
formation onset mechanism that can be solely associated to the presence of the
spiral density wave. This suggests that other (more localized) mechanisms are
important to delay star formation such that it occurs in spurs. The evidence of
star formation proceeding over several million years within individual spurs
implies that the mechanism that leads to star formation acts or is sustained
over a longer time-scale.
",0,1,0,0,0,0
18,Higher structure in the unstable Adams spectral sequence,"  We describe a variant construction of the unstable Adams spectral the
sequence for a space $Y$, associated to any free simplicial resolution of
$H^*(Y;R)$ for $R=\mathbb{F}_p$ or $\mathbb{Q}$. We use this construction to
describe the differentials and filtration in the spectral sequence in terms of
appropriate systems of higher cohomology operations.
",0,0,1,0,0,0
19,Comparing Covariate Prioritization via Matching to Machine Learning Methods for Causal Inference using Five Empirical Applications,"  When investigators seek to estimate causal effects, they often assume that
selection into treatment is based only on observed covariates. Under this
identification strategy, analysts must adjust for observed confounders. While
basic regression models have long been the dominant method of statistical
adjustment, more robust methods based on matching or weighting have become more
common. Of late, even more flexible methods based on machine learning methods
have been developed for statistical adjustment. These machine learning methods
are designed to be black box methods with little input from the researcher.
Recent research used a data competition to evaluate various methods of
statistical adjustment and found that black box methods out performed all other
methods of statistical adjustment. Matching methods with covariate
prioritization are designed for direct input from substantive investigators in
direct contrast to black methods. In this article, we use a different research
design to compare matching with covariate prioritization to black box methods.
We use black box methods to replicate results from five studies where matching
with covariate prioritization was used to customize the statistical adjustment
in direct response to substantive expertise. We find little difference across
the methods. We conclude with advice for investigators.
",0,0,0,1,0,0
20,Acoustic Impedance Calculation via Numerical Solution of the Inverse Helmholtz Problem,"  Assigning homogeneous boundary conditions, such as acoustic impedance, to the
thermoviscous wave equations (TWE) derived by transforming the linearized
Navier-Stokes equations (LNSE) to the frequency domain yields a so-called
Helmholtz solver, whose output is a discrete set of complex eigenfunction and
eigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) --
reverses such procedure by returning the value of acoustic impedance at one or
more unknown impedance boundaries (IBs) of a given domain via spatial
integration of the TWE for a given real-valued frequency with assigned
conditions on other boundaries. The iHS procedure is applied to a second-order
spatial discretization of the TWEs derived on an unstructured grid with
staggered grid arrangement. The momentum equation only is extended to the
center of each IB face where pressure and velocity components are co-located
and treated as unknowns. One closure condition considered for the iHS is the
assignment of the surface gradient of pressure phase over the IBs,
corresponding to assigning the shape of the acoustic waveform at the IB. The
iHS procedure is carried out independently for each frequency in order to
return the complete broadband complex impedance distribution at the IBs in any
desired frequency range. The iHS approach is first validated against Rott's
theory for both inviscid and viscous, rectangular and circular ducts. The
impedance of a geometrically complex toy cavity is then reconstructed and
verified against companion full compressible unstructured Navier-Stokes
simulations resolving the cavity geometry and one-dimensional impedance test
tube calculations based on time-domain impedance boundary conditions (TDIBC).
The iHS methodology is also shown to capture thermoacoustic effects, with
reconstructed impedance values quantitatively in agreement with thermoacoustic
growth rates.
",0,1,0,0,0,0
21,Deciphering noise amplification and reduction in open chemical reaction networks,"  The impact of random fluctuations on the dynamical behavior a complex
biological systems is a longstanding issue, whose understanding would shed
light on the evolutionary pressure that nature imposes on the intrinsic noise
levels and would allow rationally designing synthetic networks with controlled
noise. Using the It stochastic differential equation formalism, we performed
both analytic and numerical analyses of several model systems containing
different molecular species in contact with the environment and interacting
with each other through mass-action kinetics. These systems represent for
example biomolecular oligomerization processes, complex-breakage reactions,
signaling cascades or metabolic networks. For chemical reaction networks with
zero deficiency values, which admit a detailed- or complex-balanced steady
state, all molecular species are uncorrelated. The number of molecules of each
species follow a Poisson distribution and their Fano factors, which measure the
intrinsic noise, are equal to one. Systems with deficiency one have an
unbalanced non-equilibrium steady state and a non-zero S-flux, defined as the
flux flowing between the complexes multiplied by an adequate stoichiometric
coefficient. In this case, the noise on each species is reduced if the flux
flows from the species of lowest to highest complexity, and is amplified is the
flux goes in the opposite direction. These results are generalized to systems
of deficiency two, which possess two independent non-vanishing S-fluxes, and we
conjecture that a similar relation holds for higher deficiency systems.
",0,0,0,0,1,0
22,Many-Body Localization: Stability and Instability,"  Rare regions with weak disorder (Griffiths regions) have the potential to
spoil localization. We describe a non-perturbative construction of local
integrals of motion (LIOMs) for a weakly interacting spin chain in one
dimension, under a physically reasonable assumption on the statistics of
eigenvalues. We discuss ideas about the situation in higher dimensions, where
one can no longer ensure that interactions involving the Griffiths regions are
much smaller than the typical energy-level spacing for such regions. We argue
that ergodicity is restored in dimension d > 1, although equilibration should
be extremely slow, similar to the dynamics of glasses.
",0,1,1,0,0,0
23,Fault Detection and Isolation Tools (FDITOOLS) User's Guide,"  The Fault Detection and Isolation Tools (FDITOOLS) is a collection of MATLAB
functions for the analysis and solution of fault detection and model detection
problems. The implemented functions are based on the computational procedures
described in the Chapters 5, 6 and 7 of the book: ""A. Varga, Solving Fault
Diagnosis Problems - Linear Synthesis Techniques, Springer, 2017"". This
document is the User's Guide for the version V1.0 of FDITOOLS. First, we
present the mathematical background for solving several basic exact and
approximate synthesis problems of fault detection filters and model detection
filters. Then, we give in-depth information on the command syntax of the main
analysis and synthesis functions. Several examples illustrate the use of the
main functions of FDITOOLS.
",1,0,0,0,0,0
24,Complexity of Deciding Detectability in Discrete Event Systems,"  Detectability of discrete event systems (DESs) is a question whether the
current and subsequent states can be determined based on observations. Shu and
Lin designed a polynomial-time algorithm to check strong (periodic)
detectability and an exponential-time (polynomial-space) algorithm to check
weak (periodic) detectability. Zhang showed that checking weak (periodic)
detectability is PSpace-complete. This intractable complexity opens a question
whether there are structurally simpler DESs for which the problem is tractable.
In this paper, we show that it is not the case by considering DESs represented
as deterministic finite automata without non-trivial cycles, which are
structurally the simplest deadlock-free DESs. We show that even for such very
simple DESs, checking weak (periodic) detectability remains intractable. On the
contrary, we show that strong (periodic) detectability of DESs can be
efficiently verified on a parallel computer.
",1,0,0,0,0,0
25,The Knaster-Tarski theorem versus monotone nonexpansive mappings,"  Let $X$ be a partially ordered set with the property that each family of
order intervals of the form $[a,b],[a,\rightarrow )$ with the finite
intersection property has a nonempty intersection. We show that every directed
subset of $X$ has a supremum. Then we apply the above result to prove that if
$X$ is a topological space with a partial order $\preceq $ for which the order
intervals are compact, $\mathcal{F}$ a nonempty commutative family of monotone
maps from $X$ into $X$ and there exists $c\in X$ such that $c\preceq Tc$ for
every $T\in \mathcal{F}$, then the set of common fixed points of $\mathcal{F}$
is nonempty and has a maximal element. The result, specialized to the case of
Banach spaces gives a general fixed point theorem that drops almost all
assumptions from the recent results in this area. An application to the theory
of integral equations of Urysohn's type is also given.
",0,0,1,0,0,0
26,Efficient methods for computing integrals in electronic structure calculations,"  Efficient methods are proposed, for computing integrals appeaing in
electronic structure calculations. The methods consist of two parts: the first
part is to represent the integrals as contour integrals and the second one is
to evaluate the contour integrals by the Clenshaw-Curtis quadrature. The
efficiency of the proposed methods is demonstrated through numerical
experiments.
",0,1,0,0,0,0
27,Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source,"  We present a novel sound localization algorithm for a non-line-of-sight
(NLOS) sound source in indoor environments. Our approach exploits the
diffraction properties of sound waves as they bend around a barrier or an
obstacle in the scene. We combine a ray tracing based sound propagation
algorithm with a Uniform Theory of Diffraction (UTD) model, which simulate
bending effects by placing a virtual sound source on a wedge in the
environment. We precompute the wedges of a reconstructed mesh of an indoor
scene and use them to generate diffraction acoustic rays to localize the 3D
position of the source. Our method identifies the convergence region of those
generated acoustic rays as the estimated source position based on a particle
filter. We have evaluated our algorithm in multiple scenarios consisting of a
static and dynamic NLOS sound source. In our tested cases, our approach can
localize a source position with an average accuracy error, 0.7m, measured by
the L2 distance between estimated and actual source locations in a 7m*7m*3m
room. Furthermore, we observe 37% to 130% improvement in accuracy over a
state-of-the-art localization method that does not model diffraction effects,
especially when a sound source is not visible to the robot.
",1,0,0,0,0,0
28,"Jacob's ladders, crossbreeding in the set of $??$-factorization formulas and selection of families of $??$-kindred real continuous functions","  In this paper we introduce the notion of $\zeta$-crossbreeding in a set of
$\zeta$-factorization formulas and also the notion of complete hybrid formula
as the final result of that crossbreeding. The last formula is used as a
criterion for selection of families of $\zeta$-kindred elements in class of
real continuous functions.
Dedicated to recalling of Gregory Mendel's pea-crossbreeding.
",0,0,1,0,0,0
29,Minimax Estimation of the $L_1$ Distance,"  We consider the problem of estimating the $L_1$ distance between two discrete
probability measures $P$ and $Q$ from empirical data in a nonasymptotic and
large alphabet setting. When $Q$ is known and one obtains $n$ samples from $P$,
we show that for every $Q$, the minimax rate-optimal estimator with $n$ samples
achieves performance comparable to that of the maximum likelihood estimator
(MLE) with $n\ln n$ samples. When both $P$ and $Q$ are unknown, we construct
minimax rate-optimal estimators whose worst case performance is essentially
that of the known $Q$ case with $Q$ being uniform, implying that $Q$ being
uniform is essentially the most difficult case. The \emph{effective sample size
enlargement} phenomenon, identified in Jiao \emph{et al.} (2015), holds both in
the known $Q$ case for every $Q$ and the $Q$ unknown case. However, the
construction of optimal estimators for $\|P-Q\|_1$ requires new techniques and
insights beyond the approximation-based method of functional estimation in Jiao
\emph{et al.} (2015).
",0,0,1,1,0,0
30,Density large deviations for multidimensional stochastic hyperbolic conservation laws,"  We investigate the density large deviation function for a multidimensional
conservation law in the vanishing viscosity limit, when the probability
concentrates on weak solutions of a hyperbolic conservation law conservation
law. When the conductivity and dif-fusivity matrices are proportional, i.e. an
Einstein-like relation is satisfied, the problem has been solved in [4]. When
this proportionality does not hold, we compute explicitly the large deviation
function for a step-like density profile, and we show that the associated
optimal current has a non trivial structure. We also derive a lower bound for
the large deviation function, valid for a general weak solution, and leave the
general large deviation function upper bound as a conjecture.
",0,1,1,0,0,0
31,mixup: Beyond Empirical Risk Minimization,"  Large deep neural networks are powerful, but exhibit undesirable behaviors
such as memorization and sensitivity to adversarial examples. In this work, we
propose mixup, a simple learning principle to alleviate these issues. In
essence, mixup trains a neural network on convex combinations of pairs of
examples and their labels. By doing so, mixup regularizes the neural network to
favor simple linear behavior in-between training examples. Our experiments on
the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show
that mixup improves the generalization of state-of-the-art neural network
architectures. We also find that mixup reduces the memorization of corrupt
labels, increases the robustness to adversarial examples, and stabilizes the
training of generative adversarial networks.
",1,0,0,1,0,0
32,Equality of the usual definitions of Brakke flow,"  In 1978 Brakke introduced the mean curvature flow in the setting of geometric
measure theory. There exist multiple variants of the original definition. Here
we prove that most of them are indeed equal. One central point is to correct
the proof of Brakke's ??3.5, where he develops an estimate for the evolution
of the measure of time-dependent test functions.
",0,0,1,0,0,0
33,Dynamic Base Station Repositioning to Improve Spectral Efficiency of Drone Small Cells,"  With recent advancements in drone technology, researchers are now considering
the possibility of deploying small cells served by base stations mounted on
flying drones. A major advantage of such drone small cells is that the
operators can quickly provide cellular services in areas of urgent demand
without having to pre-install any infrastructure. Since the base station is
attached to the drone, technically it is feasible for the base station to
dynamic reposition itself in response to the changing locations of users for
reducing the communication distance, decreasing the probability of signal
blocking, and ultimately increasing the spectral efficiency. In this paper, we
first propose distributed algorithms for autonomous control of drone movements,
and then model and analyse the spectral efficiency performance of a drone small
cell to shed new light on the fundamental benefits of dynamic repositioning. We
show that, with dynamic repositioning, the spectral efficiency of drone small
cells can be increased by nearly 100\% for realistic drone speed, height, and
user traffic model and without incurring any major increase in drone energy
consumption.
",1,0,0,0,0,0
34,An Unsupervised Homogenization Pipeline for Clustering Similar Patients using Electronic Health Record Data,"  Electronic health records (EHR) contain a large variety of information on the
clinical history of patients such as vital signs, demographics, diagnostic
codes and imaging data. The enormous potential for discovery in this rich
dataset is hampered by its complexity and heterogeneity.
We present the first study to assess unsupervised homogenization pipelines
designed for EHR clustering. To identify the optimal pipeline, we tested
accuracy on simulated data with varying amounts of redundancy, heterogeneity,
and missingness. We identified two optimal pipelines: 1) Multiple Imputation by
Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,
Z-scoring, and Deep Autoencoders.
",0,0,0,0,1,0
35,Deep Neural Network Optimized to Resistive Memory with Nonlinear Current-Voltage Characteristics,"  Artificial Neural Network computation relies on intensive vector-matrix
multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array
showed a feasibility of implementing such operations with high energy
efficiency, thus there are many works on efficiently utilizing emerging NVM
crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V
characteristics restrain critical design parameters, such as the read voltage
and weight range, resulting in substantial accuracy loss. In this paper,
instead of optimizing hardware parameters to a given neural network, we propose
a methodology of reconstructing a neural network itself optimized to resistive
memory crossbar arrays. To verify the validity of the proposed method, we
simulated various neural network with MNIST and CIFAR-10 dataset using two
different specific Resistive Random Access Memory (RRAM) model. Simulation
results show that our proposed neural network produces significantly higher
inference accuracies than conventional neural network when the synapse devices
have nonlinear I-V characteristics.
",1,0,0,0,0,0
36,Rate-Distortion Region of a Gray-Wyner Model with Side Information,"  In this work, we establish a full single-letter characterization of the
rate-distortion region of an instance of the Gray-Wyner model with side
information at the decoders. Specifically, in this model an encoder observes a
pair of memoryless, arbitrarily correlated, sources $(S^n_1,S^n_2)$ and
communicates with two receivers over an error-free rate-limited link of
capacity $R_0$, as well as error-free rate-limited individual links of
capacities $R_1$ to the first receiver and $R_2$ to the second receiver. Both
receivers reproduce the source component $S^n_2$ losslessly; and Receiver $1$
also reproduces the source component $S^n_1$ lossily, to within some prescribed
fidelity level $D_1$. Also, Receiver $1$ and Receiver $2$ are equipped
respectively with memoryless side information sequences $Y^n_1$ and $Y^n_2$.
Important in this setup, the side information sequences are arbitrarily
correlated among them, and with the source pair $(S^n_1,S^n_2)$; and are not
assumed to exhibit any particular ordering. Furthermore, by specializing the
main result to two Heegard-Berger models with successive refinement and
scalable coding, we shed light on the roles of the common and private
descriptions that the encoder should produce and what they should carry
optimally. We develop intuitions by analyzing the developed single-letter
optimal rate-distortion regions of these models, and discuss some insightful
binary examples.
",1,0,1,0,0,0
37,Fourier-based numerical approximation of the Weertman equation for moving dislocations,"  This work discusses the numerical approximation of a nonlinear
reaction-advection-diffusion equation, which is a dimensionless form of the
Weertman equation. This equation models steadily-moving dislocations in
materials science. It reduces to the celebrated Peierls-Nabarro equation when
its advection term is set to zero. The approach rests on considering a
time-dependent formulation, which admits the equation under study as its
long-time limit. Introducing a Preconditioned Collocation Scheme based on
Fourier transforms, the iterative numerical method presented solves the
time-dependent problem, delivering at convergence the desired numerical
solution to the Weertman equation. Although it rests on an explicit
time-evolution scheme, the method allows for large time steps, and captures the
solution in a robust manner. Numerical results illustrate the efficiency of the
approach for several types of nonlinearities.
",0,1,0,0,0,0
38,Design Decisions for Weave: A Real-Time Web-based Collaborative Visualization Framework,"  There are many web-based visualization systems available to date, each having
its strengths and limitations. The goals these systems set out to accomplish
influence design decisions and determine how reusable and scalable they are.
Weave is a new web-based visualization platform with the broad goal of enabling
visualization of any available data by anyone for any purpose. Our open source
framework supports highly interactive linked visualizations for users of
varying skill levels. What sets Weave apart from other systems is its
consideration for real-time remote collaboration with session history. We
provide a detailed account of the various framework designs we considered with
comparisons to existing state-of-the-art systems.
",1,0,0,0,0,0
39,Suzaku Analysis of the Supernova Remnant G306.3-0.9 and the Gamma-ray View of Its Neighborhood,"  We present an investigation of the supernova remnant (SNR) G306.3$-$0.9 using
archival multi-wavelength data. The Suzaku spectra are well described by
two-component thermal plasma models: The soft component is in ionization
equilibrium and has a temperature $\sim$0.59 keV, while the hard component has
temperature $\sim$3.2 keV and ionization time-scale $\sim$$2.6\times10^{10}$
cm$^{-3}$ s. We clearly detected Fe K-shell line at energy of $\sim$6.5 keV
from this remnant. The overabundances of Si, S, Ar, Ca, and Fe confirm that the
X-ray emission has an ejecta origin. The centroid energy of the Fe-K line
supports that G306.3$-$0.9 is a remnant of a Type Ia supernova (SN) rather than
a core-collapse SN. The GeV gamma-ray emission from G306.3$-$0.9 and its
surrounding were analyzed using about 6 years of Fermi data. We report about
the non-detection of G306.3$-$0.9 and the detection of a new extended gamma-ray
source in the south-west of G306.3$-$0.9 with a significance of
$\sim$13$\sigma$. We discuss several scenarios for these results with the help
of data from other wavebands to understand the SNR and its neighborhood.
",0,1,0,0,0,0
40,Japanese Sentiment Classification using a Tree-Structured Long Short-Term Memory with Attention,"  Previous approaches to training syntax-based sentiment classification models
required phrase-level annotated corpora, which are not readily available in
many languages other than English. Thus, we propose the use of tree-structured
Long Short-Term Memory with an attention mechanism that pays attention to each
subtree of the parse tree. Experimental results indicate that our model
achieves the state-of-the-art performance in a Japanese sentiment
classification task.
",1,0,0,0,0,0
41,"Covariances, Robustness, and Variational Bayes","  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale datasets. However, even when MFVB provides accurate posterior means
for certain parameters, it often mis-estimates variances and covariances.
Furthermore, prior robustness measures have remained undeveloped for MFVB. By
deriving a simple formula for the effect of infinitesimal model perturbations
on MFVB posterior means, we provide both improved covariance estimates and
local robustness measures for MFVB, thus greatly expanding the practical
usefulness of MFVB posterior approximations. The estimates for MFVB posterior
covariances rely on a result from the classical Bayesian robustness literature
relating derivatives of posterior expectations to posterior covariances and
include the Laplace approximation as a special case. Our key condition is that
the MFVB approximation provides good estimates of a select subset of posterior
means---an assumption that has been shown to hold in many practical settings.
In our experiments, we demonstrate that our methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness
measures with runtimes that can be an order of magnitude faster than MCMC.
",0,0,0,1,0,0
42,Are multi-factor Gaussian term structure models still useful? An empirical analysis on Italian BTPs,"  In this paper, we empirically study models for pricing Italian sovereign
bonds under a reduced form framework, by assuming different dynamics for the
short-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek
multi-factor models, with a focus on optimization algorithms applied in the
calibration exercise. The Kalman filter algorithm together with a maximum
likelihood estimation method are considered to fit the Italian term-structure
over a 12-year horizon, including the global financial crisis and the euro area
sovereign debt crisis. Analytic formulas for the gradient vector and the
Hessian matrix of the likelihood function are provided.
",0,0,0,0,0,1
43,Probing valley filtering effect by Andreev reflection in zigzag graphene nanoribbon,"  Ballistic point contact (BPC) with zigzag edges in graphene is a main
candidate of a valley filter, in which the polarization of the valley degree of
freedom can be selected by using a local gate voltage. Here, we propose to
detect the valley filtering effect by Andreev reflection. Because electrons in
the lowest conduction band and the highest valence band of the BPC possess
opposite chirality, the inter-band Andreev reflection is strongly suppressed,
after multiple scattering and interference. We draw this conclusion by both the
scattering matrix analysis and the numerical simulation. The Andreev reflection
as a function of the incident energy of electrons and the local gate voltage at
the BPC is obtained, by which the parameter region for a perfect valley filter
and the direction of valley polarization can be determined. The Andreev
reflection exhibits an oscillatory decay with the length of the BPC, indicating
a negative correlation to valley polarization.
",0,1,0,0,0,0
44,Generalized Approximate Message-Passing Decoder for Universal Sparse Superposition Codes,"  Sparse superposition (SS) codes were originally proposed as a
capacity-achieving communication scheme over the additive white Gaussian noise
channel (AWGNC) [1]. Very recently, it was discovered that these codes are
universal, in the sense that they achieve capacity over any memoryless channel
under generalized approximate message-passing (GAMP) decoding [2], although
this decoder has never been stated for SS codes. In this contribution we
introduce the GAMP decoder for SS codes, we confirm empirically the
universality of this communication scheme through its study on various channels
and we provide the main analysis tools: state evolution and potential. We also
compare the performance of GAMP with the Bayes-optimal MMSE decoder. We
empirically illustrate that despite the presence of a phase transition
preventing GAMP to reach the optimal performance, spatial coupling allows to
boost the performance that eventually tends to capacity in a proper limit. We
also prove that, in contrast with the AWGNC case, SS codes for binary input
channels have a vanishing error floor in the limit of large codewords.
Moreover, the performance of Hadamard-based encoders is assessed for practical
implementations.
",1,0,1,0,0,0
45,LAAIR: A Layered Architecture for Autonomous Interactive Robots,"  When developing general purpose robots, the overarching software architecture
can greatly affect the ease of accomplishing various tasks. Initial efforts to
create unified robot systems in the 1990s led to hybrid architectures,
emphasizing a hierarchy in which deliberative plans direct the use of reactive
skills. However, since that time there has been significant progress in the
low-level skills available to robots, including manipulation and perception,
making it newly feasible to accomplish many more tasks in real-world domains.
There is thus renewed optimism that robots will be able to perform a wide array
of tasks while maintaining responsiveness to human operators. However, the top
layer in traditional hybrid architectures, designed to achieve long-term goals,
can make it difficult to react quickly to human interactions during goal-driven
execution. To mitigate this difficulty, we propose a novel architecture that
supports such transitions by adding a top-level reactive module which has
flexible access to both reactive skills and a deliberative control module. To
validate this architecture, we present a case study of its application on a
domestic service robot platform.
",1,0,0,0,0,0
46,3D Human Pose Estimation in RGBD Images for Robotic Task Learning,"  We propose an approach to estimate 3D human pose in real world units from a
single RGBD image and show that it exceeds performance of monocular 3D pose
estimation approaches from color as well as pose estimation exclusively from
depth. Our approach builds on robust human keypoint detectors for color images
and incorporates depth for lifting into 3D. We combine the system with our
learning from demonstration framework to instruct a service robot without the
need of markers. Experiments in real world settings demonstrate that our
approach enables a PR2 robot to imitate manipulation actions observed from a
human teacher.
",1,0,0,0,0,0
47,Simultaneous non-vanishing for Dirichlet L-functions,"  We extend the work of Fouvry, Kowalski and Michel on correlation between
Hecke eigenvalues of modular forms and algebraic trace functions in order to
establish an asymptotic formula for a generalized cubic moment of modular
L-functions at the central point s = 1/2 and for prime moduli q. As an
application, we exploit our recent result on the mollification of the fourth
moment of Dirichlet L-functions to derive that for any pair
$(\omega_1,\omega_2)$ of multiplicative characters modulo q, there is a
positive proportion of $\chi$ (mod q) such that $L(\chi, 1/2 ), L(\chi\omega_1,
1/2 )$ and $L(\chi\omega_2, 1/2)$ are simultaneously not too small.
",0,0,1,0,0,0
48,Wehrl Entropy Based Quantification of Nonclassicality for Single Mode Quantum Optical States,"  Nonclassical states of a quantized light are described in terms of
Glauber-Sudarshan P distribution which is not a genuine classical probability
distribution. Despite several attempts, defining a uniform measure of
nonclassicality (NC) for the single mode quantum states of light is yet an open
task. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that
the existing well-known measures fail to quantify the NC of single mode states
that are generated under multiple NC-inducing operations. Recently, Ivan et.
al. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of
non-Gaussian character of quantum optical states in terms of Wehrl entropy.
Here, we adopt this concept in the context of single mode NC. In this paper, we
propose a new quantification of NC for the single mode quantum states of light
as the difference between the total Wehrl entropy of the state and the maximum
Wehrl entropy arising due to its classical characteristics. This we achieve by
subtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any
classical state that has same randomness as measured in terms of von-Neumann
entropy. We obtain analytic expressions of NC for most of the states, in
particular, all pure states and Gaussian mixed states. However, the evaluation
of NC for the non-Gaussian mixed states is subject to extensive numerical
computation that lies beyond the scope of the current work. We show that, along
with the states generated under single NC-inducing operations, also for the
broader class of states that are generated under multiple NC-inducing
operations, our quantification enumerates the NC consistently.
",1,1,0,0,0,0
49,Attention-based Natural Language Person Retrieval,"  Following the recent progress in image classification and captioning using
deep learning, we develop a novel natural language person retrieval system
based on an attention mechanism. More specifically, given the description of a
person, the goal is to localize the person in an image. To this end, we first
construct a benchmark dataset for natural language person retrieval. To do so,
we generate bounding boxes for persons in a public image dataset from the
segmentation masks, which are then annotated with descriptions and attributes
using the Amazon Mechanical Turk. We then adopt a region proposal network in
Faster R-CNN as a candidate region generator. The cropped images based on the
region proposals as well as the whole images with attention weights are fed
into Convolutional Neural Networks for visual feature extraction, while the
natural language expression and attributes are input to Bidirectional Long
Short- Term Memory (BLSTM) models for text feature extraction. The visual and
text features are integrated to score region proposals, and the one with the
highest score is retrieved as the output of our system. The experimental
results show significant improvement over the state-of-the-art method for
generic object retrieval and this line of research promises to benefit search
in surveillance video footage.
",1,0,0,0,0,0
50,Large Scale Automated Forecasting for Monitoring Network Safety and Security,"  Real time large scale streaming data pose major challenges to forecasting, in
particular defying the presence of human experts to perform the corresponding
analysis. We present here a class of models and methods used to develop an
automated, scalable and versatile system for large scale forecasting oriented
towards safety and security monitoring. Our system provides short and long term
forecasts and uses them to detect safety and security issues in relation with
multiple internet connected devices well in advance they might take place.
",0,0,0,1,0,0
51,Contextual Regression: An Accurate and Conveniently Interpretable Nonlinear Model for Mining Discovery from Scientific Data,"  Machine learning algorithms such as linear regression, SVM and neural network
have played an increasingly important role in the process of scientific
discovery. However, none of them is both interpretable and accurate on
nonlinear datasets. Here we present contextual regression, a method that joins
these two desirable properties together using a hybrid architecture of neural
network embedding and dot product layer. We demonstrate its high prediction
accuracy and sensitivity through the task of predictive feature selection on a
simulated dataset and the application of predicting open chromatin sites in the
human genome. On the simulated data, our method achieved high fidelity recovery
of feature contributions under random noise levels up to 200%. On the open
chromatin dataset, the application of our method not only outperformed the
state of the art method in terms of accuracy, but also unveiled two previously
unfound open chromatin related histone marks. Our method can fill the blank of
accurate and interpretable nonlinear modeling in scientific data mining tasks.
",1,0,0,1,0,0
52,Multi-time correlators in continuous measurement of qubit observables,"  We consider multi-time correlators for output signals from linear detectors,
continuously measuring several qubit observables at the same time. Using the
quantum Bayesian formalism, we show that for unital (symmetric) evolution in
the absence of phase backaction, an $N$-time correlator can be expressed as a
product of two-time correlators when $N$ is even. For odd $N$, there is a
similar factorization, which also includes a single-time average. Theoretical
predictions agree well with experimental results for two detectors, which
simultaneously measure non-commuting qubit observables.
",0,1,0,0,0,0
53,"Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey","  Constraint Handling Rules is an effective concurrent declarative programming
language and a versatile computational logic formalism. CHR programs consist of
guarded reactive rules that transform multisets of constraints. One of the main
features of CHR is its inherent concurrency. Intuitively, rules can be applied
to parts of a multiset in parallel. In this comprehensive survey, we give an
overview of concurrent and parallel as well as distributed CHR semantics,
standard and more exotic, that have been proposed over the years at various
levels of refinement. These semantics range from the abstract to the concrete.
They are related by formal soundness results. Their correctness is established
as correspondence between parallel and sequential computations. We present
common concise sample CHR programs that have been widely used in experiments
and benchmarks. We review parallel CHR implementations in software and
hardware. The experimental results obtained show a consistent parallel speedup.
Most implementations are available online. The CHR formalism can also be used
to implement and reason with models for concurrency. To this end, the Software
Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus
have been faithfully encoded in CHR. Under consideration in Theory and Practice
of Logic Programming (TPLP).
",1,0,0,0,0,0
54,Robustness against the channel effect in pathological voice detection,"  Many people are suffering from voice disorders, which can adversely affect
the quality of their lives. In response, some researchers have proposed
algorithms for automatic assessment of these disorders, based on voice signals.
However, these signals can be sensitive to the recording devices. Indeed, the
channel effect is a pervasive problem in machine learning for healthcare. In
this study, we propose a detection system for pathological voice, which is
robust against the channel effect. This system is based on a bidirectional LSTM
network. To increase the performance robustness against channel mismatch, we
integrate domain adversarial training (DAT) to eliminate the differences
between the devices. When we train on data recorded on a high-quality
microphone and evaluate on smartphone data without labels, our robust detection
system increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target
sample labels). To the best of our knowledge, this is the first study applying
unsupervised domain adaptation to pathological voice detection. Notably, our
system does not need target device sample labels, which allows for
generalization to many new devices.
",1,0,0,0,0,0
55,An Effective Framework for Constructing Exponent Lattice Basis of Nonzero Algebraic Numbers,"  Computing a basis for the exponent lattice of algebraic numbers is a basic
problem in the field of computational number theory with applications to many
other areas. The main cost of a well-known algorithm
\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on
computing the primitive element of the extended field generated by the given
algebraic numbers. When the extended field is of large degree, the problem
seems intractable by the tool implementing the algorithm. In this paper, a
special kind of exponent lattice basis is introduced. An important feature of
the basis is that it can be inductively constructed, which allows us to deal
with the given algebraic numbers one by one when computing the basis. Based on
this, an effective framework for constructing exponent lattice basis is
proposed. Through computing a so-called pre-basis first and then solving some
linear Diophantine equations, the basis can be efficiently constructed. A new
certificate for multiplicative independence and some techniques for decreasing
degrees of algebraic numbers are provided to speed up the computation. The new
algorithm has been implemented with Mathematica and its effectiveness is
verified by testing various examples. Moreover, the algorithm is applied to
program verification for finding invariants of linear loops.
",1,0,0,0,0,0
56,Competing evolutionary paths in growing populations with applications to multidrug resistance,"  Investigating the emergence of a particular cell type is a recurring theme in
models of growing cellular populations. The evolution of resistance to therapy
is a classic example. Common questions are: when does the cell type first
occur, and via which sequence of steps is it most likely to emerge? For growing
populations, these questions can be formulated in a general framework of
branching processes spreading through a graph from a root to a target vertex.
Cells have a particular fitness value on each vertex and can transition along
edges at specific rates. Vertices represents cell states, say \mic{genotypes
}or physical locations, while possible transitions are acquiring a mutation or
cell migration. We focus on the setting where cells at the root vertex have the
highest fitness and transition rates are small. Simple formulas are derived for
the time to reach the target vertex and for the probability that it is reached
along a given path in the graph. We demonstrate our results on \mic{several
scenarios relevant to the emergence of drug resistance}, including: the
orderings of resistance-conferring mutations in bacteria and the impact of
imperfect drug penetration in cancer.
",0,0,0,0,1,0
57,Transient flows in active porous media,"  Stimuli-responsive materials that modify their shape in response to changes
in environmental conditions -- such as solute concentration, temperature, pH,
and stress -- are widespread in nature and technology. Applications include
micro- and nanoporous materials used in filtration and flow control. The
physiochemical mechanisms that induce internal volume modifications have been
widely studies. The coupling between induced volume changes and solute
transport through porous materials, however, is not well understood. Here, we
consider advective and diffusive transport through a small channel linking two
large reservoirs. A section of stimulus-responsive material regulates the
channel permeability, which is a function of the local solute concentration. We
derive an exact solution to the coupled transport problem and demonstrate the
existence of a flow regime in which the steady state is reached via a damped
oscillation around the equilibrium concentration value. Finally, the
feasibility of an experimental observation of the phenomena is discussed.
Please note that this version of the paper has not been formally peer reviewed,
revised or accepted by a journal.
",0,1,0,0,0,0
58,An information model for modular robots: the Hardware Robot Information Model (HRIM),"  Today's landscape of robotics is dominated by vertical integration where
single vendors develop the final product leading to slow progress, expensive
products and customer lock-in. Opposite to this, an horizontal integration
would result in a rapid development of cost-effective mass-market products with
an additional consumer empowerment. The transition of an industry from vertical
integration to horizontal integration is typically catalysed by de facto
industry standards that enable a simplified and seamless integration of
products. However, in robotics there is currently no leading candidate for a
global plug-and-play standard.
This paper tackles the problem of incompatibility between robot components
that hinder the reconfigurability and flexibility demanded by the robotics
industry. Particularly, it presents a model to create plug-and-play robot
hardware components. Rather than iteratively evolving previous ontologies, our
proposed model answers the needs identified by the industry while facilitating
interoperability, measurability and comparability of robotics technology. Our
approach differs significantly with the ones presented before as it is
hardware-oriented and establishes a clear set of actions towards the
integration of this model in real environments and with real manufacturers.
",1,0,0,0,0,0
59,Detecting Adversarial Samples Using Density Ratio Estimates,"  Machine learning models, especially based on deep architectures are used in
everyday applications ranging from self driving cars to medical diagnostics. It
has been shown that such models are dangerously susceptible to adversarial
samples, indistinguishable from real samples to human eye, adversarial samples
lead to incorrect classifications with high confidence. Impact of adversarial
samples is far-reaching and their efficient detection remains an open problem.
We propose to use direct density ratio estimation as an efficient model
agnostic measure to detect adversarial samples. Our proposed method works
equally well with single and multi-channel samples, and with different
adversarial sample generation methods. We also propose a method to use density
ratio estimates for generating adversarial samples with an added constraint of
preserving density ratio.
",1,0,0,1,0,0
60,The Query Complexity of Cake Cutting,"  We study the query complexity of cake cutting and give lower and upper bounds
for computing approximately envy-free, perfect, and equitable allocations with
the minimum number of cuts. The lower bounds are tight for computing connected
envy-free allocations among n=3 players and for computing perfect and equitable
allocations with minimum number of cuts between n=2 players.
We also formalize moving knife procedures and show that a large subclass of
this family, which captures all the known moving knife procedures, can be
simulated efficiently with arbitrarily small error in the Robertson-Webb query
model.
",1,0,0,0,0,0
61,Stacked Convolutional and Recurrent Neural Networks for Music Emotion Recognition,"  This paper studies the emotion recognition from musical tracks in the
2-dimensional valence-arousal (V-A) emotional space. We propose a method based
on convolutional (CNN) and recurrent neural networks (RNN), having
significantly fewer parameters compared with the state-of-the-art method for
the same task. We utilize one CNN layer followed by two branches of RNNs
trained separately for arousal and valence. The method was evaluated using the
'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for
arousal and 0.268 for valence, which is the best result reported on this
dataset.
",1,0,0,0,0,0
62,Timed Automata with Polynomial Delay and their Expressiveness,"  We consider previous models of Timed, Probabilistic and Stochastic Timed
Automata, we introduce our model of Timed Automata with Polynomial Delay and we
characterize the expressiveness of these models relative to each other.
",1,0,0,0,0,0
63,Superconducting properties of Cu intercalated Bi$_2$Se$_3$ studied by Muon Spin Spectroscopy,"  We present muon spin rotation measurements on superconducting Cu intercalated
Bi$_2$Se$_3$, which was suggested as a realization of a topological
superconductor. We observe a clear evidence of the superconducting transition
below 4 K, where the width of magnetic field distribution increases as the
temperature is decreased. The measured broadening at mK temperatures suggests a
large London penetration depth in the $ab$ plane ($\lambda_{\mathrm{eff}}\sim
1.6$ $\mathrm{\mu}$m). We show that the temperature dependence of this
broadening follows the BCS prediction, but could be consistent with several gap
symmetries.
",0,1,0,0,0,0
64,Time-domain THz spectroscopy reveals coupled protein-hydration dielectric response in solutions of native and fibrils of human lyso-zyme,"  Here we reveal details of the interaction between human lysozyme proteins,
both native and fibrils, and their water environment by intense terahertz time
domain spectroscopy. With the aid of a rigorous dielectric model, we determine
the amplitude and phase of the oscillating dipole induced by the THz field in
the volume containing the protein and its hydration water. At low
concentrations, the amplitude of this induced dipolar response decreases with
increasing concentration. Beyond a certain threshold, marking the onset of the
interactions between the extended hydration shells, the amplitude remains fixed
but the phase of the induced dipolar response, which is initially in phase with
the applied THz field, begins to change. The changes observed in the THz
response reveal protein-protein interactions me-diated by extended hydration
layers, which may control fibril formation and may have an important role in
chemical recognition phenomena.
",0,1,0,0,0,0
65,Inversion of Qubit Energy Levels in Qubit-Oscillator Circuits in the Deep-Strong-Coupling Regime,"  We report on experimentally measured light shifts of superconducting flux
qubits deep-strongly coupled to LC oscillators, where the coupling constants
are comparable to the qubit and oscillator resonance frequencies. By using
two-tone spectroscopy, the energies of the six lowest levels of each circuit
are determined. We find huge Lamb shifts that exceed 90% of the bare qubit
frequencies and inversions of the qubits' ground and excited states when there
are a finite number of photons in the oscillator. Our experimental results
agree with theoretical predictions based on the quantum Rabi model.
",0,1,0,0,0,0
66,Deep Multiple Instance Feature Learning via Variational Autoencoder,"  We describe a novel weakly supervised deep learning framework that combines
both the discriminative and generative models to learn meaningful
representation in the multiple instance learning (MIL) setting. MIL is a weakly
supervised learning problem where labels are associated with groups of
instances (referred as bags) instead of individual instances. To address the
essential challenge in MIL problems raised from the uncertainty of positive
instances label, we use a discriminative model regularized by variational
autoencoders (VAEs) to maximize the differences between latent representations
of all instances and negative instances. As a result, the hidden layer of the
variational autoencoder learns meaningful representation. This representation
can effectively be used for MIL problems as illustrated by better performance
on the standard benchmark datasets comparing to the state-of-the-art
approaches. More importantly, unlike most related studies, the proposed
framework can be easily scaled to large dataset problems, as illustrated by the
audio event detection and segmentation task. Visualization also confirms the
effectiveness of the latent representation in discriminating positive and
negative classes.
",0,0,0,1,0,0
67,Regularity of envelopes in K??hler classes,"  We establish the C^{1,1} regularity of quasi-psh envelopes in a Kahler class,
confirming a conjecture of Berman.
",0,0,1,0,0,0
68,$S^1$-equivariant Index theorems and Morse inequalities on complex manifolds with boundary,"  Let $M$ be a complex manifold of dimension $n$ with smooth connected boundary
$X$. Assume that $\overline M$ admits a holomorphic $S^1$-action preserving the
boundary $X$ and the $S^1$-action is transversal and CR on $X$. We show that
the $\overline\partial$-Neumann Laplacian on $M$ is transversally elliptic and
as a consequence, the $m$-th Fourier component of the $q$-th Dolbeault
cohomology group $H^q_m(\overline M)$ is finite dimensional, for every
$m\in\mathbb Z$ and every $q=0,1,\ldots,n$. This enables us to define
$\sum^{n}_{j=0}(-1)^j{\rm dim\,}H^q_m(\overline M)$ the $m$-th Fourier
component of the Euler characteristic on $M$ and to study large $m$-behavior of
$H^q_m(\overline M)$. In this paper, we establish an index formula for
$\sum^{n}_{j=0}(-1)^j{\rm dim\,}H^q_m(\overline M)$ and Morse inequalities for
$H^q_m(\overline M)$.
",0,0,1,0,0,0
69,Internal Model from Observations for Reward Shaping,"  Reinforcement learning methods require careful design involving a reward
function to obtain the desired action policy for a given task. In the absence
of hand-crafted reward functions, prior work on the topic has proposed several
methods for reward estimation by using expert state trajectories and action
pairs. However, there are cases where complete or good action information
cannot be obtained from expert demonstrations. We propose a novel reinforcement
learning method in which the agent learns an internal model of observation on
the basis of expert-demonstrated state trajectories to estimate rewards without
completely learning the dynamics of the external environment from state-action
pairs. The internal model is obtained in the form of a predictive model for the
given expert state distribution. During reinforcement learning, the agent
predicts the reward as a function of the difference between the actual state
and the state predicted by the internal model. We conducted multiple
experiments in environments of varying complexity, including the Super Mario
Bros and Flappy Bird games. We show our method successfully trains good
policies directly from expert game-play videos.
",1,0,0,1,0,0
70,Characterizations of quasitrivial symmetric nondecreasing associative operations,"  In this paper we are interested in the class of n-ary operations on an
arbitrary chain that are quasitrivial, symmetric, nondecreasing, and
associative. We first provide a description of these operations. We then prove
that associativity can be replaced with bisymmetry in the definition of this
class. Finally we investigate the special situation where the chain is finite.
",0,0,1,0,0,0
71,Multivariate Dependency Measure based on Copula and Gaussian Kernel,"  We propose a new multivariate dependency measure. It is obtained by
considering a Gaussian kernel based distance between the copula transform of
the given d-dimensional distribution and the uniform copula and then
appropriately normalizing it. The resulting measure is shown to satisfy a
number of desirable properties. A nonparametric estimate is proposed for this
dependency measure and its properties (finite sample as well as asymptotic) are
derived. Some comparative studies of the proposed dependency measure estimate
with some widely used dependency measure estimates on artificial datasets are
included. A non-parametric test of independence between two or more random
variables based on this measure is proposed. A comparison of the proposed test
with some existing nonparametric multivariate test for independence is
presented.
",0,0,1,1,0,0
72,The nature of the tensor order in Cd2Re2O7,"  The pyrochlore metal Cd2Re2O7 has been recently investigated by
second-harmonic generation (SHG) reflectivity. In this paper, we develop a
general formalism that allows for the identification of the relevant tensor
components of the SHG from azimuthal scans. We demonstrate that the secondary
order parameter identified by SHG at the structural phase transition is the
x2-y2 component of the axial toroidal quadrupole. This differs from the 3z2-r2
symmetry of the atomic displacements associated with the I-4m2 crystal
structure that was previously thought to be its origin. Within the same
formalism, we suggest that the primary order parameter detected in the SHG
experiment is the 3z2-r2 component of the magnetic quadrupole. We discuss the
general mechanism driving the phase transition in our proposed framework, and
suggest experiments, particularly resonant X-ray scattering ones, that could
clarify this issue.
",0,1,0,0,0,0
73,Efficient and consistent inference of ancestral sequences in an evolutionary model with insertions and deletions under dense taxon sampling,"  In evolutionary biology, the speciation history of living organisms is
represented graphically by a phylogeny, that is, a rooted tree whose leaves
correspond to current species and branchings indicate past speciation events.
Phylogenies are commonly estimated from molecular sequences, such as DNA
sequences, collected from the species of interest. At a high level, the idea
behind this inference is simple: the further apart in the Tree of Life are two
species, the greater is the number of mutations to have accumulated in their
genomes since their most recent common ancestor. In order to obtain accurate
estimates in phylogenetic analyses, it is standard practice to employ
statistical approaches based on stochastic models of sequence evolution on a
tree. For tractability, such models necessarily make simplifying assumptions
about the evolutionary mechanisms involved. In particular, commonly omitted are
insertions and deletions of nucleotides -- also known as indels.
Properly accounting for indels in statistical phylogenetic analyses remains a
major challenge in computational evolutionary biology. Here we consider the
problem of reconstructing ancestral sequences on a known phylogeny in a model
of sequence evolution incorporating nucleotide substitutions, insertions and
deletions, specifically the classical TKF91 process. We focus on the case of
dense phylogenies of bounded height, which we refer to as the taxon-rich
setting, where statistical consistency is achievable. We give the first
polynomial-time ancestral reconstruction algorithm with provable guarantees
under constant rates of mutation. Our algorithm succeeds when the phylogeny
satisfies the ""big bang"" condition, a necessary and sufficient condition for
statistical consistency in this context.
",1,0,1,1,0,0
74,Flow Characteristics and Cores of Complex Network and Multiplex Type Systems,"  Subject of research is complex networks and network systems. The network
system is defined as a complex network in which flows are moved. Classification
of flows in the network is carried out on the basis of ordering and continuity.
It is shown that complex networks with different types of flows generate
various network systems. Flow analogues of the basic concepts of the theory of
complex networks are introduced and the main problems of this theory in terms
of flow characteristics are formulated. Local and global flow characteristics
of networks bring closer the theory of complex networks to the systems theory
and systems analysis. Concept of flow core of network system is introduced and
defined how it simplifies the process of its investigation. Concepts of kernel
and flow core of multiplex are determined. Features of operation of multiplex
type systems are analyzed.
",1,1,0,0,0,0
75,Pattern-forming fronts in a Swift-Hohenberg equation with directional quenching - parallel and oblique stripes,"  We study the effect of domain growth on the orientation of striped phases in
a Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter
dependence that allows stripe formation in a half plane, and suppresses
patterns in the complement, while the boundary of the pattern-forming region is
propagating with fixed normal velocity. We construct front solutions that leave
behind stripes in the pattern-forming region that are parallel to or at a small
oblique angle to the boundary.
Technically, the construction of stripe formation parallel to the boundary
relies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at
a small oblique angle are constructed using a functional-analytic, perturbative
approach. Here, the main difficulties are the presence of continuous spectrum
and the fact that small oblique angles appear as a singular perturbation in a
traveling-wave problem. We resolve the former difficulty using a farfield-core
decomposition and Fredholm theory in weighted spaces. The singular perturbation
problem is resolved using preconditioners and boot-strapping.
",0,1,0,0,0,0
76,Generalized Minimum Distance Estimators in Linear Regression with Dependent Errors,"  This paper discusses minimum distance estimation method in the linear
regression model with dependent errors which are strongly mixing. The
regression parameters are estimated through the minimum distance estimation
method, and asymptotic distributional properties of the estimators are
discussed. A simulation study compares the performance of the minimum distance
estimator with other well celebrated estimator. This simulation study shows the
superiority of the minimum distance estimator over another estimator. KoulMde
(R package) which was used for the simulation study is available online. See
section 4 for the detail.
",0,0,1,1,0,0
77,Live Service Migration in Mobile Edge Clouds,"  Mobile edge clouds (MECs) bring the benefits of the cloud closer to the user,
by installing small cloud infrastructures at the network edge. This enables a
new breed of real-time applications, such as instantaneous object recognition
and safety assistance in intelligent transportation systems, that require very
low latency. One key issue that comes with proximity is how to ensure that
users always receive good performance as they move across different locations.
Migrating services between MECs is seen as the means to achieve this. This
article presents a layered framework for migrating active service applications
that are encapsulated either in virtual machines (VMs) or containers. This
layering approach allows a substantial reduction in service downtime. The
framework is easy to implement using readily available technologies, and one of
its key advantages is that it supports containers, which is a promising
emerging technology that offers tangible benefits over VMs. The migration
performance of various real applications is evaluated by experiments under the
presented framework. Insights drawn from the experimentation results are
discussed.
",1,0,0,0,0,0
78,Induced density correlations in a sonic black hole condensate,"  Analog black/white hole pairs, consisting of a region of supersonic flow,
have been achieved in a recent experiment by J. Steinhauer using an elongated
Bose-Einstein condensate. A growing standing density wave, and a checkerboard
feature in the density-density correlation function, were observed in the
supersonic region. We model the density-density correlation function, taking
into account both quantum fluctuations and the shot-to-shot variation of atom
number normally present in ultracold-atom experiments. We find that quantum
fluctuations alone produce some, but not all, of the features of the
correlation function, whereas atom-number fluctuation alone can produce all the
observed features, and agreement is best when both are included. In both cases,
the density-density correlation is not intrinsic to the fluctuations, but
rather is induced by modulation of the standing wave caused by the
fluctuations.
",0,1,0,0,0,0
79,Genus growth in $\mathbb{Z}_p$-towers of function fields,"  Let $K$ be a function field over a finite field $k$ of characteristic $p$ and
let $K_{\infty}/K$ be a geometric extension with Galois group $\mathbb{Z}_p$.
Let $K_n$ be the corresponding subextension with Galois group
$\mathbb{Z}/p^n\mathbb{Z}$ and genus $g_n$. In this paper, we give a simple
explicit formula $g_n$ in terms of an explicit Witt vector construction of the
$\mathbb{Z}_p$-tower. This formula leads to a tight lower bound on $g_n$ which
is quadratic in $p^n$. Furthermore, we determine all $\mathbb{Z}_p$-towers for
which the genus sequence is stable, in the sense that there are $a,b,c \in
\mathbb{Q}$ such that $g_n=a p^{2n}+b p^n +c$ for $n$ large enough. Such genus
stable towers are expected to have strong stable arithmetic properties for
their zeta functions. A key technical contribution of this work is a new
simplified formula for the Schmid-Witt symbol coming from local class field
theory.
",0,0,1,0,0,0
80,Topological Phases emerging from Spin-Orbital Physics,"  We study the evolution of spin-orbital correlations in an inhomogeneous
quantum system with an impurity replacing a doublon by a holon orbital degree
of freedom. Spin-orbital entanglement is large when spin correlations are
antiferromagnetic, while for a ferromagnetic host we obtain a pure orbital
description. In this regime the orbital model can be mapped on spinless
fermions and we uncover topological phases with zero energy modes at the edge
or at the domain between magnetically inequivalent regions.
",0,1,0,0,0,0
81,"Accurate and Diverse Sampling of Sequences based on a ""Best of Many"" Sample Objective","  For autonomous agents to successfully operate in the real world, anticipation
of future events and states of their environment is a key competence. This
problem has been formalized as a sequence extrapolation problem, where a number
of observations are used to predict the sequence into the future. Real-world
scenarios demand a model of uncertainty of such predictions, as predictions
become increasingly uncertain -- in particular on long time horizons. While
impressive results have been shown on point estimates, scenarios that induce
multi-modal distributions over future sequences remain challenging. Our work
addresses these challenges in a Gaussian Latent Variable model for sequence
prediction. Our core contribution is a ""Best of Many"" sample objective that
leads to more accurate and more diverse predictions that better capture the
true variations in real-world sequence data. Beyond our analysis of improved
model fit, our models also empirically outperform prior work on three diverse
tasks ranging from traffic scenes to weather data.
",0,0,0,1,0,0
82,Exploring RNN-Transducer for Chinese Speech Recognition,"  End-to-end approaches have drawn much attention recently for significantly
simplifying the construction of an automatic speech recognition (ASR) system.
RNN transducer (RNN-T) is one of the popular end-to-end methods. Previous
studies have shown that RNN-T is difficult to train and a very complex training
process is needed for a reasonable performance. In this paper, we explore RNN-T
for a Chinese large vocabulary continuous speech recognition (LVCSR) task and
aim to simplify the training process while maintaining performance. First, a
new strategy of learning rate decay is proposed to accelerate the model
convergence. Second, we find that adding convolutional layers at the beginning
of the network and using ordered data can discard the pre-training process of
the encoder without loss of performance. Besides, we design experiments to find
a balance among the usage of GPU memory, training circle and model performance.
Finally, we achieve 16.9% character error rate (CER) on our test set which is
2% absolute improvement from a strong BLSTM CE system with language model
trained on the same text corpus.
",1,0,0,0,0,0
83,A Debt-Aware Learning Approach for Resource Adaptations in Cloud Elasticity Management,"  Elasticity is a cloud property that enables applications and its execution
systems to dynamically acquire and release shared computational resources on
demand. Moreover, it unfolds the advantage of economies of scale in the cloud
through a drop in the average costs of these shared resources. However, it is
still an open challenge to achieve a perfect match between resource demand and
provision in autonomous elasticity management. Resource adaptation decisions
essentially involve a trade-off between economics and performance, which
produces a gap between the ideal and actual resource provisioning. This gap, if
not properly managed, can negatively impact the aggregate utility of a cloud
customer in the long run. To address this limitation, we propose a technical
debt-aware learning approach for autonomous elasticity management based on a
reinforcement learning of elasticity debts in resource provisioning; the
adaptation pursues strategic decisions that trades off economics against
performance. We extend CloudSim and Burlap to evaluate our approach. The
evaluation shows that a reinforcement learning of technical debts in elasticity
obtains a higher utility for a cloud customer, while conforming expected levels
of performance.
",1,0,0,0,0,0
84,Semi-simplicial spaces,"  This is an exposition of homotopical results on the geometric realization of
semi-simplicial spaces. We then use these to derive basic foundational results
about classifying spaces of topological categories, possibly without units. The
topics considered include: fibrancy conditions on topological categories; the
effect on classifying spaces of freely adjoining units; approximate notions of
units; Quillen's Theorems A and B for non-unital topological categories; the
effect on classifying spaces of changing the topology on the space of objects;
the Group-Completion Theorem.
",0,0,1,0,0,0
85,"Constraints, Lazy Constraints, or Propagators in ASP Solving: An Empirical Analysis","  Answer Set Programming (ASP) is a well-established declarative paradigm. One
of the successes of ASP is the availability of efficient systems.
State-of-the-art systems are based on the ground+solve approach. In some
applications this approach is infeasible because the grounding of one or few
constraints is expensive. In this paper, we systematically compare alternative
strategies to avoid the instantiation of problematic constraints, that are
based on custom extensions of the solver. Results on real and synthetic
benchmarks highlight some strengths and weaknesses of the different strategies.
(Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)
",1,0,0,0,0,0
86,A Unified Approach to Nonlinear Transformation Materials,"  The advances in geometric approaches to optical devices due to transformation
optics has led to the development of cloaks, concentrators, and other devices.
It has also been shown that transformation optics can be used to gravitational
fields from general relativity. However, the technique is currently constrained
to linear devices, as a consistent approach to nonlinearity (including both the
case of a nonlinear background medium and a nonlinear transformation) remains
an open question. Here we show that nonlinearity can be incorporated into
transformation optics in a consistent way. We use this to illustrate a number
of novel effects, including cloaking an optical soliton, modeling nonlinear
solutions to Einstein's field equations, controlling transport in a Debye
solid, and developing a set of constitutive to relations for relativistic
cloaks in arbitrary nonlinear backgrounds.
",0,1,0,0,0,0
87,Stationary crack propagation in a two-dimensional visco-elastic network model,"  We investigate crack propagation in a simple two-dimensional visco-elastic
model and find a scaling regime in the relation between the propagation
velocity and energy release rate or fracture energy, together with lower and
upper bounds of the scaling regime. On the basis of our result, the existence
of the lower and upper bounds is expected to be universal or model-independent:
the present simple simulation model provides generic insight into the physics
of crack propagation, and the model will be a first step towards the
development of a more refined coarse-grained model. Relatively abrupt changes
of velocity are predicted near the lower and upper bounds for the scaling
regime and the positions of the bounds could be good markers for the
development of tough polymers, for which we provide simple views that could be
useful as guiding principles for toughening polymer-based materials.
",0,1,0,0,0,0
88,A note on the fundamental group of Kodaira fibrations,"  The fundamental group $\pi$ of a Kodaira fibration is, by definition, the
extension of a surface group $\Pi_b$ by another surface group $\Pi_g$, i.e. \[
1 \rightarrow \Pi_g \rightarrow \pi \rightarrow \Pi_b \rightarrow 1. \]
Conversely, we can inquire about what conditions need to be satisfied by a
group of that sort in order to be the fundamental group of a Kodaira fibration.
In this short note we collect some restriction on the image of the classifying
map $m \colon \Pi_b \to \Gamma_g$ in terms of the coinvariant homology of
$\Pi_g$. In particular, we observe that if $\pi$ is the fundamental group of a
Kodaira fibration with relative irregularity $g-s$, then $g \leq 1+ 6s$, and we
show that this effectively constrains the possible choices for $\pi$, namely
that there are group extensions as above that fail to satisfy this bound, hence
cannot be the fundamental group of a Kodaira fibration. In particular this
provides examples of symplectic $4$--manifolds that fail to admit a K??hler
structure for reasons that eschew the usual obstructions.
",0,0,1,0,0,0
89,Photo-Chemically Directed Self-Assembly of Carbon Nanotubes on Surfaces,"  Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel
material are used in a variety of electronics applications. However, a
competitive CNT-based technology requires the precise placement of CNTs at
predefined locations of a substrate. One promising placement approach is to use
chemical recognition to bind CNTs from solution at the desired locations on a
surface. Producing the chemical pattern on the substrate is challenging. Here
we describe a one-step patterning approach based on a highly photosensitive
surface monolayer. The monolayer contains chromophopric group as light
sensitive body with heteroatoms as high quantum yield photolysis center. As
deposited, the layer will bind CNTs from solution. However, when exposed to
ultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for
conventional photoresists, the monolayer cleaves and no longer binds CNTs.
These features allow standard, wafer-scale UV lithography processes to be used
to form a patterned chemical monolayer without the need for complex substrate
patterning or monolayer stamping.
",0,1,0,0,0,0
90,Split-and-augmented Gibbs sampler - Application to large-scale inference problems,"  This paper derives two new optimization-driven Monte Carlo algorithms
inspired from variable splitting and data augmentation. In particular, the
formulation of one of the proposed approaches is closely related to the
alternating direction method of multipliers (ADMM) main steps. The proposed
framework enables to derive faster and more efficient sampling schemes than the
current state-of-the-art methods and can embed the latter. By sampling
efficiently the parameter to infer as well as the hyperparameters of the
problem, the generated samples can be used to approximate Bayesian estimators
of the parameters to infer. Additionally, the proposed approach brings
confidence intervals at a low cost contrary to optimization methods.
Simulations on two often-studied signal processing problems illustrate the
performance of the two proposed samplers. All results are compared to those
obtained by recent state-of-the-art optimization and MCMC algorithms used to
solve these problems.
",0,0,0,1,0,0
91,Does a generalized Chaplygin gas correctly describe the cosmological dark sector?,"  Yes, but only for a parameter value that makes it almost coincide with the
standard model. We reconsider the cosmological dynamics of a generalized
Chaplygin gas (gCg) which is split into a cold dark matter (CDM) part and a
dark energy (DE) component with constant equation of state. This model, which
implies a specific interaction between CDM and DE, has a $\Lambda$CDM limit and
provides the basis for studying deviations from the latter. Including matter
and radiation, we use the (modified) CLASS code \cite{class} to construct the
CMB and matter power spectra in order to search for a gCg-based concordance
model that is in agreement with the SNIa data from the JLA sample and with
recent Planck data. The results reveal that the gCg parameter $\alpha$ is
restricted to $|\alpha|\lesssim 0.05$, i.e., to values very close to the
$\Lambda$CDM limit $\alpha =0$. This excludes, in particular, models in which
DE decays linearly with the Hubble rate.
",0,1,0,0,0,0
92,The effects of subdiffusion on the NTA size measurements of extracellular vesicles in biological samples,"  The interest in the extracellular vesicles (EVs) is rapidly growing as they
became reliable biomarkers for many diseases. For this reason, fast and
accurate techniques of EVs size characterization are the matter of utmost
importance. One increasingly popular technique is the Nanoparticle Tracking
Analysis (NTA), in which the diameters of EVs are calculated from their
diffusion constants. The crucial assumption here is that the diffusion in NTA
follows the Stokes-Einstein relation, i.e. that the Mean Square Displacement
(MSD) of a particle grows linearly in time (MSD $\propto t$). However, we show
that NTA violates this assumption in both artificial and biological samples,
i.e. a large population of particles show a strongly sub-diffusive behaviour
(MSD $\propto t^\alpha$, $0<\alpha<1$). To support this observation we present
a range of experimental results for both polystyrene beads and EVs. This is
also related to another problem: for the same samples there exists a huge
discrepancy (by the factor of 2-4) between the sizes measured with NTA and with
the direct imaging methods, such as AFM. This can be remedied by e.g. the
Finite Track Length Adjustment (FTLA) method in NTA, but its applicability is
limited in the biological and poly-disperse samples. On the other hand, the
models of sub-diffusion rarely provide the direct relation between the size of
a particle and the generalized diffusion constant. However, we solve this last
problem by introducing the logarithmic model of sub-diffusion, aimed at
retrieving the size data. In result, we propose a novel protocol of NTA data
analysis. The accuracy of our method is on par with FTLA for small
($\simeq$200nm) particles. We apply our method to study the EVs samples and
corroborate the results with AFM.
",0,1,0,0,0,0
93,Empirical regression quantile process with possible application to risk analysis,"  The processes of the averaged regression quantiles and of their modifications
provide useful tools in the regression models when the covariates are not fully
under our control. As an application we mention the probabilistic risk
assessment in the situation when the return depends on some exogenous
variables. The processes enable to evaluate the expected $\alpha$-shortfall
($0\leq\alpha\leq 1$) and other measures of the risk, recently generally
accepted in the financial literature, but also help to measure the risk in
environment analysis and elsewhere.
",0,0,1,1,0,0
94,Primordial perturbations from inflation with a hyperbolic field-space,"  We study primordial perturbations from hyperinflation, proposed recently and
based on a hyperbolic field-space. In the previous work, it was shown that the
field-space angular momentum supported by the negative curvature modifies the
background dynamics and enhances fluctuations of the scalar fields
qualitatively, assuming that the inflationary background is almost de Sitter.
In this work, we confirm and extend the analysis based on the standard approach
of cosmological perturbation in multi-field inflation. At the background level,
to quantify the deviation from de Sitter, we introduce the slow-varying
parameters and show that steep potentials, which usually can not drive
inflation, can drive inflation. At the linear perturbation level, we obtain the
power spectrum of primordial curvature perturbation and express the spectral
tilt and running in terms of the slow-varying parameters. We show that
hyperinflation with power-law type potentials has already been excluded by the
recent Planck observations, while exponential-type potential with the exponent
of order unity can be made consistent with observations as far as the power
spectrum is concerned. We also argue that, in the context of a simple $D$-brane
inflation, the hyperinflation requires exponentially large hyperbolic extra
dimensions but that masses of Kaluza-Klein gravitons can be kept relatively
heavy.
",0,1,0,0,0,0
95,Role of Vanadyl Oxygen in Understanding Metallic Behavior of V2O5(001) Nanorods,"  Vanadium pentoxide (V2O5), the most stable member of vanadium oxide family,
exhibits interesting semiconductor to metal transition in the temperature range
of 530-560 K. The metallic behavior originates because of the reduction of V2O5
through oxygen vacancies. In the present report, V2O5 nanorods in the
orthorhombic phase with crystal orientation of (001) are grown using vapor
transport process. Among three nonequivalent oxygen atoms in a VO5 pyramidal
formula unit in V2O5 structure, the role of terminal vanadyl oxygen (OI) in the
formation of metallic phase above the transition temperature is established
from the temperature-dependent Raman spectroscopic studies. The origin of the
metallic behavior of V2O5 is also understood due to the breakdown of pdpi bond
between OI and nearest V atom instigated by the formation of vanadyl OI
vacancy, confirmed from the downward shift of the bottom most split-off
conduction bands in the material with increasing temperature.
",0,1,0,0,0,0
96,Graph Convolution: A High-Order and Adaptive Approach,"  In this paper, we presented a novel convolutional neural network framework
for graph modeling, with the introduction of two new modules specially designed
for graph-structured data: the $k$-th order convolution operator and the
adaptive filtering module. Importantly, our framework of High-order and
Adaptive Graph Convolutional Network (HA-GCN) is a general-purposed
architecture that fits various applications on both node and graph centrics, as
well as graph generative models. We conducted extensive experiments on
demonstrating the advantages of our framework. Particularly, our HA-GCN
outperforms the state-of-the-art models on node classification and molecule
property prediction tasks. It also generates 32% more real molecules on the
molecule generation task, both of which will significantly benefit real-world
applications such as material design and drug screening.
",1,0,0,1,0,0
97,Learning Sparse Representations in Reinforcement Learning with Sparse Coding,"  A variety of representation learning approaches have been investigated for
reinforcement learning; much less attention, however, has been given to
investigating the utility of sparse coding. Outside of reinforcement learning,
sparse coding representations have been widely used, with non-convex objectives
that result in discriminative representations. In this work, we develop a
supervised sparse coding objective for policy evaluation. Despite the
non-convexity of this objective, we prove that all local minima are global
minima, making the approach amenable to simple optimization strategies. We
empirically show that it is key to use a supervised objective, rather than the
more straightforward unsupervised sparse coding approach. We compare the
learned representations to a canonical fixed sparse representation, called
tile-coding, demonstrating that the sparse coding representation outperforms a
wide variety of tilecoding representations.
",1,0,0,1,0,0
98,Almost euclidean Isoperimetric Inequalities in spaces satisfying local Ricci curvature lower bounds,"  Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove
that if a Riemannian manifold has Ricci curvature bounded below in a metric
ball which moreover has almost maximal volume, then in a smaller ball (in a
quantified sense) it holds an almost-euclidean isoperimetric inequality. The
result is actually established in the more general framework of non-smooth
spaces satisfying local Ricci curvature lower bounds in a synthetic sense via
optimal transportation.
",0,0,1,0,0,0
99,Exponential Sums and Riesz energies,"  We bound an exponential sum that appears in the study of irregularities of
distribution (the low-frequency Fourier energy of the sum of several Dirac
measures) by geometric quantities: a special case is that for all $\left\{ x_1,
\dots, x_N\right\} \subset \mathbb{T}^2$, $X \geq 1$ and a universal $c>0$ $$
\sum_{i,j=1}^{N}{ \frac{X^2}{1 + X^4 \|x_i -x_j\|^4}} \lesssim \sum_{k \in
\mathbb{Z}^2 \atop \|k\| \leq X}{ \left| \sum_{n=1}^{N}{ e^{2 \pi i
\left\langle k, x_n \right\rangle}}\right|^2} \lesssim \sum_{i,j=1}^{N}{ X^2
e^{-c X^2\|x_i -x_j\|^2}}.$$ Since this exponential sum is intimately tied to
rather subtle distribution properties of the points, we obtain nonlocal
structural statements for near-minimizers of the Riesz-type energy. In the
regime $X \gtrsim N^{1/2}$ both upper and lower bound match for
maximally-separated point sets satisfying $\|x_i -x_j\| \gtrsim N^{-1/2}$.
",0,0,1,0,0,0
100,One dimensionalization in the spin-1 Heisenberg model on the anisotropic triangular lattice,"  We investigate the effect of dimensional crossover in the ground state of the
antiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular
lattice that interpolates between the regime of weakly coupled Haldane chains
($J^{\prime}\! \!\ll\!\! J$) and the isotropic triangular lattice
($J^{\prime}\!\!=\!\!J$). We use the density-matrix renormalization group
(DMRG) and Schwinger boson theory performed at the Gaussian correction level
above the saddle-point solution. Our DMRG results show an abrupt transition
between decoupled spin chains and the spirally ordered regime at
$(J^{\prime}/J)_c\sim 0.42$, signaled by the sudden closing of the spin gap.
Coming from the magnetically ordered side, the computation of the spin
stiffness within Schwinger boson theory predicts the instability of the spiral
magnetic order toward a magnetically disordered phase with one-dimensional
features at $(J^{\prime}/J)_c \sim 0.43$. The agreement of these complementary
methods, along with the strong difference found between the intra- and the
interchain DMRG short spin-spin correlations; for sufficiently large values of
the interchain coupling, suggests that the interplay between the quantum
fluctuations and the dimensional crossover effects gives rise to the
one-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.
",0,1,0,0,0,0
101,Memory Aware Synapses: Learning what (not) to forget,"  Humans can learn in a continuous manner. Old rarely utilized knowledge can be
overwritten by new incoming information while important, frequently used
knowledge is prevented from being erased. In artificial learning systems,
lifelong learning so far has focused mainly on accumulating knowledge over
tasks and overcoming catastrophic forgetting. In this paper, we argue that,
given the limited model capacity and the unlimited new information to be
learned, knowledge has to be preserved or erased selectively. Inspired by
neuroplasticity, we propose a novel approach for lifelong learning, coined
Memory Aware Synapses (MAS). It computes the importance of the parameters of a
neural network in an unsupervised and online manner. Given a new sample which
is fed to the network, MAS accumulates an importance measure for each parameter
of the network, based on how sensitive the predicted output function is to a
change in this parameter. When learning a new task, changes to important
parameters can then be penalized, effectively preventing important knowledge
related to previous tasks from being overwritten. Further, we show an
interesting connection between a local version of our method and Hebb's
rule,which is a model for the learning process in the brain. We test our method
on a sequence of object recognition tasks and on the challenging problem of
learning an embedding for predicting $<$subject, predicate, object$>$ triplets.
We show state-of-the-art performance and, for the first time, the ability to
adapt the importance of the parameters based on unlabeled data towards what the
network needs (not) to forget, which may vary depending on test conditions.
",1,0,0,1,0,0
102,Uniform Spectral Convergence of the Stochastic Galerkin Method for the Linear Semiconductor Boltzmann Equation with Random Inputs and Diffusive Scalings,"  In this paper, we study the generalized polynomial chaos (gPC) based
stochastic Galerkin method for the linear semiconductor Boltzmann equation
under diffusive scaling and with random inputs from an anisotropic collision
kernel and the random initial condition. While the numerical scheme and the
proof of uniform-in-Knudsen-number regularity of the distribution function in
the random space has been introduced in [Jin-Liu-16'], the main goal of this
paper is to first obtain a sharper estimate on the regularity of the
solution-an exponential decay towards its local equilibrium, which then lead to
the uniform spectral convergence of the stochastic Galerkin method for the
problem under study.
",0,0,1,0,0,0
103,On Improving the Capacity of Solving Large-scale Wireless Network Design Problems by Genetic Algorithms,"  Over the last decade, wireless networks have experienced an impressive growth
and now play a main role in many telecommunications systems. As a consequence,
scarce radio resources, such as frequencies, became congested and the need for
effective and efficient assignment methods arose. In this work, we present a
Genetic Algorithm for solving large instances of the Power, Frequency and
Modulation Assignment Problem, arising in the design of wireless networks. To
our best knowledge, this is the first Genetic Algorithm that is proposed for
such problem. Compared to previous works, our approach allows a wider
exploration of the set of power solutions, while eliminating sources of
numerical problems. The performance of the algorithm is assessed by tests over
a set of large realistic instances of a Fixed WiMAX Network.
",1,0,1,0,0,0
104,Quasi two-dimensional Fermi surface topography of the delafossite PdRhO$_2$,"  We report on a combined study of the de Haas-van Alphen effect and angle
resolved photoemission spectroscopy on single crystals of the metallic
delafossite PdRhO$_2$ rounded off by \textit{ab initio} band structure
calculations. A high sensitivity torque magnetometry setup with SQUID readout
and synchrotron-based photoemission with a light spot size of
$~50\,\mu\mathrm{m}$ enabled high resolution data to be obtained from samples
as small as $150\times100\times20\,(\mu\mathrm{m})^3$. The Fermi surface shape
is nearly cylindrical with a rounded hexagonal cross section enclosing a
Luttinger volume of 1.00(1) electrons per formula unit.
",0,1,0,0,0,0
105,A Variational Characterization of R??nyi Divergences,"  Atar, Chowdhary and Dupuis have recently exhibited a variational formula for
exponential integrals of bounded measurable functions in terms of R??nyi
divergences. We develop a variational characterization of the R??nyi
divergences between two probability distributions on a measurable sace in terms
of relative entropies. When combined with the elementary variational formula
for exponential integrals of bounded measurable functions in terms of relative
entropy, this yields the variational formula of Atar, Chowdhary and Dupuis as a
corollary. We also develop an analogous variational characterization of the
R??nyi divergence rates between two stationary finite state Markov chains in
terms of relative entropy rates. When combined with Varadhan's variational
characterization of the spectral radius of square matrices with nonnegative
entries in terms of relative entropy, this yields an analog of the variational
formula of Atar, Chowdary and Dupuis in the framework of finite state Markov
chains.
",1,0,1,1,0,0
106,Interlayer coupling and gate-tunable excitons in transition metal dichalcogenide heterostructures,"  Bilayer van der Waals (vdW) heterostructures such as MoS2/WS2 and MoSe2/WSe2
have attracted much attention recently, particularly because of their type II
band alignments and the formation of interlayer exciton as the lowest-energy
excitonic state. In this work, we calculate the electronic and optical
properties of such heterostructures with the first-principles GW+Bethe-Salpeter
Equation (BSE) method and reveal the important role of interlayer coupling in
deciding the excited-state properties, including the band alignment and
excitonic properties. Our calculation shows that due to the interlayer
coupling, the low energy excitons can be widely tunable by a vertical gate
field. In particular, the dipole oscillator strength and radiative lifetime of
the lowest energy exciton in these bilayer heterostructures is varied by over
an order of magnitude within a practical external gate field. We also build a
simple model that captures the essential physics behind this tunability and
allows the extension of the ab initio results to a large range of electric
fields. Our work clarifies the physical picture of interlayer excitons in
bilayer vdW heterostructures and predicts a wide range of gate-tunable
excited-state properties of 2D optoelectronic devices.
",0,1,0,0,0,0
107,Enumeration of singular varieties with tangency conditions,"  We construct the algebraic cobordism theory of bundles and divisors on
varieties. It has a simple basis (over Q) from projective spaces and its rank
is equal to the number of Chern numbers. An application of this algebraic
cobordism theory is the enumeration of singular subvarieties with give tangent
conditions with a fixed smooth divisor, where the subvariety is the zero locus
of a section of a vector bundle. We prove that the generating series of numbers
of such subvarieties gives a homomorphism from the algebraic cobordism group to
the power series ring. This implies that the enumeration of singular
subvarieties with tangency conditions is governed by universal polynomials of
Chern numbers, when the vector bundle is sufficiently ample. This result
combines and generalizes the Caporaso-Harris recursive formula, Gottsche's
conjecture, classical De Jonquiere's Formula and node polynomials from tropical
geometry.
",0,0,1,0,0,0
108,In-home and remote use of robotic body surrogates by people with profound motor deficits,"  People with profound motor deficits could perform useful physical tasks for
themselves by controlling robots that are comparable to the human body. Whether
this is possible without invasive interfaces has been unclear, due to the
robot's complexity and the person's limitations. We developed a novel,
augmented reality interface and conducted two studies to evaluate the extent to
which it enabled people with profound motor deficits to control robotic body
surrogates. 15 novice users achieved meaningful improvements on a clinical
manipulation assessment when controlling the robot in Atlanta from locations
across the United States. Also, one expert user performed 59 distinct tasks in
his own home over seven days, including self-care tasks such as feeding. Our
results demonstrate that people with profound motor deficits can effectively
control robotic body surrogates without invasive interfaces.
",1,0,0,0,0,0
109,ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information,"  Object detection in wide area motion imagery (WAMI) has drawn the attention
of the computer vision research community for a number of years. WAMI proposes
a number of unique challenges including extremely small object sizes, both
sparse and densely-packed objects, and extremely large search spaces (large
video frames). Nearly all state-of-the-art methods in WAMI object detection
report that appearance-based classifiers fail in this challenging data and
instead rely almost entirely on motion information in the form of background
subtraction or frame-differencing. In this work, we experimentally verify the
failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a
heatmap-based fully convolutional neural network (CNN), and propose a novel
two-stage spatio-temporal CNN which effectively and efficiently combines both
appearance and motion information to significantly surpass the state-of-the-art
in WAMI object detection. To reduce the large search space, the first stage
(ClusterNet) takes in a set of extremely large video frames, combines the
motion and appearance information within the convolutional architecture, and
proposes regions of objects of interest (ROOBI). These ROOBI can contain from
one to clusters of several hundred objects due to the large video frame size
and varying object density in WAMI. The second stage (FoveaNet) then estimates
the centroid location of all objects in that given ROOBI simultaneously via
heatmap estimation. The proposed method exceeds state-of-the-art results on the
WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped
objects, as well as being the first proposed method in wide area motion imagery
to detect completely stationary objects.
",1,0,0,0,0,0
110,Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds,"  Monte Carlo Tree Search (MCTS), most famously used in game-play artificial
intelligence (e.g., the game of Go), is a well-known strategy for constructing
approximate solutions to sequential decision problems. Its primary innovation
is the use of a heuristic, known as a default policy, to obtain Monte Carlo
estimates of downstream values for states in a decision tree. This information
is used to iteratively expand the tree towards regions of states and actions
that an optimal policy might visit. However, to guarantee convergence to the
optimal action, MCTS requires the entire tree to be expanded asymptotically. In
this paper, we propose a new technique called Primal-Dual MCTS that utilizes
sampled information relaxation upper bounds on potential actions, creating the
possibility of ""ignoring"" parts of the tree that stem from highly suboptimal
choices. This allows us to prove that despite converging to a partial decision
tree in the limit, the recommended action from Primal-Dual MCTS is optimal. The
new approach shows significant promise when used to optimize the behavior of a
single driver navigating a graph while operating on a ride-sharing platform.
Numerical experiments on a real dataset of 7,000 trips in New Jersey suggest
that Primal-Dual MCTS improves upon standard MCTS by producing deeper decision
trees and exhibits a reduced sensitivity to the size of the action space.
",1,0,1,0,0,0
111,Fermi-edge singularity and the functional renormalization group,"  We study the Fermi-edge singularity, describing the response of a degenerate
electron system to optical excitation, in the framework of the functional
renormalization group (fRG). Results for the (interband) particle-hole
susceptibility from various implementations of fRG (one- and two-
particle-irreducible, multi-channel Hubbard-Stratonovich, flowing
susceptibility) are compared to the summation of all leading logarithmic (log)
diagrams, achieved by a (first-order) solution of the parquet equations. For
the (zero-dimensional) special case of the X-ray-edge singularity, we show that
the leading log formula can be analytically reproduced in a consistent way from
a truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic
structure, we show that this derivation relies on fortuitous partial
cancellations special to the form of and accuracy applied to the X-ray-edge
singularity and does not generalize.
",0,1,0,0,0,0
112,"Towards ""AlphaChem"": Chemical Synthesis Planning with Tree Search and Deep Neural Network Policies","  Retrosynthesis is a technique to plan the chemical synthesis of organic
molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a
search tree is built by analysing molecules recursively and dissecting them
into simpler molecular building blocks until one obtains a set of known
building blocks. The search space is intractably large, and it is difficult to
determine the value of retrosynthetic positions. Here, we propose to model
retrosynthesis as a Markov Decision Process. In combination with a Deep Neural
Network policy learned from essentially the complete published knowledge of
chemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In
exploratory studies, we demonstrate that MCTS with neural network policies
outperforms the traditionally used best-first search with hand-coded
heuristics.
",1,1,0,0,0,0
113,The quasi-Assouad dimension for stochastically self-similar sets,"  The class of stochastically self-similar sets contains many famous examples
of random sets, e.g. Mandelbrot percolation and general fractal percolation.
Under the assumption of the uniform open set condition and some mild
assumptions on the iterated function systems used, we show that the
quasi-Assouad dimension of self-similar random recursive sets is almost surely
equal to the almost sure Hausdorff dimension of the set. We further comment on
random homogeneous and $V$-variable sets and the removal of overlap conditions.
",0,0,1,0,0,0
114,Influence of Spin Orbit Coupling in the Iron-Based Superconductors,"  We report on the influence of spin-orbit coupling (SOC) in the Fe-based
superconductors (FeSCs) via application of circularly-polarized spin and
angle-resolved photoemission spectroscopy. We combine this technique in
representative members of both the Fe-pnictides and Fe-chalcogenides with ab
initio density functional theory and tight-binding calculations to establish an
ubiquitous modification of the electronic structure in these materials imbued
by SOC. The influence of SOC is found to be concentrated on the hole pockets
where the superconducting gap is generally found to be largest. This result
contests descriptions of superconductivity in these materials in terms of pure
spin-singlet eigenstates, raising questions regarding the possible pairing
mechanisms and role of SOC therein.
",0,1,0,0,0,0
115,Effect of Meltdown and Spectre Patches on the Performance of HPC Applications,"  In this work we examine how the updates addressing Meltdown and Spectre
vulnerabilities impact the performance of HPC applications. To study this we
use the application kernel module of XDMoD to test the performance before and
after the application of the vulnerability patches. We tested the performance
difference for multiple application and benchmarks including: NWChem, NAMD,
HPCC, IOR, MDTest and IMB. The results show that although some specific
functions can have performance decreased by as much as 74%, the majority of
individual metrics indicates little to no decrease in performance. The
real-world applications show a 2-3% decrease in performance for single node
jobs and a 5-11% decrease for parallel multi node jobs.
",1,0,0,0,0,0
116,Gene regulatory network inference: an introductory survey,"  Gene regulatory networks are powerful abstractions of biological systems.
Since the advent of high-throughput measurement technologies in biology in the
late 90s, reconstructing the structure of such networks has been a central
computational problem in systems biology. While the problem is certainly not
solved in its entirety, considerable progress has been made in the last two
decades, with mature tools now available. This chapter aims to provide an
introduction to the basic concepts underpinning network inference tools,
attempting a categorisation which highlights commonalities and relative
strengths. While the chapter is meant to be self-contained, the material
presented should provide a useful background to the later, more specialised
chapters of this book.
",0,0,0,0,1,0
117,Optic Disc and Cup Segmentation Methods for Glaucoma Detection with Modification of U-Net Convolutional Neural Network,"  Glaucoma is the second leading cause of blindness all over the world, with
approximately 60 million cases reported worldwide in 2010. If undiagnosed in
time, glaucoma causes irreversible damage to the optic nerve leading to
blindness. The optic nerve head examination, which involves measurement of
cup-to-disc ratio, is considered one of the most valuable methods of structural
diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation
of optic disc and optic cup on eye fundus images and can be performed by modern
computer vision algorithms. This work presents universal approach for automatic
optic disc and cup segmentation, which is based on deep learning, namely,
modification of U-Net convolutional neural network. Our experiments include
comparison with the best known methods on publicly available databases
DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,
our method achieves quality comparable to current state-of-the-art methods,
outperforming them in terms of the prediction time.
",1,0,0,1,0,0
118,"Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks","  The life of the modern world essentially depends on the work of the large
artificial homogeneous networks, such as wired and wireless communication
systems, networks of roads and pipelines. The support of their effective
continuous functioning requires automatic screening and permanent optimization
with processing of the huge amount of data by high-performance distributed
systems. We propose new meta-algorithm of large homogeneous network analysis,
its decomposition into alternative sets of loosely connected subnets, and
parallel optimization of the most independent elements. This algorithm is based
on a network-specific correlation function, Simulated Annealing technique, and
is adapted to work in the computer cluster. On the example of large wireless
network, we show that proposed algorithm essentially increases speed of
parallel optimization. The elaborated general approach can be used for analysis
and optimization of the wide range of networks, including such specific types
as artificial neural networks or organized in networks physiological systems of
living organisms.
",1,0,1,0,0,0
119,Robust Contextual Bandit via the Capped-$\ell_{2}$ norm,"  This paper considers the actor-critic contextual bandit for the mobile health
(mHealth) intervention. The state-of-the-art decision-making methods in mHealth
generally assume that the noise in the dynamic system follows the Gaussian
distribution. Those methods use the least-square-based algorithm to estimate
the expected reward, which is prone to the existence of outliers. To deal with
the issue of outliers, we propose a novel robust actor-critic contextual bandit
method for the mHealth intervention. In the critic updating, the
capped-$\ell_{2}$ norm is used to measure the approximation error, which
prevents outliers from dominating our objective. A set of weights could be
achieved from the critic updating. Considering them gives a weighted objective
for the actor updating. It provides the badly noised sample in the critic
updating with zero weights for the actor updating. As a result, the robustness
of both actor-critic updating is enhanced. There is a key parameter in the
capped-$\ell_{2}$ norm. We provide a reliable method to properly set it by
making use of one of the most fundamental definitions of outliers in
statistics. Extensive experiment results demonstrate that our method can
achieve almost identical results compared with the state-of-the-art methods on
the dataset without outliers and dramatically outperform them on the datasets
noised by outliers.
",1,0,0,1,0,0
120,Improper posteriors are not improper,"  In 1933 Kolmogorov constructed a general theory that defines the modern
concept of conditional expectation. In 1955 Renyi fomulated a new axiomatic
theory for probability motivated by the need to include unbounded measures. We
introduce a general concept of conditional expectation in Renyi spaces. In this
theory improper priors are allowed, and the resulting posterior can also be
improper.
In 1965 Lindley published his classic text on Bayesian statistics using the
theory of Renyi, but retracted this idea in 1973 due to the appearance of
marginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes
are investigated, and the seemingly conflicting results are explained. The
theory of Renyi can hence be used as an axiomatic basis for statistics that
allows use of unbounded priors.
Keywords: Haldane's prior; Poisson intensity; Marginalization paradox;
Measure theory; conditional probability space; axioms for statistics;
conditioning on a sigma field; improper prior
",0,0,1,1,0,0
121,Fault Tolerant Consensus Agreement Algorithm,"  Recently a new fault tolerant and simple mechanism was designed for solving
commit consensus problem. It is based on replicated validation of messages sent
between transaction participants and a special dispatcher validator manager
node. This paper presents a correctness, safety proofs and performance analysis
of this algorithm.
",1,0,0,0,0,0
122,Congestion Barcodes: Exploring the Topology of Urban Congestion Using Persistent Homology,"  This work presents a new method to quantify connectivity in transportation
networks. Inspired by the field of topological data analysis, we propose a
novel approach to explore the robustness of road network connectivity in the
presence of congestion on the roadway. The robustness of the pattern is
summarized in a congestion barcode, which can be constructed directly from
traffic datasets commonly used for navigation. As an initial demonstration, we
illustrate the main technique on a publicly available traffic dataset in a
neighborhood in New York City.
",1,0,1,0,0,0
123,Once in a blue moon: detection of 'bluing' during debris transits in the white dwarf WD1145+017,"  The first transiting planetesimal orbiting a white dwarf was recently
detected in K2 data of WD1145+017 and has been followed up intensively. The
multiple, long, and variable transits suggest the transiting objects are dust
clouds, probably produced by a disintegrating asteroid. In addition, the system
contains circumstellar gas, evident by broad absorption lines, mostly in the
u'-band, and a dust disc, indicated by an infrared excess. Here we present the
first detection of a change in colour of WD1145+017 during transits, using
simultaneous multi-band fast-photometry ULTRACAM measurements over the
u'g'r'i'-bands. The observations reveal what appears to be 'bluing' during
transits; transits are deeper in the redder bands, with a u'-r' colour
difference of up to ~-0.05 mag. We explore various possible explanations for
the bluing. 'Spectral' photometry obtained by integrating over bandpasses in
the spectroscopic data in- and out-of-transit, compared to the photometric
data, shows that the observed colour difference is most likely the result of
reduced circumstellar absorption in the spectrum during transits. This
indicates that the transiting objects and the gas share the same line-of-sight,
and that the gas covers the white dwarf only partially, as would be expected if
the gas, the transiting debris, and the dust emitting the infrared excess, are
part of the same general disc structure (although possibly at different radii).
In addition, we present the results of a week-long monitoring campaign of the
system.
",0,1,0,0,0,0
124,"Viscous dynamics of drops and bubbles in Hele-Shaw cells: drainage, drag friction, coalescence, and bursting","  In this review article, we discuss recent studies on drops and bubbles in
Hele-Shaw cells, focusing on how scaling laws exhibit crossovers from the
three-dimensional counterparts and focusing on topics in which viscosity plays
an important role. By virtue of progresses in analytical theory and high-speed
imaging, dynamics of drops and bubbles have actively been studied with the aid
of scaling arguments. However, compared with three dimensional problems,
studies on the corresponding problems in Hele-Shaw cells are still limited.
This review demonstrates that the effect of confinement in the Hele-Shaw cell
introduces new physics allowing different scaling regimes to appear. For this
purpose, we discuss various examples that are potentially important for
industrial applications handling drops and bubbles in confined spaces by
showing agreement between experiments and scaling theories. As a result, this
review provides a collection of problems in hydrodynamics that may be
analytically solved or that may be worth studying numerically in the near
future.
",0,1,0,0,0,0
125,Stacking-based Deep Neural Network: Deep Analytic Network on Convolutional Spectral Histogram Features,"  Stacking-based deep neural network (S-DNN), in general, denotes a deep neural
network (DNN) resemblance in terms of its very deep, feedforward network
architecture. The typical S-DNN aggregates a variable number of individually
learnable modules in series to assemble a DNN-alike alternative to the targeted
object recognition tasks. This work likewise devises an S-DNN instantiation,
dubbed deep analytic network (DAN), on top of the spectral histogram (SH)
features. The DAN learning principle relies on ridge regression, and some key
DNN constituents, specifically, rectified linear unit, fine-tuning, and
normalization. The DAN aptitude is scrutinized on three repositories of varying
domains, including FERET (faces), MNIST (handwritten digits), and CIFAR10
(natural objects). The empirical results unveil that DAN escalates the SH
baseline performance over a sufficiently deep layer.
",1,0,0,0,0,0
126,Superconductivity and Frozen Electronic States at the (111) LaAlO$_3$/SrTiO$_3$ Interface,"  In spite of Anderson's theorem, disorder is known to affect superconductivity
in conventional s-wave superconductors. In most superconductors, the degree of
disorder is fixed during sample preparation. Here we report measurements of the
superconducting properties of the two-dimensional gas that forms at the
interface between LaAlO$_3$ (LAO) and SrTiO$_3$ (STO) in the (111) crystal
orientation, a system that permits \emph{in situ} tuning of carrier density and
disorder by means of a back gate voltage $V_g$. Like the (001) oriented LAO/STO
interface, superconductivity at the (111) LAO/STO interface can be tuned by
$V_g$. In contrast to the (001) interface, superconductivity in these (111)
samples is anisotropic, being different along different interface crystal
directions, consistent with the strong anisotropy already observed other
transport properties at the (111) LAO/STO interface. In addition, we find that
the (111) interface samples ""remember"" the backgate voltage $V_F$ at which they
are cooled at temperatures near the superconducting transition temperature
$T_c$, even if $V_g$ is subsequently changed at lower temperatures. The low
energy scale and other characteristics of this memory effect ($<1$ K)
distinguish it from charge-trapping effects previously observed in (001)
interface samples.
",0,1,0,0,0,0
127,Emittance preservation of an electron beam in a loaded quasi-linear plasma wakefield,"  We investigate beam loading and emittance preservation for a high-charge
electron beam being accelerated in quasi-linear plasma wakefields driven by a
short proton beam. The structure of the studied wakefields are similar to those
of a long, modulated proton beam, such as the AWAKE proton driver. We show that
by properly choosing the electron beam parameters and exploiting two well known
effects, beam loading of the wakefield and full blow out of plasma electrons by
the accelerated beam, the electron beam can gain large amounts of energy with a
narrow final energy spread (%-level) and without significant emittance growth.
",0,1,0,0,0,0
128,Detection of Nonlinearly Distorted OFDM Signals via Generalized Approximate Message Passing,"  In this paper, we propose a practical receiver for multicarrier signals
subjected to a strong memoryless nonlinearity. The receiver design is based on
a generalized approximate message passing (GAMP) framework, and this allows
real-time algorithm implementation in software or hardware with moderate
complexity. We demonstrate that the proposed receiver can provide more than a
2dB gain compared with an ideal uncoded linear OFDM transmission at a BER range
$10^{-4}\div10^{-6}$ in the AWGN channel, when the OFDM signal is subjected to
clipping nonlinearity and the crest-factor of the clipped waveform is only
1.9dB. Simulation results also demonstrate that the proposed receiver provides
significant performance gain in frequency-selective multipath channels
",1,0,0,0,0,0
129,Nonlinear fractal meaning of the Hubble constant,"  According to astrophysical observations value of recession velocity in a
certain point is proportional to a distance to this point. The proportionality
coefficient is the Hubble constant measured with 5% accuracy. It is used in
many cosmological theories describing dark energy, dark matter, baryons, and
their relation with the cosmological constant introduced by Einstein.
In the present work we have determined a limit value of the global Hubble
constant (in a big distance from a point of observations) theoretically without
using any empirical constants on the base of our own fractal model used for the
description a relation between distance to an observed galaxy and coordinate of
its center. The distance has been defined as a nonlinear fractal measure with
scale of measurement corresponding to a deviation of the measure from its fixed
value (zero-gravity radius). We have suggested a model of specific anisotropic
fractal for simulation a radial Universe expansion. Our theoretical results
have shown existence of an inverse proportionality between accuracy of
determination the Hubble constant and accuracy of calculation a coordinates of
galaxies leading to ambiguity results obtained at cosmological observations.
",0,1,0,0,0,0
130,SEA: String Executability Analysis by Abstract Interpretation,"  Dynamic languages often employ reflection primitives to turn dynamically
generated text into executable code at run-time. These features make standard
static analysis extremely hard if not impossible because its essential data
structures, i.e., the control-flow graph and the system of recursive equations
associated with the program to analyse, are themselves dynamically mutating
objects. We introduce SEA, an abstract interpreter for automatic sound string
executability analysis of dynamic languages employing bounded (i.e, finitely
nested) reflection and dynamic code generation. Strings are statically
approximated in an abstract domain of finite state automata with basic
operations implemented as symbolic transducers. SEA combines standard program
analysis together with string executability analysis. The analysis of a call to
reflection determines a call to the same abstract interpreter over a code which
is synthesised directly from the result of the static string executability
analysis at that program point. The use of regular languages for approximating
dynamically generated code structures allows SEA to soundly approximate safety
properties of self modifying programs yet maintaining efficiency. Soundness
here means that the semantics of the code synthesised by the analyser to
resolve reflection over-approximates the semantics of the code dynamically
built at run-rime by the program at that point.
",1,0,0,0,0,0
131,On the trade-off between labels and weights in quantitative bisimulation,"  Reductions for transition systems have been recently introduced as a uniform
and principled method for comparing the expressiveness of system models with
respect to a range of properties, especially bisimulations. In this paper we
study the expressiveness (w.r.t. bisimulations) of models for quantitative
computations such as weighted labelled transition systems (WLTSs), uniform
labelled transition systems (ULTraSs), and state-to-function transition systems
(FuTSs). We prove that there is a trade-off between labels and weights: at one
extreme lays the class of (unlabelled) weighted transition systems where
information is presented using weights only; at the other lays the class of
labelled transition systems (LTSs) where information is shifted on labels.
These categories of systems cannot be further reduced in any significant way
and subsume all the aforementioned models.
",1,0,0,0,0,0
132,Poynting's theorem in magnetic turbulence,"  Poynting's theorem is used to obtain an expression for the turbulent
power-spectral density as function of frequency and wavenumber in low-frequency
magnetic turbulence. No reference is made to Elsasser variables as is usually
done in magnetohydrodynamic turbulence mixing mechanical and electromagnetic
turbulence. We rather stay with an implicit form of the mechanical part of
turbulence as suggested by electromagnetic theory in arbitrary media. All of
mechanics and flows is included into a turbulent response function which by
appropriate observations can be determined from knowledge of the turbulent
fluctuation spectra. This approach is not guided by the wish of developing a
complete theory of turbulence. It aims on the identification of the response
function from observations as input into a theory which afterwards attempts its
interpretation. Combination of both the magnetic and electric power spectral
densities leads to a representation of the turbulent response function, i.e.
the turbulent conductivity spectrum $\sigma_{\omega k}$ as function of
frequency $\omega$ and wavenumber $k$. {It is given as the ratio of magnetic to
electric power spectral densities in frequency space. This knowledge allows for
formally writing down a turbulent dispersion relation. Power law inertial range
spectra result in a power law turbulent conductivity spectrum. These can be
compared with observations in the solar wind. Keywords: MHD turbulence,
turbulent dispersion relation, turbulent response function, solar wind
turbulence
",0,1,0,0,0,0
133,Polar factorization of conformal and projective maps of the sphere in the sense of optimal mass transport,"  Let M be a compact Riemannian manifold and let $\mu$,d be the associated
measure and distance on M. Robert McCann obtained, generalizing results for the
Euclidean case by Yann Brenier, the polar factorization of Borel maps S : M ->
M pushing forward $\mu$ to a measure $\nu$: each S factors uniquely a.e. into
the composition S = T \circ U, where U : M -> M is volume preserving and T : M
-> M is the optimal map transporting $\mu$ to $\nu$ with respect to the cost
function d^2/2.
In this article we study the polar factorization of conformal and projective
maps of the sphere S^n. For conformal maps, which may be identified with
elements of the identity component of O(1,n+1), we prove that the polar
factorization in the sense of optimal mass transport coincides with the
algebraic polar factorization (Cartan decomposition) of this Lie group. For the
projective case, where the group GL_+(n+1) is involved, we find necessary and
sufficient conditions for these two factorizations to agree.
",0,0,1,0,0,0
134,Representing numbers as the sum of squares and powers in the ring $\mathbb{Z}_n$,"  We examine the representation of numbers as the sum of two squares in
$\mathbb{Z}_n$ for a general positive integer $n$. Using this information we
make some comments about the density of positive integers which can be
represented as the sum of two squares and powers of $2$ in $\mathbb{N}$.
",0,0,1,0,0,0
135,Spatial Regression and the Bayesian Filter,"  Regression for spatially dependent outcomes poses many challenges, for
inference and for computation. Non-spatial models and traditional spatial
mixed-effects models each have their advantages and disadvantages, making it
difficult for practitioners to determine how to carry out a spatial regression
analysis. We discuss the data-generating mechanisms implicitly assumed by
various popular spatial regression models, and discuss the implications of
these assumptions. We propose Bayesian spatial filtering as an approximate
middle way between non-spatial models and traditional spatial mixed models. We
show by simulation that our Bayesian spatial filtering model has several
desirable properties and hence may be a useful addition to a spatial
statistician's toolkit.
",0,0,0,1,0,0
136,Behaviour of electron content in the ionospheric D-region during solar X-ray flares,"  One of the most important parameters in ionospheric plasma research also
having a wide practical application in wireless satellite telecommunications is
the total electron content (TEC) representing the columnal electron number
density. The F region with high electron density provides the biggest
contribution to TEC while the relatively weakly ionized plasma of the D region
(60 km - 90 km above Earths surface) is often considered as a negligible cause
of satellite signal disturbances. However, sudden intensive ionization
processes like those induced by solar X ray flares can cause relative increases
of electron density that are significantly larger in the D-region than in
regions at higher altitudes. Therefore, one cannot exclude a priori the D
region from investigations of ionospheric influences on propagation of
electromagnetic signals emitted by satellites. We discuss here this problem
which has not been sufficiently treated in literature so far. The obtained
results are based on data collected from the D region monitoring by very low
frequency radio waves and on vertical TEC calculations from the Global
Navigation Satellite System (GNSS) signal analyses, and they show noticeable
variations in the D region electron content (TECD) during activity of a solar X
ray flare (it rises by a factor of 136 in the considered case) when TECD
contribution to TEC can reach several percent and which cannot be neglected in
practical applications like global positioning procedures by satellites.
",0,1,0,0,0,0
137,Fractional compound Poisson processes with multiple internal states,"  For the particles undergoing the anomalous diffusion with different waiting
time distributions for different internal states, we derive the Fokker-Planck
and Feymann-Kac equations, respectively, describing positions of the particles
and functional distributions of the trajectories of particles; in particular,
the equations governing the functional distribution of internal states are also
obtained. The dynamics of the stochastic processes are analyzed and the
applications, calculating the distribution of the first passage time and the
distribution of the fraction of the occupation time, of the equations are
given.
",0,0,1,1,0,0
138,Zero-point spin-fluctuations of single adatoms,"  Stabilizing the magnetic signal of single adatoms is a crucial step towards
their successful usage in widespread technological applications such as
high-density magnetic data storage devices. The quantum mechanical nature of
these tiny objects, however, introduces intrinsic zero-point spin-fluctuations
that tend to destabilize the local magnetic moment of interest by dwindling the
magnetic anisotropy potential barrier even at absolute zero temperature. Here,
we elucidate the origins and quantify the effect of the fundamental ingredients
determining the magnitude of the fluctuations, namely the ($i$) local magnetic
moment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner
excitations. Based on a systematic first-principles study of 3d and 4d adatoms,
we demonstrate that the transverse contribution of the fluctuations is
comparable in size to the magnetic moment itself, leading to a remarkable
$\gtrsim$50$\%$ reduction of the magnetic anisotropy energy. Our analysis gives
rise to a comprehensible diagram relating the fluctuation magnitude to
characteristic features of adatoms, providing practical guidelines for
designing magnetically stable nanomagnets with minimal quantum fluctuations.
",0,1,0,0,0,0
139,Exploration-exploitation tradeoffs dictate the optimal distributions of phenotypes for populations subject to fitness fluctuations,"  We study a minimal model for the growth of a phenotypically heterogeneous
population of cells subject to a fluctuating environment in which they can
replicate (by exploiting available resources) and modify their phenotype within
a given landscape (thereby exploring novel configurations). The model displays
an exploration-exploitation trade-off whose specifics depend on the statistics
of the environment. Most notably, the phenotypic distribution corresponding to
maximum population fitness (i.e. growth rate) requires a non-zero exploration
rate when the magnitude of environmental fluctuations changes randomly over
time, while a purely exploitative strategy turns out to be optimal in two-state
environments, independently of the statistics of switching times. We obtain
analytical insight into the limiting cases of very fast and very slow
exploration rates by directly linking population growth to the features of the
environment.
",0,0,0,0,1,0
140,Evaluating openEHR for storing computable representations of electronic health record phenotyping algorithms,"  Electronic Health Records (EHR) are data generated during routine clinical
care. EHR offer researchers unprecedented phenotypic breadth and depth and have
the potential to accelerate the pace of precision medicine at scale. A main EHR
use-case is creating phenotyping algorithms to define disease status, onset and
severity. Currently, no common machine-readable standard exists for defining
phenotyping algorithms which often are stored in human-readable formats. As a
result, the translation of algorithms to implementation code is challenging and
sharing across the scientific community is problematic. In this paper, we
evaluate openEHR, a formal EHR data specification, for computable
representations of EHR phenotyping algorithms.
",1,0,0,0,0,0
141,Optimizing Mission Critical Data Dissemination in Massive IoT Networks,"  Mission critical data dissemination in massive Internet of things (IoT)
networks imposes constraints on the message transfer delay between devices. Due
to low power and communication range of IoT devices, data is foreseen to be
relayed over multiple device-to-device (D2D) links before reaching the
destination. The coexistence of a massive number of IoT devices poses a
challenge in maximizing the successful transmission capacity of the overall
network alongside reducing the multi-hop transmission delay in order to support
mission critical applications. There is a delicate interplay between the
carrier sensing threshold of the contention based medium access protocol and
the choice of packet forwarding strategy selected at each hop by the devices.
The fundamental problem in optimizing the performance of such networks is to
balance the tradeoff between conflicting performance objectives such as the
spatial frequency reuse, transmission quality, and packet progress towards the
destination. In this paper, we use a stochastic geometry approach to quantify
the performance of multi-hop massive IoT networks in terms of the spatial
frequency reuse and the transmission quality under different packet forwarding
schemes. We also develop a comprehensive performance metric that can be used to
optimize the system to achieve the best performance. The results can be used to
select the best forwarding scheme and tune the carrier sensing threshold to
optimize the performance of the network according to the delay constraints and
transmission quality requirements.
",1,0,0,0,0,0
142,Interference of two co-directional exclusion processes in the presence of a static bottleneck: a biologically motivated model,"  We develope a two-species exclusion process with a distinct pair of entry and
exit sites for each species of rigid rods. The relatively slower forward
stepping of the rods in an extended bottleneck region, located in between the
two entry sites, controls the extent of interference of the co-directional flow
of the two species of rods. The relative positions of the sites of entry of the
two species of rods with respect to the location of the bottleneck are
motivated by a biological phenomenon. However, the primary focus of the study
here is to explore the effects of the interference of the flow of the two
species of rods on their spatio-temporal organization and the regulations of
this interference by the extended bottleneck. By a combination of mean-field
theory and computer simulation we calculate the flux of both species of rods
and their density profiles as well as the composite phase diagrams of the
system. If the bottleneck is sufficiently stringent some of the phases become
practically unrealizable although not ruled out on the basis of any fundamental
physical principle. Moreover the extent of suppression of flow of the
downstream entrants by the flow of the upstream entrants can also be regulated
by the strength of the bottleneck. We speculate on the possible implications of
the results in the context of the biological phenomenon that motivated the
formulation of the theoretical model.
",0,1,0,0,0,0
143,Gaussian fluctuations of Jack-deformed random Young diagrams,"  We introduce a large class of random Young diagrams which can be regarded as
a natural one-parameter deformation of some classical Young diagram ensembles;
a deformation which is related to Jack polynomials and Jack characters. We show
that each such a random Young diagram converges asymptotically to some limit
shape and that the fluctuations around the limit are asymptotically Gaussian.
",0,0,1,0,0,0
144,Revisiting (logarithmic) scaling relations using renormalization group,"  We explicitly compute the critical exponents associated with logarithmic
corrections (the so-called hatted exponents) starting from the renormalization
group equations and the mean field behavior for a wide class of models at the
upper critical behavior (for short and long range $\phi^n$-theories) and below
it. This allows us to check the scaling relations among these critical
exponents obtained by analysing the complex singularities (Lee-Yang and Fisher
zeroes) of these models. Moreover, we have obtained an explicit method to
compute the $\hat{\coppa}$ exponent [defined by $\xi\sim L (\log
L)^{\hat{\coppa}}$] and, finally, we have found a new derivation of the scaling
law associated with it.
",0,1,0,0,0,0
145,Concentration of weakly dependent Banach-valued sums and applications to statistical learning methods,"  We obtain a Bernstein-type inequality for sums of Banach-valued random
variables satisfying a weak dependence assumption of general type and under
certain smoothness assumptions of the underlying Banach norm. We use this
inequality in order to investigate in the asymptotical regime the error upper
bounds for the broad family of spectral regularization methods for reproducing
kernel decision rules, when trained on a sample coming from a $\tau-$mixing
process.
",0,0,1,1,0,0
146,Evolution of the Kondo lattice electronic structure above the transport coherence temperature,"  The temperature-dependent evolution of the Kondo lattice is a long-standing
topic of theoretical and experimental investigation and yet it lacks a truly
microscopic description of the relation of the basic $f$-$d$ hybridization
processes to the fundamental temperature scales of Kondo screening and
Fermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$
hybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo
lattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved
photoemission (ARPES) with sufficient detail to allow direct comparison to
first principles dynamical mean field theory (DMFT) calculations containing
full realism of crystalline electric field states. The ARPES results, for two
orthogonal (001) and (100) cleaved surfaces and three different $f$-$d$
hybridization scenarios, with additional microscopic insight provided by DMFT,
reveal $f$ participation in the Fermi surface at temperatures much higher than
the lattice coherence temperature, $T^*\approx$ 45 K, commonly believed to be
the onset for such behavior. The identification of a $T$-dependent crystalline
electric field degeneracy crossover in the DMFT theory $below$ $T^*$ is
specifically highlighted.
",0,1,0,0,0,0
147,On A Conjecture Regarding Permutations Which Destroy Arithmetic Progressions,"  Hegarty conjectured for $n\neq 2, 3, 5, 7$ that $\mathbb{Z}/n\mathbb{Z}$ has
a permutation which destroys all arithmetic progressions mod $n$. For $n\ge
n_0$, Hegarty and Martinsson demonstrated that $\mathbb{Z}/n\mathbb{Z}$ has an
arithmetic-progression destroying permutation. However $n_0\approx 1.4\times
10^{14}$ and thus resolving the conjecture in full remained out of reach of any
computational techniques. However, this paper using constructions modeled after
those used by Elkies and Swaminathan for the case of $\mathbb{Z}/p\mathbb{Z}$
with $p$ being prime, establish the conjecture in full. Furthermore our results
do not rely on the fact that it suffices to study when $n<n_0$ and thus our
results completely independent of the proof given by Hegarty and Martinsson.
",0,0,1,0,0,0
148,Inverse monoids and immersions of cell complexes,"  An immersion $f : {\mathcal D} \rightarrow \mathcal C$ between cell complexes
is a local homeomorphism onto its image that commutes with the characteristic
maps of the cell complexes. We study immersions between finite-dimensional
connected $\Delta$-complexes by replacing the fundamental group of the base
space by an appropriate inverse monoid. We show how conjugacy classes of the
closed inverse submonoids of this inverse monoid may be used to classify
connected immersions into the complex. This extends earlier results of Margolis
and Meakin for immersions between graphs and of Meakin and Szak?­cs on
immersions into $2$-dimensional $CW$-complexes.
",0,0,1,0,0,0
149,Not even wrong: The spurious link between biodiversity and ecosystem functioning,"  Resolving the relationship between biodiversity and ecosystem functioning has
been one of the central goals of modern ecology. Early debates about the
relationship were finally resolved with the advent of a statistical
partitioning scheme that decomposed the biodiversity effect into a ""selection""
effect and a ""complementarity"" effect. We prove that both the biodiversity
effect and its statistical decomposition into selection and complementarity are
fundamentally flawed because these methods use a na??ve null expectation based
on neutrality, likely leading to an overestimate of the net biodiversity
effect, and they fail to account for the nonlinear abundance-ecosystem
functioning relationships observed in nature. Furthermore, under such
nonlinearity no statistical scheme can be devised to partition the biodiversity
effects. We also present an alternative metric providing a more reasonable
estimate of biodiversity effect. Our results suggest that all studies conducted
since the early 1990s likely overestimated the positive effects of biodiversity
on ecosystem functioning.
",0,0,0,0,1,0
150,Evidence of Fraud in Brazil's Electoral Campaigns Via the Benford's Law,"  The principle of democracy is that the people govern through elected
representatives. Therefore, a democracy is healthy as long as the elected
politicians do represent the people. We have analyzed data from the Brazilian
electoral court (Tribunal Superior Eleitoral, TSE) concerning money donations
for the electoral campaigns and the election results. Our work points to two
disturbing conclusions: money is a determining factor on whether a candidate is
elected or not (as opposed to representativeness); secondly, the use of
Benford's Law to analyze the declared donations received by the parties and
electoral campaigns shows evidence of fraud in the declarations. A better term
to define Brazil's government system is what we define as chrimatocracy (govern
by money).
",1,0,0,1,0,0
151,A Berkeley View of Systems Challenges for AI,"  With the increasing commoditization of computer vision, speech recognition
and machine translation systems and the widespread deployment of learning-based
back-end technologies such as digital advertising and intelligent
infrastructures, AI (Artificial Intelligence) has moved from research labs to
production. These changes have been made possible by unprecedented levels of
data and computation, by methodological advances in machine learning, by
innovations in systems software and architectures, and by the broad
accessibility of these technologies.
The next generation of AI systems promises to accelerate these developments
and increasingly impact our lives via frequent interactions and making (often
mission-critical) decisions on our behalf, often in highly personalized
contexts. Realizing this promise, however, raises daunting challenges. In
particular, we need AI systems that make timely and safe decisions in
unpredictable environments, that are robust against sophisticated adversaries,
and that can process ever increasing amounts of data across organizations and
individuals without compromising confidentiality. These challenges will be
exacerbated by the end of the Moore's Law, which will constrain the amount of
data these technologies can store and process. In this paper, we propose
several open research directions in systems, architectures, and security that
can address these challenges and help unlock AI's potential to improve lives
and society.
",1,0,0,0,0,0
152,"Equivariant infinite loop space theory, I. The space level story","  We rework and generalize equivariant infinite loop space theory, which shows
how to construct G-spectra from G-spaces with suitable structure. There is a
naive version which gives naive G-spectra for any topological group G, but our
focus is on the construction of genuine G-spectra when G is finite.
We give new information about the Segal and operadic equivariant infinite
loop space machines, supplying many details that are missing from the
literature, and we prove by direct comparison that the two machines give
equivalent output when fed equivalent input. The proof of the corresponding
nonequivariant uniqueness theorem, due to May and Thomason, works for naive
G-spectra for general G but fails hopelessly for genuine G-spectra when G is
finite. Even in the nonequivariant case, our comparison theorem is considerably
more precise, giving a direct point-set level comparison.
We have taken the opportunity to update this general area, equivariant and
nonequivariant, giving many new proofs, filling in some gaps, and giving some
corrections to results in the literature.
",0,0,1,0,0,0
153,Arithmetic purity of strong approximation for homogeneous spaces,"  We prove that any open subset $U$ of a semi-simple simply connected
quasi-split linear algebraic group $G$ with ${codim} (G\setminus U, G)\geq 2$
over a number field satisfies strong approximation by establishing a fibration
of $G$ over a toric variety. We also prove a similar result of strong
approximation with Brauer-Manin obstruction for a partial equivariant smooth
compactification of a homogeneous space where all invertible functions are
constant and the semi-simple part of the linear algebraic group is quasi-split.
Some semi-abelian varieties of any given dimension where the complements of a
rational point do not satisfy strong approximation with Brauer-Manin
obstruction are given.
",0,0,1,0,0,0
154,Flatness results for nonlocal minimal cones and subgraphs,"  We show that nonlocal minimal cones which are non-singular subgraphs outside
the origin are necessarily halfspaces.
The proof is based on classical ideas of~\cite{DG1} and on the computation of
the linearized nonlocal mean curvature operator, which is proved to satisfy a
suitable maximum principle.
With this, we obtain new, and somehow simpler, proofs of the Bernstein-type
results for nonlocal minimal surfaces which have been recently established
in~\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type
result which classifies Lipschitz nonlocal minimal subgraphs outside a ball.
",0,0,1,0,0,0
155,Effective Asymptotic Formulae for Multilinear Averages of Multiplicative Functions,"  Let $f_1,\ldots,f_k : \mathbb{N} \rightarrow \mathbb{C}$ be multiplicative
functions taking values in the closed unit disc. Using an analytic approach in
the spirit of Hal?­sz' mean value theorem, we compute multidimensional
averages of the shape $$x^{-l} \sum_{\mathbf{n} \in [x]^l} \prod_{1 \leq j \leq
k} f_j(L_j(\mathbf{n}))$$ as $x \rightarrow \infty$, where $[x] := [1,x]$ and
$L_1,\ldots, L_k$ are affine linear forms that satisfy some natural conditions.
Our approach gives a new proof of a result of Frantzikinakis and Host that is
distinct from theirs, with \emph{explicit} main and error terms. \\ As an
application of our formulae, we establish a \emph{local-to-global} principle
for Gowers norms of multiplicative functions. We also compute the asymptotic
densities of the sets of integers $n$ such that a given multiplicative function
$f: \mathbb{N} \rightarrow \{-1, 1\}$ yields a fixed sign pattern of length 3
or 4 on almost all 3- and 4-term arithmetic progressions, respectively, with
first term $n$.
",0,0,1,0,0,0
156,On the apparent permeability of porous media in rarefied gas flows,"  The apparent gas permeability of the porous medium is an important parameter
in the prediction of unconventional gas production, which was first
investigated systematically by Klinkenberg in 1941 and found to increase with
the reciprocal mean gas pressure (or equivalently, the Knudsen number).
Although the underlying rarefaction effects are well-known, the reason that the
correction factor in Klinkenberg's famous equation decreases when the Knudsen
number increases has not been fully understood. Most of the studies idealize
the porous medium as a bundle of straight cylindrical tubes, however, according
to the gas kinetic theory, this only results in an increase of the correction
factor with the Knudsen number, which clearly contradicts Klinkenberg's
experimental observations. Here, by solving the Bhatnagar-Gross-Krook equation
in simplified (but not simple) porous media, we identify, for the first time,
two key factors that can explain Klinkenberg's experimental results: the
tortuous flow path and the non-unitary tangential momentum accommodation
coefficient for the gas-surface interaction. Moreover, we find that
Klinkenberg's results can only be observed when the ratio between the apparent
and intrinsic permeabilities is $\lesssim30$; at large ratios (or Knudsen
numbers) the correction factor increases with the Knudsen number. Our numerical
results could also serve as benchmarking cases to assess the accuracy of
macroscopic models and/or numerical schemes for the modeling/simulation of
rarefied gas flows in complex geometries over a wide range of gas rarefaction.
",0,1,0,0,0,0
157,Small subgraphs and their extensions in a random distance graph,"  In previous papers, threshold probabilities for the properties of a random
distance graph to contain strictly balanced graphs were found. We extend this
result to arbitrary graphs and prove that the number of copies of a strictly
balanced graph has asymptotically Poisson distribution at the threshold.
",0,0,1,0,0,0
158,Increasing the Reusability of Enforcers with Lifecycle Events,"  Runtime enforcement can be effectively used to improve the reliability of
software applications. However, it often requires the definition of ad hoc
policies and enforcement strategies, which might be expensive to identify and
implement. This paper discusses how to exploit lifecycle events to obtain
useful enforcement strategies that can be easily reused across applications,
thus reducing the cost of adoption of the runtime enforcement technology. The
paper finally sketches how this idea can be used to define libraries that can
automatically overcome problems related to applications misusing them.
",1,0,0,0,0,0
159,A Fast Interior Point Method for Atomic Norm Soft Thresholding,"  The atomic norm provides a generalization of the $\ell_1$-norm to continuous
parameter spaces. When applied as a sparse regularizer for line spectral
estimation the solution can be obtained by solving a convex optimization
problem. This problem is known as atomic norm soft thresholding (AST). It can
be cast as a semidefinite program and solved by standard methods. In the
semidefinite formulation there are $O(N^2)$ dual variables and a standard
primal-dual interior point method requires at least $O(N^6)$ flops per
iteration. That has lead researcher to consider alternating direction method of
multipliers (ADMM) for the solution of AST, but this method is still somewhat
slow for large problem sizes. To obtain a faster algorithm we reformulate AST
as a non-symmetric conic program. That has two properties of key importance to
its numerical solution: the conic formulation has only $O(N)$ dual variables
and the Toeplitz structure inherent to AST is preserved. Based on it we derive
FastAST which is a primal-dual interior point method for solving AST. Two
variants are considered with the fastest one requiring only $O(N^2)$ flops per
iteration. Extensive numerical experiments demonstrate that FastAST solves AST
significantly faster than a state-of-the-art solver based on ADMM.
",1,0,0,0,0,0
160,Optimal Experiment Design for Causal Discovery from Fixed Number of Experiments,"  We study the problem of causal structure learning over a set of random
variables when the experimenter is allowed to perform at most $M$ experiments
in a non-adaptive manner. We consider the optimal learning strategy in terms of
minimizing the portions of the structure that remains unknown given the limited
number of experiments in both Bayesian and minimax setting. We characterize the
theoretical optimal solution and propose an algorithm, which designs the
experiments efficiently in terms of time complexity. We show that for bounded
degree graphs, in the minimax case and in the Bayesian case with uniform
priors, our proposed algorithm is a $\rho$-approximation algorithm, where
$\rho$ is independent of the order of the underlying graph. Simulations on both
synthetic and real data show that the performance of our algorithm is very
close to the optimal solution.
",1,0,0,1,0,0
161,Economically Efficient Combined Plant and Controller Design Using Batch Bayesian Optimization: Mathematical Framework and Airborne Wind Energy Case Study,"  We present a novel data-driven nested optimization framework that addresses
the problem of coupling between plant and controller optimization. This
optimization strategy is tailored towards instances where a closed-form
expression for the system dynamic response is unobtainable and simulations or
experiments are necessary. Specifically, Bayesian Optimization, which is a
data-driven technique for finding the optimum of an unknown and
expensive-to-evaluate objective function, is employed to solve a nested
optimization problem. The underlying objective function is modeled by a
Gaussian Process (GP); then, Bayesian Optimization utilizes the predictive
uncertainty information from the GP to determine the best subsequent control or
plant parameters. The proposed framework differs from the majority of co-design
literature where there exists a closed-form model of the system dynamics.
Furthermore, we utilize the idea of Batch Bayesian Optimization at the plant
optimization level to generate a set of plant designs at each iteration of the
overall optimization process, recognizing that there will exist economies of
scale in running multiple experiments in each iteration of the plant design
process. We validate the proposed framework for a Buoyant Airborne Turbine
(BAT). We choose the horizontal stabilizer area, longitudinal center of mass
relative to center of buoyancy (plant parameters), and the pitch angle
set-point (controller parameter) as our decision variables. Our results
demonstrate that these plant and control parameters converge to their
respective optimal values within only a few iterations.
",1,0,0,0,0,0
162,The 10 phases of spin chains with two Ising symmetries,"  We explore the topological properties of quantum spin-1/2 chains with two
Ising symmetries. This class of models does not possess any of the symmetries
that are required to protect the Haldane phase. Nevertheless, we show that
there are 4 symmetry-protected topological phases, in addition to 6 phases that
spontaneously break one or both Ising symmetries. By mapping the model to
one-dimensional interacting fermions with particle-hole and time-reversal
symmetry, we obtain integrable parent Hamiltonians for the conventional and
topological phases of the spin model. We use these Hamiltonians to characterize
the physical properties of all 10 phases, identify their local and nonlocal
order parameters, and understand the effects of weak perturbations that respect
the Ising symmetries. Our study provides the first explicit example of a class
of spin chains with several topologically non-trivial phases, and binds
together the topological classifications of interacting bosons and fermions.
",0,1,0,0,0,0
163,Generalized subspace subcodes with application in cryptology,"  Most of the codes that have an algebraic decoding algorithm are derived from
the Reed Solomon codes. They are obtained by taking equivalent codes, for
example the generalized Reed Solomon codes, or by using the so-called subfield
subcode method, which leads to Alternant codes and Goppa codes over the
underlying prime field, or over some intermediate subfield. The main advantages
of these constructions is to preserve both the minimum distance and the
decoding algorithm of the underlying Reed Solomon code. In this paper, we
propose a generalization of the subfield subcode construction by introducing
the notion of subspace subcodes and a generalization of the equivalence of
codes which leads to the notion of generalized subspace subcodes. When the
dimension of the selected subspaces is equal to one, we show that our approach
gives exactly the family of the codes obtained by equivalence and subfield
subcode technique. However, our approach highlights the links between the
subfield subcode of a code defined over an extension field and the operation of
puncturing the $q$-ary image of this code. When the dimension of the subspaces
is greater than one, we obtain codes whose alphabet is no longer a finite
field, but a set of r-uples. We explain why these codes are practically as
efficient for applications as the codes defined on an extension of degree r. In
addition, they make it possible to obtain decodable codes over a large alphabet
having parameters previously inaccessible. As an application, we give some
examples that can be used in public key cryptosystems such as McEliece.
",1,0,0,0,0,0
164,Lagrangian fibers of Gelfand-Cetlin systems,"  Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of
Gelfand-Cetlin systems over complex partial flag manifolds, we provide a
complete description of the topology of Gelfand-Cetlin fibers. We prove that
all fibers are \emph{smooth} isotropic submanifolds and give a complete
description of the fiber to be Lagrangian in terms of combinatorics of
Gelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian
fibers. After a few combinatorial and numercal tests for the displaceability,
using the bulk-deformation of Floer cohomology by Schubert cycles, we prove
that every full flag manifold $\mathcal{F}(n)$ ($n \geq 3$) with a monotone
Kirillov-Kostant-Souriau symplectic form carries a continuum of
non-displaceable Lagrangian tori which degenerates to a non-torus fiber in the
Hausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\mathcal{F}(3)$
is non-displaceable the question of which was raised by Nohara-Ueda who
computed its Floer cohomology to be vanishing.
",0,0,1,0,0,0
165,A local ensemble transform Kalman particle filter for convective scale data assimilation,"  Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)
are a key component of probabilistic weather forecasting. They represent the
uncertainty in the initial conditions by an ensemble which incorporates
information coming from the physical model with the latest observations.
High-resolution numerical weather prediction models ran at operational centers
are able to resolve non-linear and non-Gaussian physical phenomena such as
convection. There is therefore a growing need to develop ensemble assimilation
algorithms able to deal with non-Gaussianity while staying computationally
feasible. In the present paper we address some of these needs by proposing a
new hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully
formulated in ensemble space and uses a deterministic scheme such that it has
the ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a
limiting case. A new criterion for choosing the proportion of particle filter
and ETKF update is also proposed. The new algorithm is implemented in the COSMO
framework and numerical experiments in a quasi-operational convective-scale
setup are conducted. The results show the feasibility of the new algorithm in
practice and indicate a strong potential for such local hybrid methods, in
particular for forecasting non-Gaussian variables such as wind and hourly
precipitation.
",0,1,0,1,0,0
166,Tensor Robust Principal Component Analysis with A New Tensor Nuclear Norm,"  In this paper, we consider the Tensor Robust Principal Component Analysis
(TRPCA) problem, which aims to exactly recover the low-rank and sparse
components from their sum. Our model is based on the recently proposed
tensor-tensor product (or t-product) [13]. Induced by the t-product, we first
rigorously deduce the tensor spectral norm, tensor nuclear norm, and tensor
average rank, and show that the tensor nuclear norm is the convex envelope of
the tensor average rank within the unit ball of the tensor spectral norm. These
definitions, their relationships and properties are consistent with matrix
cases. Equipped with the new tensor nuclear norm, we then solve the TRPCA
problem by solving a convex program and provide the theoretical guarantee for
the exact recovery. Our TRPCA model and recovery guarantee include matrix RPCA
as a special case. Numerical experiments verify our results, and the
applications to image recovery and background modeling problems demonstrate the
effectiveness of our method.
",0,0,0,1,0,0
167,Resolving the age bimodality of galaxy stellar populations on kpc scales,"  Galaxies in the local Universe are known to follow bimodal distributions in
the global stellar populations properties. We analyze the distribution of the
local average stellar-population ages of 654,053 sub-galactic regions resolved
on ~1-kpc scales in a volume-corrected sample of 394 galaxies, drawn from the
CALIFA-DR3 integral-field-spectroscopy survey and complemented by SDSS imaging.
We find a bimodal local-age distribution, with an old and a young peak
primarily due to regions in early-type galaxies and star-forming regions of
spirals, respectively. Within spiral galaxies, the older ages of bulges and
inter-arm regions relative to spiral arms support an internal age bimodality.
Although regions of higher stellar-mass surface-density, mu*, are typically
older, mu* alone does not determine the stellar population age and a bimodal
distribution is found at any fixed mu*. We identify an ""old ridge"" of regions
of age ~9 Gyr, independent of mu*, and a ""young sequence"" of regions with age
increasing with mu* from 1-1.5 Gyr to 4-5 Gyr. We interpret the former as
regions containing only old stars, and the latter as regions where the relative
contamination of old stellar populations by young stars decreases as mu*
increases. The reason why this bimodal age distribution is not inconsistent
with the unimodal shape of the cosmic-averaged star-formation history is that
i) the dominating contribution by young stars biases the age low with respect
to the average epoch of star formation, and ii) the use of a single average age
per region is unable to represent the full time-extent of the star-formation
history of ""young-sequence"" regions.
",0,1,0,0,0,0
168,Hidden long evolutionary memory in a model biochemical network,"  We introduce a minimal model for the evolution of functional
protein-interaction networks using a sequence-based mutational algorithm, and
apply the model to study neutral drift in networks that yield oscillatory
dynamics. Starting with a functional core module, random evolutionary drift
increases network complexity even in the absence of specific selective
pressures. Surprisingly, we uncover a hidden order in sequence space that gives
rise to long-term evolutionary memory, implying strong constraints on network
evolution due to the topology of accessible sequence space.
",0,1,0,0,0,0
169,On Study of the Reliable Fully Convolutional Networks with Tree Arranged Outputs (TAO-FCN) for Handwritten String Recognition,"  The handwritten string recognition is still a challengeable task, though the
powerful deep learning tools were introduced. In this paper, based on TAO-FCN,
we proposed an end-to-end system for handwritten string recognition. Compared
with the conventional methods, there is no preprocess nor manually designed
rules employed. With enough labelled data, it is easy to apply the proposed
method to different applications. Although the performance of the proposed
method may not be comparable with the state-of-the-art approaches, it's
usability and robustness are more meaningful for practical applications.
",1,0,0,0,0,0
170,Marcel Riesz on N??rlund Means,"  We note that the necessary and sufficient conditions established by Marcel
Riesz for the inclusion of regular N??rlund summation methods are in fact
applicable quite generally.
",0,0,1,0,0,0
171,Mathematics of Isogeny Based Cryptography,"  These lectures notes were written for a summer school on Mathematics for
post-quantum cryptography in Thi??s, Senegal. They try to provide a guide for
Masters' students to get through the vast literature on elliptic curves,
without getting lost on their way to learning isogeny based cryptography. They
are by no means a reference text on the theory of elliptic curves, nor on
cryptography; students are encouraged to complement these notes with some of
the books recommended in the bibliography.
The presentation is divided in three parts, roughly corresponding to the
three lectures given. In an effort to keep the reader interested, each part
alternates between the fundamental theory of elliptic curves, and applications
in cryptography. We often prefer to have the main ideas flow smoothly, rather
than having a rigorous presentation as one would have in a more classical book.
The reader will excuse us for the inaccuracies and the omissions.
",1,0,0,0,0,0
172,Modeling of drug diffusion in a solid tumor leading to tumor cell death,"  It has been shown recently that changing the fluidic properties of a drug can
improve its efficacy in ablating solid tumors. We develop a modeling framework
for tumor ablation, and present the simplest possible model for drug diffusion
in a spherical tumor with leaky boundaries and assuming cell death eventually
leads to ablation of that cell effectively making the two quantities
numerically equivalent. The death of a cell after a given exposure time depends
on both the concentration of the drug and the amount of oxygen available to the
cell. Higher oxygen availability leads to cell death at lower drug
concentrations. It can be assumed that a minimum concentration is required for
a cell to die, effectively connecting diffusion with efficacy. The
concentration threshold decreases as exposure time increases, which allows us
to compute dose-response curves. Furthermore, these curves can be plotted at
much finer time intervals compared to that of experiments, which is used to
produce a dose-threshold-response surface giving an observer a complete picture
of the drug's efficacy for an individual. In addition, since the diffusion,
leak coefficients, and the availability of oxygen is different for different
individuals and tumors, we produce artificial replication data through
bootstrapping to simulate error. While the usual data-driven model with
Sigmoidal curves use 12 free parameters, our mechanistic model only has two
free parameters, allowing it to be open to scrutiny rather than forcing
agreement with data. Even so, the simplest model in our framework, derived
here, shows close agreement with the bootstrapped curves, and reproduces well
established relations, such as Haber's rule.
",0,0,0,0,1,0
173,Sensitivity analysis for inverse probability weighting estimators via the percentile bootstrap,"  To identify the estimand in missing data problems and observational studies,
it is common to base the statistical estimation on the ""missing at random"" and
""no unmeasured confounder"" assumptions. However, these assumptions are
unverifiable using empirical data and pose serious threats to the validity of
the qualitative conclusions of the statistical inference. A sensitivity
analysis asks how the conclusions may change if the unverifiable assumptions
are violated to a certain degree. In this paper we consider a marginal
sensitivity model which is a natural extension of Rosenbaum's sensitivity model
that is widely used for matched observational studies. We aim to construct
confidence intervals based on inverse probability weighting estimators, such
that asymptotically the intervals have at least nominal coverage of the
estimand whenever the data generating distribution is in the collection of
marginal sensitivity models. We use a percentile bootstrap and a generalized
minimax/maximin inequality to transform this intractable problem to a linear
fractional programming problem, which can be solved very efficiently. We
illustrate our method using a real dataset to estimate the causal effect of
fish consumption on blood mercury level.
",0,0,1,1,0,0
174,From 4G to 5G: Self-organized Network Management meets Machine Learning,"  In this paper, we provide an analysis of self-organized network management,
with an end-to-end perspective of the network. Self-organization as applied to
cellular networks is usually referred to Self-organizing Networks (SONs), and
it is a key driver for improving Operations, Administration, and Maintenance
(OAM) activities. SON aims at reducing the cost of installation and management
of 4G and future 5G networks, by simplifying operational tasks through the
capability to configure, optimize and heal itself. To satisfy 5G network
management requirements, this autonomous management vision has to be extended
to the end to end network. In literature and also in some instances of products
available in the market, Machine Learning (ML) has been identified as the key
tool to implement autonomous adaptability and take advantage of experience when
making decisions. In this paper, we survey how network management can
significantly benefit from ML solutions. We review and provide the basic
concepts and taxonomy for SON, network management and ML. We analyse the
available state of the art in the literature, standardization, and in the
market. We pay special attention to 3rd Generation Partnership Project (3GPP)
evolution in the area of network management and to the data that can be
extracted from 3GPP networks, in order to gain knowledge and experience in how
the network is working, and improve network performance in a proactive way.
Finally, we go through the main challenges associated with this line of
research, in both 4G and in what 5G is getting designed, while identifying new
directions for research.
",1,0,0,0,0,0
175,Cyber Risk Analysis of Combined Data Attacks Against Power System State Estimation,"  Understanding smart grid cyber attacks is key for developing appropriate
protection and recovery measures. Advanced attacks pursue maximized impact at
minimized costs and detectability. This paper conducts risk analysis of
combined data integrity and availability attacks against the power system state
estimation. We compare the combined attacks with pure integrity attacks - false
data injection (FDI) attacks. A security index for vulnerability assessment to
these two kinds of attacks is proposed and formulated as a mixed integer linear
programming problem. We show that such combined attacks can succeed with fewer
resources than FDI attacks. The combined attacks with limited knowledge of the
system model also expose advantages in keeping stealth against the bad data
detection. Finally, the risk of combined attacks to reliable system operation
is evaluated using the results from vulnerability assessment and attack impact
analysis. The findings in this paper are validated and supported by a detailed
case study.
",1,0,0,0,0,0
176,A New Family of Near-metrics for Universal Similarity,"  We propose a family of near-metrics based on local graph diffusion to capture
similarity for a wide class of data sets. These quasi-metametrics, as their
names suggest, dispense with one or two standard axioms of metric spaces,
specifically distinguishability and symmetry, so that similarity between data
points of arbitrary type and form could be measured broadly and effectively.
The proposed near-metric family includes the forward k-step diffusion and its
reverse, typically on the graph consisting of data objects and their features.
By construction, this family of near-metrics is particularly appropriate for
categorical data, continuous data, and vector representations of images and
text extracted via deep learning approaches. We conduct extensive experiments
to evaluate the performance of this family of similarity measures and compare
and contrast with traditional measures of similarity used for each specific
application and with the ground truth when available. We show that for
structured data including categorical and continuous data, the near-metrics
corresponding to normalized forward k-step diffusion (k small) work as one of
the best performing similarity measures; for vector representations of text and
images including those extracted from deep learning, the near-metrics derived
from normalized and reverse k-step graph diffusion (k very small) exhibit
outstanding ability to distinguish data points from different classes.
",1,0,0,1,0,0
177,Poisoning Attacks to Graph-Based Recommender Systems,"  Recommender system is an important component of many web services to help
users locate items that match their interests. Several studies showed that
recommender systems are vulnerable to poisoning attacks, in which an attacker
injects fake data to a given system such that the system makes recommendations
as the attacker desires. However, these poisoning attacks are either agnostic
to recommendation algorithms or optimized to recommender systems that are not
graph-based. Like association-rule-based and matrix-factorization-based
recommender systems, graph-based recommender system is also deployed in
practice, e.g., eBay, Huawei App Store. However, how to design optimized
poisoning attacks for graph-based recommender systems is still an open problem.
In this work, we perform a systematic study on poisoning attacks to graph-based
recommender systems. Due to limited resources and to avoid detection, we assume
the number of fake users that can be injected into the system is bounded. The
key challenge is how to assign rating scores to the fake users such that the
target item is recommended to as many normal users as possible. To address the
challenge, we formulate the poisoning attacks as an optimization problem,
solving which determines the rating scores for the fake users. We also propose
techniques to solve the optimization problem. We evaluate our attacks and
compare them with existing attacks under white-box (recommendation algorithm
and its parameters are known), gray-box (recommendation algorithm is known but
its parameters are unknown), and black-box (recommendation algorithm is
unknown) settings using two real-world datasets. Our results show that our
attack is effective and outperforms existing attacks for graph-based
recommender systems. For instance, when 1% fake users are injected, our attack
can make a target item recommended to 580 times more normal users in certain
scenarios.
",0,0,0,1,0,0
178,SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models,"  This paper describes the Stockholm University/University of Groningen
(SU-RUG) system for the SIGMORPHON 2017 shared task on morphological
inflection. Our system is based on an attentional sequence-to-sequence neural
network model using Long Short-Term Memory (LSTM) cells, with joint training of
morphological inflection and the inverse transformation, i.e. lemmatization and
morphological analysis. Our system outperforms the baseline with a large
margin, and our submission ranks as the 4th best team for the track we
participate in (task 1, high-resource).
",1,0,0,0,0,0
179,"Neural system identification for large populations separating ""what"" and ""where""","  Neuroscientists classify neurons into different types that perform similar
computations at different locations in the visual field. Traditional methods
for neural system identification do not capitalize on this separation of 'what'
and 'where'. Learning deep convolutional feature spaces that are shared among
many neurons provides an exciting path forward, but the architectural design
needs to account for data limitations: While new experimental techniques enable
recordings from thousands of neurons, experimental time is limited so that one
can sample only a small fraction of each neuron's response space. Here, we show
that a major bottleneck for fitting convolutional neural networks (CNNs) to
neural data is the estimation of the individual receptive field locations, a
problem that has been scratched only at the surface thus far. We propose a CNN
architecture with a sparse readout layer factorizing the spatial (where) and
feature (what) dimensions. Our network scales well to thousands of neurons and
short recordings and can be trained end-to-end. We evaluate this architecture
on ground-truth data to explore the challenges and limitations of CNN-based
system identification. Moreover, we show that our network model outperforms
current state-of-the art system identification models of mouse primary visual
cortex.
",1,0,0,1,0,0
180,On the Deployment of Distributed Antennas for Wireless Power Transfer with Safety Electromagnetic Radiation Level Requirement,"  The extremely low efficiency is regarded as the bottleneck of Wireless Power
Transfer (WPT) technology. To tackle this problem, either enlarging the
transfer power or changing the infrastructure of WPT system could be an
intuitively proposed way. However, the drastically important issue on the user
exposure of electromagnetic radiation is rarely considered while we try to
improve the efficiency of WPT. In this paper, a Distributed Antenna Power
Beacon (DA-PB) based WPT system where these antennas are uniformly distributed
on a circle is analyzed and optimized with the safety electromagnetic radiation
level (SERL) requirement. In this model, three key questions are intended to be
answered: 1) With the SERL, what is the performance of the harvested power at
the users ? 2) How do we configure the parameters to maximize the efficiency of
WPT? 3) Under the same constraints, does the DA-PB still have performance gain
than the Co-located Antenna PB (CA-PB)? First, the minimum antenna height of
DA-PB is derived to make the radio frequency (RF) electromagnetic radiation
power density at any location of the charging cell lower than the SERL
published by the Federal Communications Commission (FCC). Second, the
closed-form expressions of average harvested Direct Current (DC) power per user
in the charging cell for pass-loss exponent 2 and 4 are also provided. In order
to maximize the average efficiency of WPT, the optimal radii for distributed
antennas elements (DAEs) are derived when the pass-loss exponent takes the
typical value $2$ and $4$. For comparison, the CA-PB is also analyzed as a
benchmark. Simulation results verify our derived theoretical results. And it is
shown that the proposed DA-PB indeed achieves larger average harvested DC power
than CA-PB and can improve the efficiency of WPT.
",1,0,0,0,0,0
181,A simulation technique for slurries interacting with moving parts and deformable solids with applications,"  A numerical method for particle-laden fluids interacting with a deformable
solid domain and mobile rigid parts is proposed and implemented in a full
engineering system. The fluid domain is modeled with a lattice Boltzmann
representation, the particles and rigid parts are modeled with a discrete
element representation, and the deformable solid domain is modeled using a
Lagrangian mesh. The main issue of this work, since separately each of these
methods is a mature tool, is to develop coupling and model-reduction approaches
in order to efficiently simulate coupled problems of this nature, as occur in
various geological and engineering applications. The lattice Boltzmann method
incorporates a large-eddy simulation technique using the Smagorinsky turbulence
model. The discrete element method incorporates spherical and polyhedral
particles for stiff contact interactions. A neo-Hookean hyperelastic model is
used for the deformable solid. We provide a detailed description of how to
couple the three solvers within a unified algorithm. The technique we propose
for rubber modeling/coupling exploits a simplification that prevents having to
solve a finite-element problem each time step. We also develop a technique to
reduce the domain size of the full system by replacing certain zones with
quasi-analytic solutions, which act as effective boundary conditions for the
lattice Boltzmann method. The major ingredients of the routine are are
separately validated. To demonstrate the coupled method in full, we simulate
slurry flows in two kinds of piston-valve geometries. The dynamics of the valve
and slurry are studied and reported over a large range of input parameters.
",1,0,0,0,0,0
182,Dissipative hydrodynamics in superspace,"  We construct a Schwinger-Keldysh effective field theory for relativistic
hydrodynamics for charged matter in a thermal background using a superspace
formalism. Superspace allows us to efficiently impose the symmetries of the
problem and to obtain a simple expression for the effective action. We show
that the theory we obtain is compatible with the Kubo-Martin-Schwinger
condition, which in turn implies that Green's functions obey the
fluctuation-dissipation theorem. Our approach complements and extends existing
formulations found in the literature.
",0,1,0,0,0,0
183,The Two-fold Role of Observables in Classical and Quantum Kinematics,"  Observables have a dual nature in both classical and quantum kinematics: they
are at the same time \emph{quantities}, allowing to separate states by means of
their numerical values, and \emph{generators of transformations}, establishing
relations between different states. In this work, we show how this two-fold
role of observables constitutes a key feature in the conceptual analysis of
classical and quantum kinematics, shedding a new light on the distinguishing
feature of the quantum at the kinematical level. We first take a look at the
algebraic description of both classical and quantum observables in terms of
Jordan-Lie algebras and show how the two algebraic structures are the precise
mathematical manifestation of the two-fold role of observables. Then, we turn
to the geometric reformulation of quantum kinematics in terms of K??hler
manifolds. A key achievement of this reformulation is to show that the two-fold
role of observables is the constitutive ingredient defining what an observable
is. Moreover, it points to the fact that, from the restricted point of view of
the transformational role of observables, classical and quantum kinematics
behave in exactly the same way. Finally, we present Landsman's general
framework of Poisson spaces with transition probability, which highlights with
unmatched clarity that the crucial difference between the two kinematics lies
in the way the two roles of observables are related to each other.
",0,1,0,0,0,0
184,On the isoperimetric quotient over scalar-flat conformal classes,"  Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with
smooth boundary $\partial M$. Suppose that $(M,g)$ admits a scalar-flat
conformal metric. We prove that the supremum of the isoperimetric quotient over
the scalar-flat conformal class is strictly larger than the best constant of
the isoperimetric inequality in the Euclidean space, and consequently is
achieved, if either (i) $n\ge 12$ and $\partial M$ has a nonumbilic point; or
(ii) $n\ge 10$, $\partial M$ is umbilic and the Weyl tensor does not vanish at
some boundary point.
",0,0,1,0,0,0
185,On the Spectrum of Random Features Maps of High Dimensional Data,"  Random feature maps are ubiquitous in modern statistical machine learning,
where they generalize random projections by means of powerful, yet often
difficult to analyze nonlinear operators. In this paper, we leverage the
""concentration"" phenomenon induced by random matrix theory to perform a
spectral analysis on the Gram matrix of these random feature maps, here for
Gaussian mixture models of simultaneously large dimension and size. Our results
are instrumental to a deeper understanding on the interplay of the nonlinearity
and the statistics of the data, thereby allowing for a better tuning of random
feature-based techniques.
",0,0,0,1,0,0
186,Minimum energy path calculations with Gaussian process regression,"  The calculation of minimum energy paths for transitions such as atomic and/or
spin re-arrangements is an important task in many contexts and can often be
used to determine the mechanism and rate of transitions. An important challenge
is to reduce the computational effort in such calculations, especially when ab
initio or electron density functional calculations are used to evaluate the
energy since they can require large computational effort. Gaussian process
regression is used here to reduce significantly the number of energy
evaluations needed to find minimum energy paths of atomic rearrangements. By
using results of previous calculations to construct an approximate energy
surface and then converge to the minimum energy path on that surface in each
Gaussian process iteration, the number of energy evaluations is reduced
significantly as compared with regular nudged elastic band calculations. For a
test problem involving rearrangements of a heptamer island on a crystal
surface, the number of energy evaluations is reduced to less than a fifth. The
scaling of the computational effort with the number of degrees of freedom as
well as various possible further improvements to this approach are discussed.
",0,1,0,1,0,0
187,Evaluating Roles of Central Users in Online Communication Networks: A Case Study of #PanamaLeaks,"  Social media has changed the ways of communication, where everyone is
equipped with the power to express their opinions to others in online
discussion platforms. Previously, a number of stud- ies have been presented to
identify opinion leaders in online discussion networks. Feng (""Are you
connected? Evaluating information cascade in online discussion about the
#RaceTogether campaign"", Computers in Human Behavior, 2016) identified five
types of central users and their communication patterns in an online
communication network of a limited time span. However, to trace the change in
communication pattern, a long-term analysis is required. In this study, we
critically analyzed framework presented by Feng based on five types of central
users in online communication network and their communication pattern in a
long-term manner. We take another case study presented by Udnor et al.
(""Determining social media impact on the politics of developing countries using
social network analytics"", Program, 2016) to further understand the dynamics as
well as to perform validation . Results indicate that there may not exist all
of these central users in an online communication network in a long-term
manner. Furthermore, we discuss the changing positions of opinion leaders and
their power to keep isolates interested in an online discussion network.
",1,1,0,1,0,0
188,Best polynomial approximation on the triangle,"  Let $E_n(f)_{\alpha,\beta,\gamma}$ denote the error of best approximation by
polynomials of degree at most $n$ in the space
$L^2(\varpi_{\alpha,\beta,\gamma})$ on the triangle $\{(x,y): x, y \ge 0, x+y
\le 1\}$, where $\varpi_{\alpha,\beta,\gamma}(x,y) := x^\alpha y ^\beta
(1-x-y)^\gamma$ for $\alpha,\beta,\gamma > -1$. Our main result gives a sharp
estimate of $E_n(f)_{\alpha,\beta,\gamma}$ in terms of the error of best
approximation for higher order derivatives of $f$ in appropriate Sobolev
spaces. The result also leads to a characterization of
$E_n(f)_{\alpha,\beta,\gamma}$ by a weighted $K$-functional.
",0,0,1,0,0,0
189,SecureTime: Secure Multicast Time Synchronization,"  Due to the increasing dependency of critical infrastructure on synchronized
clocks, network time synchronization protocols have become an attractive target
for attackers. We identify data origin authentication as the key security
objective and suggest to employ recently proposed high-performance digital
signature schemes (Ed25519 and MQQ-SIG)) as foundation of a novel set of
security measures to secure multicast time synchronization. We conduct
experiments to verify the computational and communication efficiency for using
these signatures in the standard time synchronization protocols NTP and PTP. We
propose additional security measures to prevent replay attacks and to mitigate
delay attacks. Our proposed solutions cover 1-step mode for NTP and PTP and we
extend our security measures specifically to 2-step mode (PTP) and show that
they have no impact on time synchronization's precision.
",1,0,0,0,0,0
190,Solving the multi-site and multi-orbital Dynamical Mean Field Theory using Density Matrix Renormalization,"  We implement an efficient numerical method to calculate response functions of
complex impurities based on the Density Matrix Renormalization Group (DMRG) and
use it as the impurity-solver of the Dynamical Mean Field Theory (DMFT). This
method uses the correction vector to obtain precise Green's functions on the
real frequency axis at zero temperature. By using a self-consistent bath
configuration with very low entanglement, we take full advantage of the DMRG to
calculate dynamical response functions paving the way to treat large effective
impurities such as those corresponding to multi-orbital interacting models and
multi-site or multi-momenta clusters. This method leads to reliable
calculations of non-local self energies at arbitrary dopings and interactions
and at any energy scale.
",0,1,0,0,0,0
191,Topologically Invariant Double Dirac States in Bismuth based Perovskites: Consequence of Ambivalent Charge States and Covalent Bonding,"  Bulk and surface electronic structures, calculated using density functional
theory and a tight-binding model Hamiltonian, reveal the existence of two
topologically invariant (TI) surface states in the family of cubic Bi
perovskites (ABiO$_3$; A = Na, K, Rb, Cs, Mg, Ca, Sr and Ba). The two TI
states, one lying in the valence band (TI-V) and other lying in the conduction
band (TI-C) are formed out of bonding and antibonding states of the
Bi-$\{$s,p$\}$ - O-$\{$p$\}$ coordinated covalent interaction. Below a certain
critical thickness of the film, which varies with A, TI states of top and
bottom surfaces couple to destroy the Dirac type linear dispersion and
consequently to open surface energy gaps. The origin of s-p band inversion,
necessary to form a TI state, classifies the family of ABiO$_3$ into two. For
class-I (A = Na, K, Rb, Cs and Mg) the band inversion, leading to TI-C state,
is induced by spin-orbit coupling of the Bi-p states and for class-II (A = Ca,
Sr and Ba) the band inversion is induced through weak but sensitive second
neighbor Bi-Bi interactions.
",0,1,0,0,0,0
192,Identitas: A Better Way To Be Meaningless,"  It is often recommended that identifiers for ontology terms should be
semantics-free or meaningless. In practice, ontology developers tend to use
numeric identifiers, starting at 1 and working upwards. In this paper we
present a critique of current ontology semantics-free identifiers;
monotonically increasing numbers have a number of significant usability flaws
which make them unsuitable as a default option, and we present a series of
alternatives. We have provide an implementation of these alternatives which can
be freely combined.
",1,0,0,0,0,0
193,Learning from Between-class Examples for Deep Sound Recognition,"  Deep learning methods have achieved high performance in sound recognition
tasks. Deciding how to feed the training data is important for further
performance improvement. We propose a novel learning method for deep sound
recognition: Between-Class learning (BC learning). Our strategy is to learn a
discriminative feature space by recognizing the between-class sounds as
between-class sounds. We generate between-class sounds by mixing two sounds
belonging to different classes with a random ratio. We then input the mixed
sound to the model and train the model to output the mixing ratio. The
advantages of BC learning are not limited only to the increase in variation of
the training data; BC learning leads to an enlargement of Fisher's criterion in
the feature space and a regularization of the positional relationship among the
feature distributions of the classes. The experimental results show that BC
learning improves the performance on various sound recognition networks,
datasets, and data augmentation schemes, in which BC learning proves to be
always beneficial. Furthermore, we construct a new deep sound recognition
network (EnvNet-v2) and train it with BC learning. As a result, we achieved a
performance surpasses the human level.
",1,0,0,1,0,0
194,DAGGER: A sequential algorithm for FDR control on DAGs,"  We propose a linear-time, single-pass, top-down algorithm for multiple
testing on directed acyclic graphs (DAGs), where nodes represent hypotheses and
edges specify a partial ordering in which hypotheses must be tested. The
procedure is guaranteed to reject a sub-DAG with bounded false discovery rate
(FDR) while satisfying the logical constraint that a rejected node's parents
must also be rejected. It is designed for sequential testing settings, when the
DAG structure is known a priori, but the $p$-values are obtained selectively
(such as in a sequence of experiments), but the algorithm is also applicable in
non-sequential settings when all $p$-values can be calculated in advance (such
as variable/model selection). Our DAGGER algorithm, shorthand for Greedily
Evolving Rejections on DAGs, provably controls the false discovery rate under
independence, positive dependence or arbitrary dependence of the $p$-values.
The DAGGER procedure specializes to known algorithms in the special cases of
trees and line graphs, and simplifies to the classical Benjamini-Hochberg
procedure when the DAG has no edges. We explore the empirical performance of
DAGGER using simulations, as well as a real dataset corresponding to a gene
ontology, showing favorable performance in terms of time and power.
",0,0,1,1,0,0
195,On nonlinear profile decompositions and scattering for a NLS-ODE model,"  In this paper, we consider a Hamiltonian system combining a nonlinear Schr\""
odinger equation (NLS) and an ordinary differential equation (ODE). This system
is a simplified model of the NLS around soliton solutions. Following Nakanishi
\cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions.
The proof is based on Nakanishi's framework and Fermi Golden Rule estimates on
$L^4$ in time norms.
",0,0,1,0,0,0
196,Blockchain and human episodic memory,"  We relate the concepts used in decentralized ledger technology to studies of
episodic memory in the mammalian brain. Specifically, we introduce the standard
concepts of linked list, hash functions, and sharding, from computer science.
We argue that these concepts may be more relevant to studies of the neural
mechanisms of memory than has been previously appreciated. In turn, we also
highlight that certain phenomena studied in the brain, namely metacognition,
reality monitoring, and how perceptual conscious experiences come about, may
inspire development in blockchain technology too, specifically regarding
probabilistic consensus protocols.
",0,0,0,0,1,0
197,Epidemic Spreading and Aging in Temporal Networks with Memory,"  Time-varying network topologies can deeply influence dynamical processes
mediated by them. Memory effects in the pattern of interactions among
individuals are also known to affect how diffusive and spreading phenomena take
place. In this paper we analyze the combined effect of these two ingredients on
epidemic dynamics on networks. We study the susceptible-infected-susceptible
(SIS) and the susceptible-infected-removed (SIR) models on the recently
introduced activity-driven networks with memory. By means of an activity-based
mean-field approach we derive, in the long time limit, analytical predictions
for the epidemic threshold as a function of the parameters describing the
distribution of activities and the strength of the memory effects. Our results
show that memory reduces the threshold, which is the same for SIS and SIR
dynamics, therefore favouring epidemic spreading. The theoretical approach
perfectly agrees with numerical simulations in the long time asymptotic regime.
Strong aging effects are present in the preasymptotic regime and the epidemic
threshold is deeply affected by the starting time of the epidemics. We discuss
in detail the origin of the model-dependent preasymptotic corrections, whose
understanding could potentially allow for epidemic control on correlated
temporal networks.
",1,0,0,0,1,0
198,"The Shattered Gradients Problem: If resnets are the answer, then what is the question?","  A long-standing obstacle to progress in deep learning is the problem of
vanishing and exploding gradients. Although, the problem has largely been
overcome via carefully constructed initializations and batch normalization,
architectures incorporating skip-connections such as highway and resnets
perform much better than standard feedforward architectures despite well-chosen
initialization and batch normalization. In this paper, we identify the
shattered gradients problem. Specifically, we show that the correlation between
gradients in standard feedforward networks decays exponentially with depth
resulting in gradients that resemble white noise whereas, in contrast, the
gradients in architectures with skip-connections are far more resistant to
shattering, decaying sublinearly. Detailed empirical evidence is presented in
support of the analysis, on both fully-connected networks and convnets.
Finally, we present a new ""looks linear"" (LL) initialization that prevents
shattering, with preliminary experiments showing the new initialization allows
to train very deep networks without the addition of skip-connections.
",1,0,0,1,0,0
199,Pr$_2$Ir$_2$O$_7$: when Luttinger semimetal meets Melko-Hertog-Gingras spin ice state,"  We study the band structure topology and engineering from the interplay
between local moments and itinerant electrons in the context of pyrochlore
iridates. For the metallic iridate Pr$_2$Ir$_2$O$_7$, the Ir $5d$ conduction
electrons interact with the Pr $4f$ local moments via the $f$-$d$ exchange.
While the Ir electrons form a Luttinger semimetal, the Pr moments can be tuned
into an ordered spin ice with a finite ordering wavevector, dubbed
""Melko-Hertog-Gingras"" state, by varying Ir and O contents. We point out that
the ordered spin ice of the Pr local moments generates an internal magnetic
field that reconstructs the band structure of the Luttinger semimetal. Besides
the broad existence of Weyl nodes, we predict that the magnetic translation of
the ""Melko-Hertog-Gingras"" state for the Pr moments protects the Dirac band
touching at certain time reversal invariant momenta for the Ir conduction
electrons. We propose the magnetic fields to control the Pr magnetic structure
and thereby indirectly influence the topological and other properties of the Ir
electrons. Our prediction may be immediately tested in the ordered
Pr$_2$Ir$_2$O$_7$ samples. We expect our work to stimulate a detailed
examination of the band structure, magneto-transport, and other properties of
Pr$_2$Ir$_2$O$_7$.
",0,1,0,0,0,0
200,A 2-edge partial inverse problem for the Sturm-Liouville operators with singular potentials on a star-shaped graph,"  Boundary value problems for Sturm-Liouville operators with potentials from
the class $W_2^{-1}$ on a star-shaped graph are considered. We assume that the
potentials are known on all the edges of the graph except two, and show that
the potentials on the remaining edges can be constructed by fractional parts of
two spectra. A uniqueness theorem is proved, and an algorithm for the
constructive solution of the partial inverse problem is provided. The main
ingredient of the proofs is the Riesz-basis property of specially constructed
systems of functions.
",0,0,1,0,0,0
201,Jastrow form of the Ground State Wave Functions for Fractional Quantum Hall States,"  The topological morphology--order of zeros at the positions of electrons with
respect to a specific electron--of Laughlin state at filling fractions $1/m$
($m$ odd) is homogeneous as every electron feels zeros of order $m$ at the
positions of other electrons. Although fairly accurate ground state wave
functions for most of the other quantum Hall states in the lowest Landau level
are quite well-known, it had been an open problem in expressing the ground
state wave functions in terms of flux-attachment to particles, {\em a la}, this
morphology of Laughlin state. With a very general consideration of
flux-particle relations only, in spherical geometry, we here report a novel
method for determining morphologies of these states. Based on these, we
construct almost exact ground state wave-functions for the Coulomb interaction.
Although the form of interaction may change the ground state wave-function, the
same morphology constructs the latter irrespective of the nature of the
interaction between electrons.
",0,1,0,0,0,0
202,On a common refinement of Stark units and Gross-Stark units,"  The purpose of this paper is to formulate and study a common refinement of a
version of Stark's conjecture and its $p$-adic analogue, in terms of Fontaine's
$p$-adic period ring and $p$-adic Hodge theory. We construct period-ring-valued
functions under a generalization of Yoshida's conjecture on the transcendental
parts of CM-periods. Then we conjecture a reciprocity law on their special
values concerning the absolute Frobenius action. We show that our conjecture
implies a part of Stark's conjecture when the base field is an arbitrary real
field and the splitting place is its real place. It also implies a refinement
of the Gross-Stark conjecture under a certain assumption. When the base field
is the rational number field, our conjecture follows from Coleman's formula on
Fermat curves. We also prove some partial results in other cases.
",0,0,1,0,0,0
203,An Integrated Decision and Control Theoretic Solution to Multi-Agent Co-Operative Search Problems,"  This paper considers the problem of autonomous multi-agent cooperative target
search in an unknown environment using a decentralized framework under a
no-communication scenario. The targets are considered as static targets and the
agents are considered to be homogeneous. The no-communication scenario
translates as the agents do not exchange either the information about the
environment or their actions among themselves. We propose an integrated
decision and control theoretic solution for a search problem which generates
feasible agent trajectories. In particular, a perception based algorithm is
proposed which allows an agent to estimate the probable strategies of other
agents' and to choose a decision based on such estimation. The algorithm shows
robustness with respect to the estimation accuracy to a certain degree. The
performance of the algorithm is compared with random strategies and numerical
simulation shows considerable advantages.
",1,0,0,0,0,0
204,Chain effects of clean water: The Mills-Reincke phenomenon in early twentieth-century Japan,"  This study explores the validity of chain effects of clean water, which are
known as the ""Mills-Reincke phenomenon,"" in early twentieth-century Japan.
Recent studies have reported that water purifications systems are responsible
for huge contributions to human capital. Although a few studies have
investigated the short-term effects of water-supply systems in pre-war Japan,
little is known about the benefits associated with these systems. By analyzing
city-level cause-specific mortality data from the years 1922-1940, we found
that eliminating typhoid fever infections decreased the risk of deaths due to
non-waterborne diseases. Our estimates show that for one additional typhoid
death, there were approximately one to three deaths due to other causes, such
as tuberculosis and pneumonia. This suggests that the observed Mills-Reincke
phenomenon could have resulted from the prevention typhoid fever in a
previously-developing Asian country.
",0,0,0,1,1,0
205,Learning Transferable Architectures for Scalable Image Recognition,"  Developing neural network image classification models often requires
significant architecture engineering. In this paper, we study a method to learn
the model architectures directly on the dataset of interest. As this approach
is expensive when the dataset is large, we propose to search for an
architectural building block on a small dataset and then transfer the block to
a larger dataset. The key contribution of this work is the design of a new
search space (the ""NASNet search space"") which enables transferability. In our
experiments, we search for the best convolutional layer (or ""cell"") on the
CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking
together more copies of this cell, each with their own parameters to design a
convolutional architecture, named ""NASNet architecture"". We also introduce a
new regularization technique called ScheduledDropPath that significantly
improves generalization in the NASNet models. On CIFAR-10 itself, NASNet
achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet
achieves, among the published works, state-of-the-art accuracy of 82.7% top-1
and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than
the best human-invented architectures while having 9 billion fewer FLOPS - a
reduction of 28% in computational demand from the previous state-of-the-art
model. When evaluated at different levels of computational cost, accuracies of
NASNets exceed those of the state-of-the-art human-designed models. For
instance, a small version of NASNet also achieves 74% top-1 accuracy, which is
3.1% better than equivalently-sized, state-of-the-art models for mobile
platforms. Finally, the learned features by NASNet used with the Faster-RCNN
framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO
dataset.
",1,0,0,0,0,0
206,Fast Multi-frame Stereo Scene Flow with Motion Segmentation,"  We propose a new multi-frame method for efficiently computing scene flow
(dense depth and optical flow) and camera ego-motion for a dynamic scene
observed from a moving stereo camera rig. Our technique also segments out
moving objects from the rigid scene. In our method, we first estimate the
disparity map and the 6-DOF camera motion using stereo matching and visual
odometry. We then identify regions inconsistent with the estimated camera
motion and compute per-pixel optical flow only at these regions. This flow
proposal is fused with the camera motion-based flow proposal using fusion moves
to obtain the final optical flow and motion segmentation. This unified
framework benefits all four tasks - stereo, optical flow, visual odometry and
motion segmentation leading to overall higher accuracy and efficiency. Our
method is currently ranked third on the KITTI 2015 scene flow benchmark.
Furthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3
orders of magnitude faster than the top six methods. We also report a thorough
evaluation on challenging Sintel sequences with fast camera and object motion,
where our method consistently outperforms OSF [Menze and Geiger, 2015], which
is currently ranked second on the KITTI benchmark.
",1,0,0,0,0,0
207,Pointed $p^2q$-dimensional Hopf algebras in positive characteristic,"  Let $\K$ be an algebraically closed field of positive characteristic $p$. We
mainly classify pointed Hopf algebras over $\K$ of dimension $p^2q$, $pq^2$ and
$pqr$ where $p,q,r$ are distinct prime numbers. We obtain a complete
classification of such Hopf algebras except two subcases when they are not
generated by the first terms of coradical filtration. In particular, we obtain
many new examples of non-commutative and non-cocommutative finite-dimensional
Hopf algebras.
",0,0,1,0,0,0
208,Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems,"  We present the mixed Galerkin discretization of distributed parameter
port-Hamiltonian systems. On the prototypical example of hyperbolic systems of
two conservation laws in arbitrary spatial dimension, we derive the main
contributions: (i) A weak formulation of the underlying geometric
(Stokes-Dirac) structure with a segmented boundary according to the causality
of the boundary ports. (ii) The geometric approximation of the Stokes-Dirac
structure by a finite-dimensional Dirac structure is realized using a mixed
Galerkin approach and power-preserving linear maps, which define minimal
discrete power variables. (iii) With a consistent approximation of the
Hamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.
By the degrees of freedom in the power-preserving maps, the resulting family of
structure-preserving schemes allows for trade-offs between centered
approximations and upwinding. We illustrate the method on the example of
Whitney finite elements on a 2D simplicial triangulation and compare the
eigenvalue approximation in 1D with a related approach.
",1,0,0,0,0,0
209,Clamped seismic metamaterials: Ultra-low broad frequency stop-bands,"  The regularity of earthquakes, their destructive power, and the nuisance of
ground vibration in urban environments, all motivate designs of defence
structures to lessen the impact of seismic and ground vibration waves on
buildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and
up to a few tens of Hz for vibrations generated by human activities, cause a
large amount of damage, or inconvenience, depending on the geological
conditions they can travel considerable distances and may match the resonant
fundamental frequency of buildings. The ultimate aim of any seismic
metamaterial, or any other seismic shield, is to protect over this entire range
of frequencies, the long wavelengths involved, and low frequency, have meant
this has been unachievable to date.
Elastic flexural waves, applicable in the mechanical vibrations of thin
elastic plates, can be designed to have a broad zero-frequency stop-band using
a periodic array of very small clamped circles. Inspired by this experimental
and theoretical observation, all be it in a situation far removed from seismic
waves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)
and body (pressure P and shear S) wave reflectors at very large wavelengths in
structured soils modelled as a fully elastic layer periodically clamped to
bedrock.
We identify zero frequency stop-bands that only exist in the limit of columns
of concrete clamped at their base to the bedrock. In a realistic configuration
of a sedimentary basin 15 meters deep we observe a zero frequency stop-band
covering a broad frequency range of $0$ to $30$ Hz.
",0,1,0,0,0,0
210,Difference analogue of second main theorems for meromorphic mapping into algebraic variety,"  In this paper, we prove some difference analogue of second main theorems of
meromorphic mapping from Cm into an algebraic variety V intersecting a finite
set of fixed hypersurfaces in subgeneral position. As an application, we prove
a result on algebraically degenerate of holomorphic curves intersecting
hypersurfaces and difference analogue of Picard's theorem on holomorphic
curves. Furthermore, we obtain a second main theorem of meromorphic mappings
intersecting hypersurfaces in N-subgeneral position for Veronese embedding in
Pn(C) and a uniqueness theorem sharing hypersurfaces.
",0,0,1,0,0,0
211,An Effective Way to Improve YouTube-8M Classification Accuracy in Google Cloud Platform,"  Large-scale datasets have played a significant role in progress of neural
network and deep learning areas. YouTube-8M is such a benchmark dataset for
general multi-label video classification. It was created from over 7 million
YouTube videos (450,000 hours of video) and includes video labels from a
vocabulary of 4716 classes (3.4 labels/video on average). It also comes with
pre-extracted audio & visual features from every second of video (3.2 billion
feature vectors in total). Google cloud recently released the datasets and
organized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.
Competitors are challenged to develop classification algorithms that assign
video-level labels using the new and improved Youtube-8M V2 dataset. Inspired
by the competition, we started exploration of audio understanding and
classification using deep learning algorithms and ensemble methods. We built
several baseline predictions according to the benchmark paper and public github
tensorflow code. Furthermore, we improved global prediction accuracy (GAP) from
base level 77% to 80.7% through approaches of ensemble.
",1,0,0,1,0,0
212,Experimental Design of a Prescribed Burn Instrumentation,"  Observational data collected during experiments, such as the planned Fire and
Smoke Model Evaluation Experiment (FASMEE), are critical for progressing and
transitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM
into operational use. Historical meteorological data, representing typical
weather conditions for the anticipated burn locations and times, have been
processed to initialize and run a set of simulations representing the planned
experimental burns. Based on an analysis of these numerical simulations, this
paper provides recommendations on the experimental setup that include the
ignition procedures, size and duration of the burns, and optimal sensor
placement. New techniques are developed to initialize coupled fire-atmosphere
simulations with weather conditions typical of the planned burn locations and
time of the year. Analysis of variation and sensitivity analysis of simulation
design to model parameters by repeated Latin Hypercube Sampling are used to
assess the locations of the sensors. The simulations provide the locations of
the measurements that maximize the expected variation of the sensor outputs
with the model parameters.
",0,0,0,1,0,0
213,Seifert surgery on knots via Reidemeister torsion and Casson-Walker-Lescop invariant III,"  For a knot $K$ in a homology $3$-sphere $\Sigma$, let $M$ be the result of
$2/q$-surgery on $K$, and let $X$ be the universal abelian covering of $M$. Our
first theorem is that if the first homology of $X$ is finite cyclic and $M$ is
a Seifert fibered space with $N\ge 3$ singular fibers, then $N\ge 4$ if and
only if the first homology of the universal abelian covering of $X$ is
infinite. Our second theorem is that under an appropriate assumption on the
Alexander polynomial of $K$, if $M$ is a Seifert fibered space, then $q=\pm 1$
(i.e.\ integral surgery).
",0,0,1,0,0,0
214,Sparse mean localization by information theory,"  Sparse feature selection is necessary when we fit statistical models, we have
access to a large group of features, don't know which are relevant, but assume
that most are not. Alternatively, when the number of features is larger than
the available data the model becomes over parametrized and the sparse feature
selection task involves selecting the most informative variables for the model.
When the model is a simple location model and the number of relevant features
does not grow with the total number of features, sparse feature selection
corresponds to sparse mean estimation. We deal with a simplified mean
estimation problem consisting of an additive model with gaussian noise and mean
that is in a restricted, finite hypothesis space. This restriction simplifies
the mean estimation problem into a selection problem of combinatorial nature.
Although the hypothesis space is finite, its size is exponential in the
dimension of the mean. In limited data settings and when the size of the
hypothesis space depends on the amount of data or on the dimension of the data,
choosing an approximation set of hypotheses is a desirable approach. Choosing a
set of hypotheses instead of a single one implies replacing the bias-variance
trade off with a resolution-stability trade off. Generalization capacity
provides a resolution selection criterion based on allowing the learning
algorithm to communicate the largest amount of information in the data to the
learner without error. In this work the theory of approximation set coding and
generalization capacity is explored in order to understand this approach. We
then apply the generalization capacity criterion to the simplified sparse mean
estimation problem and detail an importance sampling algorithm which at once
solves the difficulty posed by large hypothesis spaces and the slow convergence
of uniform sampling algorithms.
",0,0,0,1,0,0
215,Joint Power and Admission Control based on Channel Distribution Information: A Novel Two-Timescale Approach,"  In this letter, we consider the joint power and admission control (JPAC)
problem by assuming that only the channel distribution information (CDI) is
available. Under this assumption, we formulate a new chance (probabilistic)
constrained JPAC problem, where the signal to interference plus noise ratio
(SINR) outage probability of the supported links is enforced to be not greater
than a prespecified tolerance. To efficiently deal with the chance SINR
constraint, we employ the sample approximation method to convert them into
finitely many linear constraints. Then, we propose a convex approximation based
deflation algorithm for solving the sample approximation JPAC problem. Compared
to the existing works, this letter proposes a novel two-timescale JPAC
approach, where admission control is performed by the proposed deflation
algorithm based on the CDI in a large timescale and transmission power is
adapted instantly with fast fadings in a small timescale. The effectiveness of
the proposed algorithm is illustrated by simulations.
",1,0,1,0,0,0
216,A Closer Look at the Alpha Persei Coronal Conundrum,"  A ROSAT survey of the Alpha Per open cluster in 1993 detected its brightest
star, mid-F supergiant Alpha Persei: the X-ray luminosity and spectral hardness
were similar to coronally active late-type dwarf members. Later, in 2010, a
Hubble Cosmic Origins Spectrograph SNAPshot of Alpha Persei found
far-ultraviolet coronal proxy SiIV unexpectedly weak. This, and a suspicious
offset of the ROSAT source, suggested that a late-type companion might be
responsible for the X-rays. Recently, a multi-faceted program tested that
premise. Groundbased optical coronography, and near-UV imaging with HST Wide
Field Camera 3, searched for any close-in faint candidate coronal objects, but
without success. Then, a Chandra pointing found the X-ray source single and
coincident with the bright star. Significantly, the SiIV emissions of Alpha
Persei, in a deeper FUV spectrum collected by HST COS as part of the joint
program, aligned well with chromospheric atomic oxygen (which must be intrinsic
to the luminous star), within the context of cooler late-F and early-G
supergiants, including Cepheid variables. This pointed to the X-rays as the
fundamental anomaly. The over-luminous X-rays still support the case for a
hyperactive dwarf secondary, albeit now spatially unresolved. However, an
alternative is that Alpha Persei represents a novel class of coronal source.
Resolving the first possibility now has become more difficult, because the easy
solution -- a well separated companion -- has been eliminated. Testing the
other possibility will require a broader high-energy census of the early-F
supergiants.
",0,1,0,0,0,0
217,The challenge of realistic music generation: modelling raw audio at scale,"  Realistic music generation is a challenging task. When building generative
models of music that are learnt from data, typically high-level representations
such as scores or MIDI are used that abstract away the idiosyncrasies of a
particular performance. But these nuances are very important for our perception
of musicality and realism, so in this work we embark on modelling music in the
raw audio domain. It has been shown that autoregressive models excel at
generating raw audio waveforms of speech, but when applied to music, we find
them biased towards capturing local signal structure at the expense of
modelling long-range correlations. This is problematic because music exhibits
structure at many different timescales. In this work, we explore autoregressive
discrete autoencoders (ADAs) as a means to enable autoregressive models to
capture long-range correlations in waveforms. We find that they allow us to
unconditionally generate piano music directly in the raw audio domain, which
shows stylistic consistency across tens of seconds.
",0,0,0,1,0,0
218,Interpretations of family size distributions: The Datura example,"  Young asteroid families are unique sources of information about fragmentation
physics and the structure of their parent bodies, since their physical
properties have not changed much since their birth. Families have different
properties such as age, size, taxonomy, collision severity and others, and
understanding the effect of those properties on our observations of the
size-frequency distribution (SFD) of family fragments can give us important
insights into the hypervelocity collision processes at scales we cannot achieve
in our laboratories. Here we take as an example the very young Datura family,
with a small 8-km parent body, and compare its size distribution to other
families, with both large and small parent bodies, and created by both
catastrophic and cratering formation events. We conclude that most likely
explanation for the shallower size distribution compared to larger families is
a more pronounced observational bias because of its small size. Its size
distribution is perfectly normal when its parent body size is taken into
account. We also discuss some other possibilities. In addition, we study
another common feature: an offset or ""bump"" in the distribution occurring for a
few of the larger elements. We hypothesize that it can be explained by a newly
described regime of cratering, ""spall cratering"", which controls the majority
of impact craters on the surface of small asteroids like Datura.
",0,1,0,0,0,0
219,"Intersections of $??$ classes in $\overline{\mathcal{M}}_{g,n}$","  We provide a graph formula which describes an arbitrary monomial in {\omega}
classes (also referred to as stable {\psi} classes) in terms of a simple family
of dual graphs (pinwheel graphs) with edges decorated by rational functions in
{\psi} classes. We deduce some numerical consequences and in particular a
combinatorial formula expressing top intersections of \k{appa} classes on Mg in
terms of top intersections of {\psi} classes.
",0,0,1,0,0,0
220,GENFIRE: A generalized Fourier iterative reconstruction algorithm for high-resolution 3D imaging,"  Tomography has made a radical impact on diverse fields ranging from the study
of 3D atomic arrangements in matter to the study of human health in medicine.
Despite its very diverse applications, the core of tomography remains the same,
that is, a mathematical method must be implemented to reconstruct the 3D
structure of an object from a number of 2D projections. In many scientific
applications, however, the number of projections that can be measured is
limited due to geometric constraints, tolerable radiation dose and/or
acquisition speed. Thus it becomes an important problem to obtain the
best-possible reconstruction from a limited number of projections. Here, we
present the mathematical implementation of a tomographic algorithm, termed
GENeralized Fourier Iterative REconstruction (GENFIRE). By iterating between
real and reciprocal space, GENFIRE searches for a global solution that is
concurrently consistent with the measured data and general physical
constraints. The algorithm requires minimal human intervention and also
incorporates angular refinement to reduce the tilt angle error. We demonstrate
that GENFIRE can produce superior results relative to several other popular
tomographic reconstruction techniques by numerical simulations, and by
experimentally by reconstructing the 3D structure of a porous material and a
frozen-hydrated marine cyanobacterium. Equipped with a graphical user
interface, GENFIRE is freely available from our website and is expected to find
broad applications across different disciplines.
",0,1,0,0,0,0
221,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,"  Generative Adversarial Networks (GANs) excel at creating realistic images
with complex models for which maximum likelihood is infeasible. However, the
convergence of GAN training has still not been proved. We propose a two
time-scale update rule (TTUR) for training GANs with stochastic gradient
descent on arbitrary GAN loss functions. TTUR has an individual learning rate
for both the discriminator and the generator. Using the theory of stochastic
approximation, we prove that the TTUR converges under mild assumptions to a
stationary local Nash equilibrium. The convergence carries over to the popular
Adam optimization, for which we prove that it follows the dynamics of a heavy
ball with friction and thus prefers flat minima in the objective landscape. For
the evaluation of the performance of GANs at image generation, we introduce the
""Fr??chet Inception Distance"" (FID) which captures the similarity of generated
images to real ones better than the Inception Score. In experiments, TTUR
improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)
outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN
Bedrooms, and the One Billion Word Benchmark.
",1,0,0,1,0,0
222,"SPIRou Input Catalog: Activity, Rotation and Magnetic Field of Cool Dwarfs","  Based on optical high-resolution spectra obtained with CFHT/ESPaDOnS, we
present new measurements of activity and magnetic field proxies of 442 low-mass
K5-M7 dwarfs. The objects were analysed as potential targets to search for
planetary-mass companions with the new spectropolarimeter and high-precision
velocimeter, SPIRou. We have analysed their high-resolution spectra in an
homogeneous way: circular polarisation, chromospheric features, and Zeeman
broadening of the FeH infrared line. The complex relationship between these
activity indicators is analysed: while no strong connection is found between
the large-scale and small-scale magnetic fields, the latter relates with the
non-thermal flux originating in the chromosphere.
We then examine the relationship between various activity diagnostics and the
optical radial-velocity jitter available in the literature, especially for
planet host stars. We use this to derive for all stars an activity merit
function (higher for quieter stars) with the goal of identifying the most
favorable stars where the radial-velocity jitter is low enough for planet
searches. We find that the main contributors to the RV jitter are the
large-scale magnetic field and the chromospheric non-thermal emission.
In addition, three stars (GJ 1289, GJ 793, and GJ 251) have been followed
along their rotation using the spectropolarimetric mode, and we derive their
magnetic topology. These very slow rotators are good representatives of future
SPIRou targets. They are compared to other stars where the magnetic topology is
also known. The poloidal component of the magnetic field is predominent in all
three stars.
",0,1,0,0,0,0
223,Objective Procedure for Reconstructing Couplings in Complex Systems,"  Inferring directional connectivity from point process data of multiple
elements is desired in various scientific fields such as neuroscience,
geography, economics, etc. Here, we propose an inference procedure for this
goal based on the kinetic Ising model. The procedure is composed of two steps:
(1) determination of the time-bin size for transforming the point-process data
to discrete time binary data and (2) screening of relevant couplings from the
estimated networks. For these, we develop simple methods based on information
theory and computational statistics. Applications to data from artificial and
\textit{in vitro} neuronal networks show that the proposed procedure performs
fairly well when identifying relevant couplings, including the discrimination
of their signs, with low computational cost. These results highlight the
potential utility of the kinetic Ising model to analyze real interacting
systems with event occurrences.
",0,0,0,0,1,0
224,Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines: A Majorization--Minimization Algorithm Approach,"  Support vector machines (SVMs) are an important tool in modern data analysis.
Traditionally, support vector machines have been fitted via quadratic
programming, either using purpose-built or off-the-shelf algorithms. We present
an alternative approach to SVM fitting via the majorization--minimization (MM)
paradigm. Algorithms that are derived via MM algorithm constructions can be
shown to monotonically decrease their objectives at each iteration, as well as
be globally convergent to stationary points. We demonstrate the construction of
iteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,
for SVM risk minimization problems involving the hinge, least-square,
squared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net
penalizations. Successful implementations of our algorithms are presented via
some numerical examples.
",1,0,0,1,0,0
225,Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search Queries,"  Estimating vaccination uptake is an integral part of ensuring public health.
It was recently shown that vaccination uptake can be estimated automatically
from web data, instead of slowly collected clinical records or population
surveys. All prior work in this area assumes that features of vaccination
uptake collected from the web are temporally regular. We present the first ever
method to remove this assumption from vaccination uptake estimation: our method
dynamically adapts to temporal fluctuations in time series web data used to
estimate vaccination uptake. We show our method to outperform the state of the
art compared to competitive baselines that use not only web data but also
curated clinical data. This performance improvement is more pronounced for
vaccines whose uptake has been irregular due to negative media attention (HPV-1
and HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of
12 years old (whose vaccination is more irregular compared to younger
children).
",1,0,0,1,0,0
226,Over Recurrence for Mixing Transformations,"  We show that every invertible strong mixing transformation on a Lebesgue
space has strictly over-recurrent sets. Also, we give an explicit procedure for
constructing strong mixing transformations with no under-recurrent sets. This
answers both parts of a question of V. Bergelson.
We define $\epsilon$-over-recurrence and show that given $\epsilon > 0$, any
ergodic measure preserving invertible transformation (including discrete
spectrum) has $\epsilon$-over-recurrent sets of arbitrarily small measure.
Discrete spectrum transformations and rotations do not have over-recurrent
sets, but we construct a weak mixing rigid transformation with strictly
over-recurrent sets.
",0,0,1,0,0,0
227,Joint Atlas-Mapping of Multiple Histological Series combined with Multimodal MRI of Whole Marmoset Brains,"  Development of a mesoscale neural circuitry map of the common marmoset is an
essential task due to the ideal characteristics of the marmoset as a model
organism for neuroscience research. To facilitate this development there is a
need for new computational tools to cross-register multi-modal data sets
containing MRI volumes as well as multiple histological series, and to register
the combined data set to a common reference atlas. We present a fully automatic
pipeline for same-subject-MRI guided reconstruction of image volumes from a
series of histological sections of different modalities, followed by
diffeomorphic mapping to a reference atlas. We show registration results for
Nissl, myelin, CTB, and fluorescent tracer images using a same-subject ex-vivo
MRI as our reference and show that our method achieves accurate registration
and eliminates artifactual warping that may be result from the absence of a
reference MRI data set. Examination of the determinant of the local metric
tensor of the diffeomorphic mapping between each subject's ex-vivo MRI and
resultant Nissl reconstruction allows an unprecedented local quantification of
geometrical distortions resulting from the histological processing, showing a
slight shrinkage, a median linear scale change of ~-1% in going from the
ex-vivo MRI to the tape-transfer generated histological image data.
",0,0,0,0,1,0
228,A Practical Approach for Successive Omniscience,"  The system that we study in this paper contains a set of users that observe a
discrete memoryless multiple source and communicate via noise-free channels
with the aim of attaining omniscience, the state that all users recover the
entire multiple source. We adopt the concept of successive omniscience (SO),
i.e., letting the local omniscience in some user subset be attained before the
global omniscience in the entire system, and consider the problem of how to
efficiently attain omniscience in a successive manner. Based on the existing
results on SO, we propose a CompSetSO algorithm for determining a complimentary
set, a user subset in which the local omniscience can be attained first without
increasing the sum-rate, the total number of communications, for the global
omniscience. We also derive a sufficient condition for a user subset to be
complimentary so that running the CompSetSO algorithm only requires a lower
bound, instead of the exact value, of the minimum sum-rate for attaining global
omniscience. The CompSetSO algorithm returns a complimentary user subset in
polynomial time. We show by example how to recursively apply the CompSetSO
algorithm so that the global omniscience can be attained by multi-stages of SO.
",1,0,0,0,0,0
229,Scholars on Twitter: who and how many are they?,"  In this paper we present a novel methodology for identifying scholars with a
Twitter account. By combining bibliometric data from Web of Science and Twitter
users identified by Altmetric.com we have obtained the largest set of
individual scholars matched with Twitter users made so far. Our methodology
consists of a combination of matching algorithms, considering different
linguistic elements of both author names and Twitter names; followed by a
rule-based scoring system that weights the common occurrence of several
elements related with the names, individual elements and activities of both
Twitter users and scholars matched. Our results indicate that about 2% of the
overall population of scholars in the Web of Science is active on Twitter. By
domain we find a strong presence of researchers from the Social Sciences and
the Humanities. Natural Sciences is the domain with the lowest level of
scholars on Twitter. Researchers on Twitter also tend to be younger than those
that are not on Twitter. As this is a bibliometric-based approach, it is
important to highlight the reliance of the method on the number of publications
produced and tweeted by the scholars, thus the share of scholars on Twitter
ranges between 1% and 5% depending on their level of productivity. Further
research is suggested in order to improve and expand the methodology.
",1,0,0,0,0,0
230,General notions of regression depth function,"  As a measure for the centrality of a point in a set of multivariate data,
statistical depth functions play important roles in multivariate analysis,
because one may conveniently construct descriptive as well as inferential
procedures relying on them. Many depth notions have been proposed in the
literature to fit to different applications. However, most of them are mainly
developed for the location setting. In this paper, we discuss the possibility
of extending some of them into the regression setting. A general concept of
regression depth function is also provided.
",0,0,0,1,0,0
231,Photonic topological pumping through the edges of a dynamical four-dimensional quantum Hall system,"  When a two-dimensional electron gas is exposed to a perpendicular magnetic
field and an in-plane electric field, its conductance becomes quantized in the
transverse in-plane direction: this is known as the quantum Hall (QH) effect.
This effect is a result of the nontrivial topology of the system's electronic
band structure, where an integer topological invariant known as the first Chern
number leads to the quantization of the Hall conductance. Interestingly, it was
shown that the QH effect can be generalized mathematically to four spatial
dimensions (4D), but this effect has never been realized for the obvious reason
that experimental systems are bound to three spatial dimensions. In this work,
we harness the high tunability and control offered by photonic waveguide arrays
to experimentally realize a dynamically-generated 4D QH system using a 2D array
of coupled optical waveguides. The inter-waveguide separation is constructed
such that the propagation of light along the device samples over
higher-dimensional momenta in the directions orthogonal to the two physical
dimensions, thus realizing a 2D topological pump. As a result, the device's
band structure is associated with 4D topological invariants known as second
Chern numbers which support a quantized bulk Hall response with a 4D symmetry.
In a finite-sized system, the 4D topological bulk response is carried by
localized edges modes that cross the sample as a function of of the modulated
auxiliary momenta. We directly observe this crossing through photon pumping
from edge-to-edge and corner-to-corner of our system. These are equivalent to
the pumping of charge across a 4D system from one 3D hypersurface to the
opposite one and from one 2D hyperedge to another, and serve as first
experimental realization of higher-dimensional topological physics.
",0,1,0,0,0,0
232,On Scalable Inference with Stochastic Gradient Descent,"  In many applications involving large dataset or online updating, stochastic
gradient descent (SGD) provides a scalable way to compute parameter estimates
and has gained increasing popularity due to its numerical convenience and
memory efficiency. While the asymptotic properties of SGD-based estimators have
been established decades ago, statistical inference such as interval estimation
remains much unexplored. The traditional resampling method such as the
bootstrap is not computationally feasible since it requires to repeatedly draw
independent samples from the entire dataset. The plug-in method is not
applicable when there are no explicit formulas for the covariance matrix of the
estimator. In this paper, we propose a scalable inferential procedure for
stochastic gradient descent, which, upon the arrival of each observation,
updates the SGD estimate as well as a large number of randomly perturbed SGD
estimates. The proposed method is easy to implement in practice. We establish
its theoretical properties for a general class of models that includes
generalized linear models and quantile regression models as special cases. The
finite-sample performance and numerical utility is evaluated by simulation
studies and two real data applications.
",1,0,0,1,0,0
233,The g-Good-Neighbor Conditional Diagnosability of Locally Twisted Cubes,"  In the work of Peng et al. in 2012, a new measure was proposed for fault
diagnosis of systems: namely, g-good-neighbor conditional diagnosability, which
requires that any fault-free vertex has at least g fault-free neighbors in the
system. In this paper, we establish the g-good-neighbor conditional
diagnosability of locally twisted cubes under the PMC model and the MM^* model.
",1,0,1,0,0,0
234,Coherence for lenses and open games,"  Categories of polymorphic lenses in computer science, and of open games in
compositional game theory, have a curious structure that is reminiscent of
compact closed categories, but differs in some crucial ways. Specifically they
have a family of morphisms that behave like the counits of a compact closed
category, but have no corresponding units; and they have a `partial' duality
that behaves like transposition in a compact closed category when it is
defined. We axiomatise this structure, which we refer to as a `teleological
category'. We precisely define a diagrammatic language suitable for these
categories, and prove a coherence theorem for them. This underpins the use of
diagrammatic reasoning in compositional game theory, which has previously been
used only informally.
",1,0,0,0,0,0
235,Streaming Algorithm for Euler Characteristic Curves of Multidimensional Images,"  We present an efficient algorithm to compute Euler characteristic curves of
gray scale images of arbitrary dimension. In various applications the Euler
characteristic curve is used as a descriptor of an image.
Our algorithm is the first streaming algorithm for Euler characteristic
curves. The usage of streaming removes the necessity to store the entire image
in RAM. Experiments show that our implementation handles terabyte scale images
on commodity hardware. Due to lock-free parallelism, it scales well with the
number of processor cores. Our software---CHUNKYEuler---is available as open
source on Bitbucket.
Additionally, we put the concept of the Euler characteristic curve in the
wider context of computational topology. In particular, we explain the
connection with persistence diagrams.
",1,0,1,0,0,0
236,An automata group of intermediate growth and exponential activity,"  We give a new example of an automata group of intermediate growth. It is
generated by an automaton with 4 states on an alphabet with 8 letters. This
automata group has exponential activity and its limit space is not simply
connected.
",0,0,1,0,0,0
237,Tuning across the BCS-BEC crossover in the multiband superconductor Fe$_{1+y}$Se$_x$Te$_{1-x}$ : An angle-resolved photoemission study,"  The crossover from Bardeen-Cooper-Schrieffer (BCS) superconductivity to
Bose-Einstein condensation (BEC) is difficult to realize in quantum materials
because, unlike in ultracold atoms, one cannot tune the pairing interaction. We
realize the BCS-BEC crossover in a nearly compensated semimetal
Fe$_{1+y}$Se$_x$Te$_{1-x}$ by tuning the Fermi energy, $\epsilon_F$, via
chemical doping, which permits us to systematically change $\Delta /
\epsilon_F$ from 0.16 to 0.5 were $\Delta$ is the superconducting (SC) gap. We
use angle-resolved photoemission spectroscopy to measure the Fermi energy, the
SC gap and characteristic changes in the SC state electronic dispersion as the
system evolves from a BCS to a BEC regime. Our results raise important
questions about the crossover in multiband superconductors which go beyond
those addressed in the context of cold atoms.
",0,1,0,0,0,0
238,GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking,"  Model compression is essential for serving large deep neural nets on devices
with limited resources or applications that require real-time responses. As a
case study, a state-of-the-art neural language model usually consists of one or
more recurrent layers sandwiched between an embedding layer used for
representing input tokens and a softmax layer for generating output tokens. For
problems with a very large vocabulary size, the embedding and the softmax
matrices can account for more than half of the model size. For instance, the
bigLSTM model achieves state-of- the-art performance on the One-Billion-Word
(OBW) dataset with around 800k vocabulary, and its word embedding and softmax
matrices use more than 6GBytes space, and are responsible for over 90% of the
model parameters. In this paper, we propose GroupReduce, a novel compression
method for neural language models, based on vocabulary-partition (block) based
low-rank matrix approximation and the inherent frequency distribution of tokens
(the power-law distribution of words). The experimental results show our method
can significantly outperform traditional compression methods such as low-rank
approximation and pruning. On the OBW dataset, our method achieved 6.6 times
compression rate for the embedding and softmax matrices, and when combined with
quantization, our method can achieve 26 times compression rate, which
translates to a factor of 12.8 times compression for the entire model with very
little degradation in perplexity.
",0,0,0,1,0,0
239,Morphological characterization of Ge ion implanted SiO2 matrix using multifractal technique,"  200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy
were implanted into SiO2 matrix with Different fluences. The implanted samples
were annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of
implanted as well as annealed samples were captured by the atomic force
microscopy (AFM). Two dimension (2D) multifractal detrended fluctuation
analysis (MFDFA) based on the partition function approach has been used to
study the surfaces of ion implanted and annealed samples. The partition
function is used to calculate generalized Hurst exponent with the segment size.
Moreover, it is seen that the generalized Hurst exponents vary nonlinearly with
the moment, thereby exhibiting the multifractal nature. The multifractality of
surface is pronounced after annealing for the surface implanted with fluence
7.5X1016 ions/cm^2.
",0,1,0,0,0,0
240,Preliminary corrosion studies of IN-RAFM steel with stagnant Lead Lithium at 550 C,"  Corrosion of Indian RAFMS (reduced activation ferritic martensitic steel)
material with liquid metal, Lead Lithium ( Pb-Li) has been studied under static
condition, maintaining Pb-Li at 550 C for different time durations, 2500, 5000
and 9000 hours. Corrosion rate was calculated from weight loss measurements.
Microstructure analysis was carried out using SEM and chemical composition by
SEM-EDX measurements. Micro Vickers hardness and tensile testing were also
carried out. Chromium was found leaching from the near surface regions and
surface hardness was found to decrease in all the three cases. Grain boundaries
were affected. Some grains got detached from the surface giving rise to pebble
like structures in the surface micrographs. There was no significant reduction
in the tensile strength, after exposure to liquid metal. This paper discusses
the experimental details and the results obtained.
",0,1,0,0,0,0
241,Magnetocapillary self-assemblies: locomotion and micromanipulation along a liquid interface,"  This paper presents an overview and discussion of magnetocapillary
self-assemblies. New results are presented, in particular concerning the
possible development of future applications. These self-organizing structures
possess the notable ability to move along an interface when powered by an
oscillatory, uniform magnetic field. The system is constructed as follows. Soft
magnetic particles are placed on a liquid interface, and submitted to a
magnetic induction field. An attractive force due to the curvature of the
interface around the particles competes with an interaction between magnetic
dipoles. Ordered structures can spontaneously emerge from these conditions.
Furthermore, time-dependent magnetic fields can produce a wide range of dynamic
behaviours, including non-time-reversible deformation sequences that produce
translational motion at low Reynolds number. In other words, due to a
spontaneous breaking of time-reversal symmetry, the assembly can turn into a
surface microswimmer. Trajectories have been shown to be precisely
controllable. As a consequence, this system offers a way to produce microrobots
able to perform different tasks. This is illustrated in this paper by the
capture, transport and release of a floating cargo, and the controlled mixing
of fluids at low Reynolds number.
",0,1,0,0,0,0
242,On asymptotically minimax nonparametric detection of signal in Gaussian white noise,"  For the problem of nonparametric detection of signal in Gaussian white noise
we point out strong asymptotically minimax tests. The sets of alternatives are
a ball in Besov space $B^r_{2\infty}$ with ""small"" balls in $L_2$ removed.
",0,0,1,1,0,0
243,Bayesian Metabolic Flux Analysis reveals intracellular flux couplings,"  Metabolic flux balance analyses are a standard tool in analysing metabolic
reaction rates compatible with measurements, steady-state and the metabolic
reaction network stoichiometry. Flux analysis methods commonly place
unrealistic assumptions on fluxes due to the convenience of formulating the
problem as a linear programming model, and most methods ignore the notable
uncertainty in flux estimates. We introduce a novel paradigm of Bayesian
metabolic flux analysis that models the reactions of the whole genome-scale
cellular system in probabilistic terms, and can infer the full flux vector
distribution of genome-scale metabolic systems based on exchange and
intracellular (e.g. 13C) flux measurements, steady-state assumptions, and
target function assumptions. The Bayesian model couples all fluxes jointly
together in a simple truncated multivariate posterior distribution, which
reveals informative flux couplings. Our model is a plug-in replacement to
conventional metabolic balance methods, such as flux balance analysis (FBA).
Our experiments indicate that we can characterise the genome-scale flux
covariances, reveal flux couplings, and determine more intracellular unobserved
fluxes in C. acetobutylicum from 13C data than flux variability analysis. The
COBRA compatible software is available at github.com/markusheinonen/bamfa
",0,0,0,1,0,0
244,Robust Estimation of Change-Point Location,"  We introduce a robust estimator of the location parameter for the
change-point in the mean based on the Wilcoxon statistic and establish its
consistency for $L_1$ near epoch dependent processes. It is shown that the
consistency rate depends on the magnitude of change. A simulation study is
performed to evaluate finite sample properties of the Wilcoxon-type estimator
in standard cases, as well as under heavy-tailed distributions and disturbances
by outliers, and to compare it with a CUSUM-type estimator. It shows that the
Wilcoxon-type estimator is equivalent to the CUSUM-type estimator in standard
cases, but outperforms the CUSUM-type estimator in presence of heavy tails or
outliers in the data.
",0,0,1,1,0,0
245,Growing length scale accompanying the vitrification: A perspective based on non-singular density fluctuations,"  In glass forming liquids close to the glass transition point, even a very
slight increase in the macroscopic density results in a dramatic slowing down
of the macroscopic relaxation. Concomitantly, the local density itself
fluctuates in space. Therefore, one can imagine that even very small local
density variations control the local glassy nature. Based on this perspective,
a model for describing growing length scale accompanying the vitrification is
introduced, in which we assume that in a subsystem whose density is above a
certain threshold value, $\rho_{\rm c}$, owing to steric constraints, particle
rearrangements are highly suppressed for a sufficiently long time period
($\sim$ structural relaxation time). We regard such a subsystem as a glassy
cluster. Then, based on the statistics of the subsystem-density, we predict
that with compression (increasing average density $\rho$) at a fixed
temperature $T$ in supercooled states, the characteristic length of the
clusters, $\xi$, diverges as $\xi\sim(\rho_{\rm c}-\rho)^{-2/d}$, where $d$ is
the spatial dimensionality. This $\xi$ measures the average persistence length
of the steric constraints in blocking the rearrangement motions and is
determined by the subsystem density. Additionally, with decreasing $T$ at a
fixed $\rho$, the length scale diverges in the same manner as $\xi\sim(T-T_{\rm
c})^{-2/d}$, for which $\rho$ is identical to $\rho_{\rm c}$ at $T=T_{\rm c}$.
The exponent describing the diverging length scale is the same as the one
predicted by some theoretical models and indeed has been observed in some
simulations and experiments. However, the basic mechanism for this divergence
is different; that is, we do not invoke thermodynamic anomalies associated with
the thermodynamic phase transition as the origin of the growing length scale.
We further present arguements for the cooperative properties based on the
clusters.
",0,1,0,0,0,0
246,Many-Objective Pareto Local Search,"  We propose a new Pareto Local Search Algorithm for the many-objective
combinatorial optimization. Pareto Local Search proved to be a very effective
tool in the case of the bi-objective combinatorial optimization and it was used
in a number of the state-of-the-art algorithms for problems of this kind. On
the other hand, the standard Pareto Local Search algorithm becomes very
inefficient for problems with more than two objectives. We build an effective
Many-Objective Pareto Local Search algorithm using three new mechanisms: the
efficient update of large Pareto archives with ND-Tree data structure, a new
mechanism for the selection of the promising solutions for the neighborhood
exploration, and a partial exploration of the neighborhoods. We apply the
proposed algorithm to the instances of two different problems, i.e. the
traveling salesperson problem and the traveling salesperson problem with
profits with up to 5 objectives showing high effectiveness of the proposed
algorithm.
",1,0,0,0,0,0
247,From Natural to Artificial Camouflage: Components and Systems,"  We identify the components of bio-inspired artificial camouflage systems
including actuation, sensing, and distributed computation. After summarizing
recent results in understanding the physiology and system-level performance of
a variety of biological systems, we describe computational algorithms that can
generate similar patterns and have the potential for distributed
implementation. We find that the existing body of work predominately treats
component technology in an isolated manner that precludes a material-like
implementation that is scale-free and robust. We conclude with open research
challenges towards the realization of integrated camouflage solutions.
",1,0,0,0,1,0
248,Bayesian nonparametric inference for the M/G/1 queueing systems based on the marked departure process,"  In the present work we study Bayesian nonparametric inference for the
continuous-time M/G/1 queueing system. In the focus of the study is the
unobservable service time distribution. We assume that the only available data
of the system are the marked departure process of customers with the marks
being the queue lengths just after departure instants. These marks constitute
an embedded Markov chain whose distribution may be parametrized by stochastic
matrices of a special delta form. We develop the theory in order to obtain
integral mixtures of Markov measures with respect to suitable prior
distributions. We have found a sufficient statistic with a distribution of a
so-called S-structure sheding some new light on the inner statistical structure
of the M/G/1 queue. Moreover, it allows to update suitable prior distributions
to the posterior. Our inference methods are validated by large sample results
as posterior consistency and posterior normality.
",0,0,1,1,0,0
249,On some polynomials and series of Bloch-Polya Type,"  We will show that $(1-q)(1-q^2)\dots (1-q^m)$ is a polynomial in $q$ with
coefficients from $\{-1,0,1\}$ iff $m=1,\ 2,\ 3,$ or $5$ and explore some
interesting consequences of this result. We find explicit formulas for the
$q$-series coefficients of $(1-q^2)(1-q^3)(1-q^4)(1-q^5)\dots$ and
$(1-q^3)(1-q^4)(1-q^5)(1-q^6)\dots$. In doing so, we extend certain
observations made by Sudler in 1964. We also discuss the classification of the
products $(1-q)(1-q^2)\dots (1-q^m)$ and some related series with respect to
their absolute largest coefficients.
",0,0,1,0,0,0
250,"Improvement in the UAV position estimation with low-cost GPS, INS and vision-based system: Application to a quadrotor UAV","  In this paper, we develop a position estimation system for Unmanned Aerial
Vehicles formed by hardware and software. It is based on low-cost devices: GPS,
commercial autopilot sensors and dense optical flow algorithm implemented in an
onboard microcomputer. Comparative tests were conducted using our approach and
the conventional one, where only fusion of GPS and inertial sensors are used.
Experiments were conducted using a quadrotor in two flying modes: hovering and
trajectory tracking in outdoor environments. Results demonstrate the
effectiveness of the proposed approach in comparison with the conventional
approaches presented in the vast majority of commercial drones.
",1,0,0,0,0,0
251,Structured low rank decomposition of multivariate Hankel matrices,"  We study the decomposition of a multivariate Hankel matrix H\_$\sigma$ as a
sum of Hankel matrices of small rank in correlation with the decomposition of
its symbol $\sigma$ as a sum of polynomial-exponential series. We present a new
algorithm to compute the low rank decomposition of the Hankel operator and the
decomposition of its symbol exploiting the properties of the associated
Artinian Gorenstein quotient algebra A\_$\sigma$. A basis of A\_$\sigma$ is
computed from the Singular Value Decomposition of a sub-matrix of the Hankel
matrix H\_$\sigma$. The frequencies and the weights are deduced from the
generalized eigenvectors of pencils of shifted sub-matrices of H $\sigma$.
Explicit formula for the weights in terms of the eigenvectors avoid us to solve
a Vandermonde system. This new method is a multivariate generalization of the
so-called Pencil method for solving Prony-type decomposition problems. We
analyse its numerical behaviour in the presence of noisy input moments, and
describe a rescaling technique which improves the numerical quality of the
reconstruction for frequencies of high amplitudes. We also present a new Newton
iteration, which converges locally to the closest multivariate Hankel matrix of
low rank and show its impact for correcting errors on input moments.
",0,0,1,0,0,0
252,Linear time-periodic dynamical systems: An H2 analysis and a model reduction framework,"  Linear time-periodic (LTP) dynamical systems frequently appear in the
modeling of phenomena related to fluid dynamics, electronic circuits, and
structural mechanics via linearization centered around known periodic orbits of
nonlinear models. Such LTP systems can reach orders that make repeated
simulation or other necessary analysis prohibitive, motivating the need for
model reduction.
We develop here an algorithmic framework for constructing reduced models that
retains the linear time-periodic structure of the original LTP system. Our
approach generalizes optimal approaches that have been established previously
for linear time-invariant (LTI) model reduction problems. We employ an
extension of the usual H2 Hardy space defined for the LTI setting to
time-periodic systems and within this broader framework develop an a posteriori
error bound expressible in terms of related LTI systems. Optimization of this
bound motivates our algorithm. We illustrate the success of our method on two
numerical examples.
",1,0,0,0,0,0
253,Software metadata: How much is enough?,"  Broad efforts are underway to capture metadata about research software and
retain it across services; notable in this regard is the CodeMeta project. What
metadata are important to have about (research) software? What metadata are
useful for searching for codes? What would you like to learn about astronomy
software? This BoF sought to gather information on metadata most desired by
researchers and users of astro software and others interested in registering,
indexing, capturing, and doing research on this software. Information from this
BoF could conceivably result in changes to the Astrophysics Source Code Library
(ASCL) or other resources for the benefit of the community or provide input
into other projects concerned with software metadata.
",1,1,0,0,0,0
254,A Categorical Approach for Recognizing Emotional Effects of Music,"  Recently, digital music libraries have been developed and can be plainly
accessed. Latest research showed that current organization and retrieval of
music tracks based on album information are inefficient. Moreover, they
demonstrated that people use emotion tags for music tracks in order to search
and retrieve them. In this paper, we discuss separability of a set of emotional
labels, proposed in the categorical emotion expression, using Fisher's
separation theorem. We determine a set of adjectives to tag music parts: happy,
sad, relaxing, exciting, epic and thriller. Temporal, frequency and energy
features have been extracted from the music parts. It could be seen that the
maximum separability within the extracted features occurs between relaxing and
epic music parts. Finally, we have trained a classifier using Support Vector
Machines to automatically recognize and generate emotional labels for a music
part. Accuracy for recognizing each label has been calculated; where the
results show that epic music can be recognized more accurately (77.4%),
comparing to the other types of music.
",1,0,0,1,0,0
255,Utilizing artificial neural networks to predict demand for weather-sensitive products at retail stores,"  One key requirement for effective supply chain management is the quality of
its inventory management. Various inventory management methods are typically
employed for different types of products based on their demand patterns,
product attributes, and supply network. In this paper, our goal is to develop
robust demand prediction methods for weather sensitive products at retail
stores. We employ historical datasets from Walmart, whose customers and markets
are often exposed to extreme weather events which can have a huge impact on
sales regarding the affected stores and products. We want to accurately predict
the sales of 111 potentially weather-sensitive products around the time of
major weather events at 45 of Walmart retails locations in the U.S.
Intuitively, we may expect an uptick in the sales of umbrellas before a big
thunderstorm, but it is difficult for replenishment managers to predict the
level of inventory needed to avoid being out-of-stock or overstock during and
after that storm. While they rely on a variety of vendor tools to predict sales
around extreme weather events, they mostly employ a time-consuming process that
lacks a systematic measure of effectiveness. We employ all the methods critical
to any analytics project and start with data exploration. Critical features are
extracted from the raw historical dataset for demand forecasting accuracy and
robustness. In particular, we employ Artificial Neural Network for forecasting
demand for each product sold around the time of major weather events. Finally,
we evaluate our model to evaluate their accuracy and robustness.
",1,0,0,1,0,0
256,Deformable Generator Network: Unsupervised Disentanglement of Appearance and Geometry,"  We propose a deformable generator model to disentangle the appearance and
geometric information from images into two independent latent vectors. The
appearance generator produces the appearance information, including color,
illumination, identity or category, of an image. The geometric generator
produces displacement of the coordinates of each pixel and performs geometric
warping, such as stretching and rotation, on the appearance generator to obtain
the final synthesized image. The proposed model can learn both representations
from image data in an unsupervised manner. The learned geometric generator can
be conveniently transferred to the other image datasets to facilitate
downstream AI tasks.
",0,0,0,1,0,0
257,Gaussian Kernel in Quantum Paradigm,"  The Gaussian kernel is a very popular kernel function used in many
machine-learning algorithms, especially in support vector machines (SVM). For
nonlinear training instances in machine learning, it often outperforms
polynomial kernels in model accuracy. We use Gaussian kernel profoundly in
formulating nonlinear classical SVM. In the recent research, P. Rebentrost
et.al. discuss a very elegant quantum version of least square support vector
machine using the quantum version of polynomial kernel, which is exponentially
faster than the classical counterparts. In this paper, we have demonstrated a
quantum version of the Gaussian kernel and analyzed its complexity in the
context of quantum SVM. Our analysis shows that the computational complexity of
the quantum Gaussian kernel is O(\epsilon^(-1)logN) with N-dimensional
instances and \epsilon with a Taylor remainder error term |R_m (\epsilon^(-1)
logN)|.
",1,0,0,0,0,0
258,Learning to Succeed while Teaching to Fail: Privacy in Closed Machine Learning Systems,"  Security, privacy, and fairness have become critical in the era of data
science and machine learning. More and more we see that achieving universally
secure, private, and fair systems is practically impossible. We have seen for
example how generative adversarial networks can be used to learn about the
expected private training data; how the exploitation of additional data can
reveal private information in the original one; and how what looks like
unrelated features can teach us about each other. Confronted with this
challenge, in this paper we open a new line of research, where the security,
privacy, and fairness is learned and used in a closed environment. The goal is
to ensure that a given entity (e.g., the company or the government), trusted to
infer certain information with our data, is blocked from inferring protected
information from it. For example, a hospital might be allowed to produce
diagnosis on the patient (the positive task), without being able to infer the
gender of the subject (negative task). Similarly, a company can guarantee that
internally it is not using the provided data for any undesired task, an
important goal that is not contradicting the virtually impossible challenge of
blocking everybody from the undesired task. We design a system that learns to
succeed on the positive task while simultaneously fail at the negative one, and
illustrate this with challenging cases where the positive task is actually
harder than the negative one being blocked. Fairness, to the information in the
negative task, is often automatically obtained as a result of this proposed
approach. The particular framework and examples open the door to security,
privacy, and fairness in very important closed scenarios, ranging from private
data accumulation companies like social networks to law-enforcement and
hospitals.
",1,0,0,1,0,0
259,Performance of Energy Harvesting Receivers with Power Optimization,"  The difficulty of modeling energy consumption in communication systems leads
to challenges in energy harvesting (EH) systems, in which nodes scavenge energy
from their environment. An EH receiver must harvest enough energy for
demodulating and decoding. The energy required depends upon factors, like code
rate and signal-to-noise ratio, which can be adjusted dynamically. We consider
a receiver which harvests energy from ambient sources and the transmitter,
meaning the received signal is used for both EH and information decoding.
Assuming a generalized function for energy consumption, we maximize the total
number of information bits decoded, under both average and peak power
constraints at the transmitter, by carefully optimizing the power used for EH,
power used for information transmission, fraction of time for EH, and code
rate. For transmission over a single block, we find there exist problem
parameters for which either maximizing power for information transmission or
maximizing power for EH is optimal. In the general case, the optimal solution
is a tradeoff of the two. For transmission over multiple blocks, we give an
upper bound on performance and give sufficient and necessary conditions to
achieve this bound. Finally, we give some numerical results to illustrate our
results and analysis.
",1,0,0,0,0,0
260,On Convergence Rate of a Continuous-Time Distributed Self-Appraisal Model with Time-Varying Relative Interaction Matrices,"  This paper studies a recently proposed continuous-time distributed
self-appraisal model with time-varying interactions among a network of $n$
individuals which are characterized by a sequence of time-varying relative
interaction matrices. The model describes the evolution of the
social-confidence levels of the individuals via a reflected appraisal mechanism
in real time. We first show by example that when the relative interaction
matrices are stochastic (not doubly stochastic), the social-confidence levels
of the individuals may not converge to a steady state. We then show that when
the relative interaction matrices are doubly stochastic, the $n$ individuals'
self-confidence levels will all converge to $1/n$, which indicates a democratic
state, exponentially fast under appropriate assumptions, and provide an
explicit expression of the convergence rate.
",0,0,1,0,0,0
261,Closing the loop on multisensory interactions: A neural architecture for multisensory causal inference and recalibration,"  When the brain receives input from multiple sensory systems, it is faced with
the question of whether it is appropriate to process the inputs in combination,
as if they originated from the same event, or separately, as if they originated
from distinct events. Furthermore, it must also have a mechanism through which
it can keep sensory inputs calibrated to maintain the accuracy of its internal
representations. We have developed a neural network architecture capable of i)
approximating optimal multisensory spatial integration, based on Bayesian
causal inference, and ii) recalibrating the spatial encoding of sensory
systems. The architecture is based on features of the dorsal processing
hierarchy, including the spatial tuning properties of unisensory neurons and
the convergence of different sensory inputs onto multisensory neurons.
Furthermore, we propose that these unisensory and multisensory neurons play
dual roles in i) encoding spatial location as separate or integrated estimates
and ii) accumulating evidence for the independence or relatedness of
multisensory stimuli. We further propose that top-down feedback connections
spanning the dorsal pathway play key a role in recalibrating spatial encoding
at the level of early unisensory cortices. Our proposed architecture provides
possible explanations for a number of human electrophysiological and
neuroimaging results and generates testable predictions linking neurophysiology
with behaviour.
",0,0,0,0,1,0
262,Block CUR: Decomposing Matrices using Groups of Columns,"  A common problem in large-scale data analysis is to approximate a matrix
using a combination of specifically sampled rows and columns, known as CUR
decomposition. Unfortunately, in many real-world environments, the ability to
sample specific individual rows or columns of the matrix is limited by either
system constraints or cost. In this paper, we consider matrix approximation by
sampling predefined \emph{blocks} of columns (or rows) from the matrix. We
present an algorithm for sampling useful column blocks and provide novel
guarantees for the quality of the approximation. This algorithm has application
in problems as diverse as biometric data analysis to distributed computing. We
demonstrate the effectiveness of the proposed algorithms for computing the
Block CUR decomposition of large matrices in a distributed setting with
multiple nodes in a compute cluster, where such blocks correspond to columns
(or rows) of the matrix stored on the same node, which can be retrieved with
much less overhead than retrieving individual columns stored across different
nodes. In the biometric setting, the rows correspond to different users and
columns correspond to users' biometric reaction to external stimuli, {\em
e.g.,}~watching video content, at a particular time instant. There is
significant cost in acquiring each user's reaction to lengthy content so we
sample a few important scenes to approximate the biometric response. An
individual time sample in this use case cannot be queried in isolation due to
the lack of context that caused that biometric reaction. Instead, collections
of time segments ({\em i.e.,} blocks) must be presented to the user. The
practical application of these algorithms is shown via experimental results
using real-world user biometric data from a content testing environment.
",1,0,0,1,0,0
263,Synchronous Observation on the Spontaneous Transformation of Liquid Metal under Free Falling Microgravity Situation,"  The unusually high surface tension of room temperature liquid metal is
molding it as unique material for diverse newly emerging areas. However, unlike
its practices on earth, such metal fluid would display very different behaviors
when working in space where gravity disappears and surface property dominates
the major physics. So far, few direct evidences are available to understand
such effect which would impede further exploration of liquid metal use for
space. Here to preliminarily probe into this intriguing issue, a low cost
experimental strategy to simulate microgravity environment on earth was
proposed through adopting bridges with high enough free falling distance as the
test platform. Then using digital cameras amounted along x, y, z directions on
outside wall of the transparent container with liquid metal and allied solution
inside, synchronous observations on the transient flow and transformational
activities of liquid metal were performed. Meanwhile, an unmanned aerial
vehicle was adopted to record the whole free falling dynamics of the test
capsule from the far end which can help justify subsequent experimental
procedures. A series of typical fundamental phenomena were thus observed as:
(a) A relatively large liquid metal object would spontaneously transform from
its original planar pool state into a sphere and float in the container if
initiating the free falling; (b) The liquid metal changes its three-dimensional
shape due to dynamic microgravity strength due to free falling and rebound of
the test capsule; and (c) A quick spatial transformation of liquid metal
immersed in the solution can easily be induced via external electrical fields.
The mechanisms of the surface tension driven liquid metal actuation in space
were interpreted. All these findings indicated that microgravity effect should
be fully treated in developing future generation liquid metal space
technologies.
",0,1,0,0,0,0
264,Continuously tempered Hamiltonian Monte Carlo,"  Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC)
method for performing approximate inference in complex probabilistic models of
continuous variables. In common with many MCMC methods, however, the standard
HMC approach performs poorly in distributions with multiple isolated modes. We
present a method for augmenting the Hamiltonian system with an extra continuous
temperature control variable which allows the dynamic to bridge between
sampling a complex target distribution and a simpler unimodal base
distribution. This augmentation both helps improve mixing in multimodal targets
and allows the normalisation constant of the target distribution to be
estimated. The method is simple to implement within existing HMC code,
requiring only a standard leapfrog integrator. We demonstrate experimentally
that the method is competitive with annealed importance sampling and simulating
tempering methods at sampling from challenging multimodal distributions and
estimating their normalising constants.
",0,0,0,1,0,0
265,Automated Synthesis of Safe Digital Controllers for Sampled-Data Stochastic Nonlinear Systems,"  We present a new method for the automated synthesis of digital controllers
with formal safety guarantees for systems with nonlinear dynamics, noisy output
measurements, and stochastic disturbances. Our method derives digital
controllers such that the corresponding closed-loop system, modeled as a
sampled-data stochastic control system, satisfies a safety specification with
probability above a given threshold. The proposed synthesis method alternates
between two steps: generation of a candidate controller pc, and verification of
the candidate. pc is found by maximizing a Monte Carlo estimate of the safety
probability, and by using a non-validated ODE solver for simulating the system.
Such a candidate is therefore sub-optimal but can be generated very rapidly. To
rule out unstable candidate controllers, we prove and utilize Lyapunov's
indirect method for instability of sampled-data nonlinear systems. In the
subsequent verification step, we use a validated solver based on SMT
(Satisfiability Modulo Theories) to compute a numerically and statistically
valid confidence interval for the safety probability of pc. If the probability
so obtained is not above the threshold, we expand the search space for
candidates by increasing the controller degree. We evaluate our technique on
three case studies: an artificial pancreas model, a powertrain control model,
and a quadruple-tank process.
",1,0,0,0,0,0
266,Magnus integrators on multicore CPUs and GPUs,"  In the present paper we consider numerical methods to solve the discrete
Schr??dinger equation with a time dependent Hamiltonian (motivated by problems
encountered in the study of spin systems). We will consider both short-range
interactions, which lead to evolution equations involving sparse matrices, and
long-range interactions, which lead to dense matrices. Both of these settings
show very different computational characteristics. We use Magnus integrators
for time integration and employ a framework based on Leja interpolation to
compute the resulting action of the matrix exponential. We consider both
traditional Magnus integrators (which are extensively used for these types of
problems in the literature) as well as the recently developed commutator-free
Magnus integrators and implement them on modern CPU and GPU (graphics
processing unit) based systems.
We find that GPUs can yield a significant speed-up (up to a factor of $10$ in
the dense case) for these types of problems. In the sparse case GPUs are only
advantageous for large problem sizes and the achieved speed-ups are more
modest. In most cases the commutator-free variant is superior but especially on
the GPU this advantage is rather small. In fact, none of the advantage of
commutator-free methods on GPUs (and on multi-core CPUs) is due to the
elimination of commutators. This has important consequences for the design of
more efficient numerical methods.
",1,1,0,0,0,0
267,High Dimensional Estimation and Multi-Factor Models,"  This paper re-investigates the estimation of multiple factor models relaxing
the convention that the number of factors is small and using a new approach for
identifying factors. We first obtain the collection of all possible factors and
then provide a simultaneous test, security by security, of which factors are
significant. Since the collection of risk factors is large and highly
correlated, high-dimension methods (including the LASSO and prototype
clustering) have to be used. The multi-factor model is shown to have a
significantly better fit than the Fama-French 5-factor model. Robustness tests
are also provided.
",0,0,0,1,0,1
268,Scaling Law for Three-body Collisions in Identical Fermions with $p$-wave Interactions,"  We experimentally confirmed the threshold behavior and scattering length
scaling law of the three-body loss coefficients in an ultracold spin-polarized
gas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the
three-body loss coefficients as functions of temperature and scattering volume,
and found that the threshold law and the scattering length scaling law hold in
limited temperature and magnetic field regions. We also found that the
breakdown of the scaling laws is due to the emergence of the effective-range
term. This work is an important first step toward full understanding of the
loss of identical fermions with $p$-wave interactions.
",0,1,0,0,0,0
269,An Expanded Local Variance Gamma model,"  The paper proposes an expanded version of the Local Variance Gamma model of
Carr and Nadtochiy by adding drift to the governing underlying process. Still
in this new model it is possible to derive an ordinary differential equation
for the option price which plays a role of Dupire's equation for the standard
local volatility model. It is shown how calibration of multiple smiles (the
whole local volatility surface) can be done in such a case. Further, assuming
the local variance to be a piecewise linear function of strike and piecewise
constant function of time this ODE is solved in closed form in terms of
Confluent hypergeometric functions. Calibration of the model to market smiles
does not require solving any optimization problem and, in contrast, can be done
term-by-term by solving a system of non-linear algebraic equations for each
maturity, which is fast.
",0,0,0,0,0,1
270,An attentive neural architecture for joint segmentation and parsing and its application to real estate ads,"  In processing human produced text using natural language processing (NLP)
techniques, two fundamental subtasks that arise are (i) segmentation of the
plain text into meaningful subunits (e.g., entities), and (ii) dependency
parsing, to establish relations between subunits. In this paper, we develop a
relatively simple and effective neural joint model that performs both
segmentation and dependency parsing together, instead of one after the other as
in most state-of-the-art works. We will focus in particular on the real estate
ad setting, aiming to convert an ad to a structured description, which we name
property tree, comprising the tasks of (1) identifying important entities of a
property (e.g., rooms) from classifieds and (2) structuring them into a tree
format. In this work, we propose a new joint model that is able to tackle the
two tasks simultaneously and construct the property tree by (i) avoiding the
error propagation that would arise from the subtasks one after the other in a
pipelined fashion, and (ii) exploiting the interactions between the subtasks.
For this purpose, we perform an extensive comparative study of the pipeline
methods and the new proposed joint model, reporting an improvement of over
three percentage points in the overall edge F1 score of the property tree.
Also, we propose attention methods, to encourage our model to focus on salient
tokens during the construction of the property tree. Thus we experimentally
demonstrate the usefulness of attentive neural architectures for the proposed
joint model, showcasing a further improvement of two percentage points in edge
F1 score for our application.
",1,0,0,0,0,0
271,Multilevel maximum likelihood estimation with application to covariance matrices,"  The asymptotic variance of the maximum likelihood estimate is proved to
decrease when the maximization is restricted to a subspace that contains the
true parameter value. Maximum likelihood estimation allows a systematic fitting
of covariance models to the sample, which is important in data assimilation.
The hierarchical maximum likelihood approach is applied to the spectral
diagonal covariance model with different parameterizations of eigenvalue decay,
and to the sparse inverse covariance model with specified parameter values on
different sets of nonzero entries. It is shown computationally that using
smaller sets of parameters can decrease the sampling noise in high dimension
substantially.
",0,0,1,1,0,0
272,Auto-Meta: Automated Gradient Based Meta Learner Search,"  Fully automating machine learning pipelines is one of the key challenges of
current artificial intelligence research, since practical machine learning
often requires costly and time-consuming human-powered processes such as model
design, algorithm development, and hyperparameter tuning. In this paper, we
verify that automated architecture search synergizes with the effect of
gradient-based meta learning. We adopt the progressive neural architecture
search \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal
architectures for meta-learners. The gradient based meta-learner whose
architecture was automatically found achieved state-of-the-art results on the
5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy,
which is $11.54\%$ improvement over the result obtained by the first
gradient-based meta-learner called MAML
\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is
the first successful neural architecture search implementation in the context
of meta learning.
",0,0,0,1,0,0
273,Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry,"  We provide a comprehensive study of the convergence of forward-backward
algorithm under suitable geometric conditions leading to fast rates. We present
several new results and collect in a unified view a variety of results
scattered in the literature, often providing simplified proofs. Novel
contributions include the analysis of infinite dimensional convex minimization
problems, allowing the case where minimizers might not exist. Further, we
analyze the relation between different geometric conditions, and discuss novel
connections with a priori conditions in linear inverse problems, including
source conditions, restricted isometry properties and partial smoothness.
",0,0,1,1,0,0
274,Calibration-Free Relaxation-Based Multi-Color Magnetic Particle Imaging,"  Magnetic Particle Imaging (MPI) is a novel imaging modality with important
applications such as angiography, stem cell tracking, and cancer imaging.
Recently, there have been efforts to increase the functionality of MPI via
multi-color imaging methods that can distinguish the responses of different
nanoparticles, or nanoparticles in different environmental conditions. The
proposed techniques typically rely on extensive calibrations that capture the
differences in the harmonic responses of the nanoparticles. In this work, we
propose a method to directly estimate the relaxation time constant of the
nanoparticles from the MPI signal, which is then used to generate a multi-color
relaxation map. The technique is based on the underlying mirror symmetry of the
adiabatic MPI signal when the same region is scanned back and forth. We
validate the proposed method via extensive simulations, and via experiments on
our in-house Magnetic Particle Spectrometer (MPS) setup at 550 Hz and our
in-house MPI scanner at 9.7 kHz. Our results show that nanoparticles can be
successfully distinguished with the proposed technique, without any calibration
or prior knowledge about the nanoparticles.
",0,1,0,0,0,0
275,Neural Machine Translation,"  Draft of textbook chapter on neural machine translation. a comprehensive
treatment of the topic, ranging from introduction to neural networks,
computation graphs, description of the currently dominant attentional
sequence-to-sequence model, recent refinements, alternative architectures and
challenges. Written as chapter for the textbook Statistical Machine
Translation. Used in the JHU Fall 2017 class on machine translation.
",1,0,0,0,0,0
276,On algebraically integrable domains in Euclidean spaces,"  Let $D$ be a bounded domain $D$ in $\mathbb R^n $ with infinitely smooth
boundary and $n$ is odd. We prove that if the volume cut off from the domain by
a hyperplane is an algebraic function of the hyperplane, free of real singular
points, then the domain is an ellipsoid. This partially answers a question of
V.I. Arnold: whether odd-dimensional ellipsoids are the only algebraically
integrable domains?
",0,0,1,0,0,0
277,Vocabulary-informed Extreme Value Learning,"  The novel unseen classes can be formulated as the extreme values of known
classes. This inspired the recent works on open-set recognition
\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no
way of naming the novel unseen classes. To solve this problem, we propose the
Extreme Value Learning (EVL) formulation to learn the mapping from visual
feature to semantic space. To model the margin and coverage distributions of
each class, the Vocabulary-informed Learning (ViL) is adopted by using vast
open vocabulary in the semantic space. Essentially, by incorporating the EVL
and ViL, we for the first time propose a novel semantic embedding paradigm --
Vocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual
features into semantic space in a probabilistic way. The learned embedding can
be directly used to solve supervised learning, zero-shot and open set
recognition simultaneously. Experiments on two benchmark datasets demonstrate
the effectiveness of proposed frameworks.
",1,0,1,1,0,0
278,Cross-layer optimized routing with low duty cycle TDMA across multiple wireless body area networks,"  In this paper, we study the performance of two cross-layer optimized dynamic
routing techniques for radio interference mitigation across multiple coexisting
wireless body area networks (BANs), based on real-life measurements. At the
network layer, the best route is selected according to channel state
information from the physical layer, associated with low duty cycle TDMA at the
MAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel
cooperative multi-path routing (CMR) incorporating 3-branch selection
combining) perform real-time and reliable data transfer across BANs operating
near the 2.4 GHz ISM band. An open-access experimental data set of 'everyday'
mixed-activities is used for analyzing the proposed cross-layer optimization.
We show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and
even 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage
probability at a realistic signal-to-interference-plus-noise ratio (SINR).
Acceptable packet delivery ratios (PDR) and spectral efficiencies are obtained
from SPR and CMR with reasonably sensitive receivers across a range of TDMA low
duty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The
distribution fits for received SINR through routing are also derived and
validated with theoretical analysis.
",1,0,0,0,0,0
279,A Team-Formation Algorithm for Faultline Minimization,"  In recent years, the proliferation of online resumes and the need to evaluate
large populations of candidates for on-site and virtual teams have led to a
growing interest in automated team-formation. Given a large pool of candidates,
the general problem requires the selection of a team of experts to complete a
given task. Surprisingly, while ongoing research has studied numerous
variations with different constraints, it has overlooked a factor with a
well-documented impact on team cohesion and performance: team faultlines.
Addressing this gap is challenging, as the available measures for faultlines in
existing teams cannot be efficiently applied to faultline optimization. In this
work, we meet this challenge with a new measure that can be efficiently used
for both faultline measurement and minimization. We then use the measure to
solve the problem of automatically partitioning a large population into
low-faultline teams. By introducing faultlines to the team-formation
literature, our work creates exciting opportunities for algorithmic work on
faultline optimization, as well as on work that combines and studies the
connection of faultlines with other influential team characteristics.
",1,0,0,0,0,0
280,A Survey of Model Compression and Acceleration for Deep Neural Networks,"  Deep convolutional neural networks (CNNs) have recently achieved great
success in many visual recognition tasks. However, existing deep neural network
models are computationally expensive and memory intensive, hindering their
deployment in devices with low memory resources or in applications with strict
latency requirements. Therefore, a natural thought is to perform model
compression and acceleration in deep networks without significantly decreasing
the model performance. During the past few years, tremendous progress has been
made in this area. In this paper, we survey the recent advanced techniques for
compacting and accelerating CNNs model developed. These techniques are roughly
categorized into four schemes: parameter pruning and sharing, low-rank
factorization, transferred/compact convolutional filters, and knowledge
distillation. Methods of parameter pruning and sharing will be described at the
beginning, after that the other techniques will be introduced. For each scheme,
we provide insightful analysis regarding the performance, related applications,
advantages, and drawbacks etc. Then we will go through a few very recent
additional successful methods, for example, dynamic capacity networks and
stochastic depths networks. After that, we survey the evaluation matrix, the
main datasets used for evaluating the model performance and recent benchmarking
efforts. Finally, we conclude this paper, discuss remaining challenges and
possible directions on this topic.
",1,0,0,0,0,0
281,Non-Parametric Calibration of Probabilistic Regression,"  The task of calibration is to retrospectively adjust the outputs from a
machine learning model to provide better probability estimates on the target
variable. While calibration has been investigated thoroughly in classification,
it has not yet been well-established for regression tasks. This paper considers
the problem of calibrating a probabilistic regression model to improve the
estimated probability densities over the real-valued targets. We propose to
calibrate a regression model through the cumulative probability density, which
can be derived from calibrating a multi-class classifier. We provide three
non-parametric approaches to solve the problem, two of which provide empirical
estimates and the third providing smooth density estimates. The proposed
approaches are experimentally evaluated to show their ability to improve the
performance of regression models on the predictive likelihood.
",0,0,0,1,0,0
282,Cyclotron resonant scattering feature simulations. II. Description of the CRSF simulation process,"  Cyclotron resonant scattering features (CRSFs) are formed by scattering of
X-ray photons off quantized plasma electrons in the strong magnetic field (of
the order 10^12 G) close to the surface of an accreting X-ray pulsar. The line
profiles of CRSFs cannot be described by an analytic expression. Numerical
methods such as Monte Carlo (MC) simulations of the scattering processes are
required in order to predict precise line shapes for a given physical setup,
which can be compared to observations to gain information about the underlying
physics in these systems.
A versatile simulation code is needed for the generation of synthetic
cyclotron lines. Sophisticated geometries should be investigatable by making
their simulation possible for the first time.
The simulation utilizes the mean free path tables described in the first
paper of this series for the fast interpolation of propagation lengths. The
code is parallelized to make the very time consuming simulations possible on
convenient time scales. Furthermore, it can generate responses to
mono-energetic photon injections, producing Green's functions, which can be
used later to generate spectra for arbitrary continua.
We develop a new simulation code to generate synthetic cyclotron lines for
complex scenarios, allowing for unprecedented physical interpretation of the
observed data. An associated XSPEC model implementation is used to fit
synthetic line profiles to NuSTAR data of Cep X-4. The code has been developed
with the main goal of overcoming previous geometrical constraints in MC
simulations of CRSFs. By applying this code also to more simple, classic
geometries used in previous works, we furthermore address issues of code
verification and cross-comparison of various models. The XSPEC model and the
Green's function tables are available online at
this http URL .
",0,1,0,0,0,0
283,New quantum mds constacylŽñc codes,"  This paper is devoted to the study of the construction of new quantum MDS
codes. Based on constacyclic codes over Fq2 , we derive four new families of
quantum MDS codes, one of which is an explicit generalization of the
construction given in Theorem 7 in [22]. We also extend the result of Theorem
3:3 given in [17].
",1,0,0,0,0,0
284,Infinitary first-order categorical logic,"  We present a unified categorical treatment of completeness theorems for
several classical and intuitionistic infinitary logics with a proposed
axiomatization. This provides new completeness theorems and subsumes previous
ones by G??del, Kripke, Beth, Karp, Joyal, Makkai and Fourman/Grayson. As an
application we prove, using large cardinals assumptions, the disjunction and
existence properties for infinitary intuitionistic first-order logics.
",0,0,1,0,0,0
285,Stochastic Gradient Monomial Gamma Sampler,"  Recent advances in stochastic gradient techniques have made it possible to
estimate posterior distributions from large datasets via Markov Chain Monte
Carlo (MCMC). However, when the target posterior is multimodal, mixing
performance is often poor. This results in inadequate exploration of the
posterior distribution. A framework is proposed to improve the sampling
efficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A
generalized kinetic function is leveraged, delivering superior stationary
mixing, especially for multimodal distributions. Techniques are also discussed
to overcome the practical issues introduced by this generalization. It is shown
that the proposed approach is better at exploring complex multimodal posterior
distributions, as demonstrated on multiple applications and in comparison with
other stochastic gradient MCMC methods.
",1,0,0,1,0,0
286,Gini estimation under infinite variance,"  We study the problems related to the estimation of the Gini index in presence
of a fat-tailed data generating process, i.e. one in the stable distribution
class with finite mean but infinite variance (i.e. with tail index
$\alpha\in(1,2)$). We show that, in such a case, the Gini coefficient cannot be
reliably estimated using conventional nonparametric methods, because of a
downward bias that emerges under fat tails. This has important implications for
the ongoing discussion about economic inequality.
We start by discussing how the nonparametric estimator of the Gini index
undergoes a phase transition in the symmetry structure of its asymptotic
distribution, as the data distribution shifts from the domain of attraction of
a light-tailed distribution to that of a fat-tailed one, especially in the case
of infinite variance. We also show how the nonparametric Gini bias increases
with lower values of $\alpha$. We then prove that maximum likelihood estimation
outperforms nonparametric methods, requiring a much smaller sample size to
reach efficiency.
Finally, for fat-tailed data, we provide a simple correction mechanism to the
small sample bias of the nonparametric estimator based on the distance between
the mode and the mean of its asymptotic distribution.
",0,0,0,1,0,0
287,Training Neural Networks Using Features Replay,"  Training a neural network using backpropagation algorithm requires passing
error gradients sequentially through the network. The backward locking prevents
us from updating network layers in parallel and fully leveraging the computing
resources. Recently, there are several works trying to decouple and parallelize
the backpropagation algorithm. However, all of them suffer from severe accuracy
loss or memory explosion when the neural network is deep. To address these
challenging issues, we propose a novel parallel-objective formulation for the
objective function of the neural network. After that, we introduce features
replay algorithm and prove that it is guaranteed to converge to critical points
for the non-convex problem under certain conditions. Finally, we apply our
method to training deep convolutional neural networks, and the experimental
results show that the proposed method achieves {faster} convergence, {lower}
memory consumption, and {better} generalization error than compared methods.
",0,0,0,1,0,0
288,The anti-spherical category,"  We study a diagrammatic categorification (the ""anti-spherical category"") of
the anti-spherical module for any Coxeter group. We deduce that Deodhar's
(sign) parabolic Kazhdan-Lusztig polynomials have non-negative coefficients,
and that a monotonicity conjecture of Brenti's holds. The main technical
observation is a localisation procedure for the anti-spherical category, from
which we construct a ""light leaves"" basis of morphisms. Our techniques may be
used to calculate many new elements of the $p$-canonical basis in the
anti-spherical module. The results use generators and relations for Soergel
bimodules (""Soergel calculus"") in a crucial way.
",0,0,1,0,0,0
289,Trajectories and orbital angular momentum of necklace beams in nonlinear colloidal suspensions,"  Recently, we have predicted that the modulation instability of optical vortex
solitons propagating in nonlinear colloidal suspensions with exponential
saturable nonlinearity leads to formation of necklace beams (NBs)
[S.~Z.~Silahli, W.~Walasik and N.~M.~Litchinitser, Opt.~Lett., \textbf{40},
5714 (2015)]. Here, we investigate the dynamics of NB formation and
propagation, and show that the distance at which the NB is formed depends on
the input power of the vortex beam. Moreover, we show that the NB trajectories
are not necessarily tangent to the initial vortex ring, and that their
velocities have components stemming both from the beam diffraction and from the
beam orbital angular momentum. We also demonstrate the generation of twisted
solitons and analyze the influence of losses on their propagation. Finally, we
investigate the conservation of the orbital angular momentum in necklace and
twisted beams. Our studies, performed in ideal lossless media and in realistic
colloidal suspensions with losses, provide a detailed description of NB
dynamics and may be useful in studies of light propagation in highly scattering
colloids and biological samples.
",0,1,0,0,0,0
290,Unified Treatment of Spin Torques using a Coupled Magnetisation Dynamics and Three-Dimensional Spin Current Solver,"  A three-dimensional spin current solver based on a generalised spin
drift-diffusion description, including the spin Hall effect, is integrated with
a magnetisation dynamics solver. The resulting model is shown to simultaneously
reproduce the spin-orbit torques generated using the spin Hall effect, spin
pumping torques generated by magnetisation dynamics in multilayers, as well as
the spin transfer torques acting on magnetisation regions with spatial
gradients, whilst field-like and spin-like torques are reproduced in a spin
valve geometry. Two approaches to modelling interfaces are analysed, one based
on the spin mixing conductance and the other based on continuity of spin
currents where the spin dephasing length governs the absorption of transverse
spin components. In both cases analytical formulas are derived for the
spin-orbit torques in a heavy metal / ferromagnet bilayer geometry, showing in
general both field-like and damping-like torques are generated. The limitations
of the analytical approach are discussed, showing that even in a simple bilayer
geometry, due to the non-uniformity of the spin currents, a full
three-dimensional treatment is required. Finally the model is applied to the
quantitative analysis of the spin Hall angle in Pt by reproducing published
experimental data on the ferromagnetic resonance linewidth in the bilayer
geometry.
",0,1,0,0,0,0
291,A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization,"  This paper aims to explore models based on the extreme gradient boosting
(XGBoost) approach for business risk classification. Feature selection (FS)
algorithms and hyper-parameter optimizations are simultaneously considered
during model training. The five most commonly used FS methods including weight
by Gini, weight by Chi-square, hierarchical variable clustering, weight by
correlation, and weight by information are applied to alleviate the effect of
redundant features. Two hyper-parameter optimization approaches, random search
(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in
XGBoost. The effect of different FS and hyper-parameter optimization methods on
the model performance are investigated by the Wilcoxon Signed Rank Test. The
performance of XGBoost is compared to the traditionally utilized logistic
regression (LR) model in terms of classification accuracy, area under the curve
(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results
show that hierarchical clustering is the optimal FS method for LR while weight
by Chi-square achieves the best performance in XG-Boost. Both TPE and RS
optimization in XGBoost outperform LR significantly. TPE optimization shows a
superiority over RS since it results in a significantly higher accuracy and a
marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE
tuning shows a lower variability than the RS method. Finally, the ranking of
feature importance based on XGBoost enhances the model interpretation.
Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an
operative while powerful approach for business risk modeling.
",1,0,0,1,0,0
292,Rheology of High-Capillary Number Flow in Porous Media,"  Immiscible fluids flowing at high capillary numbers in porous media may be
characterized by an effective viscosity. We demonstrate that the effective
viscosity is well described by the Lichtenecker-Rother equation. The exponent
$\alpha$ in this equation takes either the value 1 or 0.6 in two- and 0.5 in
three-dimensional systems depending on the pore geometry. Our arguments are
based on analytical and numerical methods.
",0,1,0,0,0,0
293,Quantum Charge Pumps with Topological Phases in Creutz Ladder,"  Quantum charge pumping phenomenon connects band topology through the dynamics
of a one-dimensional quantum system. In terms of a microscopic model, the
Su-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful
starting point for many considerations of topological physics. Here we present
a generalized Creutz scheme as a distinct two-band quantum pump model. By
noting that it undergoes two kinds of topological band transitions accompanying
with a Zak-phase-difference of $\pi$ and $2\pi$, respectively, various charge
pumping schemes are studied by applying an elaborate Peierl's phase
substitution. Translating into real space, the transportation of quantized
charges is a result of cooperative quantum interference effect. In particular,
an all-flux quantum pump emerges which operates with time-varying fluxes only
and transports two charge units. This puts cold atoms with artificial gauge
fields as an unique system where this kind of phenomena can be realized.
",0,1,0,0,0,0
294,On Deep Neural Networks for Detecting Heart Disease,"  Heart disease is the leading cause of death, and experts estimate that
approximately half of all heart attacks and strokes occur in people who have
not been flagged as ""at risk."" Thus, there is an urgent need to improve the
accuracy of heart disease diagnosis. To this end, we investigate the potential
of using data analysis, and in particular the design and use of deep neural
networks (DNNs) for detecting heart disease based on routine clinical data. Our
main contribution is the design, evaluation, and optimization of DNN
architectures of increasing depth for heart disease diagnosis. This work led to
the discovery of a novel five layer DNN architecture - named Heart Evaluation
for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields
best prediction accuracy. HEARO-5's design employs regularization optimization
and automatically deals with missing data and/or data outliers. To evaluate and
tune the architectures we use k-way cross-validation as well as Matthews
correlation coefficient (MCC) to measure the quality of our classifications.
The study is performed on the publicly available Cleveland dataset of medical
information, and we are making our developments open source, to further
facilitate openness and research on the use of DNNs in medicine. The HEARO-5
architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms
currently published research in the area.
",1,0,0,1,0,0
295,Exponential Stability Analysis via Integral Quadratic Constraints,"  The theory of integral quadratic constraints (IQCs) allows verification of
stability and gain-bound properties of systems containing nonlinear or
uncertain elements. Gain bounds often imply exponential stability, but it can
be challenging to compute useful numerical bounds on the exponential decay
rate. This work presents a generalization of the classical IQC results of
Megretski and Rantzer that leads to a tractable computational procedure for
finding exponential rate certificates that are far less conservative than ones
computed from $L_2$ gain bounds alone. An expanded library of IQCs for
certifying exponential stability is also provided and the effectiveness of the
technique is demonstrated via numerical examples.
",1,0,1,0,0,0
296,Fermi acceleration of electrons inside foreshock transient cores,"  Foreshock transients upstream of Earth's bow shock have been recently
observed to accelerate electrons to many times their thermal energy. How such
acceleration occurs is unknown, however. Using THEMIS case studies, we examine
a subset of acceleration events (31 of 247 events) in foreshock transients with
cores that exhibit gradual electron energy increases accompanied by low
background magnetic field strength and large-amplitude magnetic fluctuations.
Using the evolution of electron distributions and the energy increase rates at
multiple spacecraft, we suggest that Fermi acceleration between a converging
foreshock transient's compressional boundary and the bow shock is responsible
for the observed electron acceleration. We then show that a one-dimensional
test particle simulation of an ideal Fermi acceleration model in fluctuating
fields prescribed by the observations can reproduce the observed evolution of
electron distributions, energy increase rate, and pitch-angle isotropy,
providing further support for our hypothesis. Thus, Fermi acceleration is
likely the principal electron acceleration mechanism in at least this subset of
foreshock transient cores.
",0,1,0,0,0,0
297,Quantum Speed Limit is Not Quantum,"  The quantum speed limit (QSL), or the energy-time uncertainty relation,
describes the fundamental maximum rate for quantum time evolution and has been
regarded as being unique in quantum mechanics. In this study, we obtain a
classical speed limit corresponding to the QSL using the Hilbert space for the
classical Liouville equation. Thus, classical mechanics has a fundamental speed
limit, and QSL is not a purely quantum phenomenon but a universal dynamical
property of the Hilbert space. Furthermore, we obtain similar speed limits for
the imaginary-time Schroedinger equations such as the master equation.
",0,1,0,0,0,0
298,Adaptive Diffusion Processes of Time-Varying Local Information on Networks,"  This paper mainly discusses the diffusion on complex networks with
time-varying couplings. We propose a model to describe the adaptive diffusion
process of local topological and dynamical information, and find that the
Barabasi-Albert scale-free network (BA network) is beneficial to the diffusion
and leads nodes to arrive at a larger state value than other networks do. The
ability of diffusion for a node is related to its own degree. Specifically,
nodes with smaller degrees are more likely to change their states and reach
larger values, while those with larger degrees tend to stick to their original
states. We introduce state entropy to analyze the thermodynamic mechanism of
the diffusion process, and interestingly find that this kind of diffusion
process is a minimization process of state entropy. We use the inequality
constrained optimization method to reveal the restriction function of the
minimization and find that it has the same form as the Gibbs free energy. The
thermodynamical concept allows us to understand dynamical processes on complex
networks from a brand-new perspective. The result provides a convenient means
of optimizing relevant dynamical processes on practical circuits as well as
related complex systems.
",1,0,0,0,0,0
299,ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary classifier variational autoencoder,"  This paper proposes a non-parallel many-to-many voice conversion (VC) method
using a variant of the conditional variational autoencoder (VAE) called an
auxiliary classifier VAE (ACVAE). The proposed method has three key features.
First, it adopts fully convolutional architectures to construct the encoder and
decoder networks so that the networks can learn conversion rules that capture
time dependencies in the acoustic feature sequences of source and target
speech. Second, it uses an information-theoretic regularization for the model
training to ensure that the information in the attribute class label will not
be lost in the conversion process. With regular CVAEs, the encoder and decoder
are free to ignore the attribute class label input. This can be problematic
since in such a situation, the attribute class label will have little effect on
controlling the voice characteristics of input speech at test time. Such
situations can be avoided by introducing an auxiliary classifier and training
the encoder and decoder so that the attribute classes of the decoder outputs
are correctly predicted by the classifier. Third, it avoids producing
buzzy-sounding speech at test time by simply transplanting the spectral details
of the input speech into its converted version. Subjective evaluation
experiments revealed that this simple method worked reasonably well in a
non-parallel many-to-many speaker identity conversion task.
",1,0,0,1,0,0
300,Spectral analysis of jet turbulence,"  Informed by LES data and resolvent analysis of the mean flow, we examine the
structure of turbulence in jets in the subsonic, transonic, and supersonic
regimes. Spectral (frequency-space) proper orthogonal decomposition is used to
extract energy spectra and decompose the flow into energy-ranked coherent
structures. The educed structures are generally well predicted by the resolvent
analysis. Over a range of low frequencies and the first few azimuthal mode
numbers, these jets exhibit a low-rank response characterized by
Kelvin-Helmholtz (KH) type wavepackets associated with the annular shear layer
up to the end of the potential core and that are excited by forcing in the
very-near-nozzle shear layer. These modes too the have been experimentally
observed before and predicted by quasi-parallel stability theory and other
approximations--they comprise a considerable portion of the total turbulent
energy. At still lower frequencies, particularly for the axisymmetric mode, and
again at high frequencies for all azimuthal wavenumbers, the response is not
low rank, but consists of a family of similarly amplified modes. These modes,
which are primarily active downstream of the potential core, are associated
with the Orr mechanism. They occur also as sub-dominant modes in the range of
frequencies dominated by the KH response. Our global analysis helps tie
together previous observations based on local spatial stability theory, and
explains why quasi-parallel predictions were successful at some frequencies and
azimuthal wavenumbers, but failed at others.
",0,1,0,0,0,0
301,A dual framework for low-rank tensor completion,"  One of the popular approaches for low-rank tensor completion is to use the
latent trace norm regularization. However, most existing works in this
direction learn a sparse combination of tensors. In this work, we fill this gap
by proposing a variant of the latent trace norm that helps in learning a
non-sparse combination of tensors. We develop a dual framework for solving the
low-rank tensor completion problem. We first show a novel characterization of
the dual solution space with an interesting factorization of the optimal
solution. Overall, the optimal solution is shown to lie on a Cartesian product
of Riemannian manifolds. Furthermore, we exploit the versatile Riemannian
optimization framework for proposing computationally efficient trust region
algorithm. The experiments illustrate the efficacy of the proposed algorithm on
several real-world datasets across applications.
",1,0,0,1,0,0
302,La notion d'involution dans le Brouillon Project de Girard Desargues,"  Nous tentons dans cet article de proposer une th??se coh??rente concernant
la formation de la notion d'involution dans le Brouillon Project de Desargues.
Pour cela, nous donnons une analyse d??taill??e des dix premi??res pages
dudit Brouillon, comprenant les d??veloppements de cas particuliers qui aident
?ÿ comprendre l'intention de Desargues. Nous mettons cette analyse en regard
de la lecture qu'en fait Jean de Beaugrand et que l'on trouve dans les Advis
Charitables.
The purpose of this article is to propose a coherent thesis on how Girard
Desargues arrived at the notion of involution in his Brouillon Project of 1639.
To this purpose we give a detailed analysis of the ten first pages of the
Brouillon, including developments of particular cases which help to understand
the goal of Desargues, as well as to clarify the links between the notion of
involution and that of harmonic division. We compare the conclusions of this
analysis with the very critical reading Jean de Beaugrand made of the Brouillon
Project in the Advis Charitables of 1640.
",0,0,1,0,0,0
303,Framing U-Net via Deep Convolutional Framelets: Application to Sparse-view CT,"  X-ray computed tomography (CT) using sparse projection views is a recent
approach to reduce the radiation dose. However, due to the insufficient
projection views, an analytic reconstruction approach using the filtered back
projection (FBP) produces severe streaking artifacts. Recently, deep learning
approaches using large receptive field neural networks such as U-Net have
demonstrated impressive performance for sparse- view CT reconstruction.
However, theoretical justification is still lacking. Inspired by the recent
theory of deep convolutional framelets, the main goal of this paper is,
therefore, to reveal the limitation of U-Net and propose new multi-resolution
deep learning schemes. In particular, we show that the alternative U- Net
variants such as dual frame and the tight frame U-Nets satisfy the so-called
frame condition which make them better for effective recovery of high frequency
edges in sparse view- CT. Using extensive experiments with real patient data
set, we demonstrate that the new network architectures provide better
reconstruction performance.
",1,0,0,1,0,0
304,Lie $\infty$-algebroids and singular foliations,"  A singular (or Hermann) foliation on a smooth manifold $M$ can be seen as a
subsheaf of the sheaf $\mathfrak{X}$ of vector fields on $M$. We show that if
this singular foliation admits a resolution (in the sense of sheaves)
consisting of sections of a graded vector bundle of finite type, then one can
lift the Lie bracket of vector fields to a Lie $\infty$-algebroid structure on
this resolution, that we call a universal Lie $\infty$-algebroid associated to
the foliation. The name is justified because it is isomorphic (up to homotopy)
to any other Lie $\infty$-algebroid structure built on any other resolution of
the given singular foliation.
",0,0,1,0,0,0
305,Distinct evolutions of Weyl fermion quasiparticles and Fermi arcs with bulk band topology in Weyl semimetals,"  The Weyl semimetal phase is a recently discovered topological quantum state
of matter characterized by the presence of topologically protected degeneracies
near the Fermi level. These degeneracies are the source of exotic phenomena,
including the realization of chiral Weyl fermions as quasiparticles in the bulk
and the formation of Fermi arc states on the surfaces. Here, we demonstrate
that these two key signatures show distinct evolutions with the bulk band
topology by performing angle-resolved photoemission spectroscopy, supported by
first-principle calculations, on transition-metal monophosphides. While Weyl
fermion quasiparticles exist only when the chemical potential is located
between two saddle points of the Weyl cone features, the Fermi arc states
extend in a larger energy scale and are robust across the bulk Lifshitz
transitions associated with the recombination of two non-trivial Fermi surfaces
enclosing one Weyl point into a single trivial Fermi surface enclosing two Weyl
points of opposite chirality. Therefore, in some systems (e.g. NbP),
topological Fermi arc states are preserved even if Weyl fermion quasiparticles
are absent in the bulk. Our findings not only provide insight into the
relationship between the exotic physical phenomena and the intrinsic bulk band
topology in Weyl semimetals, but also resolve the apparent puzzle of the
different magneto-transport properties observed in TaAs, TaP and NbP, where the
Fermi arc states are similar.
",0,1,0,0,0,0
306,Assessing inter-modal and inter-regional dependencies in prodromal Alzheimer's disease using multimodal MRI/PET and Gaussian graphical models,"  A sequence of pathological changes takes place in Alzheimer's disease, which
can be assessed in vivo using various brain imaging methods. Currently, there
is no appropriate statistical model available that can easily integrate
multiple imaging modalities, being able to utilize the additional information
provided from the combined data. We applied Gaussian graphical models (GGMs)
for analyzing the conditional dependency networks of multimodal neuroimaging
data and assessed alterations of the network structure in mild cognitive
impairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy
controls.
Data from N=667 subjects were obtained from the Alzheimer's Disease
Neuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism
(FDG-PET), and gray matter volume (MRI) was calculated for each brain region.
Separate GGMs were estimated using a Bayesian framework for the combined
multimodal data for each diagnostic category. Graph-theoretical statistics were
calculated to determine network alterations associated with disease severity.
Network measures clustering coefficient, path length and small-world
coefficient were significantly altered across diagnostic groups, with a
biphasic u-shape trajectory, i.e. increased small-world coefficient in early
MCI, intermediate values in late MCI, and decreased values in AD patients
compared to controls. In contrast, no group differences were found for
clustering coefficient and small-world coefficient when estimating conditional
dependency networks on single imaging modalities.
GGMs provide a useful methodology to analyze the conditional dependency
networks of multimodal neuroimaging data.
",0,0,0,1,1,0
307,On the economics of knowledge creation and sharing,"  This work bridges the technical concepts underlying distributed computing and
blockchain technologies with their profound socioeconomic and sociopolitical
implications, particularly on academic research and the healthcare industry.
Several examples from academia, industry, and healthcare are explored
throughout this paper. The limiting factor in contemporary life sciences
research is often funding: for example, to purchase expensive laboratory
equipment and materials, to hire skilled researchers and technicians, and to
acquire and disseminate data through established academic channels. In the case
of the U.S. healthcare system, hospitals generate massive amounts of data, only
a small minority of which is utilized to inform current and future medical
practice. Similarly, corporations too expend large amounts of money to collect,
secure and transmit data from one centralized source to another. In all three
scenarios, data moves under the traditional paradigm of centralization, in
which data is hosted and curated by individuals and organizations and of
benefit to only a small subset of people.
",1,0,0,0,0,0
308,Seismic fragility curves for structures using non-parametric representations,"  Fragility curves are commonly used in civil engineering to assess the
vulnerability of structures to earthquakes. The probability of failure
associated with a prescribed criterion (e.g. the maximal inter-storey drift of
a building exceeding a certain threshold) is represented as a function of the
intensity of the earthquake ground motion (e.g. peak ground acceleration or
spectral acceleration). The classical approach relies on assuming a lognormal
shape of the fragility curves; it is thus parametric. In this paper, we
introduce two non-parametric approaches to establish the fragility curves
without employing the above assumption, namely binned Monte Carlo simulation
and kernel density estimation. As an illustration, we compute the fragility
curves for a three-storey steel frame using a large number of synthetic ground
motions. The curves obtained with the non-parametric approaches are compared
with respective curves based on the lognormal assumption. A similar comparison
is presented for a case when a limited number of recorded ground motions is
available. It is found that the accuracy of the lognormal curves depends on the
ground motion intensity measure, the failure criterion and most importantly, on
the employed method for estimating the parameters of the lognormal shape.
",0,0,0,1,0,0
309,Metastable Markov chains: from the convergence of the trace to the convergence of the finite-dimensional distributions,"  We consider continuous-time Markov chains which display a family of wells at
the same depth. We provide sufficient conditions which entail the convergence
of the finite-dimensional distributions of the order parameter to the ones of a
finite state Markov chain. We also show that the state of the process can be
represented as a time-dependent convex combination of metastable states, each
of which is supported on one well.
",0,0,1,0,0,0
310,Construction of embedded periodic surfaces in $\mathbb{R}^n$,"  We construct embedded minimal surfaces which are $n$-periodic in
$\mathbb{R}^n$. They are new for codimension $n-2\ge 2$. We start with a Jordan
curve of edges of the $n$-dimensional cube. It bounds a Plateau minimal disk
which Schwarz reflection extends to a complete minimal surface. Studying the
group of Schwarz reflections, we can characterize those Jordan curves for which
the complete surface is embedded. For example, for $n=4$ exactly five such
Jordan curves generate embedded surfaces. Our results apply to surface classes
other than minimal as well, for instance polygonal surfaces.
",0,0,1,0,0,0
311,A System of Three Super Earths Transiting the Late K-Dwarf GJ 9827 at Thirty Parsecs,"  We report the discovery of three small transiting planets orbiting GJ 9827, a
bright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\pm0.11$
$R_{\rm \oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$
$R_{\rm \oplus}$ super Earth on a 3.6 day period, and a $2.07\pm0.14$ $R_{\rm
\oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting
GJ 9827 span the transition between predominantly rocky and gaseous planets,
and GJ 9827 b and c fall in or close to the known gap in the radius
distribution of small planets between these populations. At a distance of 30
parsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making
these planets well-suited for atmospheric studies with the upcoming James Webb
Space Telescope. The GJ 9827 system provides a valuable opportunity to
characterize interior structure and atmospheric properties of coeval planets
spanning the rocky to gaseous transition.
",0,1,0,0,0,0
312,State Sum Invariants of Three Manifolds from Spherical Multi-fusion Categories,"  We define a family of quantum invariants of closed oriented $3$-manifolds
using spherical multi-fusion categories. The state sum nature of this invariant
leads directly to $(2+1)$-dimensional topological quantum field theories
($\text{TQFT}$s), which generalize the Turaev-Viro-Barrett-Westbury
($\text{TVBW}$) $\text{TQFT}$s from spherical fusion categories. The invariant
is given as a state sum over labeled triangulations, which is mostly parallel
to, but richer than the $\text{TVBW}$ approach in that here the labels live not
only on $1$-simplices but also on $0$-simplices. It is shown that a
multi-fusion category in general cannot be a spherical fusion category in the
usual sense. Thus we introduce the concept of a spherical multi-fusion category
by imposing a weakened version of sphericity. Besides containing the
$\text{TVBW}$ theory, our construction also includes the recent higher gauge
theory $(2+1)$-$\text{TQFT}$s given by Kapustin and Thorngren, which was not
known to have a categorical origin before.
",0,1,1,0,0,0
313,Sparse Neural Networks Topologies,"  We propose Sparse Neural Network architectures that are based on random or
structured bipartite graph topologies. Sparse architectures provide compression
of the models learned and speed-ups of computations, they can also surpass
their unstructured or fully connected counterparts. As we show, even more
compact topologies of the so-called SNN (Sparse Neural Network) can be achieved
with the use of structured graphs of connections between consecutive layers of
neurons. In this paper, we investigate how the accuracy and training speed of
the models depend on the topology and sparsity of the neural network. Previous
approaches using sparcity are all based on fully connected neural network
models and create sparcity during training phase, instead we explicitly define
a sparse architectures of connections before the training. Building compact
neural network models is coherent with empirical observations showing that
there is much redundancy in learned neural network models. We show
experimentally that the accuracy of the models learned with neural networks
depends on expander-like properties of the underlying topologies such as the
spectral gap and algebraic connectivity rather than the density of the graphs
of connections.
",1,0,0,1,0,0
314,Human perception in computer vision,"  Computer vision has made remarkable progress in recent years. Deep neural
network (DNN) models optimized to identify objects in images exhibit
unprecedented task-trained accuracy and, remarkably, some generalization
ability: new visual problems can now be solved more easily based on previous
learning. Biological vision (learned in life and through evolution) is also
accurate and general-purpose. Is it possible that these different learning
regimes converge to similar problem-dependent optimal computations? We
therefore asked whether the human system-level computation of visual perception
has DNN correlates and considered several anecdotal test cases. We found that
perceptual sensitivity to image changes has DNN mid-computation correlates,
while sensitivity to segmentation, crowding and shape has DNN end-computation
correlates. Our results quantify the applicability of using DNN computation to
estimate perceptual loss, and are consistent with the fascinating theoretical
view that properties of human perception are a consequence of
architecture-independent visual learning.
",1,0,0,0,0,0
315,Analyses and estimation of certain design parameters of micro-grooved heat pipes,"  A numerical analysis of heat conduction through the cover plate of a heat
pipe is carried out to determine the temperature of the working substance,
average temperature of heating and cooling surfaces, heat spread in the
transmitter, and the heat bypass through the cover plate. Analysis has been
extended for the estimation of heat transfer requirements at the outer surface
of the con- denser under different heat load conditions using Genetic
Algorithm. This paper also presents the estimation of an average heat transfer
coefficient for the boiling and condensation of the working substance inside
the microgrooves corresponding to a known temperature of the heat source. The
equation of motion of the working fluid in the meniscus of an equilateral
triangular groove has been presented from which a new term called the minimum
surface tension required for avoiding the dry out condition is defined.
Quantitative results showing the effect of thickness of cover plate, heat load,
angle of inclination and viscosity of the working fluid on the different
aspects of the heat transfer, minimum surface tension required to avoid dry
out, velocity distribution of the liquid, and radius of liquid meniscus inside
the micro-grooves have been presented and discussed.
",0,1,0,0,0,0
316,"Merge decompositions, two-sided Krohn-Rhodes, and aperiodic pointlikes","  This paper provides short proofs of two fundamental theorems of finite
semigroup theory whose previous proofs were significantly longer, namely the
two-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike
theorem, using a new algebraic technique that we call the merge decomposition.
A prototypical application of this technique decomposes a semigroup $T$ into a
two-sided semidirect product whose components are built from two subsemigroups
$T_1,T_2$, which together generate $T$, and the subsemigroup generated by their
setwise product $T_1T_2$. In this sense we decompose $T$ by merging the
subsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup
homomorphisms from free semigroups.
",1,0,1,0,0,0
317,Deep Multimodal Image-Repurposing Detection,"  Nefarious actors on social media and other platforms often spread rumors and
falsehoods through images whose metadata (e.g., captions) have been modified to
provide visual substantiation of the rumor/falsehood. This type of modification
is referred to as image repurposing, in which often an unmanipulated image is
published along with incorrect or manipulated metadata to serve the actor's
ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)
dataset, a substantially challenging dataset over that which has been
previously available to support research into image repurposing detection. The
new dataset includes location, person, and organization manipulations on
real-world data sourced from Flickr. We also present a novel, end-to-end, deep
multimodal learning model for assessing the integrity of an image by combining
information extracted from the image with related information from a knowledge
base. The proposed method is compared against state-of-the-art techniques on
existing datasets as well as MEIR, where it outperforms existing methods across
the board, with AUC improvement up to 0.23.
",1,0,0,0,0,0
318,DeepFense: Online Accelerated Defense Against Adversarial Deep Learning,"  Recent advances in adversarial Deep Learning (DL) have opened up a largely
unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. With the wide-spread usage of DL in critical and
time-sensitive applications, including unmanned vehicles, drones, and video
surveillance systems, online detection of malicious inputs is of utmost
importance. We propose DeepFense, the first end-to-end automated framework that
simultaneously enables efficient and safe execution of DL models. DeepFense
formalizes the goal of thwarting adversarial attacks as an optimization problem
that minimizes the rarely observed regions in the latent feature space spanned
by a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples in parallel with the victim DL model. DeepFense
leverages hardware/software/algorithm co-design and customized acceleration to
achieve just-in-time performance in resource-constrained settings. The proposed
countermeasure is unsupervised, meaning that no adversarial sample is leveraged
to train modular redundancies. We further provide an accompanying API to reduce
the non-recurring engineering cost and ensure automated adaptation to various
platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
of magnitude performance improvement while enabling online adversarial sample
detection.
",1,0,0,1,0,0
319,Retrospective Higher-Order Markov Processes for User Trails,"  Users form information trails as they browse the web, checkin with a
geolocation, rate items, or consume media. A common problem is to predict what
a user might do next for the purposes of guidance, recommendation, or
prefetching. First-order and higher-order Markov chains have been widely used
methods to study such sequences of data. First-order Markov chains are easy to
estimate, but lack accuracy when history matters. Higher-order Markov chains,
in contrast, have too many parameters and suffer from overfitting the training
data. Fitting these parameters with regularization and smoothing only offers
mild improvements. In this paper we propose the retrospective higher-order
Markov process (RHOMP) as a low-parameter model for such sequences. This model
is a special case of a higher-order Markov chain where the transitions depend
retrospectively on a single history state instead of an arbitrary combination
of history states. There are two immediate computational advantages: the number
of parameters is linear in the order of the Markov chain and the model can be
fit to large state spaces. Furthermore, by providing a specific structure to
the higher-order chain, RHOMPs improve the model accuracy by efficiently
utilizing history states without risks of overfitting the data. We demonstrate
how to estimate a RHOMP from data and we demonstrate the effectiveness of our
method on various real application datasets spanning geolocation data, review
sequences, and business locations. The RHOMP model uniformly outperforms
higher-order Markov chains, Kneser-Ney regularization, and tensor
factorizations in terms of prediction accuracy.
",1,0,0,1,0,0
320,Frank-Wolfe with Subsampling Oracle,"  We analyze two novel randomized variants of the Frank-Wolfe (FW) or
conditional gradient algorithm. While classical FW algorithms require solving a
linear minimization problem over the domain at each iteration, the proposed
method only requires to solve a linear minimization problem over a small
\emph{subset} of the original domain. The first algorithm that we propose is a
randomized variant of the original FW algorithm and achieves a
$\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic
counterpart. The second algorithm is a randomized variant of the Away-step FW
algorithm, and again as its deterministic counterpart, reaches linear (i.e.,
exponential) convergence rate making it the first provably convergent
randomized variant of Away-step FW. In both cases, while subsampling reduces
the convergence rate by a constant factor, the linear minimization step can be
a fraction of the cost of that of the deterministic versions, especially when
the data is streamed. We illustrate computational gains of the algorithms on
regression problems, involving both $\ell_1$ and latent group lasso penalties.
",0,0,0,1,0,0
321,A mean value formula and a Liouville theorem for the complex Monge-Amp??re equation,"  In this paper, we prove a mean value formula for bounded subharmonic
Hermitian matrix valued function on a complete Riemannian manifold with
nonnegative Ricci curvature. As its application, we obtain a Liouville type
theorem for the complex Monge-Amp??re equation on product manifolds.
",0,0,1,0,0,0
322,Bayesian Image Quality Transfer with CNNs: Exploring Uncertainty in dMRI Super-Resolution,"  In this work, we investigate the value of uncertainty modeling in 3D
super-resolution with convolutional neural networks (CNNs). Deep learning has
shown success in a plethora of medical image transformation problems, such as
super-resolution (SR) and image synthesis. However, the highly ill-posed nature
of such problems results in inevitable ambiguity in the learning of networks.
We propose to account for intrinsic uncertainty through a per-patch
heteroscedastic noise model and for parameter uncertainty through approximate
Bayesian inference in the form of variational dropout. We show that the
combined benefits of both lead to the state-of-the-art performance SR of
diffusion MR brain images in terms of errors compared to ground truth. We
further show that the reduced error scores produce tangible benefits in
downstream tractography. In addition, the probabilistic nature of the methods
naturally confers a mechanism to quantify uncertainty over the super-resolved
output. We demonstrate through experiments on both healthy and pathological
brains the potential utility of such an uncertainty measure in the risk
assessment of the super-resolved images for subsequent clinical use.
",1,0,0,0,0,0
323,Yu-Shiba-Rusinov bands in superconductors in contact with a magnetic insulator,"  Superconductor-Ferromagnet (SF) heterostructures are of interest due to
numerous phenomena related to the spin-dependent interaction of Cooper pairs
with the magnetization. Here we address the effects of a magnetic insulator on
the density of states of a superconductor based on a recently developed
boundary condition for strongly spin-dependent interfaces. We show that the
boundary to a magnetic insulator has a similar effect like the presence of
magnetic impurities. In particular we find that the impurity effects of
strongly scattering localized spins leading to the formation of Shiba bands can
be mapped onto the boundary problem.
",0,1,0,0,0,0
324,Bayesian Optimization for Probabilistic Programs,"  We present the first general purpose framework for marginal maximum a
posteriori estimation of probabilistic program variables. By using a series of
code transformations, the evidence of any probabilistic program, and therefore
of any graphical model, can be optimized with respect to an arbitrary subset of
its sampled variables. To carry out this optimization, we develop the first
Bayesian optimization package to directly exploit the source code of its
target, leading to innovations in problem-independent hyperpriors, unbounded
optimization, and implicit constraint satisfaction; delivering significant
performance improvements over prominent existing packages. We present
applications of our method to a number of tasks including engineering design
and parameter optimization.
",1,0,0,1,0,0
325,Long-Term Load Forecasting Considering Volatility Using Multiplicative Error Model,"  Long-term load forecasting plays a vital role for utilities and planners in
terms of grid development and expansion planning. An overestimate of long-term
electricity load will result in substantial wasted investment in the
construction of excess power facilities, while an underestimate of future load
will result in insufficient generation and unmet demand. This paper presents
first-of-its-kind approach to use multiplicative error model (MEM) in
forecasting load for long-term horizon. MEM originates from the structure of
autoregressive conditional heteroscedasticity (ARCH) model where conditional
variance is dynamically parameterized and it multiplicatively interacts with an
innovation term of time-series. Historical load data, accessed from a U.S.
regional transmission operator, and recession data for years 1993-2016 is used
in this study. The superiority of considering volatility is proven by
out-of-sample forecast results as well as directional accuracy during the great
economic recession of 2008. To incorporate future volatility, backtesting of
MEM model is performed. Two performance indicators used to assess the proposed
model are mean absolute percentage error (for both in-sample model fit and
out-of-sample forecasts) and directional accuracy.
",0,0,0,1,0,0
326,Geometrically stopped Markovian random growth processes and Pareto tails,"  Many empirical studies document power law behavior in size distributions of
economic interest such as cities, firms, income, and wealth. One mechanism for
generating such behavior combines independent and identically distributed
Gaussian additive shocks to log-size with a geometric age distribution. We
generalize this mechanism by allowing the shocks to be non-Gaussian (but
light-tailed) and dependent upon a Markov state variable. Our main results
provide sharp bounds on tail probabilities and simple formulas for Pareto
exponents. We present two applications: (i) we show that the tails of the
wealth distribution in a heterogeneous-agent dynamic general equilibrium model
with idiosyncratic endowment risk decay exponentially, unlike models with
investment risk where the tails may be Paretian, and (ii) we show that a random
growth model for the population dynamics of Japanese prefectures is consistent
with the observed Pareto exponent but only after allowing for Markovian
dynamics.
",0,0,1,0,0,0
327,Mathematics of Topological Quantum Computing,"  In topological quantum computing, information is encoded in ""knotted"" quantum
states of topological phases of matter, thus being locked into topology to
prevent decay. Topological precision has been confirmed in quantum Hall liquids
by experiments to an accuracy of $10^{-10}$, and harnessed to stabilize quantum
memory. In this survey, we discuss the conceptual development of this
interdisciplinary field at the juncture of mathematics, physics and computer
science. Our focus is on computing and physical motivations, basic mathematical
notions and results, open problems and future directions related to and/or
inspired by topological quantum computing.
",0,1,1,0,0,0
328,Graph Clustering using Effective Resistance,"  $ \def\vecc#1{\boldsymbol{#1}} $We design a polynomial time algorithm that
for any weighted undirected graph $G = (V, E,\vecc w)$ and sufficiently large
$\delta > 1$, partitions $V$ into subsets $V_1, \ldots, V_h$ for some $h\geq
1$, such that
$\bullet$ at most $\delta^{-1}$ fraction of the weights are between clusters,
i.e. \[ w(E - \cup_{i = 1}^h E(V_i)) \lesssim \frac{w(E)}{\delta};\]
$\bullet$ the effective resistance diameter of each of the induced subgraphs
$G[V_i]$ is at most $\delta^3$ times the average weighted degree, i.e. \[
\max_{u, v \in V_i} \mathsf{Reff}_{G[V_i]}(u, v) \lesssim \delta^3 \cdot
\frac{|V|}{w(E)} \quad \text{ for all } i=1, \ldots, h.\]
In particular, it is possible to remove one percent of weight of edges of any
given graph such that each of the resulting connected components has effective
resistance diameter at most the inverse of the average weighted degree.
Our proof is based on a new connection between effective resistance and low
conductance sets. We show that if the effective resistance between two vertices
$u$ and $v$ is large, then there must be a low conductance cut separating $u$
from $v$. This implies that very mildly expanding graphs have constant
effective resistance diameter. We believe that this connection could be of
independent interest in algorithm design.
",1,0,0,0,0,0
329,Self-supervised learning: When is fusion of the primary and secondary sensor cue useful?,"  Self-supervised learning (SSL) is a reliable learning mechanism in which a
robot enhances its perceptual capabilities. Typically, in SSL a trusted,
primary sensor cue provides supervised training data to a secondary sensor cue.
In this article, a theoretical analysis is performed on the fusion of the
primary and secondary cue in a minimal model of SSL. A proof is provided that
determines the specific conditions under which it is favorable to perform
fusion. In short, it is favorable when (i) the prior on the target value is
strong or (ii) the secondary cue is sufficiently accurate. The theoretical
findings are validated with computational experiments. Subsequently, a
real-world case study is performed to investigate if fusion in SSL is also
beneficial when assumptions of the minimal model are not met. In particular, a
flying robot learns to map pressure measurements to sonar height measurements
and then fuses the two, resulting in better height estimation. Fusion is also
beneficial in the opposite case, when pressure is the primary cue. The analysis
and results are encouraging to study SSL fusion also for other robots and
sensors.
",1,0,0,0,0,0
330,Playing Atari with Six Neurons,"  Deep reinforcement learning on Atari games maps pixel directly to actions;
internally, the deep neural network bears the responsibility of both extracting
useful information and making decisions based on it. Aiming at devoting entire
deep networks to decision making alone, we propose a new method for learning
policies and compact state representations separately but simultaneously for
policy approximation in reinforcement learning. State representations are
generated by a novel algorithm based on Vector Quantization and Sparse Coding,
trained online along with the network, and capable of growing its dictionary
size over time. We also introduce new techniques allowing both the neural
network and the evolution strategy to cope with varying dimensions. This
enables networks of only 6 to 18 neurons to learn to play a selection of Atari
games with performance comparable---and occasionally superior---to
state-of-the-art techniques using evolution strategies on deep networks two
orders of magnitude larger.
",0,0,0,1,0,0
331,Contraction and uniform convergence of isotonic regression,"  We consider the problem of isotonic regression, where the underlying signal
$x$ is assumed to satisfy a monotonicity constraint, that is, $x$ lies in the
cone $\{ x\in\mathbb{R}^n : x_1 \leq \dots \leq x_n\}$. We study the isotonic
projection operator (projection to this cone), and find a necessary and
sufficient condition characterizing all norms with respect to which this
projection is contractive. This enables a simple and non-asymptotic analysis of
the convergence properties of isotonic regression, yielding uniform confidence
bands that adapt to the local Lipschitz properties of the signal.
",0,0,1,1,0,0
332,A Systematic Approach for Exploring Tradeoffs in Predictive HVAC Control Systems for Buildings,"  Heating, Ventilation, and Cooling (HVAC) systems are often the most
significant contributor to the energy usage, and the operational cost, of large
office buildings. Therefore, to understand the various factors affecting the
energy usage, and to optimize the operational efficiency of building HVAC
systems, energy analysts and architects often create simulations (e.g.,
EnergyPlus or DOE-2), of buildings prior to construction or renovation to
determine energy savings and quantify the Return-on-Investment (ROI). While
useful, these simulations usually use static HVAC control strategies such as
lowering room temperature at night, or reactive control based on simulated room
occupancy. Recently, advances have been made in HVAC control algorithms that
predict room occupancy. However, these algorithms depend on costly sensor
installations and the tradeoffs between predictive accuracy, energy savings,
comfort and expenses are not well understood. Current simulation frameworks do
not support easy analysis of these tradeoffs. Our contribution is a simulation
framework that can be used to explore this design space by generating objective
estimates of the energy savings and occupant comfort for different levels of
HVAC prediction and control performance. We validate our framework on a
real-world occupancy dataset spanning 6 months for 235 rooms in a large
university office building. Using the gold standard of energy use modeling and
simulation (Revit and Energy Plus), we compare the energy consumption and
occupant comfort in 29 independent simulations that explore our parameter
space. Our results highlight a number of potentially useful tradeoffs with
respect to energy savings, comfort, and algorithmic performance among
predictive, reactive, and static schedules, for a stakeholder of our building.
",1,0,0,0,0,0
333,On types of degenerate critical points of real polynomial functions,"  In this paper, we consider the problem of identifying the type (local
minimizer, maximizer or saddle point) of a given isolated real critical point
$c$, which is degenerate, of a multivariate polynomial function $f$. To this
end, we introduce the definition of faithful radius of $c$ by means of the
curve of tangency of $f$. We show that the type of $c$ can be determined by the
global extrema of $f$ over the Euclidean ball centered at $c$ with a faithful
radius.We propose algorithms to compute a faithful radius of $c$ and determine
its type.
",0,0,1,0,0,0
334,Contribution of Data Categories to Readmission Prediction Accuracy,"  Identification of patients at high risk for readmission could help reduce
morbidity and mortality as well as healthcare costs. Most of the existing
studies on readmission prediction did not compare the contribution of data
categories. In this study we analyzed relative contribution of 90,101 variables
across 398,884 admission records corresponding to 163,468 patients, including
patient demographics, historical hospitalization information, discharge
disposition, diagnoses, procedures, medications and laboratory test results. We
established an interpretable readmission prediction model based on Logistic
Regression in scikit-learn, and added the available variables to the model one
by one in order to analyze the influences of individual data categories on
readmission prediction accuracy. Diagnosis related groups (c-statistic
increment of 0.0933) and discharge disposition (c-statistic increment of
0.0269) were the strongest contributors to model accuracy. Additionally, we
also identified the top ten contributing variables in every data category.
",0,0,0,0,1,0
335,Tropical recurrent sequences,"  Tropical recurrent sequences are introduced satisfying a given vector (being
a tropical counterpart of classical linear recurrent sequences). We consider
the case when Newton polygon of the vector has a single (bounded) edge. In this
case there are periodic tropical recurrent sequences which are similar to
classical linear recurrent sequences. A question is studied when there exists a
non-periodic tropical recurrent sequence satisfying a given vector, and partial
answers are provided to this question. Also an algorithm is designed which
tests existence of non-periodic tropical recurrent sequences satisfying a given
vector with integer coordinates. Finally, we introduce a tropical entropy of a
vector and provide some bounds on it.
",1,0,0,0,0,0
336,Invariance of Weight Distributions in Rectified MLPs,"  An interesting approach to analyzing neural networks that has received
renewed attention is to examine the equivalent kernel of the neural network.
This is based on the fact that a fully connected feedforward network with one
hidden layer, a certain weight distribution, an activation function, and an
infinite number of neurons can be viewed as a mapping into a Hilbert space. We
derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for
all rotationally-invariant weight distributions, generalizing a previous result
that required Gaussian weight distributions. Additionally, the Central Limit
Theorem is used to show that for certain activation functions, kernels
corresponding to layers with weight distributions having $0$ mean and finite
absolute third moment are asymptotically universal, and are well approximated
by the kernel corresponding to layers with spherical Gaussian weights. In deep
networks, as depth increases the equivalent kernel approaches a pathological
fixed point, which can be used to argue why training randomly initialized
networks can be difficult. Our results also have implications for weight
initialization.
",1,0,0,1,0,0
337,Learning to Adapt by Minimizing Discrepancy,"  We explore whether useful temporal neural generative models can be learned
from sequential data without back-propagation through time. We investigate the
viability of a more neurocognitively-grounded approach in the context of
unsupervised generative modeling of sequences. Specifically, we build on the
concept of predictive coding, which has gained influence in cognitive science,
in a neural framework. To do so we develop a novel architecture, the Temporal
Neural Coding Network, and its learning algorithm, Discrepancy Reduction. The
underlying directed generative model is fully recurrent, meaning that it
employs structural feedback connections and temporal feedback connections,
yielding information propagation cycles that create local learning signals.
This facilitates a unified bottom-up and top-down approach for information
transfer inside the architecture. Our proposed algorithm shows promise on the
bouncing balls generative modeling problem. Further experiments could be
conducted to explore the strengths and weaknesses of our approach.
",1,0,0,1,0,0
338,Incarnation of Majorana Fermions in Kitaev Quantum Spin Lattice,"  Kitaev quantum spin liquid is a topological magnetic quantum state
characterized by Majorana fermions of fractionalized spin excitations, which
are identical to their own antiparticles. Here, we demonstrate emergence of
Majorana fermions thermally fractionalized in the Kitaev honeycomb spin lattice
{\alpha}-RuCl3. The specific heat data unveil the characteristic two-stage
release of magnetic entropy involving localized and itinerant Majorana
fermions. The inelastic neutron scattering results further corroborate these
two distinct fermions by exhibiting quasielastic excitations at low energies
around the Brillouin zone center and Y-shaped magnetic continuum at high
energies, which are evident for the ferromagnetic Kitaev model. Our results
provide an opportunity to build a unified conceptual framework of
fractionalized excitations, applicable also for the quantum Hall states,
superconductors, and frustrated magnets.
",0,1,0,0,0,0
339,Haro 11: Where is the Lyman continuum source?,"  Identifying the mechanism by which high energy Lyman continuum (LyC) photons
escaped from early galaxies is one of the most pressing questions in cosmic
evolution. Haro 11 is the best known local LyC leaking galaxy, providing an
important opportunity to test our understanding of LyC escape. The observed LyC
emission in this galaxy presumably originates from one of the three bright,
photoionizing knots known as A, B, and C. It is known that Knot C has strong
Ly$\alpha$ emission, and Knot B hosts an unusually bright ultraluminous X-ray
source, which may be a low-luminosity AGN. To clarify the LyC source, we carry
out ionization-parameter mapping (IPM) by obtaining narrow-band imaging from
the Hubble Space Telescope WFC3 and ACS cameras to construct spatially resolved
ratio maps of [OIII]/[OII] emission from the galaxy. IPM traces the ionization
structure of the interstellar medium and allows us to identify optically thin
regions. To optimize the continuum subtraction, we introduce a new method for
determining the best continuum scale factor derived from the mode of the
continuum-subtracted, image flux distribution. We find no conclusive evidence
of LyC escape from Knots B or C, but instead, we identify a high-ionization
region extending over at least 1 kpc from Knot A. Knot A shows evidence of an
extremely young age ($\lesssim 1$ Myr), perhaps containing very massive stars
($>100$ M$_\odot$). It is weak in Ly$\alpha$, so if it is confirmed as the LyC
source, our results imply that LyC emission may be independent of Ly$\alpha$
emission.
",0,1,0,0,0,0
340,Typed Closure Conversion for the Calculus of Constructions,"  Dependently typed languages such as Coq are used to specify and verify the
full functional correctness of source programs. Type-preserving compilation can
be used to preserve these specifications and proofs of correctness through
compilation into the generated target-language programs. Unfortunately,
type-preserving compilation of dependent types is hard. In essence, the problem
is that dependent type systems are designed around high-level compositional
abstractions to decide type checking, but compilation interferes with the
type-system rules for reasoning about run-time terms.
We develop a type-preserving closure-conversion translation from the Calculus
of Constructions (CC) with strong dependent pairs ($\Sigma$ types)---a subset
of the core language of Coq---to a type-safe, dependently typed compiler
intermediate language named CC-CC. The central challenge in this work is how to
translate the source type-system rules for reasoning about functions into
target type-system rules for reasoning about closures. To justify these rules,
we prove soundness of CC-CC by giving a model in CC. In addition to type
preservation, we prove correctness of separate compilation.
",1,0,0,0,0,0
341,Untangling Planar Curves,"  Any generic closed curve in the plane can be transformed into a simple closed
curve by a finite sequence of local transformations called homotopy moves. We
prove that simplifying a planar closed curve with $n$ self-crossings requires
$\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the
best previous upper bound $O(n^2)$, which is already implicit in the classical
work of Steinitz; the matching lower bound follows from the construction of
closed curves with large defect, a topological invariant of generic closed
curves introduced by Aicardi and Arnold. Our lower bound also implies that
$\Omega(n^{3/2})$ facial electrical transformations are required to reduce any
plane graph with treewidth $\Omega(\sqrt{n})$ to a single vertex, matching
known upper bounds for rectangular and cylindrical grid graphs. More generally,
we prove that transforming one immersion of $k$ circles with at most $n$
self-crossings into another requires $\Theta(n^{3/2} + nk + k^2)$ homotopy
moves in the worst case. Finally, we prove that transforming one
noncontractible closed curve to another on any orientable surface requires
$\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if
the curve is homotopic to a simple closed curve.
",1,0,1,0,0,0
342,Greedy-Merge Degrading has Optimal Power-Law,"  Consider a channel with a given input distribution. Our aim is to degrade it
to a channel with at most L output letters. One such degradation method is the
so called ""greedy-merge"" algorithm. We derive an upper bound on the reduction
in mutual information between input and output. For fixed input alphabet size
and variable L, the upper bound is within a constant factor of an
algorithm-independent lower bound. Thus, we establish that greedy-merge is
optimal in the power-law sense.
",1,0,1,0,0,0
343,Radially distributed values and normal families,"  Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let
${\mathcal F}$ be the family of all functions holomorphic in the unit disk
${\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on
$L_1$. It is shown that ${\mathcal F}$ is normal in ${\mathbb
D}\backslash\{0\}$. The case where $L_0$ is the positive real axis and $L_1$ is
the negative real axis is studied in more detail.
",0,0,1,0,0,0
344,Characterizing Exoplanet Habitability,"  A habitable exoplanet is a world that can maintain stable liquid water on its
surface. Techniques and approaches to characterizing such worlds are essential,
as performing a census of Earth-like planets that may or may not have life will
inform our understanding of how frequently life originates and is sustained on
worlds other than our own. Observational techniques like high contrast imaging
and transit spectroscopy can reveal key indicators of habitability for
exoplanets. Both polarization measurements and specular reflectance from oceans
(also known as ""glint"") can provide direct evidence for surface liquid water,
while constraining surface pressure and temperature (from moderate resolution
spectra) can indicate liquid water stability. Indirect evidence for
habitability can come from a variety of sources, including observations of
variability due to weather, surface mapping studies, and/or measurements of
water vapor or cloud profiles that indicate condensation near a surface.
Approaches to making the types of measurements that indicate habitability are
diverse, and have different considerations for the required wavelength range,
spectral resolution, maximum noise levels, stellar host temperature, and
observing geometry.
",0,1,0,0,0,0
345,Deep Learning for Classification Tasks on Geospatial Vector Polygons,"  In this paper, we evaluate the accuracy of deep learning approaches on
geospatial vector geometry classification tasks. The purpose of this evaluation
is to investigate the ability of deep learning models to learn from geometry
coordinates directly. Previous machine learning research applied to geospatial
polygon data did not use geometries directly, but derived properties thereof.
These are produced by way of extracting geometry properties such as Fourier
descriptors. Instead, our introduced deep neural net architectures are able to
learn on sequences of coordinates mapped directly from polygons. In three
classification tasks we show that the deep learning architectures are
competitive with common learning algorithms that require extracted features.
",0,0,0,1,0,0
346,The Distance Standard Deviation,"  The distance standard deviation, which arises in distance correlation
analysis of multivariate data, is studied as a measure of spread. New
representations for the distance standard deviation are obtained in terms of
Gini's mean difference and in terms of the moments of spacings of order
statistics. Inequalities for the distance variance are derived, proving that
the distance standard deviation is bounded above by the classical standard
deviation and by Gini's mean difference. Further, it is shown that the distance
standard deviation satisfies the axiomatic properties of a measure of spread.
Explicit closed-form expressions for the distance variance are obtained for a
broad class of parametric distributions. The asymptotic distribution of the
sample distance variance is derived.
",0,0,1,1,0,0
347,Combining learned and analytical models for predicting action effects,"  One of the most basic skills a robot should possess is predicting the effect
of physical interactions with objects in the environment. This enables optimal
action selection to reach a certain goal state. Traditionally, dynamics are
approximated by physics-based analytical models. These models rely on specific
state representations that may be hard to obtain from raw sensory data,
especially if no knowledge of the object shape is assumed. More recently, we
have seen learning approaches that can predict the effect of complex physical
interactions directly from sensory input. It is however an open question how
far these models generalize beyond their training data. In this work, we
investigate the advantages and limitations of neural network based learning
approaches for predicting the effects of actions based on sensory input and
show how analytical and learned models can be combined to leverage the best of
both worlds. As physical interaction task, we use planar pushing, for which
there exists a well-known analytical model and a large real-world dataset. We
propose to use a convolutional neural network to convert raw depth images or
organized point clouds into a suitable representation for the analytical model
and compare this approach to using neural networks for both, perception and
prediction. A systematic evaluation of the proposed approach on a very large
real-world dataset shows two main advantages of the hybrid architecture.
Compared to a pure neural network, it significantly (i) reduces required
training data and (ii) improves generalization to novel physical interaction.
",1,0,0,0,0,0
348,Leavitt path algebras: Graded direct-finiteness and graded $?œ$-injective simple modules,"  In this paper, we give a complete characterization of Leavitt path algebras
which are graded $\Sigma $-$V$ rings, that is, rings over which a direct sum of
arbitrary copies of any graded simple module is graded injective. Specifically,
we show that a Leavitt path algebra $L$ over an arbitrary graph $E$ is a graded
$\Sigma $-$V$ ring if and only if it is a subdirect product of matrix rings of
arbitrary size but with finitely many non-zero entries over $K$ or
$K[x,x^{-1}]$ with appropriate matrix gradings. We also obtain a graphical
characterization of such a graded $\Sigma $-$V$ ring $L$% . When the graph $E$
is finite, we show that $L$ is a graded $\Sigma $-$V$ ring $\Longleftrightarrow
L$ is graded directly-finite $\Longleftrightarrow L $ has bounded index of
nilpotence $\Longleftrightarrow $ $L$ is graded semi-simple. Examples show that
the equivalence of these properties in the preceding statement no longer holds
when the graph $E$ is infinite. Following this, we also characterize Leavitt
path algebras $L$ which are non-graded $\Sigma $-$V$ rings. Graded rings which
are graded directly-finite are explored and it is shown that if a Leavitt path
algebra $L$ is a graded $\Sigma$-$V$ ring, then $L$ is always graded
directly-finite. Examples show the subtle differences between graded and
non-graded directly-finite rings. Leavitt path algebras which are graded
directly-finite are shown to be directed unions of graded semisimple rings.
Using this, we give an alternative proof of a theorem of Va­ \cite{V} on
directly-finite Leavitt path algebras.
",0,0,1,0,0,0
349,Ensemble Estimation of Mutual Information,"  We derive the mean squared error convergence rates of kernel density-based
plug-in estimators of mutual information measures between two multidimensional
random variables $\mathbf{X}$ and $\mathbf{Y}$ for two cases: 1) $\mathbf{X}$
and $\mathbf{Y}$ are both continuous; 2) $\mathbf{X}$ is continuous and
$\mathbf{Y}$ is discrete. Using the derived rates, we propose an ensemble
estimator of these information measures for the second case by taking a
weighted sum of the plug-in estimators with varied bandwidths. The resulting
ensemble estimator achieves the $1/N$ parametric convergence rate when the
conditional densities of the continuous variables are sufficiently smooth. To
the best of our knowledge, this is the first nonparametric mutual information
estimator known to achieve the parametric convergence rate for this case, which
frequently arises in applications (e.g. variable selection in classification).
The estimator is simple to implement as it uses the solution to an offline
convex optimization problem and simple plug-in estimators. A central limit
theorem is also derived for the ensemble estimator. Ensemble estimators that
achieve the parametric rate are also derived for the first case ($\mathbf{X}$
and $\mathbf{Y}$ are both continuous) and another case 3) $\mathbf{X}$ and
$\mathbf{Y}$ may have any mixture of discrete and continuous components.
",1,0,1,1,0,0
350,Social media mining for identification and exploration of health-related information from pregnant women,"  Widespread use of social media has led to the generation of substantial
amounts of information about individuals, including health-related information.
Social media provides the opportunity to study health-related information about
selected population groups who may be of interest for a particular study. In
this paper, we explore the possibility of utilizing social media to perform
targeted data collection and analysis from a particular population group --
pregnant women. We hypothesize that we can use social media to identify cohorts
of pregnant women and follow them over time to analyze crucial health-related
information. To identify potentially pregnant women, we employ simple
rule-based searches that attempt to detect pregnancy announcements with
moderate precision. To further filter out false positives and noise, we employ
a supervised classifier using a small number of hand-annotated data. We then
collect their posts over time to create longitudinal health timelines and
attempt to divide the timelines into different pregnancy trimesters. Finally,
we assess the usefulness of the timelines by performing a preliminary analysis
to estimate drug intake patterns of our cohort at different trimesters. Our
rule-based cohort identification technique collected 53,820 users over thirty
months from Twitter. Our pregnancy announcement classification technique
achieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user
timelines. Analysis of the timelines revealed that pertinent health-related
information, such as drug-intake and adverse reactions can be mined from the
data. Our approach to using user timelines in this fashion has produced very
encouraging results and can be employed for other important tasks where
cohorts, for which health-related information may not be available from other
sources, are required to be followed over time to derive population-based
estimates.
",1,0,0,0,0,0
351,The vortex method for 2D ideal flows in the exterior of a disk,"  The vortex method is a common numerical and theoretical approach used to
implement the motion of an ideal flow, in which the vorticity is approximated
by a sum of point vortices, so that the Euler equations read as a system of
ordinary differential equations. Such a method is well justified in the full
plane, thanks to the explicit representation formulas of Biot and Savart. In an
exterior domain, we also replace the impermeable boundary by a collection of
point vortices generating the circulation around the obstacle. The density of
these point vortices is chosen in order that the flow remains tangent at
midpoints between adjacent vortices. In this work, we provide a rigorous
justification for this method in exterior domains. One of the main mathematical
difficulties being that the Biot-Savart kernel defines a singular integral
operator when restricted to a curve. For simplicity and clarity, we only treat
the case of the unit disk in the plane approximated by a uniformly distributed
mesh of point vortices. The complete and general version of our work is
available in [arXiv:1707.01458].
",0,0,1,0,0,0
352,Neeman's characterization of K(R-Proj) via Bousfield localization,"  Let $R$ be an associative ring with unit and denote by $K({\rm R
\mbox{-}Proj})$ the homotopy category of complexes of projective left
$R$-modules. Neeman proved the theorem that $K({\rm R \mbox{-}Proj})$ is
$\aleph_1$-compactly generated, with the category $K^+ ({\rm R \mbox{-}proj})$
of left bounded complexes of finitely generated projective $R$-modules
providing an essentially small class of such generators. Another proof of
Neeman's theorem is explained, using recent ideas of Christensen and Holm, and
Emmanouil. The strategy of the proof is to show that every complex in $K({\rm R
\mbox{-}Proj})$ vanishes in the Bousfield localization $K({\rm R
\mbox{-}Flat})/\langle K^+ ({\rm R \mbox{-}proj}) \rangle.$
",0,0,1,0,0,0
353,Qualification Conditions in Semi-algebraic Programming,"  For an arbitrary finite family of semi-algebraic/definable functions, we
consider the corresponding inequality constraint set and we study qualification
conditions for perturbations of this set. In particular we prove that all
positive diagonal perturbations, save perhaps a finite number of them, ensure
that any point within the feasible set satisfies Mangasarian-Fromovitz
constraint qualification. Using the Milnor-Thom theorem, we provide a bound for
the number of singular perturbations when the constraints are polynomial
functions. Examples show that the order of magnitude of our exponential bound
is relevant. Our perturbation approach provides a simple protocol to build
sequences of ""regular"" problems approximating an arbitrary
semi-algebraic/definable problem. Applications to sequential quadratic
programming methods and sum of squares relaxation are provided.
",0,0,1,0,0,0
354,Curvature in Hamiltonian Mechanics And The Einstein-Maxwell-Dilaton Action,"  Riemannian geometry is a particular case of Hamiltonian mechanics: the orbits
of the hamiltonian $H=\frac{1}{2}g^{ij}p_{i}p_{j}$ are the geodesics. Given a
symplectic manifold (\Gamma,\omega), a hamiltonian $H:\Gamma\to\mathbb{R}$ and
a Lagrangian sub-manifold $M\subset\Gamma$ we find a generalization of the
notion of curvature. The particular case
$H=\frac{1}{2}g^{ij}\left[p_{i}-A_{i}\right]\left[p_{j}-A_{j}\right]+\phi $ of
a particle moving in a gravitational, electromagnetic and scalar fields is
studied in more detail. The integral of the generalized Ricci tensor w.r.t. the
Boltzmann weight reduces to the action principle
$\int\left[R+\frac{1}{4}F_{ik}F_{jl}g^{kl}g^{ij}-g^{ij}\partial_{i}\phi\partial_{j}\phi\right]e^{-\phi}\sqrt{g}d^{n}q$
for the scalar, vector and tensor fields.
",0,0,1,0,0,0
355,End-to-End Navigation in Unknown Environments using Neural Networks,"  We investigate how a neural network can learn perception actions loops for
navigation in unknown environments. Specifically, we consider how to learn to
navigate in environments populated with cul-de-sacs that represent convex local
minima that the robot could fall into instead of finding a set of feasible
actions that take it to the goal. Traditional methods rely on maintaining a
global map to solve the problem of over coming a long cul-de-sac. However, due
to errors induced from local and global drift, it is highly challenging to
maintain such a map for long periods of time. One way to mitigate this problem
is by using learning techniques that do not rely on hand engineered map
representations and instead output appropriate control policies directly from
their sensory input. We first demonstrate that such a problem cannot be solved
directly by deep reinforcement learning due to the sparse reward structure of
the environment. Further, we demonstrate that deep supervised learning also
cannot be used directly to solve this problem. We then investigate network
models that offer a combination of reinforcement learning and supervised
learning and highlight the significance of adding fully differentiable memory
units to such networks. We evaluate our networks on their ability to generalize
to new environments and show that adding memory to such networks offers huge
jumps in performance
",1,0,0,0,0,0
356,A Combinatorial Approach to the Opposite Bi-Free Partial $S$-Transform,"  In this paper, we present a combinatorial approach to the opposite 2-variable
bi-free partial $S$-transforms where the opposite multiplication is used on the
right. In addition, extensions of this partial $S$-transforms to the
conditional bi-free and operator-valued bi-free settings are discussed.
",0,0,1,0,0,0
357,Roche-lobe overflow in eccentric planet-star systems,"  Many giant exoplanets are found near their Roche limit and in mildly
eccentric orbits. In this study we examine the fate of such planets through
Roche-lobe overflow as a function of the physical properties of the binary
components, including the eccentricity and the asynchronicity of the rotating
planet. We use a direct three-body integrator to compute the trajectories of
the lost mass in the ballistic limit and investigate the possible outcomes. We
find three different outcomes for the mass transferred through the Lagrangian
point $L_{1}$: (i) self-accretion by the planet, (ii) direct impact on the
stellar surface, (iii) disk formation around the star. We explore the parameter
space of the three different regimes and find that at low eccentricities,
$e\lesssim 0.2$, mass overflow leads to disk formation for most systems, while
for higher eccentricities or retrograde orbits self-accretion is the only
possible outcome. We conclude that the assumption often made in previous work
that when a planet overflows its Roche lobe it is quickly disrupted and
accreted by the star is not always valid.
",0,1,0,0,0,0
358,A short-orbit spectrometer for low-energy pion detection in electroproduction experiments at MAMI,"  A new Short-Orbit Spectrometer (SOS) has been constructed and installed
within the experimental facility of the A1 collaboration at Mainz Microtron
(MAMI), with the goal to detect low-energy pions. It is equipped with a
Browne-Buechner magnet and a detector system consisting of two helium-ethane
based drift chambers and a scintillator telescope made of five layers. The
detector system allows detection of pions in the momentum range of 50 - 147
MeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can
be placed at a distance range of 54 - 66 cm from the target center. Two
collimators are available for the measurements, one having 1.8 msr aperture and
the other having 7 msr aperture. The Short-Orbit Spectrometer has been
successfully calibrated and used in coincidence measurements together with the
standard magnetic spectrometers of the A1 collaboration.
",0,1,0,0,0,0
359,A one-dimensional model for water desalination by flow-through electrode capacitive deionization,"  Capacitive deionization (CDI) is a fast-emerging water desalination
technology in which a small cell voltage of ~1 V across porous carbon
electrodes removes salt from feedwaters via electrosorption. In flow-through
electrode (FTE) CDI cell architecture, feedwater is pumped through macropores
or laser perforated channels in porous electrodes, enabling highly compact
cells with parallel flow and electric field, as well as rapid salt removal. We
here present a one-dimensional model describing water desalination by FTE CDI,
and a comparison to data from a custom-built experimental cell. The model
employs simple cell boundary conditions derived via scaling arguments. We show
good model-to-data fits with reasonable values for fitting parameters such as
the Stern layer capacitance, micropore volume, and attraction energy. Thus, we
demonstrate that from an engineering modeling perspective, an FTE CDI cell may
be described with simpler one-dimensional models, unlike more typical
flow-between electrodes architecture where 2D models are required.
",1,1,0,0,0,0
360,Deep Learning Scooping Motion using Bilateral Teleoperations,"  We present bilateral teleoperation system for task learning and robot motion
generation. Our system includes a bilateral teleoperation platform and a deep
learning software. The deep learning software refers to human demonstration
using the bilateral teleoperation platform to collect visual images and robotic
encoder values. It leverages the datasets of images and robotic encoder
information to learn about the inter-modal correspondence between visual images
and robot motion. In detail, the deep learning software uses a combination of
Deep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent
Neural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor
angles, to learn motion taught be human teleoperation. The learnt models are
used to predict new motion trajectories for similar tasks. Experimental results
show that our system has the adaptivity to generate motion for similar scooping
tasks. Detailed analysis is performed based on failure cases of the
experimental results. Some insights about the cans and cannots of the system
are summarized.
",1,0,0,0,0,0
361,XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual Classification,"  We propose two multimodal deep learning architectures that allow for
cross-modal dataflow (XFlow) between the feature extractors, thereby extracting
more interpretable features and obtaining a better representation than through
unimodal learning, for the same amount of training data. These models can
usefully exploit correlations between audio and visual data, which have a
different dimensionality and are therefore nontrivially exchangeable. Our work
improves on existing multimodal deep learning metholodogies in two essential
ways: (1) it presents a novel method for performing cross-modality (before
features are learned from individual modalities) and (2) extends the previously
proposed cross-connections, which only transfer information between streams
that process compatible data. Both cross-modal architectures outperformed their
baselines (by up to 7.5%) when evaluated on the AVletters dataset.
",1,0,0,1,0,0
362,Making Sense of Physics through Stories: High School Students Narratives about Electric Charges and Interactions,"  Educational research has shown that narratives are useful tools that can help
young students make sense of scientific phenomena. Based on previous research,
I argue that narratives can also become tools for high school students to make
sense of concepts such as the electric field. In this paper I examine high
school students visual and oral narratives in which they describe the
interaction among electric charges as if they were characters of a cartoon
series. The study investigates: given the prompt to produce narratives for
electrostatic phenomena during a classroom activity prior to receiving formal
instruction, (1) what ideas of electrostatics do students attend to in their
narratives?; (2) what role do students narratives play in their understanding
of electrostatics? The participants were a group of high school students
engaged in an open-ended classroom activity prior to receiving formal
instruction about electrostatics. During the activity, the group was asked to
draw comic strips for electric charges. In addition to individual work,
students shared their work within small groups as well as with the whole group.
Post activity, six students from a small group were interviewed individually
about their work. In this paper I present two cases in which students produced
narratives to express their ideas about electrostatics in different ways. In
each case, I present student work for the comic strip activity (visual
narratives), their oral descriptions of their work (oral narratives) during the
interview and/or to their peers during class, and the their ideas of the
electric interactions expressed through their narratives.
",0,1,0,0,0,0
363,Preventing Hospital Acquired Infections Through a Workflow-Based Cyber-Physical System,"  Hospital acquired infections (HAI) are infections acquired within the
hospital from healthcare workers, patients or from the environment, but which
have no connection to the initial reason for the patient's hospital admission.
HAI are a serious world-wide problem, leading to an increase in mortality
rates, duration of hospitalisation as well as significant economic burden on
hospitals. Although clear preventive guidelines exist, studies show that
compliance to them is frequently poor. This paper details the software
perspective for an innovative, business process software based cyber-physical
system that will be implemented as part of a European Union-funded research
project. The system is composed of a network of sensors mounted in different
sites around the hospital, a series of wearables used by the healthcare workers
and a server side workflow engine. For better understanding, we describe the
system through the lens of a single, simple clinical workflow that is
responsible for a significant portion of all hospital infections. The goal is
that when completed, the system will be configurable in the sense of
facilitating the creation and automated monitoring of those clinical workflows
that when combined, account for over 90\% of hospital infections.
",1,0,0,0,0,0
364,OpenML Benchmarking Suites and the OpenML100,"  We advocate the use of curated, comprehensive benchmark suites of machine
learning datasets, backed by standardized OpenML-based interfaces and
complementary software toolkits written in Python, Java and R. Major
distinguishing features of OpenML benchmark suites are (a) ease of use through
standardized data formats, APIs, and existing client libraries; (b)
machine-readable meta-information regarding the contents of the suite; and (c)
online sharing of results, enabling large scale comparisons. As a first such
suite, we propose the OpenML100, a machine learning benchmark suite of
100~classification datasets carefully curated from the thousands of datasets
available on OpenML.org.
",1,0,0,1,0,0
365,On orbifold constructions associated with the Leech lattice vertex operator algebra,"  In this article, we study orbifold constructions associated with the Leech
lattice vertex operator algebra. As an application, we prove that the structure
of a strongly regular holomorphic vertex operator algebra of central charge
$24$ is uniquely determined by its weight one Lie algebra if the Lie algebra
has the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$,
$A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse
orbifold construction. Our result also provides alternative constructions of
these vertex operator algebras (except for the case $A_{6,7}$) from the Leech
lattice vertex operator algebra.
",0,0,1,0,0,0
366,From mindless mathematics to thinking meat?,"  Deconstruction of the theme of the 2017 FQXi essay contest is already an
interesting exercise in its own right: Teleology is rarely useful in physics
--- the only known mainstream physics example (black hole event horizons) has a
very mixed score-card --- so the ""goals"" and ""aims and intentions"" alluded to
in the theme of the 2017 FQXi essay contest are already somewhat pushing the
limits. Furthermore, ""aims and intentions"" certainly carries the implication of
consciousness, and opens up a whole can of worms related to the mind-body
problem. As for ""mindless mathematical laws"", that allusion is certainly in
tension with at least some versions of the ""mathematical universe hypothesis"".
Finally ""wandering towards a goal"" again carries the implication of
consciousness, with all its attendant problems.
In this essay I will argue, simply because we do not yet have any really good
mathematical or physical theory of consciousness, that the theme of this essay
contest is premature, and unlikely to lead to any resolution that would be
widely accepted in the mathematics or physics communities.
",0,1,0,0,0,0
367,Phase correction for ALMA - Investigating water vapour radiometer scaling:The long-baseline science verification data case study,"  The Atacama Large millimetre/submillimetre Array (ALMA) makes use of water
vapour radiometers (WVR), which monitor the atmospheric water vapour line at
183 GHz along the line of sight above each antenna to correct for phase delays
introduced by the wet component of the troposphere. The application of WVR
derived phase corrections improve the image quality and facilitate successful
observations in weather conditions that were classically marginal or poor. We
present work to indicate that a scaling factor applied to the WVR solutions can
act to further improve the phase stability and image quality of ALMA data. We
find reduced phase noise statistics for 62 out of 75 datasets from the
long-baseline science verification campaign after a WVR scaling factor is
applied. The improvement of phase noise translates to an expected coherence
improvement in 39 datasets. When imaging the bandpass source, we find 33 of the
39 datasets show an improvement in the signal-to-noise ratio (S/N) between a
few to ~30 percent. There are 23 datasets where the S/N of the science image is
improved: 6 by <1%, 11 between 1 and 5%, and 6 above 5%. The higher frequencies
studied (band 6 and band 7) are those most improved, specifically datasets with
low precipitable water vapour (PWV), <1mm, where the dominance of the wet
component is reduced. Although these improvements are not profound, phase
stability improvements via the WVR scaling factor come into play for the higher
frequency (>450 GHz) and long-baseline (>5km) observations. These inherently
have poorer phase stability and are taken in low PWV (<1mm) conditions for
which we find the scaling to be most effective. A promising explanation for the
scaling factor is the mixing of dry and wet air components, although other
origins are discussed. We have produced a python code to allow ALMA users to
undertake WVR scaling tests and make improvements to their data.
",0,1,0,0,0,0
368,On the $?¬$-ordinary locus of a Shimura variety,"  In this paper, we study the $\mu$-ordinary locus of a Shimura variety with
parahoric level structure. Under the axioms in \cite{HR}, we show that
$\mu$-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport
strata introduced in \cite{HR} and we give criteria on the density of the
$\mu$-ordinary locus.
",0,0,1,0,0,0
369,Scatteract: Automated extraction of data from scatter plots,"  Charts are an excellent way to convey patterns and trends in data, but they
do not facilitate further modeling of the data or close inspection of
individual data points. We present a fully automated system for extracting the
numerical values of data points from images of scatter plots. We use deep
learning techniques to identify the key components of the chart, and optical
character recognition together with robust regression to map from pixels to the
coordinate system of the chart. We focus on scatter plots with linear scales,
which already have several interesting challenges. Previous work has done fully
automatic extraction for other types of charts, but to our knowledge this is
the first approach that is fully automatic for scatter plots. Our method
performs well, achieving successful data extraction on 89% of the plots in our
test set.
",1,0,0,1,0,0
370,When the Universe Expands Too Fast: Relentless Dark Matter,"  We consider a modification to the standard cosmological history consisting of
introducing a new species $\phi$ whose energy density red-shifts with the scale
factor $a$ like $\rho_\phi \propto a^{-(4+n)}$. For $n>0$, such a red-shift is
faster than radiation, hence the new species dominates the energy budget of the
universe at early times while it is completely negligible at late times. If
equality with the radiation energy density is achieved at low enough
temperatures, dark matter can be produced as a thermal relic during the new
cosmological phase. Dark matter freeze-out then occurs at higher temperatures
compared to the standard case, implying that reproducing the observed abundance
requires significantly larger annihilation rates. Here, we point out a
completely new phenomenon, which we refer to as $\textit{relentless}$ dark
matter: for large enough $n$, unlike the standard case where annihilation ends
shortly after the departure from thermal equilibrium, dark matter particles
keep annihilating long after leaving chemical equilibrium, with a significant
depletion of the final relic abundance. Relentless annihilation occurs for $n
\geq 2$ and $n \geq 4$ for s-wave and p-wave annihilation, respectively, and it
thus occurs in well motivated scenarios such as a quintessence with a kination
phase. We discuss a few microscopic realizations for the new cosmological
component and highlight the phenomenological consequences of our calculations
for dark matter searches.
",0,1,0,0,0,0
371,Polygons with prescribed edge slopes: configuration space and extremal points of perimeter,"  We describe the configuration space $\mathbf{S}$ of polygons with prescribed
edge slopes, and study the perimeter $\mathcal{P}$ as a Morse function on
$\mathbf{S}$. We characterize critical points of $\mathcal{P}$ (these are
\textit{tangential} polygons) and compute their Morse indices. This setup is
motivated by a number of results about critical points and Morse indices of the
oriented area function defined on the configuration space of polygons with
prescribed edge lengths (flexible polygons). As a by-product, we present an
independent computation of the Morse index of the area function (obtained
earlier by G. Panina and A. Zhukova).
",0,0,1,0,0,0
372,An Asymptotically Efficient Metropolis-Hastings Sampler for Bayesian Inference in Large-Scale Educational Measuremen,"  This paper discusses a Metropolis-Hastings algorithm developed by
\citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is
proven that the algorithm becomes more efficient with more data and meets the
growing demands of large scale educational measurement.
",0,0,0,1,0,0
373,When Streams of Optofluidics Meet the Sea of Life,"  Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National
University of Singapore. In this contribution he describes the power of
optofluidics as a research tool and reviews new insights within the areas of
single cell analysis, microphysiological analysis, and integrated systems.
",0,0,0,0,1,0
374,"L lines, C points and Chern numbers: understanding band structure topology using polarization fields","  Topology has appeared in different physical contexts. The most prominent
application is topologically protected edge transport in condensed matter
physics. The Chern number, the topological invariant of gapped Bloch
Hamiltonians, is an important quantity in this field. Another example of
topology, in polarization physics, are polarization singularities, called L
lines and C points. By establishing a connection between these two theories, we
develop a novel technique to visualize and potentially measure the Chern
number: it can be expressed either as the winding of the polarization azimuth
along L lines in reciprocal space, or in terms of the handedness and the index
of the C points. For mechanical systems, this is directly connected to the
visible motion patterns.
",0,1,0,0,0,0
375,Willis Theory via Graphs,"  We study the scale and tidy subgroups of an endomorphism of a totally
disconnected locally compact group using a geometric framework. This leads to
new interpretations of tidy subgroups and the scale function. Foremost, we
obtain a geometric tidying procedure which applies to endomorphisms as well as
a geometric proof of the fact that tidiness is equivalent to being minimizing
for a given endomorphism. Our framework also yields an endomorphism version of
the Baumgartner-Willis tree representation theorem. We conclude with a
construction of new endomorphisms of totally disconnected locally compact
groups from old via HNN-extensions.
",0,0,1,0,0,0
376,The Effect of Site-Specific Spectral Densities on the High-Dimensional Exciton-Vibrational Dynamics in the FMO Complex,"  The coupled exciton-vibrational dynamics of a three-site model of the FMO
complex is investigated using the Multi-layer Multi-configuration
Time-dependent Hartree (ML-MCTDH) approach. Emphasis is put on the effect of
the spectral density on the exciton state populations as well as on the
vibrational and vibronic non-equilibrium excitations. Models which use either a
single or site-specific spectral densities are contrasted to a spectral density
adapted from experiment. For the transfer efficiency, the total integrated
Huang-Rhys factor is found to be more important than details of the spectral
distributions. However, the latter are relevant for the obtained
non-equilibrium vibrational and vibronic distributions and thus influence the
actual pattern of population relaxation.
",0,1,0,0,0,0
377,Space dependent adhesion forces mediated by transient elastic linkages : new convergence and global existence results,"  In the first part of this work we show the convergence with respect to an
asymptotic parameter {\epsilon} of a delayed heat equation. It represents a
mathematical extension of works considered previously by the authors [Milisic
et al. 2011, Milisic et al. 2016]. Namely, this is the first result involving
delay operators approximating protein linkages coupled with a spatial elliptic
second order operator. For the sake of simplicity we choose the Laplace
operator, although more general results could be derived. The main arguments
are (i) new energy estimates and (ii) a stability result extended from the
previous work to this more involved context. They allow to prove convergence of
the delay operator to a friction term together with the Laplace operator in the
same asymptotic regime considered without the space dependence in [Milisic et
al, 2011]. In a second part we extend fixed-point results for the fully
non-linear model introduced in [Milisic et al, 2016] and prove global existence
in time. This shows that the blow-up scenario observed previously does not
occur. Since the latter result was interpreted as a rupture of adhesion forces,
we discuss the possibility of bond breaking both from the analytic and
numerical point of view.
",0,0,1,0,0,0
378,On the nonparametric maximum likelihood estimator for Gaussian location mixture densities with application to Gaussian denoising,"  We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for
estimating Gaussian location mixture densities in $d$-dimensions from
independent observations. Unlike usual likelihood-based methods for fitting
mixtures, NPMLEs are based on convex optimization. We prove finite sample
results on the Hellinger accuracy of every NPMLE. Our results imply, in
particular, that every NPMLE achieves near parametric risk (up to logarithmic
multiplicative factors) when the true density is a discrete Gaussian mixture
without any prior information on the number of mixture components. NPMLEs can
naturally be used to yield empirical Bayes estimates of the Oracle Bayes
estimator in the Gaussian denoising problem. We prove bounds for the accuracy
of the empirical Bayes estimate as an approximation to the Oracle Bayes
estimator. Here our results imply that the empirical Bayes estimator performs
at nearly the optimal level (up to logarithmic multiplicative factors) for
denoising in clustering situations without any prior knowledge of the number of
clusters.
",0,0,1,1,0,0
379,Gravitational wave production from preheating: parameter dependence,"  Parametric resonance is among the most efficient phenomena generating
gravitational waves (GWs) in the early Universe. The dynamics of parametric
resonance, and hence of the GWs, depend exclusively on the resonance parameter
$q$. The latter is determined by the properties of each scenario: the initial
amplitude and potential curvature of the oscillating field, and its coupling to
other species. Previous works have only studied the GW production for fixed
value(s) of $q$. We present an analytical derivation of the GW amplitude
dependence on $q$, valid for any scenario, which we confront against numerical
results. By running lattice simulations in an expanding grid, we study for a
wide range of $q$ values, the production of GWs in post-inflationary preheating
scenarios driven by parametric resonance. We present simple fits for the final
amplitude and position of the local maxima in the GW spectrum. Our
parametrization allows to predict the location and amplitude of the GW
background today, for an arbitrary $q$. The GW signal can be rather large, as
$h^2\Omega_{\rm GW}(f_p) \lesssim 10^{-11}$, but it is always peaked at high
frequencies $f_p \gtrsim 10^{7}$ Hz. We also discuss the case of
spectator-field scenarios, where the oscillatory field can be e.g.~a curvaton,
or the Standard Model Higgs.
",0,1,0,0,0,0
380,IP determination and 1+1 REMPI spectrum of SiO at 210-220 nm with implications for SiO$^{+}$ ion trap loading,"  The 1+1 REMPI spectrum of SiO in the 210-220 nm range is recorded. Observed
bands are assigned to the $A-X$ vibrational bands $(v``=0-3, v`=5-10)$ and a
tentative assignment is given to the 2-photon transition from $X$ to the
n=12-13 $[X^{2}{\Sigma}^{+},v^{+}=1]$ Rydberg states at 216-217 nm. We estimate
the IP of SiO to be 11.59(1) eV. The SiO$^{+}$ cation has previously been
identified as a molecular candidate amenable to laser control. Our work allows
us to identify an efficient method for loading cold SiO$^{+}$ from an ablated
sample of SiO into an ion trap via the $(5,0)$ $A-X$ band at 213.977 nm.
",0,1,0,0,0,0
381,Adaptive Model Predictive Control for High-Accuracy Trajectory Tracking in Changing Conditions,"  Robots and automated systems are increasingly being introduced to unknown and
dynamic environments where they are required to handle disturbances, unmodeled
dynamics, and parametric uncertainties. Robust and adaptive control strategies
are required to achieve high performance in these dynamic environments. In this
paper, we propose a novel adaptive model predictive controller that combines
model predictive control (MPC) with an underlying $\mathcal{L}_1$ adaptive
controller to improve trajectory tracking of a system subject to unknown and
changing disturbances. The $\mathcal{L}_1$ adaptive controller forces the
system to behave in a predefined way, as specified by a reference model. A
higher-level model predictive controller then uses this reference model to
calculate the optimal reference input based on a cost function, while taking
into account input and state constraints. We focus on the experimental
validation of the proposed approach and demonstrate its effectiveness in
experiments on a quadrotor. We show that the proposed approach has a lower
trajectory tracking error compared to non-predictive, adaptive approaches and a
predictive, non-adaptive approach, even when external wind disturbances are
applied.
",1,0,0,0,0,0
382,An enhanced method to compute the similarity between concepts of ontology,"  With the use of ontologies in several domains such as semantic web,
information retrieval, artificial intelligence, the concept of similarity
measuring has become a very important domain of research. Therefore, in the
current paper, we propose our method of similarity measuring which uses the
Dijkstra algorithm to define and compute the shortest path. Then, we use this
one to compute the semantic distance between two concepts defined in the same
hierarchy of ontology. Afterward, we base on this result to compute the
semantic similarity. Finally, we present an experimental comparison between our
method and other methods of similarity measuring.
",1,0,0,0,0,0
383,Eigenvalues of symmetric tridiagonal interval matrices revisited,"  In this short note, we present a novel method for computing exact lower and
upper bounds of eigenvalues of a symmetric tridiagonal interval matrix.
Compared to the known methods, our approach is fast, simple to present and to
implement, and avoids any assumptions. Our construction explicitly yields those
matrices for which particular lower and upper bounds are attained.
",1,0,0,0,0,0
384,PRE-render Content Using Tiles (PRECUT). 1. Large-Scale Compound-Target Relationship Analyses,"  Visualizing a complex network is computationally intensive process and
depends heavily on the number of components in the network. One way to solve
this problem is not to render the network in real time. PRE-render Content
Using Tiles (PRECUT) is a process to convert any complex network into a
pre-rendered network. Tiles are generated from pre-rendered images at different
zoom levels, and navigating the network simply becomes delivering relevant
tiles. PRECUT is exemplified by performing large-scale compound-target
relationship analyses. Matched molecular pair (MMP) networks were created using
compounds and the target class description found in the ChEMBL database. To
visualize MMP networks, the MMP network viewer has been implemented in COMBINE
and as a web application, hosted at this http URL.
",1,0,0,0,0,0
385,The list chromatic number of graphs with small clique number,"  We prove that every triangle-free graph with maximum degree $\Delta$ has list
chromatic number at most $(1+o(1))\frac{\Delta}{\ln \Delta}$. This matches the
best-known bound for graphs of girth at least 5. We also provide a new proof
that for any $r\geq 4$ every $K_r$-free graph has list-chromatic number at most
$200r\frac{\Delta\ln\ln\Delta}{\ln\Delta}$.
",0,0,1,0,0,0
386,Topology of Large-Scale Structures of Galaxies in Two Dimensions - Systematic Effects,"  We study the two-dimensional topology of the galactic distribution when
projected onto two-dimensional spherical shells. Using the latest Horizon Run 4
simulation data, we construct the genus of the two-dimensional field and
consider how this statistic is affected by late-time nonlinear effects --
principally gravitational collapse and redshift space distortion (RSD). We also
consider systematic and numerical artifacts such as shot noise, galaxy bias,
and finite pixel effects. We model the systematics using a Hermite polynomial
expansion and perform a comprehensive analysis of known effects on the
two-dimensional genus, with a view toward using the statistic for cosmological
parameter estimation. We find that the finite pixel effect is dominated by an
amplitude drop and can be made less than $1\%$ by adopting pixels smaller than
$1/3$ of the angular smoothing length. Nonlinear gravitational evolution
introduces time-dependent coefficients of the zeroth, first, and second Hermite
polynomials, but the genus amplitude changes by less than $1\%$ between $z=1$
and $z=0$ for smoothing scales $R_{\rm G} > 9 {\rm Mpc/h}$. Non-zero terms are
measured up to third order in the Hermite polynomial expansion when studying
RSD. Differences in shapes of the genus curves in real and redshift space are
small when we adopt thick redshift shells, but the amplitude change remains a
significant $\sim {\cal O}(10\%)$ effect. The combined effects of galaxy
biasing and shot noise produce systematic effects up to the second Hermite
polynomial. It is shown that, when sampling, the use of galaxy mass cuts
significantly reduces the effect of shot noise relative to random sampling.
",0,1,0,0,0,0
387,Wiki-index of authors popularity,"  The new index of the author's popularity estimation is represented in the
paper. The index is calculated on the basis of Wikipedia encyclopedia analysis
(Wikipedia Index - WI). Unlike the conventional existed citation indices, the
suggested mark allows to evaluate not only the popularity of the author, as it
can be done by means of calculating the general citation number or by the
Hirsch index, which is often used to measure the author's research rate. The
index gives an opportunity to estimate the author's popularity, his/her
influence within the sought-after area ""knowledge area"" in the Internet - in
the Wikipedia. The suggested index is supposed to be calculated in frames of
the subject domain, and it, on the one hand, avoids the mistaken computation of
the homonyms, and on the other hand - provides the entirety of the subject
area. There are proposed algorithms and the technique of the Wikipedia Index
calculation through the network encyclopedia sounding, the exemplified
calculations of the index for the prominent researchers, and also the methods
of the information networks formation - models of the subject domains by the
automatic monitoring and networks information reference resources analysis. The
considered in the paper notion network corresponds the terms-heads of the
Wikipedia articles.
",1,0,0,0,0,0
388,Belyi map for the sporadic group J1,"  We compute the genus 0 Belyi map for the sporadic Janko group J1 of degree
266 and describe the applied method. This yields explicit polynomials having J1
as a Galois group over K(t), [K:Q] = 7.
",0,0,1,0,0,0
389,"Convolution Semigroups of Probability Measures on Gelfand Pairs, Revisited","  Our goal is to find classes of convolution semigroups on Lie groups $G$ that
give rise to interesting processes in symmetric spaces $G/K$. The
$K$-bi-invariant convolution semigroups are a well-studied example. An
appealing direction for the next step is to generalise to right $K$-invariant
convolution semigroups, but recent work of Liao has shown that these are in
one-to-one correspondence with $K$-bi-invariant convolution semigroups. We
investigate a weaker notion of right $K$-invariance, but show that this is, in
fact, the same as the usual notion. Another possible approach is to use
generalised notions of negative definite functions, but this also leads to
nothing new. We finally find an interesting class of convolution semigroups
that are obtained by making use of the Cartan decomposition of a semisimple Lie
group, and the solution of certain stochastic differential equations. Examples
suggest that these are well-suited for generating random motion along geodesics
in symmetric spaces.
",0,0,1,0,0,0
390,Toward Optimal Coupon Allocation in Social Networks: An Approximate Submodular Optimization Approach,"  CMO Council reports that 71\% of internet users in the U.S. were influenced
by coupons and discounts when making their purchase decisions. It has also been
shown that offering coupons to a small fraction of users (called seed users)
may affect the purchase decisions of many other users in a social network. This
motivates us to study the optimal coupon allocation problem, and our objective
is to allocate coupons to a set of users so as to maximize the expected
cascade. Different from existing studies on influence maximizaton (IM), our
framework allows a general utility function and a more complex set of
constraints. In particular, we formulate our problem as an approximate
submodular maximization problem subject to matroid and knapsack constraints.
Existing techniques relying on the submodularity of the utility function, such
as greedy algorithm, can not work directly on a non-submodular function. We use
$\epsilon$ to measure the difference between our function and its closest
submodular function and propose a novel approximate algorithm with
approximation ratio $\beta(\epsilon)$ with $\lim_{\epsilon\rightarrow
0}\beta(\epsilon)=1-1/e$. This is the best approximation guarantee for
approximate submodular maximization subject to a partition matroid and knapsack
constraints, our results apply to a broad range of optimization problems that
can be formulated as an approximate submodular maximization problem.
",1,0,0,0,0,0
391,Lefschetz duality for intersection (co)homology,"  We prove the Lefschetz duality for intersection (co)homology in the framework
of $\partial$-pesudomanifolds. We work with general perversities and without
restriction on the coefficient ring.
",0,0,1,0,0,0
392,Empirical determination of the optimum attack for fragmentation of modular networks,"  All possible removals of $n=5$ nodes from networks of size $N=100$ are
performed in order to find the optimal set of nodes which fragments the
original network into the smallest largest connected component. The resulting
attacks are ordered according to the size of the largest connected component
and compared with the state of the art methods of network attacks. We chose
attacks of size $5$ on relative small networks of size $100$ because the number
of all possible attacks, ${100}\choose{5}$ $\approx 10^8$, is at the verge of
the possible to compute with the available standard computers. Besides, we
applied the procedure in a series of networks with controlled and varied
modularity, comparing the resulting statistics with the effect of removing the
same amount of vertices, according to the known most efficient disruption
strategies, i.e., High Betweenness Adaptive attack (HBA), Collective Index
attack (CI), and Modular Based Attack (MBA). Results show that modularity has
an inverse relation with robustness, with $Q_c \approx 0.7$ being the critical
value. For modularities lower than $Q_c$, all heuristic method gives mostly the
same results than with random attacks, while for bigger $Q$, networks are less
robust and highly vulnerable to malicious attacks.
",1,0,0,0,0,0
393,Stochastic Non-convex Optimization with Strong High Probability Second-order Convergence,"  In this paper, we study stochastic non-convex optimization with non-convex
random functions. Recent studies on non-convex optimization revolve around
establishing second-order convergence, i.e., converging to a nearly
second-order optimal stationary points. However, existing results on stochastic
non-convex optimization are limited, especially with a high probability
second-order convergence. We propose a novel updating step (named NCG-S) by
leveraging a stochastic gradient and a noisy negative curvature of a stochastic
Hessian, where the stochastic gradient and Hessian are based on a proper
mini-batch of random functions. Building on this step, we develop two
algorithms and establish their high probability second-order convergence. To
the best of our knowledge, the proposed stochastic algorithms are the first
with a second-order convergence in {\it high probability} and a time complexity
that is {\it almost linear} in the problem's dimensionality.
",1,0,0,1,0,0
394,On the optimality and sharpness of Laguerre's lower bound on the smallest eigenvalue of a symmetric positive definite matrix,"  Lower bounds on the smallest eigenvalue of a symmetric positive definite
matrices $A\in\mathbb{R}^{m\times m}$ play an important role in condition
number estimation and in iterative methods for singular value computation. In
particular, the bounds based on ${\rm Tr}(A^{-1})$ and ${\rm Tr}(A^{-2})$
attract attention recently because they can be computed in $O(m)$ work when $A$
is tridiagonal. In this paper, we focus on these bounds and investigate their
properties in detail. First, we consider the problem of finding the optimal
bound that can be computed solely from ${\rm Tr}(A^{-1})$ and ${\rm
Tr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one
in terms of sharpness. Next, we study the gap between the Laguerre bound and
the smallest eigenvalue. We characterize the situation in which the gap becomes
largest in terms of the eigenvalue distribution of $A$ and show that the gap
becomes smallest when ${\rm Tr}(A^{-2})/\{{\rm Tr}(A^{-1})\}^2$ approaches 1 or
$\frac{1}{m}$. These results will be useful, for example, in designing
efficient shift strategies for singular value computation algorithms.
",1,0,1,0,0,0
395,Attitude Control of a 2U Cubesat by Magnetic and Air Drag Torques,"  This paper describes the development of a magnetic attitude control subsystem
for a 2U cubesat. Due to the presence of gravity gradient torques, the
satellite dynamics are open-loop unstable near the desired pointing
configuration. Nevertheless the linearized time-varying system is completely
controllable, under easily verifiable conditions, and the system's disturbance
rejection capabilities can be enhanced by adding air drag panels exemplifying a
beneficial interplay between hardware design and control. In the paper,
conditions for the complete controllability for the case of a magnetically
controlled satellite with passive air drag panels are developed, and simulation
case studies with the LQR and MPC control designs applied in combination with a
nonlinear time-varying input transformation are presented to demonstrate the
ability of the closed-loop system to satisfy mission objectives despite
disturbance torques.
",0,0,1,0,0,0
396,"Random Forests, Decision Trees, and Categorical Predictors: The ""Absent Levels"" Problem","  One advantage of decision tree based methods like random forests is their
ability to natively handle categorical predictors without having to first
transform them (e.g., by using feature engineering techniques). However, in
this paper, we show how this capability can lead to an inherent ""absent levels""
problem for decision tree based methods that has never been thoroughly
discussed, and whose consequences have never been carefully explored. This
problem occurs whenever there is an indeterminacy over how to handle an
observation that has reached a categorical split which was determined when the
observation in question's level was absent during training. Although these
incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's
random forests FORTRAN code and the randomForest R package (Liaw and Wiener,
2002) as motivating case studies, we examine how overlooking the absent levels
problem can systematically bias a model. Furthermore, by using three real data
examples, we illustrate how absent levels can dramatically alter a model's
performance in practice, and we empirically demonstrate how some simple
heuristics can be used to help mitigate the effects of the absent levels
problem until a more robust theoretical solution is found.
",1,0,0,1,0,0
397,Temporal correlation detection using computational phase-change memory,"  For decades, conventional computers based on the von Neumann architecture
have performed computation by repeatedly transferring data between their
processing and their memory units, which are physically separated. As
computation becomes increasingly data-centric and as the scalability limits in
terms of performance and power are being reached, alternative computing
paradigms are searched for in which computation and storage are collocated. A
fascinating new approach is that of computational memory where the physics of
nanoscale memory devices are used to perform certain computational tasks within
the memory unit in a non-von Neumann manner. Here we present a large-scale
experimental demonstration using one million phase-change memory devices
organized to perform a high-level computational primitive by exploiting the
crystallization dynamics. Also presented is an application of such a
computational memory to process real-world data-sets. The results show that
this co-existence of computation and storage at the nanometer scale could be
the enabler for new, ultra-dense, low power, and massively parallel computing
systems.
",1,0,0,0,0,0
398,Complexity and capacity bounds for quantum channels,"  We generalise some well-known graph parameters to operator systems by
considering their underlying quantum channels. In particular, we introduce the
quantum complexity as the dimension of the smallest co-domain Hilbert space a
quantum channel requires to realise a given operator system as its
non-commutative confusability graph. We describe quantum complexity as a
generalised minimum semidefinite rank and, in the case of a graph operator
system, as a quantum intersection number. The quantum complexity and a closely
related quantum version of orthogonal rank turn out to be upper bounds for the
Shannon zero-error capacity of a quantum channel, and we construct examples for
which these bounds beat the best previously known general upper bound for the
capacity of quantum channels, given by the quantum Lov?­sz theta number.
",0,0,1,0,0,0
399,Quantum Interference of Glory Rescattering in Strong-Field Atomic Ionization,"  During the ionization of atoms irradiated by linearly polarized intense laser
fields, we find for the first time that the transverse momentum distribution of
photoelectrons can be well fitted by a squared zeroth-order Bessel function
because of the quantum interference effect of Glory rescattering. The
characteristic of the Bessel function is determined by the common angular
momentum of a bunch of semiclassical paths termed as Glory trajectories, which
are launched with different nonzero initial transverse momenta distributed on a
specific circle in the momentum plane and finally deflected to the same
asymptotic momentum, which is along the polarization direction, through
post-tunneling rescattering. Glory rescattering theory (GRT) based on the
semiclassical path-integral formalism is developed to address this effect
quantitatively. Our theory can resolve the long-standing discrepancies between
existing theories and experiments on the fringe location, predict the sudden
transition of the fringe structure in holographic patterns, and shed light on
the quantum interference aspects of low-energy structures in strong-field
atomic ionization.
",0,1,0,0,0,0
400,On vector measures and extensions of transfunctions,"  We are interested in extending operators defined on positive measures, called
here transfunctions, to signed measures and vector measures. Our methods use a
somewhat nonstandard approach to measures and vector measures. The necessary
background, including proofs of some auxiliary results, is included.
",0,0,1,0,0,0
401,Deep Within-Class Covariance Analysis for Robust Audio Representation Learning,"  Convolutional Neural Networks (CNNs) can learn effective features, though
have been shown to suffer from a performance drop when the distribution of the
data changes from training to test data. In this paper we analyze the internal
representations of CNNs and observe that the representations of unseen data in
each class, spread more (with higher variance) in the embedding space of the
CNN compared to representations of the training data. More importantly, this
difference is more extreme if the unseen data comes from a shifted
distribution. Based on this observation, we objectively evaluate the degree of
representation's variance in each class via eigenvalue decomposition on the
within-class covariance of the internal representations of CNNs and observe the
same behaviour. This can be problematic as larger variances might lead to
mis-classification if the sample crosses the decision boundary of its class. We
apply nearest neighbor classification on the representations and empirically
show that the embeddings with the high variance actually have significantly
worse KNN classification performances, although this could not be foreseen from
their end-to-end classification results. To tackle this problem, we propose
Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that
significantly reduces the within-class covariance of a DNN's representation,
improving performance on unseen test data from a shifted distribution. We
empirically evaluate DWCCA on two datasets for Acoustic Scene Classification
(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA
significantly improve the network's internal representation, it also increases
the end-to-end classification accuracy, especially when the test set exhibits a
distribution shift. By adding DWCCA to a VGG network, we achieve around 6
percentage points improvement in the case of a distribution mismatch.
",1,0,0,0,0,0
402,Efficient Online Bandit Multiclass Learning with $\tilde{O}(\sqrt{T})$ Regret,"  We present an efficient second-order algorithm with
$\tilde{O}(\frac{1}{\eta}\sqrt{T})$ regret for the bandit online multiclass
problem. The regret bound holds simultaneously with respect to a family of loss
functions parameterized by $\eta$, for a range of $\eta$ restricted by the norm
of the competitor. The family of loss functions ranges from hinge loss
($\eta=0$) to squared hinge loss ($\eta=1$). This provides a solution to the
open problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for
$\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our
algorithm experimentally, showing that it also performs favorably against
earlier algorithms.
",0,0,0,1,0,0
403,Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning,"  Swarm systems constitute a challenging problem for reinforcement learning
(RL) as the algorithm needs to learn decentralized control policies that can
cope with limited local sensing and communication abilities of the agents.
While it is often difficult to directly define the behavior of the agents,
simple communication protocols can be defined more easily using prior knowledge
about the given task. In this paper, we propose a number of simple
communication protocols that can be exploited by deep reinforcement learning to
find decentralized control policies in a multi-robot swarm environment. The
protocols are based on histograms that encode the local neighborhood relations
of the agents and can also transmit task-specific information, such as the
shortest distance and direction to a desired target. In our framework, we use
an adaptation of Trust Region Policy Optimization to learn complex
collaborative tasks, such as formation building and building a communication
link. We evaluate our findings in a simulated 2D-physics environment, and
compare the implications of different communication protocols.
",1,0,0,1,0,0
404,Towards exascale real-time RFI mitigation,"  We describe the design and implementation of an extremely scalable real-time
RFI mitigation method, based on the offline AOFlagger. All algorithms scale
linearly in the number of samples. We describe how we implemented the flagger
in the LOFAR real-time pipeline, on both CPUs and GPUs. Additionally, we
introduce a novel simple history-based flagger that helps reduce the impact of
our small window on the data.
By examining an observation of a known pulsar, we demonstrate that our
flagger can achieve much higher quality than a simple thresholder, even when
running in real time, on a distributed system. The flagger works on visibility
data, but also on raw voltages, and beam formed data. The algorithms are
scale-invariant, and work on microsecond to second time scales. We are
currently implementing a prototype for the time domain pipeline of the SKA
central signal processor.
",0,1,0,0,0,0
405,Learning body-affordances to simplify action spaces,"  Controlling embodied agents with many actuated degrees of freedom is a
challenging task. We propose a method that can discover and interpolate between
context dependent high-level actions or body-affordances. These provide an
abstract, low-dimensional interface indexing high-dimensional and time-
extended action policies. Our method is related to recent ap- proaches in the
machine learning literature but is conceptually simpler and easier to
implement. More specifically our method requires the choice of a n-dimensional
target sensor space that is endowed with a distance metric. The method then
learns an also n-dimensional embedding of possibly reactive body-affordances
that spread as far as possible throughout the target sensor space.
",1,0,0,0,0,0
406,Cayley properties of the line graphs induced by of consecutive layers of the hypercube,"  Let $n >3$ and $ 0< k < \frac{n}{2} $ be integers. In this paper, we
investigate some algebraic properties of the line graph of the graph $
{Q_n}(k,k+1) $ where $ {Q_n}(k,k+1) $ is the subgraph of the hypercube $Q_n$
which is induced by the set of vertices of weights $k$ and $k+1$. In the first
step, we determine the automorphism groups of these graphs for all values of
$k$. In the second step, we study Cayley properties of the line graph of these
graphs. In particular, we show that for $ k>2, $ if $ 2k+1 \neq n$, then the
line graph of the graph $ {Q_n}(k,k+1) $ is a vertex-transitive non Cayley
graph. Also, we show that the line graph of the graph $ {Q_n}(1,2) $ is a
Cayley graph if and only if $ n$ is a power of a prime $p$.
",0,0,1,0,0,0
407,Beyond the technical challenges for deploying Machine Learning solutions in a software company,"  Recently software development companies started to embrace Machine Learning
(ML) techniques for introducing a series of advanced functionality in their
products such as personalisation of the user experience, improved search,
content recommendation and automation. The technical challenges for tackling
these problems are heavily researched in literature. A less studied area is a
pragmatic approach to the role of humans in a complex modern industrial
environment where ML based systems are developed. Key stakeholders affect the
system from inception and up to operation and maintenance. Product managers
want to embed ""smart"" experiences for their users and drive the decisions on
what should be built next; software engineers are challenged to build or
utilise ML software tools that require skills that are well outside of their
comfort zone; legal and risk departments may influence design choices and data
access; operations teams are requested to maintain ML systems which are
non-stationary in their nature and change behaviour over time; and finally ML
practitioners should communicate with all these stakeholders to successfully
build a reliable system. This paper discusses some of the challenges we faced
in Atlassian as we started investing more in the ML space.
",1,0,0,1,0,0
408,Class-Splitting Generative Adversarial Networks,"  Generative Adversarial Networks (GANs) produce systematically better quality
samples when class label information is provided., i.e. in the conditional GAN
setup. This is still observed for the recently proposed Wasserstein GAN
formulation which stabilized adversarial training and allows considering high
capacity network architectures such as ResNet. In this work we show how to
boost conditional GAN by augmenting available class labels. The new classes
come from clustering in the representation space learned by the same GAN model.
The proposed strategy is also feasible when no class information is available,
i.e. in the unsupervised setup. Our generated samples reach state-of-the-art
Inception scores for CIFAR-10 and STL-10 datasets in both supervised and
unsupervised setup.
",0,0,0,1,0,0
409,Dynamical system analysis of dark energy models in scalar coupled metric-torsion theories,"  We study the phase space dynamics of cosmological models in the theoretical
formulations of non-minimal metric-torsion couplings with a scalar field, and
investigate in particular the critical points which yield stable solutions
exhibiting cosmic acceleration driven by the {\em dark energy}. The latter is
defined in a way that it effectively has no direct interaction with the
cosmological fluid, although in an equivalent scalar-tensor cosmological setup
the scalar field interacts with the fluid (which we consider to be the
pressureless dust). Determining the conditions for the existence of the stable
critical points we check their physical viability, in both Einstein and Jordan
frames. We also verify that in either of these frames, the evolution of the
universe at the corresponding stable points matches with that given by the
respective exact solutions we have found in an earlier work (arXiv: 1611.00654
[gr-qc]). We not only examine the regions of physical relevance for the
trajectories in the phase space when the coupling parameter is varied, but also
demonstrate the evolution profiles of the cosmological parameters of interest
along fiducial trajectories in the effectively non-interacting scenarios, in
both Einstein and Jordan frames.
",0,1,0,0,0,0
410,J-MOD$^{2}$: Joint Monocular Obstacle Detection and Depth Estimation,"  In this work, we propose an end-to-end deep architecture that jointly learns
to detect obstacles and estimate their depth for MAV flight applications. Most
of the existing approaches either rely on Visual SLAM systems or on depth
estimation models to build 3D maps and detect obstacles. However, for the task
of avoiding obstacles this level of complexity is not required. Recent works
have proposed multi task architectures to both perform scene understanding and
depth estimation. We follow their track and propose a specific architecture to
jointly estimate depth and obstacles, without the need to compute a global map,
but maintaining compatibility with a global SLAM system if needed. The network
architecture is devised to exploit the joint information of the obstacle
detection task, that produces more reliable bounding boxes, with the depth
estimation one, increasing the robustness of both to scenario changes. We call
this architecture J-MOD$^{2}$. We test the effectiveness of our approach with
experiments on sequences with different appearance and focal lengths and
compare it to SotA multi task methods that jointly perform semantic
segmentation and depth estimation. In addition, we show the integration in a
full system using a set of simulated navigation experiments where a MAV
explores an unknown scenario and plans safe trajectories by using our detection
model.
",1,0,0,0,0,0
411,The Calabi flow with rough initial data,"  In this paper, we prove that there exists a dimensional constant $\delta > 0$
such that given any background K??hler metric $\omega$, the Calabi flow with
initial data $u_0$ satisfying \begin{equation*} \partial \bar \partial u_0 \in
L^\infty (M) \text{ and } (1- \delta )\omega < \omega_{u_0} < (1+\delta
)\omega, \end{equation*} admits a unique short time solution and it becomes
smooth immediately, where $\omega_{u_0} : = \omega +\sqrt{-1}\partial
\bar\partial u_0$. The existence time depends on initial data $u_0$ and the
metric $\omega$. As a corollary, we get that Calabi flow has short time
existence for any initial data satisfying \begin{equation*} \partial \bar
\partial u_0 \in C^0(M) \text{ and } \omega_{u_0} > 0, \end{equation*} which
should be interpreted as a ""continuous K??hler metric"". A main technical
ingredient is Schauder-type estimates for biharmonic heat equation on
Riemannian manifolds with time weighted H??lder norms.
",0,0,1,0,0,0
412,Star Formation Activity in the molecular cloud G35.20$-$0.74: onset of cloud-cloud collision,"  To probe the star-formation (SF) processes, we present results of an analysis
of the molecular cloud G35.20$-$0.74 (hereafter MCG35.2) using multi-frequency
observations. The MCG35.2 is depicted in a velocity range of 30-40 km s$^{-1}$.
An almost horseshoe-like structure embedded within the MCG35.2 is evident in
the infrared and millimeter images and harbors the previously known sites,
ultra-compact/hyper-compact G35.20$-$0.74N H\,{\sc ii} region, Ap2-1, and
Mercer 14 at its base. The site, Ap2-1 is found to be excited by a radio
spectral type of B0.5V star where the distribution of 20 cm and H$\alpha$
emission is surrounded by the extended molecular hydrogen emission. Using the
{\it Herschel} 160-500 $\mu$m and photometric 1-24 $\mu$m data analysis,
several embedded clumps and clusters of young stellar objects (YSOs) are
investigated within the MCG35.2, revealing the SF activities. Majority of the
YSOs clusters and massive clumps (500-4250 M$_{\odot}$) are seen toward the
horseshoe-like structure. The position-velocity analysis of $^{13}$CO emission
shows a blue-shifted peak (at 33 km s$^{-1}$) and a red-shifted peak (at 37 km
s$^{-1}$) interconnected by lower intensity intermediated velocity emission,
tracing a broad bridge feature. The presence of such broad bridge feature
suggests the onset of a collision between molecular components in the MCG35.2.
A noticeable change in the H-band starlight mean polarization angles has also
been observed in the MCG35.2, probably tracing the interaction between
molecular components. Taken together, it seems that the cloud-cloud collision
process has influenced the birth of massive stars and YSOs clusters in the
MCG35.2.
",0,1,0,0,0,0
413,Oblivious Routing via Random Walks,"  We present novel oblivious routing algorithms for both splittable and
unsplittable multicommodity flow. Our algorithm for minimizing congestion for
\emph{unsplittable} multicommodity flow is the first oblivious routing
algorithm for this setting. As an intermediate step towards this algorithm, we
present a novel generalization of Valiant's classical load balancing scheme for
packet-switched networks to arbitrary graphs, which is of independent interest.
Our algorithm for minimizing congestion for \emph{splittable} multicommodity
flow improves upon the state-of-the-art, in terms of both running time and
performance, for graphs that exhibit good expansion guarantees. Our algorithms
rely on diffusing traffic via iterative applications of the random walk
operator. Consequently, the performance guarantees of our algorithms are
derived from the convergence of the random walk operator to the stationary
distribution and are expressed in terms of the spectral gap of the graph (which
dominates the mixing time).
",1,0,0,0,0,0
414,On Functional Graphs of Quadratic Polynomials,"  We study functional graphs generated by quadratic polynomials over prime
fields. We introduce efficient algorithms for methodical computations and
provide the values of various direct and cumulative statistical parameters of
interest. These include: the number of connected functional graphs, the number
of graphs having a maximal cycle, the number of cycles of fixed size, the
number of components of fixed size, as well as the shape of trees extracted
from functional graphs. We particularly focus on connected functional graphs,
that is, the graphs which contain only one component (and thus only one cycle).
Based on the results of our computations, we formulate several conjectures
highlighting the similarities and differences between these functional graphs
and random mappings.
",0,0,1,0,0,0
415,Helmholtz decomposition theorem and Blumenthal's extension by regularization,"  Helmholtz decomposition theorem for vector fields is usually presented with
too strong restrictions on the fields and only for time independent fields.
Blumenthal showed in 1905 that decomposition is possible for any asymptotically
weakly decreasing vector field. He used a regularization method in his proof
which can be extended to prove the theorem even for vector fields
asymptotically increasing sublinearly. Blumenthal's result is then applied to
the time-dependent fields of the dipole radiation and an artificial sublinearly
increasing field.
",0,1,0,0,0,0
416,A homotopy decomposition of the fibre of the squaring map on $??^3S^{17}$,"  We use Richter's $2$-primary proof of Gray's conjecture to give a homotopy
decomposition of the fibre $\Omega^3S^{17}\{2\}$ of the $H$-space squaring map
on the triple loop space of the $17$-sphere. This induces a splitting of the
mod-$2$ homotopy groups $\pi_\ast(S^{17}; \mathbb{Z}/2\mathbb{Z})$ in terms of
the integral homotopy groups of the fibre of the double suspension
$E^2:S^{2n-1} \to \Omega^2S^{2n+1}$ and refines a result of Cohen and Selick,
who gave similar decompositions for $S^5$ and $S^9$. We relate these
decompositions to various Whitehead products in the homotopy groups of mod-$2$
Moore spaces and Stiefel manifolds to show that the Whitehead square $[i_{2n},
i_{2n}]$ of the inclusion of the bottom cell of the Moore space $P^{2n+1}(2)$
is divisible by $2$ if and only if $2n=2, 4, 8$ or $16$.
",0,0,1,0,0,0
417,Spaces of orders of some one-relator groups,"  We show that certain orderable groups admit no isolated left orders. The
groups we consider are cyclic amalgamations of a free group with a general
orderable group, the HNN extensions of free groups over cyclic subgroups, and a
particular class of one-relator groups. In order to prove the results about
orders, we develop perturbation techniques for actions of these groups on the
line.
",0,0,1,0,0,0
418,Adversarial Attacks on Neural Network Policies,"  Machine learning classifiers are known to be vulnerable to inputs maliciously
constructed by adversaries to force misclassification. Such adversarial
examples have been extensively studied in the context of computer vision
applications. In this work, we show adversarial attacks are also effective when
targeting neural network policies in reinforcement learning. Specifically, we
show existing adversarial example crafting techniques can be used to
significantly degrade test-time performance of trained policies. Our threat
model considers adversaries capable of introducing small perturbations to the
raw input of the policy. We characterize the degree of vulnerability across
tasks and training algorithms, for a subclass of adversarial-example attacks in
white-box and black-box settings. Regardless of the learned task or training
algorithm, we observe a significant drop in performance, even with small
adversarial perturbations that do not interfere with human perception. Videos
are available at this http URL.
",1,0,0,1,0,0
419,Stellar streams as gravitational experiments I. The case of Sagittarius,"  Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy
offer a unique way to constrain the shape of galactic gravitational potentials.
Such streams can be used as leaning tower gravitational experiments on galactic
scales. The most well motivated modification of gravity proposed as an
alternative to dark matter on galactic scales is Milgromian dynamics (MOND),
and we present here the first ever N-body simulations of the dynamical
evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a
realistic baryonic mass model for the Milky Way, we attempt to reproduce the
present-day spatial and kinematic structure of the Sagittarius dwarf and its
immense tidal stream that wraps around the Milky Way. With very little freedom
on the original structure of the progenitor, constrained by the total
luminosity of the Sagittarius structure and by the observed stellar mass-size
relation for isolated dwarf galaxies, we find reasonable agreement between our
simulations and observations of this system. The observed stellar velocities in
the leading arm can be reproduced if we include a massive hot gas corona around
the Milky Way that is flattened in the direction of the principal plane of its
satellites. This is the first time that tidal dissolution in MOND has been
tested rigorously at these mass and acceleration scales.
",0,1,0,0,0,0
420,Tuning quantum non-local effects in graphene plasmonics,"  The response of an electron system to electromagnetic fields with sharp
spatial variations is strongly dependent on quantum electronic properties, even
in ambient conditions, but difficult to access experimentally. We use
propagating graphene plasmons, together with an engineered dielectric-metallic
environment, to probe the graphene electron liquid and unveil its detailed
electronic response at short wavelengths.The near-field imaging experiments
reveal a parameter-free match with the full theoretical quantum description of
the massless Dirac electron gas, in which we identify three types of quantum
effects as keys to understanding the experimental response of graphene to
short-ranged terahertz electric fields. The first type is of single-particle
nature and is related to shape deformations of the Fermi surface during a
plasmon oscillations. The second and third types are a many-body effect
controlled by the inertia and compressibility of the interacting electron
liquid in graphene. We demonstrate how, in principle, our experimental approach
can determine the full spatiotemporal response of an electron system.
",0,1,0,0,0,0
421,Flows along arch filaments observed in the GRIS 'very fast spectroscopic mode',"  A new generation of solar instruments provides improved spectral, spatial,
and temporal resolution, thus facilitating a better understanding of dynamic
processes on the Sun. High-resolution observations often reveal
multiple-component spectral line profiles, e.g., in the near-infrared He I
10830 \AA\ triplet, which provides information about the chromospheric velocity
and magnetic fine structure. We observed an emerging flux region, including two
small pores and an arch filament system, on 2015 April 17 with the 'very fast
spectroscopic mode' of the GREGOR Infrared Spectrograph (GRIS) situated at the
1.5-meter GREGOR solar telescope at Observatorio del Teide, Tenerife, Spain. We
discuss this method of obtaining fast (one per minute) spectral scans of the
solar surface and its potential to follow dynamic processes on the Sun. We
demonstrate the performance of the 'very fast spectroscopic mode' by tracking
chromospheric high-velocity features in the arch filament system.
",0,1,0,0,0,0
422,Rethinking Information Sharing for Actionable Threat Intelligence,"  In the past decade, the information security and threat landscape has grown
significantly making it difficult for a single defender to defend against all
attacks at the same time. This called for introduc- ing information sharing, a
paradigm in which threat indicators are shared in a community of trust to
facilitate defenses. Standards for representation, exchange, and consumption of
indicators are pro- posed in the literature, although various issues are
undermined. In this paper, we rethink information sharing for actionable
intelli- gence, by highlighting various issues that deserve further explo-
ration. We argue that information sharing can benefit from well- defined use
models, threat models, well-understood risk by mea- surement and robust
scoring, well-understood and preserved pri- vacy and quality of indicators and
robust mechanism to avoid free riding behavior of selfish agent. We call for
using the differential nature of data and community structures for optimizing
sharing.
",1,0,0,0,0,0
423,More new classes of permutation trinomials over $\mathbb{F}_{2^n}$,"  Permutation polynomials over finite fields have wide applications in many
areas of science and engineering. In this paper, we present six new classes of
permutation trinomials over $\mathbb{F}_{2^n}$ which have explicit forms by
determining the solutions of some equations.
",0,0,1,0,0,0
424,Distributive Aronszajn trees,"  Ben-David and Shelah proved that if $\lambda$ is a singular strong-limit
cardinal and $2^\lambda=\lambda^+$, then $\square^*_\lambda$ entails the
existence of a normal $\lambda$-distributive $\lambda^+$-Aronszajn tree. Here,
it is proved that the same conclusion remains valid after replacing the
hypothesis $\square^*_\lambda$ by $\square(\lambda^+,{<}\lambda)$.
As $\square(\lambda^+,{<}\lambda)$ does not impose a bound on the order-type
of the witnessing clubs, our construction is necessarily different from that of
Ben-David and Shelah, and instead uses walks on ordinals augmented with club
guessing.
A major component of this work is the study of postprocessing functions and
their effect on square sequences. A byproduct of this study is the finding that
for $\kappa$ regular uncountable, $\square(\kappa)$ entails the existence of a
partition of $\kappa$ into $\kappa$ many fat sets. When contrasted with a
classic model of Magidor, this shows that it is equiconsistent with the
existence of a weakly compact cardinal that $\omega_2$ cannot be split into two
fat sets.
",0,0,1,0,0,0
425,Analytical solutions for the radial Scarf II potential,"  The real Scarf II potential is discussed as a radial problem. This potential
has been studied extensively as a one-dimensional problem, and now these
results are used to construct its bound and resonance solutions for $l=0$ by
setting the origin at some arbitrary value of the coordinate. The solutions
with appropriate boundary conditions are composed as the linear combination of
the two independent solutions of the Schr??dinger equation. The asymptotic
expression of these solutions is used to construct the $S_0(k)$ s-wave
$S$-matrix, the poles of which supply the $k$ values corresponding to the
bound, resonance and anti-bound solutions. The location of the discrete energy
eigenvalues is analyzed, and the relation of the solutions of the radial and
one-dimensional Scarf II potentials is discussed. It is shown that the
generalized Woods--Saxon potential can be generated from the Rosen--Morse II
potential in the same way as the radial Scarf II potential is obtained from its
one-dimensional correspondent. Based on this analogy, possible applications are
also pointed out.
",0,1,0,0,0,0
426,Gated Multimodal Units for Information Fusion,"  This paper presents a novel model for multimodal learning based on gated
neural networks. The Gated Multimodal Unit (GMU) model is intended to be used
as an internal unit in a neural network architecture whose purpose is to find
an intermediate representation based on a combination of data from different
modalities. The GMU learns to decide how modalities influence the activation of
the unit using multiplicative gates. It was evaluated on a multilabel scenario
for genre classification of movies using the plot and the poster. The GMU
improved the macro f-score performance of single-modality approaches and
outperformed other fusion strategies, including mixture of experts models.
Along with this work, the MM-IMDb dataset is released which, to the best of our
knowledge, is the largest publicly available multimodal dataset for genre
prediction on movies.
",0,0,0,1,0,0
427,Why Condorcet Consistency is Essential,"  In a single winner election with several candidates and ranked choice or
rating scale ballots, a Condorcet winner is one who wins all their two way
races by majority rule or MR. A voting system has Condorcet consistency or CC
if it names any Condorcet winner the winner. Many voting systems lack CC, but a
three step line of reasoning is used here to show why it is necessary. In step
1 we show that we can dismiss all the electoral criteria which conflict with
CC. In step 2 we point out that CC follows almost automatically if we can agree
that MR is the only acceptable system for elections with two candidates. In
step 3 we make that argument for MR. This argument itself has three parts.
First, in races with two candidates, the only well known alternatives to MR can
sometimes name as winner a candidate who is preferred over their opponent by
only one voter, with all others preferring the opponent. That is unacceptable.
Second, those same systems are also extremely susceptible to strategic
insincere voting. Third, in simulation studies using spatial models with two
candidates, the best known alternative to MR picks the best or most centrist
candidate significantly less often than MR does.
",0,0,0,1,0,0
428,Birefringence induced by pp-wave modes in an electromagnetically active dynamic aether,"  In the framework of the Einstein-Maxwell-aether theory we study the
birefringence effect, which can occur in the pp-wave symmetric dynamic aether.
The dynamic aether is considered to be latently birefringent quasi-medium,
which displays this hidden property if and only if the aether motion is
non-uniform, i.e., when the aether flow is characterized by the non-vanishing
expansion, shear, vorticity or acceleration. In accordance with the
dynamo-optical scheme of description of the interaction between electromagnetic
waves and the dynamic aether, we shall model the susceptibility tensors by the
terms linear in the covariant derivative of the aether velocity four-vector.
When the pp-wave modes appear in the dynamic aether, we deal with a
gravitationally induced degeneracy removal with respect to hidden
susceptibility parameters. As a consequence, the phase velocities of
electromagnetic waves possessing orthogonal polarizations do not coincide, thus
displaying the birefringence effect. Two electromagnetic field configurations
are studied in detail: longitudinal and transversal with respect to the aether
pp-wave front. For both cases the solutions are found, which reveal anomalies
in the electromagnetic response on the action of the pp-wave aether mode.
",0,1,0,0,0,0
429,On generalizations of $p$-sets and their applications,"  The $p$-set, which is in a simple analytic form, is well distributed in unit
cubes. The well-known Weil's exponential sum theorem presents an upper bound of
the exponential sum over the $p$-set. Based on the result, one shows that the
$p$-set performs well in numerical integration, in compressed sensing as well
as in UQ. However, $p$-set is somewhat rigid since the cardinality of the
$p$-set is a prime $p$ and the set only depends on the prime number $p$. The
purpose of this paper is to present generalizations of $p$-sets, say
$\mathcal{P}_{d,p}^{{\mathbf a},\epsilon}$, which is more flexible.
Particularly, when a prime number $p$ is given, we have many different choices
of the new $p$-sets. Under the assumption that Goldbach conjecture holds, for
any even number $m$, we present a point set, say ${\mathcal L}_{p,q}$, with
cardinality $m-1$ by combining two different new $p$-sets, which overcomes a
major bottleneck of the $p$-set. We also present the upper bounds of the
exponential sums over $\mathcal{P}_{d,p}^{{\mathbf a},\epsilon}$ and ${\mathcal
L}_{p,q}$, which imply these sets have many potential applications.
",1,0,1,0,0,0
430,Robot human interface for housekepeer with wireless capabilities,"  This paper presents the design and implementation of a Human Interface for a
housekeeper robot. It bases on the idea of making the robot understand the
human needs without making the human go through the details of robots work, for
example, the way that the robot implements the work or the method that the
robot uses to plan the path in order to reach the work area. The interface
commands based on idioms of the natural human language and designed in a manner
that the user gives the robot several commands with their execution date/time.
",1,0,0,0,0,0
431,Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector Machine Classifiers,"  This work proposes a new algorithm for training a re-weighted L2 Support
Vector Machine (SVM), inspired on the re-weighted Lasso algorithm of Cand??s
et al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In
particular, the margin required for each training vector is set independently,
defining a new weighted SVM model. These weights are selected to be binary, and
they are automatically adapted during the training of the model, resulting in a
variation of the Frank-Wolfe optimization algorithm with essentially the same
computational complexity as the original algorithm. As shown experimentally,
this algorithm is computationally cheaper to apply since it requires less
iterations to converge, and it produces models with a sparser representation in
terms of support vectors and which are more stable with respect to the
selection of the regularization hyper-parameter.
",1,0,0,1,0,0
432,Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation,"  Learning-based approaches to robotic manipulation are limited by the
scalability of data collection and accessibility of labels. In this paper, we
present a multi-task domain adaptation framework for instance grasping in
cluttered scenes by utilizing simulated robot experiments. Our neural network
takes monocular RGB images and the instance segmentation mask of a specified
target object as inputs, and predicts the probability of successfully grasping
the specified object for each candidate motor command. The proposed transfer
learning framework trains a model for instance grasping in simulation and uses
a domain-adversarial loss to transfer the trained model to real robots using
indiscriminate grasping data, which is available both in simulation and the
real world. We evaluate our model in real-world robot experiments, comparing it
with alternative model architectures as well as an indiscriminate grasping
baseline.
",1,0,0,0,0,0
433,Bounded Depth Ascending HNN Extensions and $??_1$-Semistability at $\infty$,"  A 1-ended finitely presented group has semistable fundamental group at
$\infty$ if it acts geometrically on some (equivalently any) simply connected
and locally finite complex $X$ with the property that any two proper rays in
$X$ are properly homotopic. If $G$ has semistable fundamental group at $\infty$
then one can unambiguously define the fundamental group at $\infty$ for $G$.
The problem, asking if all finitely presented groups have semistable
fundamental group at $\infty$ has been studied for over 40 years. If $G$ is an
ascending HNN extension of a finitely presented group then indeed, $G$ has
semistable fundamental group at $\infty$, but since the early 1980's it has
been suggested that the finitely presented groups that are ascending HNN
extensions of {\it finitely generated} groups may include a group with
non-semistable fundamental group at $\infty$. Ascending HNN extensions
naturally break into two classes, those with bounded depth and those with
unbounded depth. Our main theorem shows that bounded depth finitely presented
ascending HNN extensions of finitely generated groups have semistable
fundamental group at $\infty$. Semistability is equivalent to two weaker
asymptotic conditions on the group holding simultaneously. We show one of these
conditions holds for all ascending HNN extensions, regardless of depth. We give
a technique for constructing ascending HNN extensions with unbounded depth.
This work focuses attention on a class of groups that may contain a group with
non-semistable fundamental group at $\infty$.
",0,0,1,0,0,0
434,AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles,"  Developing and testing algorithms for autonomous vehicles in real world is an
expensive and time consuming process. Also, in order to utilize recent advances
in machine intelligence and deep learning we need to collect a large amount of
annotated training data in a variety of conditions and environments. We present
a new simulator built on Unreal Engine that offers physically and visually
realistic simulations for both of these goals. Our simulator includes a physics
engine that can operate at a high frequency for real-time hardware-in-the-loop
(HITL) simulations with support for popular protocols (e.g. MavLink). The
simulator is designed from the ground up to be extensible to accommodate new
types of vehicles, hardware platforms and software protocols. In addition, the
modular design enables various components to be easily usable independently in
other projects. We demonstrate the simulator by first implementing a quadrotor
as an autonomous vehicle and then experimentally comparing the software
components with real-world flights.
",1,0,0,0,0,0
435,Hausdorff dimensions in $p$-adic analytic groups,"  Let $G$ be a finitely generated pro-$p$ group, equipped with the $p$-power
series. The associated metric and Hausdorff dimension function give rise to the
Hausdorff spectrum, which consists of the Hausdorff dimensions of closed
subgroups of $G$. In the case where $G$ is $p$-adic analytic, the Hausdorff
dimension function is well understood; in particular, the Hausdorff spectrum
consists of finitely many rational numbers closely linked to the analytic
dimensions of subgroups of $G$.
Conversely, it is a long-standing open question whether the finiteness of the
Hausdorff spectrum implies that $G$ is $p$-adic analytic. We prove that the
answer is yes, in a strong sense, under the extra condition that $G$ is
soluble.
Furthermore, we explore the problem and related questions also for other
filtration series, such as the lower $p$-series, the Frattini series, the
modular dimension subgroup series and quite general filtration series. For
instance, we prove, for odd primes $p$, that every countably based pro-$p$
group $G$ with an open subgroup mapping onto 2 copies of the $p$-adic integers
admits a filtration series such that the corresponding Hausdorff spectrum
contains an infinite real interval.
",0,0,1,0,0,0
436,Real-time brain machine interaction via social robot gesture control,"  Brain-Machine Interaction (BMI) system motivates interesting and promising
results in forward/feedback control consistent with human intention. It holds
great promise for advancements in patient care and applications to
neurorehabilitation. Here, we propose a novel neurofeedback-based BCI robotic
platform using a personalized social robot in order to assist patients having
cognitive deficits through bilateral rehabilitation and mental training. For
initial testing of the platform, electroencephalography (EEG) brainwaves of a
human user were collected in real time during tasks of imaginary movements.
First, the brainwaves associated with imagined body kinematics parameters were
decoded to control a cursor on a computer screen in training protocol. Then,
the experienced subject was able to interact with a social robot via our
real-time BMI robotic platform. Corresponding to subject's imagery performance,
he/she received specific gesture movements and eye color changes as
neural-based feedback from the robot. This hands-free neurofeedback interaction
not only can be used for mind control of a social robot's movements, but also
sets the stage for application to enhancing and recovering mental abilities
such as attention via training in humans by providing real-time neurofeedback
from a social robot.
",1,0,0,0,0,0
437,City-Scale Road Audit System using Deep Learning,"  Road networks in cities are massive and is a critical component of mobility.
Fast response to defects, that can occur not only due to regular wear and tear
but also because of extreme events like storms, is essential. Hence there is a
need for an automated system that is quick, scalable and cost-effective for
gathering information about defects. We propose a system for city-scale road
audit, using some of the most recent developments in deep learning and semantic
segmentation. For building and benchmarking the system, we curated a dataset
which has annotations required for road defects. However, many of the labels
required for road audit have high ambiguity which we overcome by proposing a
label hierarchy. We also propose a multi-step deep learning model that segments
the road, subdivide the road further into defects, tags the frame for each
defect and finally localizes the defects on a map gathered using GPS. We
analyze and evaluate the models on image tagging as well as segmentation at
different levels of the label hierarchy.
",1,0,0,0,0,0
438,Mass and moment of inertia govern the transition in the dynamics and wakes of freely rising and falling cylinders,"  In this Letter, we study the motion and wake-patterns of freely rising and
falling cylinders in quiescent fluid. We show that the amplitude of oscillation
and the overall system-dynamics are intricately linked to two parameters: the
particle's mass-density relative to the fluid $m^* \equiv \rho_p/\rho_f$ and
its relative moment-of-inertia $I^* \equiv {I}_p/{I}_f$. This supersedes the
current understanding that a critical mass density ($m^*\approx$ 0.54) alone
triggers the sudden onset of vigorous vibrations. Using over 144 combinations
of ${m}^*$ and $I^*$, we comprehensively map out the parameter space covering
very heavy ($m^* > 10$) to very buoyant ($m^* < 0.1$) particles. The entire
data collapses into two scaling regimes demarcated by a transitional Strouhal
number, $St_t \approx 0.17$. $St_t$ separates a mass-dominated regime from a
regime dominated by the particle's moment of inertia. A shift from one regime
to the other also marks a gradual transition in the wake-shedding pattern: from
the classical $2S$~(2-Single) vortex mode to a $2P$~(2-Pairs) vortex mode.
Thus, auto-rotation can have a significant influence on the trajectories and
wakes of freely rising isotropic bodies.
",0,1,0,0,0,0
439,It Takes Two to Tango: Towards Theory of AI's Mind,"  Theory of Mind is the ability to attribute mental states (beliefs, intents,
knowledge, perspectives, etc.) to others and recognize that these mental states
may differ from one's own. Theory of Mind is critical to effective
communication and to teams demonstrating higher collective performance. To
effectively leverage the progress in Artificial Intelligence (AI) to make our
lives more productive, it is important for humans and AI to work well together
in a team. Traditionally, there has been much emphasis on research to make AI
more accurate, and (to a lesser extent) on having it better understand human
intentions, tendencies, beliefs, and contexts. The latter involves making AI
more human-like and having it develop a theory of our minds. In this work, we
argue that for human-AI teams to be effective, humans must also develop a
theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,
and quirks. We instantiate these ideas within the domain of Visual Question
Answering (VQA). We find that using just a few examples (50), lay people can be
trained to better predict responses and oncoming failures of a complex VQA
model. We further evaluate the role existing explanation (or interpretability)
modalities play in helping humans build ToAIM. Explainable AI has received
considerable scientific and popular attention in recent times. Surprisingly, we
find that having access to the model's internal states - its confidence in its
top-k predictions, explicit or implicit attention maps which highlight regions
in the image (and words in the question) the model is looking at (and listening
to) while answering a question about an image - do not help people better
predict its behavior.
",1,0,0,0,0,0
440,"On variation of dynamical canonical heights, and Intersection numbers","  We study families of varieties endowed with polarized canonical eigensystems
of several maps, inducing canonical heights on the dominating variety as well
as on the ""good"" fibers of the family. We show explicitely the dependence on
the parameter for global and local canonical heights defined by Kawaguchi when
the fibers change, extending previous works of J. Silverman and others.
Finally, fixing an absolute value $v \in K$ and a variety $V/K$, we descript
the Kawaguchi`s canonical local height $\hat{\lambda}_{V,E,\mathcal{Q},}(.,v)$
as an intersection number, provided that the polarized system $(V,\mathcal{Q})$
has a certain weak N??ron model over Spec$(\mathcal{O}_v)$ to be defined and
under some conditions depending on the special fiber. With this we extend
N??ron's work strengthening Silverman's results, which were for systems
having only one map.
",0,0,1,0,0,0
441,Enhancing the Spectral Hardening of Cosmic TeV Photons by Mixing with Axionlike Particles in the Magnetized Cosmic Web,"  Large-scale extragalactic magnetic fields may induce conversions between
very-high-energy photons and axionlike particles (ALPs), thereby shielding the
photons from absorption on the extragalactic background light. However, in
simplified ""cell"" models, used so far to represent extragalactic magnetic
fields, this mechanism would be strongly suppressed by current astrophysical
bounds. Here we consider a recent model of extragalactic magnetic fields
obtained from large-scale cosmological simulations. Such simulated magnetic
fields would have large enhancement in the filaments of matter. As a result,
photon-ALP conversions would produce a significant spectral hardening for
cosmic TeV photons. This effect would be probed with the upcoming Cherenkov
Telescope Array detector. This possible detection would give a unique chance to
perform a tomography of the magnetized cosmic web with ALPs.
",0,1,0,0,0,0
442,Forecasting in the light of Big Data,"  Predicting the future state of a system has always been a natural motivation
for science and practical applications. Such a topic, beyond its obvious
technical and societal relevance, is also interesting from a conceptual point
of view. This owes to the fact that forecasting lends itself to two equally
radical, yet opposite methodologies. A reductionist one, based on the first
principles, and the naive inductivist one, based only on data. This latter view
has recently gained some attention in response to the availability of
unprecedented amounts of data and increasingly sophisticated algorithmic
analytic techniques. The purpose of this note is to assess critically the role
of big data in reshaping the key aspects of forecasting and in particular the
claim that bigger data leads to better predictions. Drawing on the
representative example of weather forecasts we argue that this is not generally
the case. We conclude by suggesting that a clever and context-dependent
compromise between modelling and quantitative analysis stands out as the best
forecasting strategy, as anticipated nearly a century ago by Richardson and von
Neumann.
",0,1,0,0,0,0
443,Adelic point groups of elliptic curves,"  We show that for an elliptic curve E defined over a number field K, the group
E(A) of points of E over the adele ring A of K is a topological group that can
be analyzed in terms of the Galois representation associated to the torsion
points of E. An explicit description of E(A) is given, and we prove that for K
of degree n, almost all elliptic curves over K have an adelic point group
topologically isomorphic to a universal group depending on n. We also show that
there exist infinitely many elliptic curves over K having a different adelic
point group.
",0,0,1,0,0,0
444,Position Aided Beam Alignment for Millimeter Wave Backhaul Systems with Large Phased Arrays,"  Wireless backhaul communication has been recently realized with large
antennas operating in the millimeter wave (mmWave) frequency band and
implementing highly directional beamforming. In this paper, we focus on the
alignment problem of narrow beams between fixed position network nodes in
mmWave backhaul systems that are subject to small displacements due to wind
flow or ground vibration. We consider nodes equipped with antenna arrays that
are capable of performing only analog processing and communicate through
wireless channels including a line-of-sight component. Aiming at minimizing the
time needed to achieve beam alignment, we present an efficient method that
capitalizes on the exchange of position information between the nodes to design
their beamforming and combining vectors. Some numerical results on the outage
probability with the proposed beam alignment method offer useful preliminary
insights on the impact of some system and operation parameters.
",1,0,1,0,0,0
445,Deep & Cross Network for Ad Click Predictions,"  Feature engineering has been the key to the success of many prediction
models. However, the process is non-trivial and often requires manual feature
engineering or exhaustive searching. DNNs are able to automatically learn
feature interactions; however, they generate all the interactions implicitly,
and are not necessarily efficient in learning all types of cross features. In
this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits
of a DNN model, and beyond that, it introduces a novel cross network that is
more efficient in learning certain bounded-degree feature interactions. In
particular, DCN explicitly applies feature crossing at each layer, requires no
manual feature engineering, and adds negligible extra complexity to the DNN
model. Our experimental results have demonstrated its superiority over the
state-of-art algorithms on the CTR prediction dataset and dense classification
dataset, in terms of both model accuracy and memory usage.
",1,0,0,1,0,0
446,Fan-type spin structure in uni-axial chiral magnets,"  We investigate the spin structure of a uni-axial chiral magnet near the
transition temperatures in low fields perpendicular to the helical axis. We
find a fan-type modulation structure where the clockwise and counterclockwise
windings appear alternatively along the propagation direction of the modulation
structure. This structure is often realized in a Yoshimori-type (non-chiral)
helimagnet but it is rarely realized in a chiral helimagnet. To discuss
underlying physics of this structure, we reconsider the phase diagram (phase
boundary and crossover lines) through the free energy and asymptotic behaviors
of isolated solitons. The fan structure appears slightly below the phase
boundary of the continuous transition of instability-type. In this region,
there are no solutions containing any types of isolated solitons to the mean
field equations.
",0,1,0,0,0,0
447,Rotation of a synchronous viscoelastic shell,"  Several natural satellites of the giant planets have shown evidence of a
global internal ocean, coated by a thin, icy crust. This crust is probably
viscoelastic, which would alter its rotational response. This response would
translate into several rotational quantities, i.e. the obliquity, and the
librations at different frequencies, for which the crustal elasticity reacts
differently. This study aims at modelling the global response of the
viscoelastic crust. For that, I derive the time-dependency of the tensor of
inertia, which I combine with the time evolution of the rotational quantities,
thanks to an iterative algorithm. This algorithm combines numerical simulations
of the rotation with a digital filtering of the resulting tensor of inertia.
The algorithm works very well in the elastic case, provided the problem is not
resonant. However, considering tidal dissipation adds different phase lags to
the oscillating contributions, which challenge the convergence of the
algorithm.
",0,1,0,0,0,0
448,Direct estimation of density functionals using a polynomial basis,"  A number of fundamental quantities in statistical signal processing and
information theory can be expressed as integral functions of two probability
density functions. Such quantities are called density functionals as they map
density functions onto the real line. For example, information divergence
functions measure the dissimilarity between two probability density functions
and are useful in a number of applications. Typically, estimating these
quantities requires complete knowledge of the underlying distribution followed
by multi-dimensional integration. Existing methods make parametric assumptions
about the data distribution or use non-parametric density estimation followed
by high-dimensional integration. In this paper, we propose a new alternative.
We introduce the concept of ""data-driven basis functions"" - functions of
distributions whose value we can estimate given only samples from the
underlying distributions without requiring distribution fitting or direct
integration. We derive a new data-driven complete basis that is similar to the
deterministic Bernstein polynomial basis and develop two methods for performing
basis expansions of functionals of two distributions. We also show that the new
basis set allows us to approximate functions of distributions as closely as
desired. Finally, we evaluate the methodology by developing data driven
estimators for the Kullback-Leibler divergences and the Hellinger distance and
by constructing empirical estimates of tight bounds on the Bayes error rate.
",1,0,0,1,0,0
449,Experimental Evidence on a Refined Conjecture of the BSD type,"  Let $E/\mathbb{Q}$ be an elliptic curve of level $N$ and rank equal to $1$.
Let $p$ be a prime of ordinary reduction. We experimentally study conjecture
$4$ of B. Mazur and J. Tate in his article ""Refined Conjectures of the Birch
and Swinnerton-Dyer Type"". We report the computational evidence.
",0,0,1,0,0,0
450,The Tu--Deng Conjecture holds almost surely,"  The Tu--Deng Conjecture is concerned with the sum of digits $w(n)$ of $n$ in
base~$2$ (the Hamming weight of the binary expansion of $n$) and states the
following: assume that $k$ is a positive integer and $1\leq t<2^k-1$. Then
\[\Bigl \lvert\Bigl\{(a,b)\in\bigl\{0,\ldots,2^k-2\bigr\}^2:a+b\equiv t\bmod
2^k-1, w(a)+w(b)<k\Bigr\}\Bigr \rvert\leq 2^{k-1}.\]
We prove that the Tu--Deng Conjecture holds almost surely in the following
sense: the proportion of $t\in[1,2^k-2]$ such that the above inequality holds
approaches $1$ as $k\rightarrow\infty$.
Moreover, we prove that the Tu--Deng Conjecture implies a conjecture due to
T.~W.~Cusick concerning the sum of digits of $n$ and $n+t$.
",1,0,1,0,0,0
451,Convergence Analysis of the Dynamics of a Special Kind of Two-Layered Neural Networks with $\ell_1$ and $\ell_2$ Regularization,"  In this paper, we made an extension to the convergence analysis of the
dynamics of two-layered bias-free networks with one $ReLU$ output. We took into
consideration two popular regularization terms: the $\ell_1$ and $\ell_2$ norm
of the parameter vector $w$, and added it to the square loss function with
coefficient $\lambda/2$. We proved that when $\lambda$ is small, the weight
vector $w$ converges to the optimal solution $\hat{w}$ (with respect to the new
loss function) with probability $\geq (1-\varepsilon)(1-A_d)/2$ under random
initiations in a sphere centered at the origin, where $\varepsilon$ is a small
value and $A_d$ is a constant. Numerical experiments including phase diagrams
and repeated simulations verified our theory.
",1,0,0,1,0,0
452,"From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter","  We further progress along the line of Ref. [Phys. Rev. {\bf A 94}, 043614
(2016)] where a functional for Fermi systems with anomalously large $s$-wave
scattering length $a_s$ was proposed that has no free parameters. The
functional is designed to correctly reproduce the unitary limit in Fermi gases
together with the leading-order contributions in the s- and p-wave channels at
low density. The functional is shown to be predictive up to densities
$\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang
functional, valid for $\rho < 10^{-6}$ fm$^{-3}$. The form of the functional
retained in this work is further motivated. It is shown that the new functional
corresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all
orders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One
conclusion from the present work is that, except in the extremely low--density
regime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with
respect to the unitary limit. Starting from the functional, we introduce
density--dependent scales and show that scales associated to the bare
interaction are strongly renormalized by medium effects. As a consequence, some
of the scales at play around saturation are dominated by the unitary gas
properties and not directly to low-energy constants. For instance, we show that
the scale in the s-wave channel around saturation is proportional to the
so-called Bertsch parameter $\xi_0$ and becomes independent of $a_s$. We also
point out that these scales are of the same order of magnitude than those
empirically obtained in the Skyrme energy density functional. We finally
propose a slight modification of the functional such that it becomes accurate
up to the saturation density $\rho\simeq 0.16$ fm$^{-3}$.
",0,1,0,0,0,0
453,EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras (Extended Abstract),"  Marker-based and marker-less optical skeletal motion-capture methods use an
outside-in arrangement of cameras placed around a scene, with viewpoints
converging on the center. They often create discomfort by possibly needed
marker suits, and their recording volume is severely restricted and often
constrained to indoor scenes with controlled backgrounds. We therefore propose
a new method for real-time, marker-less and egocentric motion capture which
estimates the full-body skeleton pose from a lightweight stereo pair of fisheye
cameras that are attached to a helmet or virtual-reality headset. It combines
the strength of a new generative pose estimation framework for fisheye views
with a ConvNet-based body-part detector trained on a new automatically
annotated and augmented dataset. Our inside-in method captures full-body motion
in general indoor and outdoor scenes, and also crowded scenes.
",1,0,0,0,0,0
454,Diffusion Maps meet Nystr??m,"  Diffusion maps are an emerging data-driven technique for non-linear
dimensionality reduction, which are especially useful for the analysis of
coherent structures and nonlinear embeddings of dynamical systems. However, the
computational complexity of the diffusion maps algorithm scales with the number
of observations. Thus, long time-series data presents a significant challenge
for fast and efficient embedding. We propose integrating the Nystr??m method
with diffusion maps in order to ease the computational demand. We achieve a
speedup of roughly two to four times when approximating the dominant diffusion
map components.
",0,0,0,1,0,0
455,Multiphase Flows of N Immiscible Incompressible Fluids: An Outflow/Open Boundary Condition and Algorithm,"  We present a set of effective outflow/open boundary conditions and an
associated algorithm for simulating the dynamics of multiphase flows consisting
of $N$ ($N\geqslant 2$) immiscible incompressible fluids in domains involving
outflows or open boundaries. These boundary conditions are devised based on the
properties of energy stability and reduction consistency. The energy stability
property ensures that the contributions of these boundary conditions to the
energy balance will not cause the total energy of the N-phase system to
increase over time. Therefore, these open/outflow boundary conditions are very
effective in overcoming the backflow instability in multiphase systems. The
reduction consistency property ensures that if some fluid components are absent
from the N-phase system then these N-phase boundary conditions will reduce to
those corresponding boundary conditions for the equivalent smaller system. Our
numerical algorithm for the proposed boundary conditions together with the
N-phase governing equations involves only the solution of a set of de-coupled
individual Helmholtz-type equations within each time step, and the resultant
linear algebraic systems after discretization involve only constant and
time-independent coefficient matrices which can be pre-computed. Therefore, the
algorithm is computationally very efficient and attractive. We present
extensive numerical experiments for flow problems involving multiple fluid
components and inflow/outflow boundaries to test the proposed method. In
particular, we compare in detail the simulation results of a three-phase
capillary wave problem with Prosperetti's exact physical solution and
demonstrate that the method developed herein produces physically accurate
results.
",0,1,0,0,0,0
456,Deadly dark matter cusps vs faint and extended star clusters: Eridanus II and Andromeda XXV,"  The recent detection of two faint and extended star clusters in the central
regions of two Local Group dwarf galaxies, Eridanus II and Andromeda XXV,
raises the question of whether clusters with such low densities can survive the
tidal field of cold dark matter haloes with central density cusps. Using both
analytic arguments and a suite of collisionless N-body simulations, I show that
these clusters are extremely fragile and quickly disrupted in the presence of
central cusps $\rho\sim r^{-\alpha}$ with $\alpha\gtrsim 0.2$. Furthermore, the
scenario in which the clusters where originally more massive and sank to the
center of the halo requires extreme fine tuning and does not naturally
reproduce the observed systems. In turn, these clusters are long lived in cored
haloes, whose central regions are safe shelters for $\alpha\lesssim 0.2$. The
only viable scenario for hosts that have preserved their primoridal cusp to the
present time is that the clusters formed at rest at the bottom of the
potential, which is easily tested by measurement of the clusters proper
velocity within the host. This offers means to readily probe the central
density profile of two dwarf galaxies as faint as $L_V\sim5\times 10^5 L_\odot$
and $L_V\sim6\times10^4 L_\odot$, in which stellar feedback is unlikely to be
effective.
",0,1,0,0,0,0
457,"Mutual Information, Relative Entropy and Estimation Error in Semi-martingale Channels","  Fundamental relations between information and estimation have been
established in the literature for the continuous-time Gaussian and Poisson
channels, in a long line of work starting from the classical representation
theorems by Duncan and Kabanov respectively. In this work, we demonstrate that
such relations hold for a much larger family of continuous-time channels. We
introduce the family of semi-martingale channels where the channel output is a
semi-martingale stochastic process, and the channel input modulates the
characteristics of the semi-martingale. For these channels, which includes as a
special case the continuous time Gaussian and Poisson models, we establish new
representations relating the mutual information between the channel input and
output to an optimal causal filtering loss, thereby unifying and considerably
extending results from the Gaussian and Poisson settings. Extensions to the
setting of mismatched estimation are also presented where the relative entropy
between the laws governing the output of the channel under two different input
distributions is equal to the cumulative difference between the estimation loss
incurred by using the mismatched and optimal causal filters respectively. The
main tool underlying these results is the Doob--Meyer decomposition of a class
of likelihood ratio sub-martingales. The results in this work can be viewed as
the continuous-time analogues of recent generalizations for relations between
information and estimation for discrete-time L??vy channels.
",1,0,0,0,0,0
458,Testing redMaPPer centring probabilities using galaxy clustering and galaxy-galaxy lensing,"  Galaxy cluster centring is a key issue for precision cosmology studies using
galaxy surveys. Mis-identification of central galaxies causes systematics in
various studies such as cluster lensing, satellite kinematics, and galaxy
clustering. The red-sequence Matched-filter Probabilistic Percolation
(redMaPPer) estimates the probability that each member galaxy is central from
photometric information rather than specifying one central galaxy. The
redMaPPer estimates can be used for calibrating the off-centring effect,
however, the centring algorithm has not previously been well-tested. We test
the centring probabilities of redMaPPer cluster catalog using the projected
cross correlation between redMaPPer clusters with photometric red galaxies and
galaxy-galaxy lensing. We focus on the subsample of redMaPPer clusters in which
the redMaPPer central galaxies (RMCGs) are not the brightest member galaxies
(BMEM) and both of them have spectroscopic redshift. This subsample represents
nearly 10% of the whole cluster sample. We find a clear difference in the
cross-correlation measurements between RMCGs and BMEMs, and the estimated
centring probability is 74$\pm$10% for RMCGs and 13$\pm$4% for BMEMs in the
Gaussian offset model and 78$\pm$9% for RMCGs and 5$\pm$5% for BMEMs in the NFW
offset model. These values are in agreement with the centring probability
values reported by redMaPPer (75% for RMCG and 10% for BMEMs) within 1$\sigma$.
Our analysis provides a strong consistency test of the redMaPPer centring
probabilities. Our results suggest that redMaPPer centring probabilities are
reliably estimated. We confirm that the brightest galaxy in the cluster is not
always the central galaxy as has been shown in previous works.
",0,1,0,0,0,0
459,Criterion of positivity for semilinear problems with applications in biology,"  The goal of this article is to provide an useful criterion of positivity and
well-posedness for a wide range of infinite dimensional semilinear abstract
Cauchy problems. This criterion is based on some weak assumptions on the
non-linear part of the semilinear problem and on the existence of a strongly
continuous semigroup generated by the differential operator. To illustrate a
large variety of applications, we exhibit the feasibility of this criterion
through three examples in mathematical biology: epidemiology, predator-prey
interactions and oncology.
",0,0,1,0,0,0
460,Axiomatic quantum mechanics: Necessity and benefits for the physics studies,"  The ongoing progress in quantum theory emphasizes the crucial role of the
very basic principles of quantum theory. However, this is not properly followed
in teaching quantum mechanics on the graduate and undergraduate levels of
physics studies. The existing textbooks typically avoid the axiomatic
presentation of the theory. We emphasize usefulness of the systematic,
axiomatic approach to the basics of quantum theory as well as its importance in
the light of the modern scientific-research context.
",0,1,0,0,0,0
461,Kinetic modelling of competition and depletion of shared miRNAs by competing endogenous RNAs,"  Non-conding RNAs play a key role in the post-transcriptional regulation of
mRNA translation and turnover in eukaryotes. miRNAs, in particular, interact
with their target RNAs through protein-mediated, sequence-specific binding,
giving rise to extended and highly heterogeneous miRNA-RNA interaction
networks. Within such networks, competition to bind miRNAs can generate an
effective positive coupling between their targets. Competing endogenous RNAs
(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.
Albeit potentially weak, ceRNA interactions can occur both dynamically,
affecting e.g. the regulatory clock, and at stationarity, in which case ceRNA
networks as a whole can be implicated in the composition of the cell's
proteome. Many features of ceRNA interactions, including the conditions under
which they become significant, can be unraveled by mathematical and in silico
models. We review the understanding of the ceRNA effect obtained within such
frameworks, focusing on the methods employed to quantify it, its role in the
processing of gene expression noise, and how network topology can determine its
reach.
",0,0,0,0,1,0
462,Shortening binary complexes and commutativity of $K$-theory with infinite products,"  We show that in Grayson's model of higher algebraic $K$-theory using binary
acyclic complexes, the complexes of length two suffice to generate the whole
group. Moreover, we prove that the comparison map from Nenashev's model for
$K_1$ to Grayson's model for $K_1$ is an isomorphism. It follows that algebraic
$K$-theory of exact categories commutes with infinite products.
",0,0,1,0,0,0
463,Cost-Effective Seed Selection in Online Social Networks,"  We study the min-cost seed selection problem in online social networks, where
the goal is to select a set of seed nodes with the minimum total cost such that
the expected number of influenced nodes in the network exceeds a predefined
threshold. We propose several algorithms that outperform the previous studies
both on the theoretical approximation ratios and on the experimental
performance. Under the case where the nodes have heterogeneous costs, our
algorithms are the first bi- criteria approximation algorithms with polynomial
running time and provable logarithmic performance bounds using a general
contagion model. Under the case where the users have uniform costs, our
algorithms achieve logarithmic approximation ratio and provable time complexity
which is smaller than that of existing algorithms in orders of magnitude. We
conduct extensive experiments using real social networks. The experimental
results show that, our algorithms significantly outperform the existing
algorithms both on the total cost and on the running time, and also scale well
to billion-scale networks.
",1,0,0,0,0,0
464,Fast Meta-Learning for Adaptive Hierarchical Classifier Design,"  We propose a new splitting criterion for a meta-learning approach to
multiclass classifier design that adaptively merges the classes into a
tree-structured hierarchy of increasingly difficult binary classification
problems. The classification tree is constructed from empirical estimates of
the Henze-Penrose bounds on the pairwise Bayes misclassification rates that
rank the binary subproblems in terms of difficulty of classification. The
proposed empirical estimates of the Bayes error rate are computed from the
minimal spanning tree (MST) of the samples from each pair of classes. Moreover,
a meta-learning technique is presented for quantifying the one-vs-rest Bayes
error rate for each individual class from a single MST on the entire dataset.
Extensive simulations on benchmark datasets show that the proposed hierarchical
method can often be learned much faster than competing methods, while achieving
competitive accuracy.
",1,0,0,1,0,0
465,Vibrational Density Matrix Renormalization Group,"  Variational approaches for the calculation of vibrational wave functions and
energies are a natural route to obtain highly accurate results with
controllable errors. However, the unfavorable scaling and the resulting high
computational cost of standard variational approaches limit their application
to small molecules with only few vibrational modes. Here, we demonstrate how
the density matrix renormalization group (DMRG) can be exploited to optimize
vibrational wave functions (vDMRG) expressed as matrix product states. We study
the convergence of these calculations with respect to the size of the local
basis of each mode, the number of renormalized block states, and the number of
DMRG sweeps required. We demonstrate the high accuracy achieved by vDMRG for
small molecules that were intensively studied in the literature. We then
proceed to show that the complete fingerprint region of the sarcosyn-glycin
dipeptide can be calculated with vDMRG.
",0,1,0,0,0,0
466,Identification and Off-Policy Learning of Multiple Objectives Using Adaptive Clustering,"  In this work, we present a methodology that enables an agent to make
efficient use of its exploratory actions by autonomously identifying possible
objectives in its environment and learning them in parallel. The identification
of objectives is achieved using an online and unsupervised adaptive clustering
algorithm. The identified objectives are learned (at least partially) in
parallel using Q-learning. Using a simulated agent and environment, it is shown
that the converged or partially converged value function weights resulting from
off-policy learning can be used to accumulate knowledge about multiple
objectives without any additional exploration. We claim that the proposed
approach could be useful in scenarios where the objectives are initially
unknown or in real world scenarios where exploration is typically a time and
energy intensive process. The implications and possible extensions of this work
are also briefly discussed.
",1,0,0,0,0,0
467,Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization,"  Recent studies on diffusion-based sampling methods have shown that Langevin
Monte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and
rigorous theoretical guarantees have been proven for both asymptotic and
finite-time regimes. Algorithmically, LMC-based algorithms resemble the
well-known gradient descent (GD) algorithm, where the GD recursion is perturbed
by an additive Gaussian noise whose variance has a particular form. Fractional
Langevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the
Gaussian noise is replaced by a heavy-tailed {\alpha}-stable noise. As opposed
to its Gaussian counterpart, these heavy-tailed perturbations can incur large
jumps and it has been empirically demonstrated that the choice of
{\alpha}-stable noise can provide several advantages in modern machine learning
problems, both in optimization and sampling contexts. However, as opposed to
LMC, only asymptotic convergence properties of FLMC have been yet established.
In this study, we analyze the non-asymptotic behavior of FLMC for non-convex
optimization and prove finite-time bounds for its expected suboptimality. Our
results show that the weak-error of FLMC increases faster than LMC, which
suggests using smaller step-sizes in FLMC. We finally extend our results to the
case where the exact gradients are replaced by stochastic gradients and show
that similar results hold in this setting as well.
",1,0,0,1,0,0
468,Spoken English Intelligibility Remediation with PocketSphinx Alignment and Feature Extraction Improves Substantially over the State of the Art,"  We use automatic speech recognition to assess spoken English learner
pronunciation based on the authentic intelligibility of the learners' spoken
responses determined from support vector machine (SVM) classifier or deep
learning neural network model predictions of transcription correctness. Using
numeric features produced by PocketSphinx alignment mode and many recognition
passes searching for the substitution and deletion of each expected phoneme and
insertion of unexpected phonemes in sequence, the SVM models achieve 82 percent
agreement with the accuracy of Amazon Mechanical Turk crowdworker
transcriptions, up from 75 percent reported by multiple independent
researchers. Using such features with SVM classifier probability prediction
models can help computer-aided pronunciation teaching (CAPT) systems provide
intelligibility remediation.
",1,0,0,1,0,0
469,Second-Order Analysis and Numerical Approximation for Bang-Bang Bilinear Control Problems,"  We consider bilinear optimal control problems, whose objective functionals do
not depend on the controls. Hence, bang-bang solutions will appear. We
investigate sufficient second-order conditions for bang-bang controls, which
guarantee local quadratic growth of the objective functional in $L^1$. In
addition, we prove that for controls that are not bang-bang, no such growth can
be expected. Finally, we study the finite-element discretization, and prove
error estimates of bang-bang controls in $L^1$-norms.
",0,0,1,0,0,0
470,On the letter frequencies and entropy of written Marathi,"  We carry out a comprehensive analysis of letter frequencies in contemporary
written Marathi. We determine sets of letters which statistically predominate
any large generic Marathi text, and use these sets to estimate the entropy of
Marathi.
",1,0,0,0,0,0
471,Robust Orchestration of Concurrent Application Workflows in Mobile Device Clouds,"  A hybrid mobile/fixed device cloud that harnesses sensing, computing,
communication, and storage capabilities of mobile and fixed devices in the
field as well as those of computing and storage servers in remote datacenters
is envisioned. Mobile device clouds can be harnessed to enable innovative
pervasive applications that rely on real-time, in-situ processing of sensor
data collected in the field. To support concurrent mobile applications on the
device cloud, a robust and secure distributed computing framework, called
Maestro, is proposed. The key components of Maestro are (i) a task scheduling
mechanism that employs controlled task replication in addition to task
reallocation for robustness and (ii) Dedup for task deduplication among
concurrent pervasive workflows. An architecture-based solution that relies on
task categorization and authorized access to the categories of tasks is
proposed for different levels of protection. Experimental evaluation through
prototype testbed of Android- and Linux-based mobile devices as well as
simulations is performed to demonstrate Maestro's capabilities.
",1,0,0,0,0,0
472,Anisotropy and multiband superconductivity in Sr2RuO4,"  Despite numerous studies the exact nature of the order parameter in
superconducting Sr2RuO4 remains unresolved. We have extended previous
small-angle neutron scattering studies of the vortex lattice in this material
to a wider field range, higher temperatures, and with the field applied close
to both the <100> and <110> basal plane directions. Measurements at high field
were made possible by the use of both spin polarization and analysis to improve
the signal-to-noise ratio. Rotating the field towards the basal plane causes a
distortion of the square vortex lattice observed for H // <001>, and also a
symmetry change to a distorted triangular symmetry for fields close to <100>.
The vortex lattice distortion allows us to determine the intrinsic
superconducting anisotropy between the c-axis and the Ru-O basal plane,
yielding a value of ~60 at low temperature and low to intermediate fields. This
greatly exceeds the upper critical field anisotropy of ~20 at low temperature,
reminiscent of Pauli limiting. Indirect evidence for Pauli paramagnetic effects
on the unpaired quasiparticles in the vortex cores are observed, but a direct
detection lies below the measurement sensitivity. The superconducting
anisotropy is found to be independent of temperature but increases for fields >
1 T, indicating multiband superconductvity in Sr2RuO4. Finally, the temperature
dependence of the scattered intensity provides further support for gap nodes or
deep minima in the superconducting gap.
",0,1,0,0,0,0
473,"Time-Reversal Breaking in QCD$_4$, Walls, and Dualities in 2+1 Dimensions","  We study $SU(N)$ Quantum Chromodynamics (QCD) in 3+1 dimensions with $N_f$
degenerate fundamental quarks with mass $m$ and a $\theta$-parameter. For
generic $m$ and $\theta$ the theory has a single gapped vacuum. However, as
$\theta$ is varied through $\theta=\pi$ for large $m$ there is a first order
transition. For $N_f=1$ the first order transition line ends at a point with a
massless $\eta'$ particle (for all $N$) and for $N_f>1$ the first order
transition ends at $m=0$, where, depending on the value of $N_f$, the IR theory
has free Nambu-Goldstone bosons, an interacting conformal field theory, or a
free gauge theory. Even when the $4d$ bulk is smooth, domain walls and
interfaces can have interesting phase transitions separating different $3d$
phases. These turn out to be the phases of the recently studied $3d$
Chern-Simons matter theories, thus relating the dynamics of QCD$_4$ and
QCD$_3$, and, in particular, making contact with the recently discussed
dualities in 2+1 dimensions. For example, when the massless $4d$ theory has an
$SU(N_f)$ sigma model, the domain wall theory at low (nonzero) mass supports a
$3d$ massless $CP^{N_f-1}$ nonlinear $\sigma$-model with a Wess-Zumino term, in
agreement with the conjectured dynamics in 2+1 dimensions.
",0,1,0,0,0,0
474,Comparative Investigation of the High Pressure Autoignition of the Butanol Isomers,"  Investigation of the autoignition delay of the butanol isomers has been
performed at elevated pressures of 15 bar and 30 bar and low to intermediate
temperatures of 680-860 K. The reactivity of the stoichiometric isomers of
butanol, in terms of inverse ignition delay, was ranked as n-butanol >
sec-butanol ~ iso-butanol > tert-butanol at a compressed pressure of 15 bar but
changed to n-butanol > tert-butanol > sec-butanol > iso-butanol at 30 bar. For
the temperature and pressure conditions in this study, no NTC or two-stage
ignition behavior were observed. However, for both of the compressed pressures
studied in this work, tert-butanol exhibited unique pre-ignition heat release
characteristics. As such, tert-butanol was further studied at two additional
equivalence ratios ($\phi$ = 0.5 and 2.0) to help determine the cause of the
heat release.
",0,1,0,0,0,0
475,Selecting optimal minimum spanning trees that share a topological correspondence with phylogenetic trees,"  Choi et. al (2011) introduced a minimum spanning tree (MST)-based method
called CLGrouping, for constructing tree-structured probabilistic graphical
models, a statistical framework that is commonly used for inferring
phylogenetic trees. While CLGrouping works correctly if there is a unique MST,
we observe an indeterminacy in the method in the case that there are multiple
MSTs. In this work we remove this indeterminacy by introducing so-called
vertex-ranked MSTs. We note that the effectiveness of CLGrouping is inversely
related to the number of leaves in the MST. This motivates the problem of
finding a vertex-ranked MST with the minimum number of leaves (MLVRMST). We
provide a polynomial time algorithm for the MLVRMST problem, and prove its
correctness for graphs whose edges are weighted with tree-additive distances.
",1,0,1,0,0,0
476,Noisy Natural Gradient as Variational Inference,"  Variational Bayesian neural nets combine the flexibility of deep learning
with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff
between cheap but simple variational families (e.g.~fully factorized) or
expensive and complicated inference procedures. We show that natural gradient
ascent with adaptive weight noise implicitly fits a variational posterior to
maximize the evidence lower bound (ELBO). This insight allows us to train
full-covariance, fully factorized, or matrix-variate Gaussian variational
posteriors using noisy versions of natural gradient, Adam, and K-FAC,
respectively, making it possible to scale up to modern-size ConvNets. On
standard regression benchmarks, our noisy K-FAC algorithm makes better
predictions and matches Hamiltonian Monte Carlo's predictive variances better
than existing methods. Its improved uncertainty estimates lead to more
efficient exploration in active learning, and intrinsic motivation for
reinforcement learning.
",1,0,0,1,0,0
477,A Game of Life on Penrose tilings,"  We define rules for cellular automata played on quasiperiodic tilings of the
plane arising from the multigrid method in such a way that these cellular
automata are isomorphic to Conway's Game of Life. Although these tilings are
nonperiodic, determining the next state of each tile is a local computation,
requiring only knowledge of the local structure of the tiling and the states of
finitely many nearby tiles. As an example, we show a version of a ""glider""
moving through a region of a Penrose tiling. This constitutes a potential
theoretical framework for a method of executing computations in
non-periodically structured substrates such as quasicrystals.
",0,1,1,0,0,0
478,"Single and Multiple Vortex Rings in Three-Dimensional Bose-Einstein Condensates: Existence, Stability and Dynamics","  In the present work, we explore the existence, stability and dynamics of
single and multiple vortex ring states that can arise in Bose-Einstein
condensates. Earlier works have illustrated the bifurcation of such states, in
the vicinity of the linear limit, for isotropic or anisotropic
three-dimensional harmonic traps. Here, we extend these states to the regime of
large chemical potentials, the so-called Thomas-Fermi limit, and explore their
properties such as equilibrium radii and inter-ring distance, for multi-ring
states, as well as their vibrational spectra and possible instabilities. In
this limit, both the existence and stability characteristics can be partially
traced to a particle picture that considers the rings as individual particles
oscillating within the trap and interacting pairwise with one another. Finally,
we examine some representative instability scenarios of the multi-ring dynamics
including breakup and reconnections, as well as the transient formation of
vortex lines.
",0,1,0,0,0,0
479,Dimension-free Wasserstein contraction of nonlinear filters,"  For a class of partially observed diffusions, sufficient conditions are given
for the map from initial condition of the signal to filtering distribution to
be contractive with respect to Wasserstein distances, with rate which has no
dependence on the dimension of the state-space and is stable under tensor
products of the model. The main assumptions are that the signal has affine
drift and constant diffusion coefficient, and that the likelihood functions are
log-concave. Contraction estimates are obtained from an $h$-process
representation of the transition probabilities of the signal reweighted so as
to condition on the observations.
",0,0,1,1,0,0
480,Vortex Nucleation Limited Mobility of Free Electron Bubbles in the Gross-Pitaevskii Model of a Superfluid,"  We study the motion of an electron bubble in the zero temperature limit where
neither phonons nor rotons provide a significant contribution to the drag
exerted on an ion moving within the superfluid. By using the Gross-Clark model,
in which a Gross-Pitaevskii equation for the superfluid wavefunction is coupled
to a Schr??dinger equation for the electron wavefunction, we study how
vortex nucleation affects the measured drift velocity of the ion. We use
parameters that give realistic values of the ratio of the radius of the bubble
with respect to the healing length in superfluid $^4$He at a pressure of one
bar. By performing fully 3D spatio-temporal simulations of the superfluid
coupled to an electron, that is modelled within an adiabatic approximation and
moving under the influence of an applied electric field, we are able to recover
the key dynamics of the ion-vortex interactions that arise and the subsequent
ion-vortex complexes that can form. Using the numerically computed drift
velocity of the ion as a function of the applied electric field, we determine
the vortex-nucleation limited mobility of the ion to recover values in
reasonable agreement with measured data.
",0,1,0,0,0,0
481,Radio variability and non-thermal components in stars evolving toward planetary nebulae,"  We present new JVLA multi-frequency measurements of a set of stars in
transition from the post-AGB to the Planetary Nebula phase monitored in the
radio range over several years. Clear variability is found for five sources.
Their light curves show increasing and decreasing patterns. New radio
observations at high angular resolution are also presented for two sources.
Among these is IRAS 18062+2410, whose radio structure is compared to
near-infrared images available in the literature. With these new maps, we can
estimate inner and outer radii of 0.03$""$ and 0.08$""$ for the ionised shell, an
ionised mass of $3.2\times10^{-4}$ M$_\odot$, and a density at the inner radius
of $7.7\times 10^{-5}$ cm$^{-3}$, obtained by modelling the radio shell with
the new morphological constraints. The combination of multi-frequency data and,
where available, spectral-index maps leads to the detection of spectral indices
not due to thermal emission, contrary to what one would expect in planetary
nebulae. Our results allow us to hypothesise the existence of a link between
radio variability and non-thermal emission mechanisms in the nebulae. This link
seems to hold for IRAS 22568+6141 and may generally hold for those nebulae
where the radio flux decreases over time.
",0,1,0,0,0,0
482,Sequential testing for structural stability in approximate factor models,"  We develop an on-line monitoring procedure to detect a change in a large
approximate factor model. Our statistics are based on a well-known property of
the $% \left( r+1\right) $-th eigenvalue of the sample covariance matrix of the
data (having defined $r$ as the number of common factors): whilst under the
null the $\left( r+1\right) $-th eigenvalue is bounded, under the alternative
of a change (either in the loadings, or in the number of factors itself) it
becomes spiked. Given that the sample eigenvalue cannot be estimated
consistently under the null, we regularise the problem by randomising the test
statistic in conjunction with sample conditioning, obtaining a sequence of
\textit{i.i.d.}, asymptotically chi-square statistics which are then employed
to build the monitoring scheme. Numerical evidence shows that our procedure
works very well in finite samples, with a very small probability of false
detections and tight detection times in presence of a genuine change-point.
",0,0,0,1,0,0
483,Susceptibility Propagation by Using Diagonal Consistency,"  A susceptibility propagation that is constructed by combining a belief
propagation and a linear response method is used for approximate computation
for Markov random fields. Herein, we formulate a new, improved susceptibility
propagation by using the concept of a diagonal matching method that is based on
mean-field approaches to inverse Ising problems. The proposed susceptibility
propagation is robust for various network structures, and it is reduced to the
ordinary susceptibility propagation and to the adaptive
Thouless-Anderson-Palmer equation in special cases.
",0,0,1,1,0,0
484,Performance Analysis of Ultra-Dense Networks with Elevated Base Stations,"  This paper analyzes the downlink performance of ultra-dense networks with
elevated base stations (BSs). We consider a general dual-slope pathloss model
with distance-dependent probability of line-of-sight (LOS) transmission between
BSs and receivers. Specifically, we consider the scenario where each link may
be obstructed by randomly placed buildings. Using tools from stochastic
geometry, we show that both coverage probability and area spectral efficiency
decay to zero as the BS density grows large. Interestingly, we show that the BS
height alone has a detrimental effect on the system performance even when the
standard single-slope pathloss model is adopted.
",1,0,0,0,0,0
485,Learning to Drive in a Day,"  We demonstrate the first application of deep reinforcement learning to
autonomous driving. From randomly initialised parameters, our model is able to
learn a policy for lane following in a handful of training episodes using a
single monocular image as input. We provide a general and easy to obtain
reward: the distance travelled by the vehicle without the safety driver taking
control. We use a continuous, model-free deep reinforcement learning algorithm,
with all exploration and optimisation performed on-vehicle. This demonstrates a
new framework for autonomous driving which moves away from reliance on defined
logical rules, mapping, and direct supervision. We discuss the challenges and
opportunities to scale this approach to a broader range of autonomous driving
tasks.
",1,0,0,1,0,0
486,Strong-coupling of WSe2 in ultra-compact plasmonic nanocavities at room temperature,"  Strong-coupling of monolayer metal dichalcogenide semiconductors with light
offers encouraging prospects for realistic exciton devices at room temperature.
However, the nature of this coupling depends extremely sensitively on the
optical confinement and the orientation of electronic dipoles and fields. Here,
we show how plasmon strong coupling can be achieved in compact robust
easily-assembled gold nano-gap resonators at room temperature. We prove that
strong coupling is impossible with monolayers due to the large exciton
coherence size, but resolve clear anti-crossings for 8 layer devices with Rabi
splittings exceeding 135 meV. We show that such structures improve on prospects
for nonlinear exciton functionalities by at least 10^4, while retaining quantum
efficiencies above 50%.
",0,1,0,0,0,0
487,Stigmergy-based modeling to discover urban activity patterns from positioning data,"  Positioning data offer a remarkable source of information to analyze crowds
urban dynamics. However, discovering urban activity patterns from the emergent
behavior of crowds involves complex system modeling. An alternative approach is
to adopt computational techniques belonging to the emergent paradigm, which
enables self-organization of data and allows adaptive analysis. Specifically,
our approach is based on stigmergy. By using stigmergy each sample position is
associated with a digital pheromone deposit, which progressively evaporates and
aggregates with other deposits according to their spatiotemporal proximity.
Based on this principle, we exploit positioning data to identify high density
areas (hotspots) and characterize their activity over time. This
characterization allows the comparison of dynamics occurring in different days,
providing a similarity measure exploitable by clustering techniques. Thus, we
cluster days according to their activity behavior, discovering unexpected urban
activity patterns. As a case study, we analyze taxi traces in New York City
during 2015.
",1,1,0,0,0,0
488,BiHom-Lie colour algebras structures,"  BiHom-Lie Colour algebra is a generalized Hom-Lie Colour algebra endowed with
two commuting multiplicative linear maps. The main purpose of this paper is to
define representations and a cohomology of BiHom-Lie colour algebras and to
study some key constructions and properties.
Moreover, we discuss $\alpha^{k}\beta^l$-generalized derivations,
$\alpha^{k}\beta^l$-quasi-derivations and $\alpha^{k}\beta^l$-quasi-centroid.
We provide some properties and their relationships with BiHom-Jordan colour
algebra.
",0,0,1,0,0,0
489,Clustering of Gamma-Ray bursts through kernel principal component analysis,"  We consider the problem related to clustering of gamma-ray bursts (from
""BATSE"" catalogue) through kernel principal component analysis in which our
proposed kernel outperforms results of other competent kernels in terms of
clustering accuracy and we obtain three physically interpretable groups of
gamma-ray bursts. The effectivity of the suggested kernel in combination with
kernel principal component analysis in revealing natural clusters in noisy and
nonlinear data while reducing the dimension of the data is also explored in two
simulated data sets.
",0,1,0,1,0,0
490,Bounded gaps between primes in short intervals,"  Baker, Harman, and Pintz showed that a weak form of the Prime Number Theorem
holds in intervals of the form $[x-x^{0.525},x]$ for large $x$. In this paper,
we extend a result of Maynard and Tao concerning small gaps between primes to
intervals of this length. More precisely, we prove that for any $\delta\in
[0.525,1]$ there exist positive integers $k,d$ such that for sufficiently large
$x$, the interval $[x-x^\delta,x]$ contains $\gg_{k} \frac{x^\delta}{(\log
x)^k}$ pairs of consecutive primes differing by at most $d$. This confirms a
speculation of Maynard that results on small gaps between primes can be refined
to the setting of short intervals of this length.
",0,0,1,0,0,0
491,Handover analysis of the Improved Phantom Cells,"  Improved Phantom cell is a new scenario which has been introduced recently to
enhance the capacity of Heterogeneous Networks (HetNets). The main trait of
this scenario is that, besides maximizing the total network capacity in both
indoor and outdoor environments, it claims to reduce the handover number
compared to the conventional scenarios. In this paper, by a comprehensive
review of the Improved Phantom cells structure, an appropriate algorithm will
be introduced for the handover procedure of this scenario. To reduce the number
of handover in the proposed algorithm, various parameters such as the received
Signal to Interference plus Noise Ratio (SINR) at the user equipment (UE),
users access conditions to the phantom cells, and users staying time in the
target cell based on its velocity, has been considered. Theoretical analyses
and simulation results show that applying the suggested algorithm the improved
phantom cell structure has a much better performance than conventional HetNets
in terms of the number of handover.
",1,0,0,0,0,0
492,Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation,"  We address the problem of localisation of objects as bounding boxes in images
with weak labels. This weakly supervised object localisation problem has been
tackled in the past using discriminative models where each object class is
localised independently from other classes. We propose a novel framework based
on Bayesian joint topic modelling. Our framework has three distinctive
advantages over previous works: (1) All object classes and image backgrounds
are modelled jointly together in a single generative model so that ""explaining
away"" inference can resolve ambiguity and lead to better learning and
localisation. (2) The Bayesian formulation of the model enables easy
integration of prior knowledge about object appearance to compensate for
limited supervision. (3) Our model can be learned with a mixture of weakly
labelled and unlabelled data, allowing the large volume of unlabelled images on
the Internet to be exploited for learning. Extensive experiments on the
challenging VOC dataset demonstrate that our approach outperforms the
state-of-the-art competitors.
",1,0,0,0,0,0
493,Psychological model of the investor and manager behavior in risk,"  All people have to make risky decisions in everyday life. And we do not know
how true they are. But is it possible to mathematically assess the correctness
of our choice? This article discusses the model of decision making under risk
on the example of project management. This is a game with two players, one of
which is Investor, and the other is the Project Manager. Each player makes a
risky decision for himself, based on his past experience. With the help of a
mathematical model, the players form a level of confidence, depending on who
the player accepts the strategy or does not accept. The project manager
assesses the costs and compares them with the level of confidence. An investor
evaluates past results. Also visit the case where the strategy of the player
accepts the part.
",0,0,0,0,0,1
494,Constraints on Super-Earths Interiors from Stellar Abundances,"  Modeling the interior of exoplanets is essential to go further than the
conclusions provided by mean density measurements. In addition to the still
limited precision on the planets' fundamental parameters, models are limited by
the existence of degeneracies on their compositions. Here we present a model of
internal structure dedicated to the study of solid planets up to ~10 Earth
masses, i.e. Super-Earths. When the measurement is available, the assumption
that the bulk Fe/Si ratio of a planet is similar to that of its host star
allows us to significantly reduce the existing degeneracy and more precisely
constrain the planet's composition. Based on our model, we provide an update of
the mass-radius relationships used to provide a first estimate of a planet's
composition from density measurements. Our model is also applied to the cases
of two well-known exoplanets, CoRoT-7b and Kepler-10b, using their recently
updated parameters. The core mass fractions of CoRoT-7b and Kepler-10b are
found to lie within the 10-37% and 10-33% ranges, respectively, allowing both
planets to be compatible with an Earth-like composition. We also extend the
recent study of Proxima Centauri b, and show that its radius may reach 1.94
Earth radii in the case of a 5 Earth masses planet, as there is a 96.7%
probability that the real mass of Proxima Centauri b is below this value.
",0,1,0,0,0,0
495,Software correlator for Radioastron mission,"  In this paper we discuss the characteristics and operation of Astro Space
Center (ASC) software FX correlator that is an important component of
space-ground interferometer for Radioastron project. This project performs
joint observations of compact radio sources using 10 meter space radio
telescope (SRT) together with ground radio telescopes at 92, 18, 6 and 1.3 cm
wavelengths. In this paper we describe the main features of space-ground VLBI
data processing of Radioastron project using ASC correlator. Quality of
implemented fringe search procedure provides positive results without
significant losses in correlated amplitude. ASC Correlator has a computational
power close to real time operation. The correlator has a number of processing
modes: ""Continuum"", ""Spectral Line"", ""Pulsars"", ""Giant Pulses"",""Coherent"".
Special attention is paid to peculiarities of Radioastron space-ground VLBI
data processing. The algorithms of time delay and delay rate calculation are
also discussed, which is a matter of principle for data correlation of
space-ground interferometers. During 5 years of Radioastron space radio
telescope (SRT) successful operation, ASC correlator showed high potential of
satisfying steady growing needs of current and future ground and space VLBI
science. Results of ASC software correlator operation are demonstrated.
",0,1,0,0,0,0
496,Isogenies for point counting on genus two hyperelliptic curves with maximal real multiplication,"  Schoof's classic algorithm allows point-counting for elliptic curves over
finite fields in polynomial time. This algorithm was subsequently improved by
Atkin, using factorizations of modular polynomials, and by Elkies, using a
theory of explicit isogenies. Moving to Jacobians of genus-2 curves, the
current state of the art for point counting is a generalization of Schoof's
algorithm. While we are currently missing the tools we need to generalize
Elkies' methods to genus 2, recently Martindale and Milio have computed
analogues of modular polynomials for genus-2 curves whose Jacobians have real
multiplication by maximal orders of small discriminant. In this article, we
prove Atkin-style results for genus-2 Jacobians with real multiplication by
maximal orders, with a view to using these new modular polynomials to improve
the practicality of point-counting algorithms for these curves.
",1,0,1,0,0,0
497,On the self-duality of rings of integers in tame and abelian extensions,"  Let $L/K$ be a tame and Galois extension of number fields with group $G$. It
is well-known that any ambiguous ideal in $L$ is locally free over
$\mathcal{O}_KG$ (of rank one), and so it defines a class in the locally free
class group of $\mathcal{O}_KG$, where $\mathcal{O}_K$ denotes the ring of
integers of $K$. In this paper, we shall study the relationship among the
classes arising from the ring of integers $\mathcal{O}_L$ of $L$, the inverse
different $\mathfrak{D}_{L/K}^{-1}$ of $L/K$, and the square root of the
inverse different $A_{L/K}$ of $L/K$ (if it exists), in the case that $G$ is
abelian. They are naturally related because $A_{L/K}^2 =
\mathfrak{D}_{L/K}^{-1} = \mathcal{O}_L^*$, and $A_{L/K}$ is special because
$A_{L/K} = A_{L/K}^*$, where $*$ denotes dual with respect to the trace of
$L/K$.
",0,0,1,0,0,0
498,Forecasting Transformative AI: An Expert Survey,"  Transformative AI technologies have the potential to reshape critical aspects
of society in the near future. However, in order to properly prepare policy
initiatives for the arrival of such technologies accurate forecasts and
timelines are necessary. A survey was administered to attendees of three AI
conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).
The survey included questions for estimating AI capabilities over the next
decade, questions for forecasting five scenarios of transformative AI and
questions concerning the impact of computational resources in AI research.
Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that
humans are currently paid to do) can be feasibly automated now, and that this
figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts
indicated a 50% probability of AI systems being capable of automating 90% of
current human tasks in 25 years and 99% of current human tasks in 50 years. The
conference of attendance was found to have a statistically significant impact
on all forecasts, with attendees of HLAI providing more optimistic timelines
with less uncertainty. These findings suggest that AI experts expect major
advances in AI technology to continue over the next decade to a degree that
will likely have profound transformative impacts on society.
",1,0,0,0,0,0
499,"Change of grading, injective dimension and dualizing complexes","  Let $G,H$ be groups, $\phi: G \rightarrow H$ a group morphism, and $A$ a
$G$-graded algebra. The morphism $\phi$ induces an $H$-grading on $A$, and on
any $G$-graded $A$-module, which thus becomes an $H$-graded $A$-module. Given
an injective $G$-graded $A$-module, we give bounds for its injective dimension
when seen as $H$-graded $A$-module. Following ideas by Van den Bergh, we give
an application of our results to the stability of dualizing complexes through
change of grading.
",0,0,1,0,0,0
500,Approximately certifying the restricted isometry property is hard,"  A matrix is said to possess the Restricted Isometry Property (RIP) if it acts
as an approximate isometry when restricted to sparse vectors. Previous work has
shown it to be NP-hard to determine whether a matrix possess this property, but
only in a narrow range of parameters. In this work, we show that it is NP-hard
to make this determination for any accuracy parameter, even when we restrict
ourselves to instances which are either RIP or far from being RIP. This result
implies that it is NP-hard to approximate the range of parameters for which a
matrix possesses the Restricted Isometry Property with accuracy better than
some constant. Ours is the first work to prove such a claim without any
additional assumptions.
",1,0,0,0,0,0
501,A compact design for velocity-map imaging energetic electrons and ions,"  We present a compact design for a velocity-map imaging spectrometer for
energetic electrons and ions. The standard geometry by Eppink and Parker [A. T.
J. B. Eppink and D. H. Parker, Rev. Sci. Instrum. 68, 3477 (1997)] is augmented
by just two extended electrodes so as to realize an additional einzel lens. In
this way, for a maximum electrode voltage of 7 kV we experimentally demonstrate
imaging of electrons with energies up to 65 eV. Simulations show that energy
acceptances of <270 and <1,200 eV with an energy resolution of dE / E <5% are
achievable for electrode voltages of <20 kV when using diameters of the
position-sensitive detector of 42 and 78 mm, respectively.
",0,1,0,0,0,0
502,Multiset Combinatorial Batch Codes,"  Batch codes, first introduced by Ishai, Kushilevitz, Ostrovsky, and Sahai,
mimic a distributed storage of a set of $n$ data items on $m$ servers, in such
a way that any batch of $k$ data items can be retrieved by reading at most some
$t$ symbols from each server. Combinatorial batch codes, are replication-based
batch codes in which each server stores a subset of the data items.
In this paper, we propose a generalization of combinatorial batch codes,
called multiset combinatorial batch codes (MCBC), in which $n$ data items are
stored in $m$ servers, such that any multiset request of $k$ items, where any
item is requested at most $r$ times, can be retrieved by reading at most $t$
items from each server. The setup of this new family of codes is motivated by
recent work on codes which enable high availability and parallel reads in
distributed storage systems. The main problem under this paradigm is to
minimize the number of items stored in the servers, given the values of
$n,m,k,r,t$, which is denoted by $N(n,k,m,t;r)$. We first give a necessary and
sufficient condition for the existence of MCBCs. Then, we present several
bounds on $N(n,k,m,t;r)$ and constructions of MCBCs. In particular, we
determine the value of $N(n,k,m,1;r)$ for any $n\geq
\left\lfloor\frac{k-1}{r}\right\rfloor{m\choose k-1}-(m-k+1)A(m,4,k-2)$, where
$A(m,4,k-2)$ is the maximum size of a binary constant weight code of length
$m$, distance four and weight $k-2$. We also determine the exact value of
$N(n,k,m,1;r)$ when $r\in\{k,k-1\}$ or $k=m$.
",1,0,1,0,0,0
503,Thermoelectric Cooperative Effect in Three-Terminal Elastic Transport through a Quantum Dot,"  The energy efficiency and power of a three-terminal thermoelectric nanodevice
are studied by considering elastic tunneling through a single quantum dot.
Facilitated by the three-terminal geometry, the nanodevice is able to generate
simultaneously two electrical powers by utilizing only one temperature bias.
These two electrical powers can add up constructively or destructively,
depending on their signs. It is demonstrated that the constructive addition
leads to the enhancement of both energy efficiency and output power for various
system parameters. In fact, such enhancement, dubbed as thermoelectric
cooperative effect, can lead to maximum efficiency and power no less than when
only one of the electrical power is harvested.
",0,1,0,0,0,0
504,DeepSaucer: Unified Environment for Verifying Deep Neural Networks,"  In recent years, a number of methods for verifying DNNs have been developed.
Because the approaches of the methods differ and have their own limitations, we
think that a number of verification methods should be applied to a developed
DNN. To apply a number of methods to the DNN, it is necessary to translate
either the implementation of the DNN or the verification method so that one
runs in the same environment as the other. Since those translations are
time-consuming, a utility tool, named DeepSaucer, which helps to retain and
reuse implementations of DNNs, verification methods, and their environments, is
proposed. In DeepSaucer, code snippets of loading DNNs, running verification
methods, and creating their environments are retained and reused as software
assets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer
is confirmed by implementing it on the basis of Anaconda, which provides
virtual environment for loading a DNN and running a verification method. In
addition, the effectiveness of DeepSaucer is demonstrated by usecase examples.
",1,0,0,0,0,0
505,Checklists to Support Test Charter Design in Exploratory Testing,"  During exploratory testing sessions the tester simultaneously learns, designs
and executes tests. The activity is iterative and utilizes the skills of the
tester and provides flexibility and creativity.Test charters are used as a
vehicle to support the testers during the testing. The aim of this study is to
support practitioners in the design of test charters through checklists. We
aimed to identify factors allowing practitioners to critically reflect on their
designs and contents of test charters to support practitioners in making
informed decisions of what to include in test charters. The factors and
contents have been elicited through interviews. Overall, 30 factors and 35
content elements have been elicited.
",1,0,0,0,0,0
506,Fast non-destructive parallel readout of neutral atom registers in optical potentials,"  We demonstrate the parallel and non-destructive readout of the hyperfine
state for optically trapped $^{87}$Rb atoms. The scheme is based on
state-selective fluorescence imaging and achieves detection fidelities $>$98%
within 10$\,$ms, while keeping 99% of the atoms trapped. For the read-out of
dense arrays of neutral atoms in optical lattices, where the fluorescence
images of neighboring atoms overlap, we apply a novel image analysis technique
using Bayesian inference to determine the internal state of multiple atoms. Our
method is scalable to large neutral atom registers relevant for future quantum
information processing tasks requiring fast and non-destructive readout and can
also be used for the simultaneous read-out of quantum information stored in
internal qubit states and in the atoms' positions.
",0,1,0,0,0,0
507,Binaural Source Localization based on Modulation-Domain Features and Decision Pooling,"  In this work we apply Amplitude Modulation Spectrum (AMS) features to the
source localization problem. Our approach computes 36 bilateral features for 2s
long signal segments and estimates the azimuthal directions of a sound source
through a binaurally trained classifier. This directional information of a
sound source could be e.g. used to steer the beamformer in a hearing aid to the
source of interest in order to increase the SNR. We evaluated our approach on
the development set of the IEEE-AASP Challenge on sound source localization and
tracking (LOCATA) and achieved a 4.25?ø smaller MAE than the baseline
approach. Additionally, our approach is computationally less complex.
",1,0,0,0,0,0
508,Dynamic Shrinkage Processes,"  We propose a novel class of dynamic shrinkage processes for Bayesian time
series and regression analysis. Building upon a global-local framework of prior
construction, in which continuous scale mixtures of Gaussian distributions are
employed for both desirable shrinkage properties and computational
tractability, we model dependence among the local scale parameters. The
resulting processes inherit the desirable shrinkage behavior of popular
global-local priors, such as the horseshoe prior, but provide additional
localized adaptivity, which is important for modeling time series data or
regression functions with local features. We construct a computationally
efficient Gibbs sampling algorithm based on a P??lya-Gamma scale mixture
representation of the proposed process. Using dynamic shrinkage processes, we
develop a Bayesian trend filtering model that produces more accurate estimates
and tighter posterior credible intervals than competing methods, and apply the
model for irregular curve-fitting of minute-by-minute Twitter CPU usage data.
In addition, we develop an adaptive time-varying parameter regression model to
assess the efficacy of the Fama-French five-factor asset pricing model with
momentum added as a sixth factor. Our dynamic analysis of manufacturing and
healthcare industry data shows that with the exception of the market risk, no
other risk factors are significant except for brief periods.
",0,0,0,1,0,0
509,A Multiple Source Framework for the Identification of Activities of Daily Living Based on Mobile Device Data,"  The monitoring of the lifestyles may be performed based on a system for the
recognition of Activities of Daily Living (ADL) and their environments,
combining the results obtained with the user agenda. The system may be
developed with the use of the off-the-shelf mobile devices commonly used,
because they have several types of sensors available, including motion,
magnetic, acoustic, and location sensors. Data acquisition, data processing,
data fusion, and artificial intelligence methods are applied in different
stages of the system developed, which recognizes the ADL with pattern
recognition methods. The motion and magnetic sensors allow the recognition of
activities with movement, but the acoustic sensors allow the recognition of the
environments. The fusion of the motion, magnetic and acoustic sensors allows
the differentiation of other ADL. On the other hand, the location sensors
allows the recognition of ADL with large movement, and the combination of these
sensors with the other sensors increases the number of ADL recognized by the
system. This study consists on the comparison of different types of ANN for
choosing the best methods for the recognition of several ADL, which they are
implemented in a system for the recognition of ADL that combines the sensors
data with the users agenda for the monitoring of the lifestyles. Conclusions
point to the use of Deep Neural Networks (DNN) with normalized data for the
identification of ADL with 85.89% of accuracy, the use of Feedforward neural
networks with non-normalized data for the identification of the environments
with 86.50% of accuracy, and the use of DNN with normalized data for the
identification of standing activities with 100% of accuracy, proving the
reliability of the framework presented in this study.
",1,0,0,0,0,0
510,Maximum likelihood estimators based on the block maxima method,"  The extreme value index is a fundamental parameter in univariate Extreme
Value Theory (EVT). It captures the tail behavior of a distribution and is
central in the extrapolation beyond observed data. Among other semi-parametric
methods (such as the popular Hill's estimator), the Block Maxima (BM) and
Peaks-Over-Threshold (POT) methods are widely used for assessing the extreme
value index and related normalizing constants. We provide asymptotic theory for
the maximum likelihood estimators (MLE) based on the BM method. Our main result
is the asymptotic normality of the MLE with a non-trivial bias depending on the
extreme value index and on the so-called second order parameter. Our approach
combines asymptotic expansions of the likelihood process and of the empirical
quantile process of block maxima. The results permit to complete the comparison
of most common semi-parametric estimators in EVT (MLE and probability weighted
moment estimators based on the POT or BM methods) through their asymptotic
variances, biases and optimal mean square errors.
",0,0,1,1,0,0
511,Hipsters on Networks: How a Small Group of Individuals Can Lead to an Anti-Establishment Majority,"  The spread of opinions, memes, diseases, and ""alternative facts"" in a
population depends both on the details of the spreading process and on the
structure of the social and communication networks on which they spread. In
this paper, we explore how \textit{anti-establishment} nodes (e.g.,
\textit{hipsters}) influence the spreading dynamics of two competing products.
We consider a model in which spreading follows a deterministic rule for
updating node states (which describe which product has been adopted) in which
an adjustable fraction $p_{\rm Hip}$ of the nodes in a network are hipsters,
who choose to adopt the product that they believe is the less popular of the
two. The remaining nodes are conformists, who choose which product to adopt by
considering which products their immediate neighbors have adopted. We simulate
our model on both synthetic and real networks, and we show that the hipsters
have a major effect on the final fraction of people who adopt each product:
even when only one of the two products exists at the beginning of the
simulations, a very small fraction of hipsters in a network can still cause the
other product to eventually become the more popular one. To account for this
behavior, we construct an approximation for the steady-state adoption fraction
on $k$-regular trees in the limit of few hipsters. Additionally, our
simulations demonstrate that a time delay $\tau$ in the knowledge of the
product distribution in a population, as compared to immediate knowledge of
product adoption among nearest neighbors, can have a large effect on the final
distribution of product adoptions. Our simple model and analysis may help shed
light on the road to success for anti-establishment choices in elections, as
such success can arise rather generically in our model from a small number of
anti-establishment individuals and ordinary processes of social influence on
normal individuals.
",1,1,0,0,0,0
512,Optimal Identity Testing with High Probability,"  We study the problem of testing identity against a given distribution with a
focus on the high confidence regime. More precisely, given samples from an
unknown distribution $p$ over $n$ elements, an explicitly given distribution
$q$, and parameters $0< \epsilon, \delta < 1$, we wish to distinguish, {\em
with probability at least $1-\delta$}, whether the distributions are identical
versus $\varepsilon$-far in total variation distance. Most prior work focused
on the case that $\delta = \Omega(1)$, for which the sample complexity of
identity testing is known to be $\Theta(\sqrt{n}/\epsilon^2)$. Given such an
algorithm, one can achieve arbitrarily small values of $\delta$ via black-box
amplification, which multiplies the required number of samples by
$\Theta(\log(1/\delta))$.
We show that black-box amplification is suboptimal for any $\delta = o(1)$,
and give a new identity tester that achieves the optimal sample complexity. Our
new upper and lower bounds show that the optimal sample complexity of identity
testing is \[
\Theta\left( \frac{1}{\epsilon^2}\left(\sqrt{n \log(1/\delta)} +
\log(1/\delta) \right)\right) \] for any $n, \varepsilon$, and $\delta$. For
the special case of uniformity testing, where the given distribution is the
uniform distribution $U_n$ over the domain, our new tester is surprisingly
simple: to test whether $p = U_n$ versus $d_{\mathrm TV}(p, U_n) \geq
\varepsilon$, we simply threshold $d_{\mathrm TV}(\widehat{p}, U_n)$, where
$\widehat{p}$ is the empirical probability distribution. The fact that this
simple ""plug-in"" estimator is sample-optimal is surprising, even in the
constant $\delta$ case. Indeed, it was believed that such a tester would not
attain sublinear sample complexity even for constant values of $\varepsilon$
and $\delta$.
",1,0,1,1,0,0
513,Handling state space explosion in verification of component-based systems: A review,"  Component-based design is a different way of constructing systems which
offers numerous benefits, in particular, decreasing the complexity of system
design. However, deploying components into a system is a challenging and
error-prone task. Model checking is one of the reliable methods that
automatically and systematically analyse the correctness of a given system. Its
brute-force check of the state space significantly expands the level of
confidence in the system. Nevertheless, model checking is limited by a critical
problem so-called State Space Explosion (SSE). To benefit from model checking,
appropriate methods to reduce SSE, is required. In two last decades, a great
number of methods to mitigate the state space explosion have been proposed
which have many similarities, dissimilarities, and unclear concepts in some
cases. This research, firstly, aims at present a review and brief discussion of
the methods of handling SSE problem and classify them based on their
similarities, principle and characteristics. Second, it investigates the
methods for handling SSE problem in verifying Component-based system (CBS) and
provides insight into CBS verification limitations that have not been addressed
yet. The analysis in this research has revealed the patterns, specific
features, and gaps in the state-of-the-art methods. In addition, we identified
and discussed suitable methods to soften SSE problem in CBS and underlined the
key challenges for future research efforts.
",1,0,1,0,0,0
514,A Framework for Relating the Structures and Recovery Statistics in Pressure Time-Series Surveys for Dust Devils,"  Dust devils are likely the dominant source of dust for the martian
atmosphere, but the amount and frequency of dust-lifting depend on the
statistical distribution of dust devil parameters. Dust devils exhibit pressure
perturbations and, if they pass near a barometric sensor, they may register as
a discernible dip in a pressure time-series. Leveraging this fact, several
surveys using barometric sensors on landed spacecraft have revealed dust devil
structures and occurrence rates. However powerful they are, though, such
surveys suffer from non-trivial biases that skew the inferred dust devil
properties. For example, such surveys are most sensitive to dust devils with
the widest and deepest pressure profiles, but the recovered profiles will be
distorted, broader and shallow than the actual profiles. In addition, such
surveys often do not provide wind speed measurements alongside the pressure
time series, and so the durations of the dust devil signals in the time series
cannot be directly converted to profile widths. Fortunately, simple statistical
and geometric considerations can de-bias these surveys, allowing conversion of
the duration of dust devil signals into physical widths, given only a
distribution of likely translation velocities, and the recovery of the
underlying distributions of physical parameters. In this study, we develop a
scheme for de-biasing such surveys. Applying our model to an in-situ survey
using data from the Phoenix lander suggests a larger dust flux and a dust devil
occurrence rate about ten times larger than previously inferred. Comparing our
results to dust devil track surveys suggests only about one in five
low-pressure cells lifts sufficient dust to leave a visible track.
",0,1,0,0,0,0
515,"Investigation on different physical aspects such as structural, elastic, mechanical, optical properties and Debye temperature of Fe2ScM (M = P and As) semiconductors: a DFT based first principles study","  With the help of first principles calculation method based on the density
functional theory we have investigated the structural, elastic, mechanical
properties and Debye temperature of Fe2ScM (M = P and As) compounds under
pressure up to 60 GPa. The optical properties have been investigated under zero
pressure. Our calculated optimized structural parameters of both the compounds
are in good agreement with the other theoretical results. The calculated
elastic constants show that Fe2ScM (M = P and As) compounds are mechanically
stable up to 60 GPa.
",0,1,0,0,0,0
516,Optimal Envelope Approximation in Fourier Basis with Applications in TV White Space,"  Lowpass envelope approximation of smooth continuous-variable signals are
introduced in this work. Envelope approximations are necessary when a given
signal has to be approximated always to a larger value (such as in TV white
space protection regions). In this work, a near-optimal approximate algorithm
for finding a signal's envelope, while minimizing a mean-squared cost function,
is detailed. The sparse (lowpass) signal approximation is obtained in the
linear Fourier series basis. This approximate algorithm works by discretizing
the envelope property from an infinite number of points to a large (but finite)
number of points. It is shown that this approximate algorithm is near-optimal
and can be solved by using efficient convex optimization programs available in
the literature. Simulation results are provided towards the end to gain more
insights into the analytical results presented.
",1,0,0,0,0,0
517,The curl operator on odd-dimensional manifolds,"  We study the spectral properties of curl, a linear differential operator of
first order acting on differential forms of appropriate degree on an
odd-dimensional closed oriented Riemannian manifold. In three dimensions its
eigenvalues are the electromagnetic oscillation frequencies in vacuum without
external sources. In general, the spectrum consists of the eigenvalue 0 with
infinite multiplicity and further real discrete eigenvalues of finite
multiplicity. We compute the Weyl asymptotics and study the zeta-function. We
give a sharp lower eigenvalue bound for positively curved manifolds and analyze
the equality case. Finally, we compute the spectrum for flat tori, round
spheres and 3-dimensional spherical space forms.
",0,0,1,0,0,0
518,Topology optimization for transient response of structures subjected to dynamic loads,"  This paper presents a topology optimization framework for structural problems
subjected to transient loading. The mechanical model assumes a linear elastic
isotropic material, infinitesimal strains, and a dynamic response. The
optimization problem is solved using the gradient-based optimizer Method of
Moving Asymptotes (MMA) with time-dependent sensitivities provided via the
adjoint method. The stiffness of materials is interpolated using the Solid
Isotropic Material with Penalization (SIMP) method and the Heaviside Projection
Method (HPM) is used to stabilize the problem numerically and improve the
manufacturability of the topology-optimized designs. Both static and dynamic
optimization examples are considered here. The resulting optimized designs
demonstrate the ability of topology optimization to tailor the transient
response of structures.
",0,1,1,0,0,0
519,Generalized Lambert series and arithmetic nature of odd zeta values,"  It is pointed out that the generalized Lambert series
$\displaystyle\sum_{n=1}^{\infty}\frac{n^{N-2h}}{e^{n^{N}x}-1}$ studied by
Kanemitsu, Tanigawa and Yoshimoto can be found on page $332$ of Ramanujan's
Lost Notebook in a slightly more general form. We extend an important
transformation of this series obtained by Kanemitsu, Tanigawa and Yoshimoto by
removing restrictions on the parameters $N$ and $h$ that they impose. From our
extension we deduce a beautiful new generalization of Ramanujan's famous
formula for odd zeta values which, for $N$ odd and $m>0$, gives a relation
between $\zeta(2m+1)$ and $\zeta(2Nm+1)$. A result complementary to the
aforementioned generalization is obtained for any even $N$ and
$m\in\mathbb{Z}$. It generalizes a transformation of Wigert and can be regarded
as a formula for $\zeta\left(2m+1-\frac{1}{N}\right)$. Applications of these
transformations include a generalization of the transformation for the
logarithm of Dedekind eta-function $\eta(z)$, Zudilin- and Rivoal-type results
on transcendence of certain values, and a transcendence criterion for Euler's
constant $\gamma$.
",0,0,1,0,0,0
520,Perfect phylogenies via branchings in acyclic digraphs and a generalization of Dilworth's theorem,"  Motivated by applications in cancer genomics and following the work of
Hajirasouliha and Raphael (WABI 2014), HujduroviŽ? et al. (IEEE TCBB, to
appear) introduced the minimum conflict-free row split (MCRS) problem: split
each row of a given binary matrix into a bitwise OR of a set of rows so that
the resulting matrix corresponds to a perfect phylogeny and has the minimum
possible number of rows among all matrices with this property. Hajirasouliha
and Raphael also proposed the study of a similar problem, in which the task is
to minimize the number of distinct rows of the resulting matrix. HujduroviŽ?
et al. proved that both problems are NP-hard, gave a related characterization
of transitively orientable graphs, and proposed a polynomial-time heuristic
algorithm for the MCRS problem based on coloring cocomparability graphs.
We give new, more transparent formulations of the two problems, showing that
the problems are equivalent to two optimization problems on branchings in a
derived directed acyclic graph. Building on these formulations, we obtain new
results on the two problems, including: (i) a strengthening of the heuristic by
HujduroviŽ? et al. via a new min-max result in digraphs generalizing
Dilworth's theorem, which may be of independent interest, (ii) APX-hardness
results for both problems, (iii) approximation algorithms, and (iv)
exponential-time algorithms solving the two problems to optimality faster than
the na??ve brute-force approach. Our work relates to several well studied
notions in combinatorial optimization: chain partitions in partially ordered
sets, laminar hypergraphs, and (classical and weighted) colorings of graphs.
",1,0,1,0,0,0
521,Towards a Service-oriented Platform for Intelligent Apps in Intermediate Cities,"  Smart cities are a growing trend in many cities in Argentina. In particular,
the so-called intermediate cities present a context and requirements different
from those of large cities with respect to smart cities. One aspect of
relevance is to encourage the development of applications (generally for mobile
devices) that enable citizens to take advantage of data and services normally
associated with the city, for example, in the urban mobility domain. In this
work, a platform is proposed for intermediate cities that provide ""high level""
services and that allow the construction of software applications that consume
those services. Our platform-centric strategy focused aims to integrate systems
and heterogeneous data sources, and provide ""intelligent"" services to different
applications. Examples of these services include: construction of user
profiles, recommending local events, and collaborative sensing based on data
mining techniques, among others. In this work, the design of this platform
(currently in progress) is described, and experiences of applications for urban
mobility are discussed, which are being migrated in the form of reusable
services provided by the platform
",1,0,0,0,0,0
522,Fully Bayesian Estimation Under Informative Sampling,"  Bayesian estimation is increasingly popular for performing model based
inference to support policymaking. These data are often collected from surveys
under informative sampling designs where subject inclusion probabilities are
designed to be correlated with the response variable of interest. Sampling
weights constructed from marginal inclusion probabilities are typically used to
form an exponentiated pseudo likelihood that adjusts the population likelihood
for estimation on the sample due to ease-of-estimation. We propose an
alternative adjustment based on a Bayes rule construction that simultaneously
performs weight smoothing and estimates the population model parameters in a
fully Bayesian construction. We formulate conditions on known marginal and
pairwise inclusion probabilities that define a class of sampling designs where
$L_{1}$ consistency of the joint posterior is guaranteed. We compare
performances between the two approaches on synthetic data, which reveals that
our fully Bayesian approach better estimates posterior uncertainty without a
requirement to calibrate the normalization of the sampling weights. We
demonstrate our method on an application concerning the National Health and
Nutrition Examination Survey exploring the relationship between caffeine
consumption and systolic blood pressure.
",0,0,1,1,0,0
523,Bistable reaction equations with doubly nonlinear diffusion,"  Reaction-diffusion equations appear in biology and chemistry, and combine
linear diffusion with different kind of reaction terms. Some of them are
remarkable from the mathematical point of view, since they admit families of
travelling waves that describe the asymptotic behaviour of a larger class of
solutions $0\leq u(x,t)\leq 1$ of the problem posed in the real line. We
investigate here the existence of waves with constant propagation speed, when
the linear diffusion is replaced by the ""slow"" doubly nonlinear diffusion. In
the present setting we consider bistable reaction terms, which present
interesting differences w.r.t. the Fisher-KPP framework recently studied in
\cite{AA-JLV:art}. We find different families of travelling waves that are
employed to describe the wave propagation of more general solutions and to
study the stability/instability of the steady states, even when we extend the
study to several space dimensions. A similar study is performed in the critical
case that we call ""pseudo-linear"", i.e., when the operator is still nonlinear
but has homogeneity one. With respect to the classical model and the
""pseudo-linear"" case, the travelling waves of the ""slow"" diffusion setting
exhibit free boundaries. \\ Finally, as a complement of \cite{AA-JLV:art}, we
study the asymptotic behaviour of more general solutions in the presence of a
""heterozygote superior"" reaction function and doubly nonlinear diffusion
(""slow"" and ""pseudo-linear"").
",0,0,1,0,0,0
524,Static Free Space Detection with Laser Scanner using Occupancy Grid Maps,"  Drivable free space information is vital for autonomous vehicles that have to
plan evasive maneuvers in real-time. In this paper, we present a new efficient
method for environmental free space detection with laser scanner based on 2D
occupancy grid maps (OGM) to be used for Advanced Driving Assistance Systems
(ADAS) and Collision Avoidance Systems (CAS). Firstly, we introduce an enhanced
inverse sensor model tailored for high-resolution laser scanners for building
OGM. It compensates the unreflected beams and deals with the ray casting to
grid cells accuracy and computational effort problems. Secondly, we introduce
the 'vehicle on a circle for grid maps' map alignment algorithm that allows
building more accurate local maps by avoiding the computationally expensive
inaccurate operations of image sub-pixel shifting and rotation. The resulted
grid map is more convenient for ADAS features than existing methods, as it
allows using less memory sizes, and hence, results into a better real-time
performance. Thirdly, we present an algorithm to detect what we call the
'in-sight edges'. These edges guarantee modeling the free space area with a
single polygon of a fixed number of vertices regardless the driving situation
and map complexity. The results from real world experiments show the
effectiveness of our approach.
",1,0,0,0,0,0
525,Proactive Edge Computing in Latency-Constrained Fog Networks,"  In this paper, the fundamental problem of distribution and proactive caching
of computing tasks in fog networks is studied under latency and reliability
constraints. In the proposed scenario, computing can be executed either locally
at the user device or offloaded to an edge cloudlet. Moreover, cloudlets
exploit both their computing and storage capabilities by proactively caching
popular task computation results to minimize computing latency. To this end, a
clustering method to group spatially proximate user devices with mutual task
popularity interests and their serving cloudlets is proposed. Then, cloudlets
can proactively cache the popular tasks' computations of their cluster members
to minimize computing latency. Additionally, the problem of distributing tasks
to cloudlets is formulated as a matching game in which a cost function of
computing delay is minimized under latency and reliability constraints.
Simulation results show that the proposed scheme guarantees reliable
computations with bounded latency and achieves up to 91% decrease in computing
latency as compared to baseline schemes.
",1,0,0,0,0,0
526,Deuterium fractionation and H2D+ evolution in turbulent and magnetized cloud cores,"  High-mass stars are expected to form from dense prestellar cores. Their
precise formation conditions are widely discussed, including their virial
condition, which results in slow collapse for super-virial cores with strong
support by turbulence or magnetic fields, or fast collapse for sub-virial
sources. To disentangle their formation processes, measurements of the
deuterium fractions are frequently employed to approximately estimate the ages
of these cores and to obtain constraints on their dynamical evolution. We here
present 3D magneto-hydrodynamical simulations including for the first time an
accurate non-equilibrium chemical network with 21 gas-phase species plus dust
grains and 213 reactions. With this network we model the deuteration process in
fully depleted prestellar cores in great detail and determine its response to
variations in the initial conditions. We explore the dependence on the initial
gas column density, the turbulent Mach number, the mass-to-magnetic flux ratio
and the distribution of the magnetic field, as well as the initial
ortho-to-para ratio of H2. We find excellent agreement with recent observations
of deuterium fractions in quiescent sources. Our results show that deuteration
is rather efficient, even when assuming a conservative ortho-to-para ratio of 3
and highly sub-virial initial conditions, leading to large deuterium fractions
already within roughly a free-fall time. We discuss the implications of our
results and give an outlook to relevant future investigations.
",0,1,0,0,0,0
527,Experimental data over quantum mechanics simulations for inferring the repulsive exponent of the Lennard-Jones potential in Molecular Dynamics,"  The Lennard-Jones (LJ) potential is a cornerstone of Molecular Dynamics (MD)
simulations and among the most widely used computational kernels in science.
The potential models atomistic attraction and repulsion with century old
prescribed parameters ($q=6, \; p=12$, respectively), originally related by a
factor of two for simplicity of calculations. We re-examine the value of the
repulsion exponent through data driven uncertainty quantification. We perform
Hierarchical Bayesian inference on MD simulations of argon using experimental
data of the radial distribution function (RDF) for a range of thermodynamic
conditions, as well as dimer interaction energies from quantum mechanics
simulations. The experimental data suggest a repulsion exponent ($p \approx
6.5$), in contrast to the quantum simulations data that support values closer
to the original ($p=12$) exponent. Most notably, we find that predictions of
RDF, diffusion coefficient and density of argon are more accurate and robust in
producing the correct argon phase around its triple point, when using the
values inferred from experimental data over those from quantum mechanics
simulations. The present results suggest the need for data driven recalibration
of the LJ potential across MD simulations.
",0,1,0,1,0,0
528,Ensemble Sampling,"  Thompson sampling has emerged as an effective heuristic for a broad range of
online decision problems. In its basic form, the algorithm requires computing
and sampling from a posterior distribution over models, which is tractable only
for simple special cases. This paper develops ensemble sampling, which aims to
approximate Thompson sampling while maintaining tractability even in the face
of complex models such as neural networks. Ensemble sampling dramatically
expands on the range of applications for which Thompson sampling is viable. We
establish a theoretical basis that supports the approach and present
computational results that offer further insight.
",1,0,0,1,0,0
529,Failures of Gradient-Based Deep Learning,"  In recent years, Deep Learning has become the go-to solution for a broad
range of applications, often outperforming state-of-the-art. However, it is
important, for both theoreticians and practitioners, to gain a deeper
understanding of the difficulties and limitations associated with common
approaches and algorithms. We describe four types of simple problems, for which
the gradient-based algorithms commonly used in deep learning either fail or
suffer from significant difficulties. We illustrate the failures through
practical experiments, and provide theoretical insights explaining their
source, and how they might be remedied.
",1,0,0,1,0,0
530,Searching for the Transit of the Earth--mass exoplanet Proxima~Centauri~b in Antarctica: Preliminary Result,"  Proxima Centauri is known as the closest star from the Sun. Recently, radial
velocity observations revealed the existence of an Earth-mass planet around it.
With an orbital period of ~11 days, the surface of Proxima Centauri b is
temperate and might be habitable. We took a photometric monitoring campaign to
search for its transit, using the Bright Star Survey Telescope at the Zhongshan
Station in Antarctica. A transit-like signal appearing on 2016 September 8th,
is identified tentatively. Its midtime, $T_{C}=2,457,640.1990\pm0.0017$ HJD, is
consistent with the predicted ephemeris based on RV orbit in a 1$\sigma$
confidence interval. Time-correlated noise is pronounced in the light curve of
Proxima Centauri, affecting detection of transits. We develop a technique, in a
Gaussian process framework, to gauge the statistical significance of potential
transit detection. The tentative transit signal reported here, has a confidence
level of $2.5\sigma$. Further detection of its periodic signals is necessary to
confirm the planetary transit of Proxima Centauri b. We plan to monitor Proxima
Centauri in next Polar night at Dome A in Antarctica, taking the advantage of
continuous darkness. \citet{Kipping17} reported two tentative transit-like
signals of Proxima Centauri b, observed by the Microvariability and Oscillation
of Stars space Telescope in 2014 and 2015, respectively. The midtransit time of
our detection is 138 minutes later than that predicted by their transit
ephemeris. If all the signals are real transits, the misalignment of the epochs
plausibly suggests transit timing variations of Proxima Centauri b induced by
an outer planet in this system.
",0,1,0,0,0,0
531,A Data-Driven Sparse-Learning Approach to Model Reduction in Chemical Reaction Networks,"  In this paper, we propose an optimization-based sparse learning approach to
identify the set of most influential reactions in a chemical reaction network.
This reduced set of reactions is then employed to construct a reduced chemical
reaction mechanism, which is relevant to chemical interaction network modeling.
The problem of identifying influential reactions is first formulated as a
mixed-integer quadratic program, and then a relaxation method is leveraged to
reduce the computational complexity of our approach. Qualitative and
quantitative validation of the sparse encoding approach demonstrates that the
model captures important network structural properties with moderate
computational load.
",1,0,0,0,0,0
532,Parity-Forbidden Transitions and Their Impacts on the Optical Absorption Properties of Lead-Free Metal Halide Perovskites and Double Perovskites,"  Using density-functional theory calculations, we analyze the optical
absorption properties of lead (Pb)-free metal halide perovskites
(AB$^{2+}$X$_3$) and double perovskites (AB$^+$B$^{3+}$X$_6$) (A = Cs or
monovalent organic ion, B$^{2+}$ = non-Pb divalent metal, B$^+$ = monovalent
metal, B$^{3+}$ = trivalent metal, X = halogen). We show that, if B$^{2+}$ is
not Sn or Ge, Pb-free metal halide perovskites exhibit poor optical absorptions
because of their indirect bandgap nature. Among the nine possible types of
Pb-free metal halide double perovskites, six have direct bandgaps. Of these six
types, four show inversion symmetry-induced parity-forbidden or weak
transitions between band edges, making them not ideal for thin-film solar cell
application. Only one type of Pb-free double perovskite shows optical
absorption and electronic properties suitable for solar cell applications,
namely those with B$^+$ = In, Tl and B$^{3+}$ = Sb, Bi. Our results provide
important insights for designing new metal halide perovskites and double
perovskites for optoelectronic applications.
",0,1,0,0,0,0
533,Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer,"  MicroRNAs play important roles in many biological processes. Their aberrant
expression can have oncogenic or tumor suppressor function directly
participating to carcinogenesis, malignant transformation, invasiveness and
metastasis. Indeed, miRNA profiles can distinguish not only between normal and
cancerous tissue but they can also successfully classify different subtypes of
a particular cancer. Here, we focus on a particular class of transcripts
encoding polycistronic miRNA genes that yields multiple miRNA components. We
describe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully
redesigned release of the MMRA computational pipeline (MiRNA Master Regulator
Analysis), developed to search for clustered miRNAs potentially driving cancer
molecular subtyping. Genomically clustered miRNAs are frequently co-expressed
to target different components of pro-tumorigenic signalling pathways. By
applying ClustMMRA to breast cancer patient data, we identified key miRNA
clusters driving the phenotype of different tumor subgroups. The pipeline was
applied to two independent breast cancer datasets, providing statistically
concordant results between the two analysis. We validated in cell lines the
miR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative
subtype phenotype through its control of proliferation and EMT.
",0,0,0,0,1,0
534,A Belief Propagation Algorithm for Multipath-Based SLAM,"  We present a simultaneous localization and mapping (SLAM) algorithm that is
based on radio signals and the association of specular multipath components
(MPCs) with geometric features. Especially in indoor scenarios, robust
localization from radio signals is challenging due to diffuse multipath
propagation, unknown MPC-feature association, and limited visibility of
features. In our approach, specular reflections at flat surfaces are described
in terms of virtual anchors (VAs) that are mirror images of the physical
anchors (PAs). The positions of these VAs and possibly also of the PAs are
unknown. We develop a Bayesian model of the SLAM problem including the unknown
MPC-VA/PA association. We represent this model by a factor graph, which enables
the use of the belief propagation (BP) scheme for efficient marginalization of
the joint posterior distribution. The resulting BP-based SLAM algorithm detects
the VAs associated with the PAs and estimates jointly the time-varying position
of the mobile agent and the positions of the VAs and possibly also of the PAs,
thereby leveraging the MPCs in the radio signal for improved accuracy and
robustness of agent localization. A core aspect of the algorithm is BP-based
probabilistic MPC-VA/PA association. Moreover, for improved initialization of
new VA positions, the states of unobserved potential VAs are modeled as a
random finite set and propagated in time by means of a ""zero-measurement""
probability hypothesis density filter. The proposed BP-based SLAM algorithm has
a low computational complexity and scales well in all relevant system
parameters. Experimental results using both synthetically generated
measurements and real ultra-wideband radio signals demonstrate the excellent
performance of the algorithm in challenging indoor environments.
",1,0,0,0,0,0
535,Local methods for blocks of finite simple groups,"  This survey is about old and new results about the modular representation
theory of finite reductive groups with a strong emphasis on local methods. This
includes subpairs, Brauer's Main Theorems, fusion, Rickard equivalences. In the
defining characteristic we describe the relation between $p$-local subgroups
and parabolic subgroups, then give classical consequences on simple modules and
blocks, including the Alperin weight conjecture in that case. In the
non-defining characteristics, we sketch a picture of the local methods
pioneered by Fong-Srinivasan in the determination of blocks and their ordinary
characters. This includes the relationship with Lusztig's twisted induction and
the determination of defect groups. We conclude with a survey of the results
and methods by Bonnaf??-Dat-Rouquier giving Morita equivalences between blocks
that preserve defect groups and the local structures.
The text grew out of the course and talks given by the author in July and
September 2016 during the program ""Local representation theory and simple
groups"" at CIB Lausanne. Written Oct 2017, to appear in a proceedings volume
published by EMS.
",0,0,1,0,0,0
536,Motion Planning for a Humanoid Mobile Manipulator System,"  A high redundant non-holonomic humanoid mobile dual-arm manipulator system is
presented in this paper where the motion planning to realize ""human-like""
autonomous navigation and manipulation tasks is studied. Firstly, an improved
MaxiMin NSGA-II algorithm, which optimizes five objective functions to solve
the problems of singularity, redundancy, and coupling between mobile base and
manipulator simultaneously, is proposed to design the optimal pose to
manipulate the target object. Then, in order to link the initial pose and that
optimal pose, an off-line motion planning algorithm is designed. In detail, an
efficient direct-connect bidirectional RRT and gradient descent algorithm is
proposed to reduce the sampled nodes largely, and a geometric optimization
method is proposed for path pruning. Besides, head forward behaviors are
realized by calculating the reasonable orientations and assigning them to the
mobile base to improve the quality of human-robot interaction. Thirdly, the
extension to on-line planning is done by introducing real-time sensing,
collision-test and control cycles to update robotic motion in dynamic
environments. Fourthly, an EEs' via-point-based multi-objective genetic
algorithm is proposed to design the ""human-like"" via-poses by optimizing four
objective functions. Finally, numerous simulations are presented to validate
the effectiveness of proposed algorithms.
",1,0,0,0,0,0
537,End-to-end Planning of Fixed Millimeter-Wave Networks,"  This article discusses a framework to support the design and end-to-end
planning of fixed millimeter-wave networks. Compared to traditional techniques,
the framework allows an organization to quickly plan a deployment in a
cost-effective way. We start by using LiDAR data---basically, a 3D point cloud
captured from a city---to estimate potential sites to deploy antennas and
whether there is line-of-sight between them. With that data on hand, we use
combinatorial optimization techniques to determine the optimal set of locations
and how they should communicate with each other, to satisfy engineering (e.g.,
latency, polarity), design (e.g., reliability) and financial (e.g., total cost
of operation) constraints. The primary goal is to connect as many people as
possible to the network. Our methodology can be used for strategic planning
when an organization is in the process of deciding whether to adopt a
millimeter-wave technology or choosing between locations, or for operational
planning when conducting a detailed design of the actual network to be deployed
in a selected location.
",1,0,1,0,0,0
538,A giant planet undergoing extreme ultraviolet irradiation by its hot massive-star host,"  The amount of ultraviolet irradiation and ablation experienced by a planet
depends strongly on the temperature of its host star. Of the thousands of
extra-solar planets now known, only four giant planets have been found that
transit hot, A-type stars (temperatures of 7300-10,000K), and none are known to
transit even hotter B-type stars. WASP-33 is an A-type star with a temperature
of ~7430K, which hosts the hottest known transiting planet; the planet is
itself as hot as a red dwarf star of type M. The planet displays a large heat
differential between its day-side and night-side, and is highly inflated,
traits that have been linked to high insolation. However, even at the
temperature of WASP-33b's day-side, its atmosphere likely resembles the
molecule-dominated atmospheres of other planets, and at the level of
ultraviolet irradiation it experiences, its atmosphere is unlikely to be
significantly ablated over the lifetime of its star. Here we report
observations of the bright star HD 195689, which reveal a close-in (orbital
period ~1.48 days) transiting giant planet, KELT-9b. At ~10,170K, the host star
is at the dividing line between stars of type A and B, and we measure the
KELT-9b's day-side temperature to be ~4600K. This is as hot as stars of stellar
type K4. The molecules in K stars are entirely dissociated, and thus the
primary sources of opacity in the day-side atmosphere of KELT-9b are likely
atomic metals. Furthermore, KELT-9b receives ~700 times more extreme
ultraviolet radiation (wavelengths shorter than 91.2 nanometers) than WASP-33b,
leading to a predicted range of mass-loss rates that could leave the planet
largely stripped of its envelope during the main-sequence lifetime of the host
star.
",0,1,0,0,0,0
539,"Characterizing videos, audience and advertising in Youtube channels for kids","  Online video services, messaging systems, games and social media services are
tremendously popular among young people and children in many countries. Most of
the digital services offered on the internet are advertising funded, which
makes advertising ubiquitous in children's everyday life. To understand the
impact of advertising-based digital services on children, we study the
collective behavior of users of YouTube for kids channels and present the
demographics of a large number of users. We collected data from 12,848 videos
from 17 channels in US and UK and 24 channels in Brazil. The channels in
English have been viewed more than 37 billion times. We also collected more
than 14 million comments made by users. Based on a combination of text-analysis
and face recognition tools, we show the presence of racial and gender biases in
our large sample of users. We also identify children actively using YouTube,
although the minimum age for using the service is 13 years in most countries.
We provide comparisons of user behavior among the three countries, which
represent large user populations in the global North and the global South.
",1,0,0,0,0,0
540,Vanishing theorems for the negative K-theory of stacks,"  We prove that the homotopy algebraic K-theory of tame quasi-DM stacks
satisfies cdh-descent. We apply this descent result to prove that if X is a
Noetherian tame quasi-DM stack and i < -dim(X), then K_i(X)[1/n] = 0 (resp.
K_i(X, Z/n) = 0) provided that n is nilpotent on X (resp. is invertible on X).
Our descent and vanishing results apply more generally to certain Artin stacks
whose stabilizers are extensions of finite group schemes by group schemes of
multiplicative type.
",0,0,1,0,0,0
541,Gravitational radiation from compact binary systems in screened modified gravity,"  Screened modified gravity (SMG) is a kind of scalar-tensor theory with
screening mechanisms, which can suppress the fifth force in dense regions and
allow theories to evade the solar system and laboratory tests. In this paper,
we investigate how the screening mechanisms in SMG affect the gravitational
radiation damping effects, calculate in detail the rate of the energy loss due
to the emission of tensor and scalar gravitational radiations, and derive their
contributions to the change in the orbital period of the binary system. We find
that the scalar radiation depends on the screened parameters and the
propagation speed of scalar waves, and the scalar dipole radiation dominates
the orbital decay of the binary system. For strongly self-gravitating bodies,
all effects of scalar sector are strongly suppressed by the screening
mechanisms in SMG. By comparing our results to observations of binary system
PSR J1738+0333, we place the stringent constraints on the screening mechanisms
in SMG. As an application of these results, we focus on three specific models
of SMG (chameleon, symmetron, and dilaton), and derive the constraints on the
model parameters, respectively.
",0,1,0,0,0,0
542,weedNet: Dense Semantic Weed Classification Using Multispectral Images and MAV for Smart Farming,"  Selective weed treatment is a critical step in autonomous crop management as
related to crop health and yield. However, a key challenge is reliable, and
accurate weed detection to minimize damage to surrounding plants. In this
paper, we present an approach for dense semantic weed classification with
multispectral images collected by a micro aerial vehicle (MAV). We use the
recently developed encoder-decoder cascaded Convolutional Neural Network (CNN),
Segnet, that infers dense semantic classes while allowing any number of input
image channels and class balancing with our sugar beet and weed datasets. To
obtain training datasets, we established an experimental field with varying
herbicide levels resulting in field plots containing only either crop or weed,
enabling us to use the Normalized Difference Vegetation Index (NDVI) as a
distinguishable feature for automatic ground truth generation. We train 6
models with different numbers of input channels and condition (fine-tune) it to
achieve about 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification
metrics. For model deployment, an embedded GPU system (Jetson TX2) is tested
for MAV integration. Dataset used in this paper is released to support the
community and future work.
",1,0,0,0,0,0
543,An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$,"  Let $G:=\widehat{SL_2}$ denote the affine Kac-Moody group associated to
$SL_2$ and $\bar{\mathcal{X}}$ the associated affine Grassmannian. We determine
an inductive formula for the Schubert basis structure constants in the
torus-equivariant Grothendieck group of $\bar{\mathcal{X}}$. In the case of
ordinary (non-equivariant) $K$-theory we find an explicit closed form for the
structure constants. We also determine an inductive formula for the structure
constants in the torus-equivariant cohomology ring, and use this formula to
find closed forms for some of the structure constants.
",0,0,1,0,0,0
544,The Correct Application of Variance Concept in Measurement Theory,"  The existing measurement theory interprets the variance as the dispersion of
measured value, which is actually contrary to a general mathematical knowledge
that the variance of a constant is 0. This paper will fully demonstrate that
the variance in measurement theory is actually the evaluation of probability
interval of an error instead of the dispersion of a measured value, point out
the key point of mistake in the existing interpretation, and fully interpret a
series of changes in conceptual logic and processing method brought about by
this new concept.
",0,0,1,1,0,0
545,A Generalized Framework for the Estimation of Causal Moderation Effects with Randomized Treatments and Non-Randomized Moderators,"  Researchers are often interested in analyzing conditional treatment effects.
One variant of this is ""causal moderation,"" which implies that intervention
upon a third (moderator) variable would alter the treatment effect. This study
presents a generalized, non-parametric framework for estimating causal
moderation effects given randomized treatments and non-randomized moderators
that achieves a number of goals. First, it highlights how conventional
approaches do not constitute unbiased or consistent estimators of causal
moderation effects. Second, it offers researchers a simple, transparent
approach for estimating causal moderation effects and lays out the assumptions
under which this can be performed consistently and/or without bias. Third, as
part of the estimation process, it allows researchers to implement their
preferred method of covariate adjustment, including parametric and
non-parametric methods, or alternative identification strategies of their
choosing. Fourth, it provides a set-up whereby sensitivity analysis designed
for the average-treatment-effect context can be extended to the moderation
context. An original application is also presented.
",0,0,0,1,0,0
546,Spherical Functions on Riemannian Symmetric Spaces,"  This paper deals with some simple results about spherical functions of type
$\delta$, namely new integral formulas, new results about behavior at infinity
and some facts about the related $C_\sigma$ functions.
",0,0,1,0,0,0
547,Modular curves with infinitely many cubic points,"  In this study, we determine all modular curves $X_0(N)$ that admit infinitely
many cubic points.
",0,0,1,0,0,0
548,Semi-Analytical Perturbative Approaches to Third Body Resonant Trajectories,"  In the framework of multi-body dynamics, successive encounters with a third
body, even if well outside of its sphere of influence, can noticeably alter the
trajectory of a spacecraft. Examples of these effects have already been
exploited by past missions such as SMART-1, as well as are proposed to benefit
future missions to Jupiter, Saturn or Neptune, and disposal strategies from
Earth's High Eccentric or Libration Point Orbits. This paper revises three
totally different descriptions of the effects of the third body gravitational
perturbation. These are the averaged dynamics of the classical third body
perturbing function, the Opik's close encounter theory and the Keplerian map
approach. The first two techniques have respectively been applied to the cases
of a spacecraft either always remaining very far or occasionally experiencing
extremely close approaches to the third body. However, the paper also seeks
solutions for trajectories that undergo one or more close approaches at
distances in the order of the sphere of influence of the third body. The paper
attempts to gain insight into the accuracy of these different perturbative
techniques into each of these scenarios, as compared with the motion in the
Circular Restricted Three Body Problem.
",0,1,0,0,0,0
549, Robust and Imperceptible Adversarial Attacks on Capsule Networks,"  Capsule Networks envision an innovative point of view about the
representation of the objects in the brain and preserve the hierarchical
spatial relationships between them. This type of networks exhibits a huge
potential for several Machine Learning tasks like image classification, while
outperforming Convolutional Neural Networks (CNNs). A large body of work has
explored adversarial examples for CNNs, but their efficacy to Capsule Networks
is not well explored. In our work, we study the vulnerabilities in Capsule
Networks to adversarial attacks. These perturbations, added to the test inputs,
are small and imperceptible to humans, but fool the network to mis-predict. We
propose a greedy algorithm to automatically generate targeted imperceptible
adversarial examples in a black-box attack scenario. We show that this kind of
attacks, when applied to the German Traffic Sign Recognition Benchmark (GTSRB),
mislead Capsule Networks. Moreover, we apply the same kind of adversarial
attacks to a 9-layer CNN and analyze the outcome, compared to the Capsule
Networks to study their differences / commonalities.
",1,0,0,1,0,0
550,Thermophoretic MHD Flow and Non-linear Radiative Heat Transfer with Convective Boundary Conditions over a Non-linearly Stretching Sheet,"  The effects of MHD boundary layer flow of non-linear thermal radiation with
convective heat transfer and non-uniform heat source/sink in presence of
thermophortic velocity and chemical reaction investigated in this study.
Suitable similarity transformation are used to solve the partial ordinary
differential equation of considered governing flow. Runge-Kutta fourth fifth
order Fehlberg method with shooting techniques are used to solved
non-dimensional governing equations. The variation of different parameters such
as thermophoretic parameter, chemical reaction parameter, non- uniform heat
source/sink parameters are studied on velocity, temperature and concentration
profiles, and are described by suitable graphs and tables. The obtained results
are in very well agreement with previous results.
",0,1,0,0,0,0
551,Resting-state ASL : Toward an optimal sequence duration,"  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily
practice and academic research stay discreet compared to resting-state BOLD.
However, by giving direct access to cerebral blood flow maps, rs-fASL leads to
significant clinical subject scaled application as CBF can be considered as a
biomarker in common neuropathology. Our work here focuses on the link between
overall quality of rs-fASL and duration of acquisition. To this end, we
consider subject self-Default Mode Network (DMN), and assess DMN quality
depletion compared to a gold standard DMN depending on the duration of
acquisition.
",0,0,0,0,1,0
552,Learning Neural Models for End-to-End Clustering,"  We propose a novel end-to-end neural network architecture that, once trained,
directly outputs a probabilistic clustering of a batch of input examples in one
pass. It estimates a distribution over the number of clusters $k$, and for each
$1 \leq k \leq k_\mathrm{max}$, a distribution over the individual cluster
assignment for each data point. The network is trained in advance in a
supervised fashion on separate data to learn grouping by any perceptual
similarity criterion based on pairwise labels (same/different group). It can
then be applied to different data containing different groups. We demonstrate
promising performance on high-dimensional data like images (COIL-100) and
speech (TIMIT). We call this ``learning to cluster'' and show its conceptual
difference to deep metric learning, semi-supervise clustering and other related
approaches while having the advantage of performing learnable clustering fully
end-to-end.
",0,0,0,1,0,0
553,Anisotropic exchange and spin-wave damping in pure and electron-doped Sr$_2$IrO$_4$,"  The collective magnetic excitations in the spin-orbit Mott insulator
(Sr$_{1-x}$La$_x$)$_2$IrO$_4$ ($x=0,\,0.01,\,0.04,\, 0.1$) were investigated by
means of resonant inelastic x-ray scattering. We report significant magnon
energy gaps at both the crystallographic and antiferromagnetic zone centers at
all doping levels, along with a remarkably pronounced momentum-dependent
lifetime broadening. The spin-wave gap is accounted for by a significant
anisotropy in the interactions between $J_\text{eff}=1/2$ isospins, thus
marking the departure of Sr$_2$IrO$_4$ from the essentially isotropic
Heisenberg model appropriate for the superconducting cuprates.
",0,1,0,0,0,0
554,Anomalous transport properties in Nb/Bi1.95Sb0.05Se3 hybrid structure,"  We report the proximity induced anomalous transport behavior in a Nb
Bi1.95Sb0.05Se3 heterostructure. Mechanically Exfoliated single crystal of
Bi1.95Sb0.05Se3 topological insulator (TI) is partially covered with a 100 nm
thick Niobium superconductor using DC magnetron sputtering by shadow masking
technique. The magnetotransport (MR) measurements have been performed
simultaneously on the TI sample with and without Nb top layer in the
temperature,T, range of 3 to 8 K, and a magnetic field B up to 15 T. MR on TI
region shows Subnikov de Haas oscillation at fields greater than 5 T. Anomalous
linear change in resistance is observed in the field range of negative 4T to
positive 4T at which Nb is superconducting. At 0 T field, the temperature
dependence of resistance on the Nb covered region revealed a superconducting
transition (TC) at 8.2 K, whereas TI area showed similar TC with the absence of
zero resistance states due to the additional resistance from superconductor
(SC) TI interface. Interestingly below the TC the R vs T measured on TI showed
an enhancement in resistance for positive field and prominent fall in
resistance for negative field direction. This indicates the directional
dependent scattering of the Cooper pairs on the surface of the TI due to the
superposition of spin singlet and triplet states in the superconductor and TI
respectively.
",0,1,0,0,0,0
555,State-dependent Priority Scheduling for Networked Control Systems,"  Networked control systems (NCS) have attracted considerable attention in
recent years. While the stabilizability and optimal control of NCS for a given
communication system has already been studied extensively, the design of the
communication system for NCS has recently seen an increase in more thorough
investigation. In this paper, we address an optimal scheduling problem for a
set of NCS sharing a dedicated communication channel, providing performance
bounds and asymptotic stability. We derive a suboptimal scheduling policy with
dynamic state-based priorities calculated at the sensors, which are then used
for stateless priority queuing in the network, making it both scalable and
efficient to implement on routers or multi-layer switches. These properties are
beneficial towards leveraging existing IP networks for control, which will be a
crucial factor for the proliferation of wide-area NCS applications. By allowing
for an arbitrary number of concurrent transmissions, we are able to investigate
the relationship between available bandwidth, transmission rate, and delay. To
demonstrate the feasibility of our approach, we provide a proof-of-concept
implementation of the priority scheduler using real networking hardware.
",1,0,0,0,0,0
556,Deformation conditions for pseudorepresentations,"  Given a property of representations satisfying a basic stability condition,
Ramakrishna developed a variant of Mazur's Galois deformation theory for
representations with that property. We introduce an axiomatic definition of
pseudorepresentations with such a property. Among other things, we show that
pseudorepresentations with a property enjoy a good deformation theory,
generalizing Ramakrishna's theory to pseudorepresentations.
",0,0,1,0,0,0
557,Adaptive local surface refinement based on LR NURBS and its application to contact,"  A novel adaptive local surface refinement technique based on Locally Refined
Non-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model
complex geometries exactly and are the rational extension of LR B-splines. The
local representation of the parameter space overcomes the drawback of
non-existent local refinement in standard NURBS-based isogeometric analysis.
For a convenient embedding into general finite element code, the B??zier
extraction operator for LR NURBS is formulated. An automatic remeshing
technique is presented that allows adaptive local refinement and coarsening of
LR NURBS. In this work, LR NURBS are applied to contact computations of 3D
solids and membranes. For solids, LR NURBS-enriched finite elements are used to
discretize the contact surfaces with LR NURBS finite elements, while the rest
of the body is discretized by linear Lagrange finite elements. For membranes,
the entire surface is discretized by LR NURBS. Various numerical examples are
shown, and they demonstrate the benefit of using LR NURBS: Compared to uniform
refinement, LR NURBS can achieve high accuracy at lower computational cost.
",1,0,0,0,0,0
558,"Approximate Program Smoothing Using Mean-Variance Statistics, with Application to Procedural Shader Bandlimiting","  This paper introduces a general method to approximate the convolution of an
arbitrary program with a Gaussian kernel. This process has the effect of
smoothing out a program. Our compiler framework models intermediate values in
the program as random variables, by using mean and variance statistics. Our
approach breaks the input program into parts and relates the statistics of the
different parts, under the smoothing process. We give several approximations
that can be used for the different parts of the program. These include the
approximation of Dorn et al., a novel adaptive Gaussian approximation, Monte
Carlo sampling, and compactly supported kernels. Our adaptive Gaussian
approximation is accurate up to the second order in the standard deviation of
the smoothing kernel, and mathematically smooth. We show how to construct a
compiler that applies chosen approximations to given parts of the input
program. Because each expression can have multiple approximation choices, we
use a genetic search to automatically select the best approximations. We apply
this framework to the problem of automatically bandlimiting procedural shader
programs. We evaluate our method on a variety of complex shaders, including
shaders with parallax mapping, animation, and spatially varying statistics. The
resulting smoothed shader programs outperform previous approaches both
numerically, and aesthetically, due to the smoothing properties of our
approximations.
",1,0,0,0,0,0
559,Trusted Multi-Party Computation and Verifiable Simulations: A Scalable Blockchain Approach,"  Large-scale computational experiments, often running over weeks and over
large datasets, are used extensively in fields such as epidemiology,
meteorology, computational biology, and healthcare to understand phenomena, and
design high-stakes policies affecting everyday health and economy. For
instance, the OpenMalaria framework is a computationally-intensive simulation
used by various non-governmental and governmental agencies to understand
malarial disease spread and effectiveness of intervention strategies, and
subsequently design healthcare policies. Given that such shared results form
the basis of inferences drawn, technological solutions designed, and day-to-day
policies drafted, it is essential that the computations are validated and
trusted. In particular, in a multi-agent environment involving several
independent computing agents, a notion of trust in results generated by peers
is critical in facilitating transparency, accountability, and collaboration.
Using a novel combination of distributed validation of atomic computation
blocks and a blockchain-based immutable audits mechanism, this work proposes a
universal framework for distributed trust in computations. In particular we
address the scalaibility problem by reducing the storage and communication
costs using a lossy compression scheme. This framework guarantees not only
verifiability of final results, but also the validity of local computations,
and its cost-benefit tradeoffs are studied using a synthetic example of
training a neural network.
",1,0,0,0,0,0
560,Empirical analysis of non-linear activation functions for Deep Neural Networks in classification tasks,"  We provide an overview of several non-linear activation functions in a neural
network architecture that have proven successful in many machine learning
applications. We conduct an empirical analysis on the effectiveness of using
these function on the MNIST classification task, with the aim of clarifying
which functions produce the best results overall. Based on this first set of
results, we examine the effects of building deeper architectures with an
increasing number of hidden layers. We also survey the impact of using, on the
same task, different initialisation schemes for the weights of our neural
network. Using these sets of experiments as a base, we conclude by providing a
optimal neural network architecture that yields impressive results in accuracy
on the MNIST classification task.
",1,0,0,1,0,0
561,A Survey of Deep Learning Techniques for Mobile Robot Applications,"  Advancements in deep learning over the years have attracted research into how
deep artificial neural networks can be used in robotic systems. This research
survey will present a summarization of the current research with a specific
focus on the gains and obstacles for deep learning to be applied to mobile
robotics.
",1,0,0,0,0,0
562,Design of the Artificial: lessons from the biological roots of general intelligence,"  Our desire and fascination with intelligent machines dates back to the
antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought
(syllogism) and Heron of Alexandria's mechanical machines and automata.
However, the quest for Artificial General Intelligence (AGI) is troubled with
repeated failures of strategies and approaches throughout the history. This
decade has seen a shift in interest towards bio-inspired software and hardware,
with the assumption that such mimicry entails intelligence. Though these steps
are fruitful in certain directions and have advanced automation, their singular
design focus renders them highly inefficient in achieving AGI. Which set of
requirements have to be met in the design of AGI? What are the limits in the
design of the artificial? Here, a careful examination of computation in
biological systems hints that evolutionary tinkering of contextual processing
of information enabled by a hierarchical architecture is the key to build AGI.
",1,1,0,0,0,0
563,Continuum of quantum fluctuations in a three-dimensional $S\!=\!1$ Heisenberg magnet,"  Conventional crystalline magnets are characterized by symmetry breaking and
normal modes of excitation called magnons with quantized angular momentum
$\hbar$. Neutron scattering correspondingly features extra magnetic Bragg
diffraction at low temperatures and dispersive inelastic scattering associated
with single magnon creation and annihilation. Exceptions are anticipated in
so-called quantum spin liquids as exemplified by the one-dimensional spin-1/2
chain which has no magnetic order and where magnons accordingly fractionalize
into spinons with angular momentum $\hbar/2$. This is spectacularly revealed by
a continuum of inelastic neutron scattering associated with two-spinon
processes and the absence of magnetic Bragg diffraction. Here, we report
evidence for these same key features of a quantum spin liquid in the
three-dimensional Heisenberg antiferromagnet NaCaNi$_2$F$_7$. Through specific
heat and neutron scattering measurements, Monte Carlo simulations, and analytic
approximations to the equal time correlations, we show that NaCaNi$_2$F$_7$ is
an almost ideal realization of the spin-1 antiferromagnetic Heisenberg model on
a pyrochlore lattice with weak connectivity and frustrated interactions.
Magnetic Bragg diffraction is absent and 90\% of the spectral weight forms a
continuum of magnetic scattering not dissimilar to that of the spin-1/2 chain
but with low energy pinch points indicating NaCaNi$_2$F$_7$ is in a Coulomb
phase. The residual entropy and diffuse elastic scattering points to an exotic
state of matter driven by frustration, quantum fluctuations and weak exchange
disorder.
",0,1,0,0,0,0
564,SimProp v2r4: Monte Carlo simulation code for UHECR propagation,"  We introduce the new version of SimProp, a Monte Carlo code for simulating
the propagation of ultra-high energy cosmic rays in intergalactic space. This
version, SimProp v2r4, together with an overall improvement of the code
capabilities with a substantial reduction in the computation time, also
computes secondary cosmogenic particles such as electron-positron pairs and
gamma rays produced during the propagation of ultra-high energy cosmic rays. As
recently pointed out by several authors, the flux of this secondary radiation
and its products, within reach of the current observatories, provides useful
information about models of ultra-high energy cosmic ray sources which would be
hard to discriminate otherwise.
",0,1,0,0,0,0
565,Non-negative Matrix Factorization via Archetypal Analysis,"  Given a collection of data points, non-negative matrix factorization (NMF)
suggests to express them as convex combinations of a small set of `archetypes'
with non-negative entries. This decomposition is unique only if the true
archetypes are non-negative and sufficiently sparse (or the weights are
sufficiently sparse), a regime that is captured by the separability condition
and its generalizations.
In this paper, we study an approach to NMF that can be traced back to the
work of Cutler and Breiman (1994) and does not require the data to be
separable, while providing a generally unique decomposition. We optimize the
trade-off between two objectives: we minimize the distance of the data points
from the convex envelope of the archetypes (which can be interpreted as an
empirical risk), while minimizing the distance of the archetypes from the
convex envelope of the data (which can be interpreted as a data-dependent
regularization). The archetypal analysis method of (Cutler, Breiman, 1994) is
recovered as the limiting case in which the last term is given infinite weight.
We introduce a `uniqueness condition' on the data which is necessary for
exactly recovering the archetypes from noiseless data. We prove that, under
uniqueness (plus additional regularity conditions on the geometry of the
archetypes), our estimator is robust. While our approach requires solving a
non-convex optimization problem, we find that standard optimization methods
succeed in finding good solutions both for real and synthetic data.
",1,0,0,0,0,0
566,Muon Reconstruction in the Daya Bay Water Pools,"  Muon reconstruction in the Daya Bay water pools would serve to verify the
simulated muon fluxes and offer the possibility of studying cosmic muons in
general. This reconstruction is, however, complicated by many optical obstacles
and the small coverage of photomultiplier tubes (PMTs) as compared to other
large water Cherenkov detectors. The PMTs' timing information is useful only in
the case of direct, unreflected Cherenkov light. This requires PMTs to be added
and removed as an hypothesized muon trajectory is iteratively improved, to
account for the changing effects of obstacles and direction of light.
Therefore, muon reconstruction in the Daya Bay water pools does not lend itself
to a general fitting procedure employing smoothly varying functions with
continuous derivatives. Here, an algorithm is described which overcomes these
complications. It employs the method of Least Mean Squares to determine an
hypothesized trajectory from the PMTs' charge-weighted positions. This
initially hypothesized trajectory is then iteratively refined using the PMTs'
timing information. Reconstructions with simulated data reproduce the simulated
trajectory to within about 5 degrees in direction and about 45 cm in position
at the pool surface, with a bias that tends to pull tracks away from the
vertical by about 3 degrees.
",0,1,0,0,0,0
567,Improving Foot-Mounted Inertial Navigation Through Real-Time Motion Classification,"  We present a method to improve the accuracy of a foot-mounted,
zero-velocity-aided inertial navigation system (INS) by varying estimator
parameters based on a real-time classification of motion type. We train a
support vector machine (SVM) classifier using inertial data recorded by a
single foot-mounted sensor to differentiate between six motion types (walking,
jogging, running, sprinting, crouch-walking, and ladder-climbing) and report
mean test classification accuracy of over 90% on a dataset with five different
subjects. From these motion types, we select two of the most common (walking
and running), and describe a method to compute optimal zero-velocity detection
parameters tailored to both a specific user and motion type by maximizing the
detector F-score. By combining the motion classifier with a set of optimal
detection parameters, we show how we can reduce INS position error during mixed
walking and running motion. We evaluate our adaptive system on a total of 5.9
km of indoor pedestrian navigation performed by five different subjects moving
along a 130 m path with surveyed ground truth markers.
",1,0,0,0,0,0
568,An Extended Low Fat Allocator API and Applications,"  The primary function of memory allocators is to allocate and deallocate
chunks of memory primarily through the malloc API. Many memory allocators also
implement other API extensions, such as deriving the size of an allocated
object from the object's pointer, or calculating the base address of an
allocation from an interior pointer. In this paper, we propose a general
purpose extended allocator API built around these common extensions. We argue
that such extended APIs have many applications and demonstrate several use
cases, such as (manual) memory error detection, meta data storage, typed
pointers and compact data-structures. Because most existing allocators were not
designed for the extended API, traditional implementations are expensive or not
possible.
Recently, the LowFat allocator for heap and stack objects has been developed.
The LowFat allocator is an implementation of the idea of low-fat pointers,
where object bounds information (size and base) are encoded into the native
machine pointer representation itself. The ""killer app"" for low-fat pointers is
automated bounds check instrumentation for program hardening and bug detection.
However, the LowFat allocator can also be used to implement highly optimized
version of the extended allocator API, which makes the new applications (listed
above) possible. In this paper, we implement and evaluate several applications
based efficient memory allocator API extensions using low-fat pointers. We also
extend the LowFat allocator to cover global objects for the first time.
",1,0,0,0,0,0
569,Exoplanet Atmosphere Retrieval using Multifractal Analysis of Secondary Eclipse Spectra,"  We extend a data-based model-free multifractal method of exoplanet detection
to probe exoplanetary atmospheres. Whereas the transmission spectrum is studied
during the primary eclipse, we analyze the emission spectrum during the
secondary eclipse, thereby probing the atmospheric limb. In addition to the
spectral structure of exoplanet atmospheres, the approach provides information
to study phenomena such as atmospheric flows, tidal-locking behavior, and the
dayside-nightside redistribution of energy. The approach is demonstrated using
Spitzer data for exoplanet HD189733b. The central advantage of the method is
the lack of model assumptions in the detection and observational schemes.
",0,1,0,1,0,0
570,Permutation Tests for Infection Graphs,"  We formulate and analyze a novel hypothesis testing problem for inferring the
edge structure of an infection graph. In our model, a disease spreads over a
network via contagion or random infection, where the random variables governing
the rates of contracting the disease from neighbors or random infection are
independent exponential random variables with unknown rate parameters. A subset
of nodes is also censored uniformly at random. Given the statuses of nodes in
the network, the goal is to determine the underlying graph. We present a
procedure based on permutation testing, and we derive sufficient conditions for
the validity of our test in terms of automorphism groups of the graphs
corresponding to the null and alternative hypotheses. Further, the test is
valid more generally for infection processes satisfying a basic symmetry
condition. Our test is easy to compute and does not involve estimating unknown
parameters governing the process. We also derive risk bounds for our
permutation test in a variety of settings, and motivate our test statistic in
terms of approximate equivalence to likelihood ratio testing and maximin tests.
We conclude with an application to real data from an HIV infection network.
",1,0,1,1,0,0
571,A novel distribution-free hybrid regression model for manufacturing process efficiency improvement,"  This work is motivated by a particular problem of a modern paper
manufacturing industry, in which maximum efficiency of the fiber-filler
recovery process is desired. A lot of unwanted materials along with valuable
fibers and fillers come out as a by-product of the paper manufacturing process
and mostly goes as waste. The job of an efficient Krofta supracell is to
separate the unwanted materials from the valuable ones so that fibers and
fillers can be collected from the waste materials and reused in the
manufacturing process. The efficiency of Krofta depends on several crucial
process parameters and monitoring them is a difficult proposition. To solve
this problem, we propose a novel hybridization of regression trees (RT) and
artificial neural networks (ANN), hybrid RT-ANN model, to solve the problem of
low recovery percentage of the supracell. This model is used to achieve the
goal of improving supracell efficiency, viz., gain in percentage recovery. In
addition, theoretical results for the universal consistency of the proposed
model are given with the optimal value of a vital model parameter. Experimental
findings show that the proposed hybrid RT-ANN model achieves higher accuracy in
predicting Krofta recovery percentage than other conventional regression models
for solving the Krofta efficiency problem. This work will help the paper
manufacturing company to become environmentally friendly with minimal
ecological damage and improved waste recovery.
",0,0,0,1,0,0
572,Exploring deep learning as an event classification method for the Cherenkov Telescope Array,"  Telescopes based on the imaging atmospheric Cherenkov technique (IACTs)
detect images of the atmospheric showers generated by gamma rays and cosmic
rays as they are absorbed by the atmosphere. The much more frequent cosmic-ray
events form the main background when looking for gamma-ray sources, and
therefore IACT sensitivity is significantly driven by the capability to
distinguish between these two types of events. Supervised learning algorithms,
like random forests and boosted decision trees, have been shown to effectively
classify IACT events. In this contribution we present results from exploratory
work using deep learning as an event classification method for the Cherenkov
Telescope Array (CTA). CTA, conceived as an array of tens of IACTs, is an
international project for a next-generation ground-based gamma-ray observatory,
aiming to improve on the sensitivity of current-generation experiments by an
order of magnitude and provide energy coverage from 20 GeV to more than 300
TeV.
",0,1,0,0,0,0
573,Photo-Induced Bandgap Renormalization Governs the Ultrafast Response of Single-Layer MoS2,"  Transition metal dichalcogenides (TMDs) are emerging as promising
two-dimensional (2d) semiconductors for optoelectronic and flexible devices.
However, a microscopic explanation of their photophysics -- of pivotal
importance for the understanding and optimization of device operation -- is
still lacking. Here we use femtosecond transient absorption spectroscopy, with
pump pulse tunability and broadband probing, to monitor the relaxation dynamics
of single-layer MoS2 over the entire visible range, upon photoexcitation of
different excitonic transitions. We find that, irrespective of excitation
photon energy, the transient absorption spectrum shows the simultaneous
bleaching of all excitonic transitions and corresponding red-shifted
photoinduced absorption bands. First-principle modeling of the ultrafast
optical response reveals that a transient bandgap renormalization, caused by
the presence of photo-excited carriers, is primarily responsible for the
observed features. Our results demonstrate the strong impact of many-body
effects in the transient optical response of TMDs even in the
low-excitation-density regime.
",0,1,0,0,0,0
574,Improved Quantile Regression Estimators when the Errors are Independently and Non-identically Distributed,"  In a classical regression model, it is usually assumed that the explanatory
variables are independent of each other and error terms are normally
distributed. But when these assumptions are not met, situations like the error
terms are not independent or they are not identically distributed or both of
these, LSE will not be robust. Hence, quantile regression has been used to
complement this deficiency of classical regression analysis and to improve the
least square estimation (LSE). In this study, we consider preliminary test and
shrinkage estimation strategies for quantile regression models with
independently and non-identically distributed (i.ni.d.) errors. A Monte Carlo
simulation study is conducted to assess the relative performance of the
estimators. Also, we numerically compare their performance with Ridge, Lasso,
Elastic Net penalty estimation strategies. A real data example is presented to
illustrate the usefulness of the suggested methods. Finally, we obtain the
asymptotic results of suggested estimators
",0,0,1,1,0,0
575,Language Modeling by Clustering with Word Embeddings for Text Readability Assessment,"  We present a clustering-based language model using word embeddings for text
readability prediction. Presumably, an Euclidean semantic space hypothesis
holds true for word embeddings whose training is done by observing word
co-occurrences. We argue that clustering with word embeddings in the metric
space should yield feature representations in a higher semantic space
appropriate for text regression. Also, by representing features in terms of
histograms, our approach can naturally address documents of varying lengths. An
empirical evaluation using the Common Core Standards corpus reveals that the
features formed on our clustering-based language model significantly improve
the previously known results for the same corpus in readability prediction. We
also evaluate the task of sentence matching based on semantic relatedness using
the Wiki-SimpleWiki corpus and find that our features lead to superior matching
performance.
",1,0,0,0,0,0
576,Discriminant circle bundles over local models of Strebel graphs and Boutroux curves,"  We study special circle bundles over two elementary moduli spaces of
meromorphic quadratic differentials with real periods denoted by $\mathcal
Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$. The space
$\mathcal Q_0^{\mathbb R}(-7)$ is the moduli space of meromorphic quadratic
differentials on the Riemann sphere with one pole of order 7 with real periods;
it appears naturally in the study of a neighbourhood of the Witten's cycle
$W_1$ in the combinatorial model based on Jenkins-Strebel quadratic
differentials of $\mathcal M_{g,n}$. The space $\mathcal Q^{\mathbb
R}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the
Riemann sphere with two poles of order at most 3 with real periods; it appears
in description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the
combinatorial model. The application of the formalism of the Bergman
tau-function to the combinatorial model (with the goal of computing
analytically Poincare dual cycles to certain combinations of tautological
classes) requires the study of special sections of circle bundles over
$\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$; in the
case of the space $\mathcal Q_0^{\mathbb R}(-7)$ a section of this circle
bundle is given by the argument of the modular discriminant. We study the
spaces $\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$,
also called the spaces of Boutroux curves, in detail, together with
corresponding circle bundles.
",0,1,1,0,0,0
577,Far-field theory for trajectories of magnetic ellipsoids in rectangular and circular channels,"  We report a method to control the positions of ellipsoidal magnets in flowing
channels of rectangular or circular cross section at low Reynolds number.A
static uniform magnetic field is used to pin the particle orientation, and the
particles move with translational drift velocities resulting from hydrodynamic
interactions with the channel walls which can be described using Blake's image
tensor.Building on his insights, we are able to present a far-field theory
predicting the particle motion in rectangular channels, and validate the
accuracy of the theory by comparing to numerical solutions using the boundary
element method.We find that, by changing the direction of the applied magnetic
field, the motion can be controlled so that particles move either to a curved
focusing region or to the channel walls.We also use simulations to show that
the particles are focused to a single line in a circular channel.Our results
suggest ways to focus and segregate magnetic particles in lab-on-a-chip
devices.
",0,1,0,0,0,0
578,On M-functions associated with modular forms,"  Let $f$ be a primitive cusp form of weight $k$ and level $N,$ let $\chi$ be a
Dirichlet character of conductor coprime with $N,$ and let
$\mathfrak{L}(f\otimes \chi, s)$ denote either $\log L(f\otimes \chi, s)$ or
$(L'/L)(f\otimes \chi, s).$ In this article we study the distribution of the
values of $\mathfrak{L}$ when either $\chi$ or $f$ vary. First, for a
quasi-character $\psi\colon \mathbb{C} \to \mathbb{C}^\times$ we find the limit
for the average $\mathrm{Avg}\_\chi \psi(L(f\otimes\chi, s)),$ when $f$ is
fixed and $\chi$ varies through the set of characters with prime conductor that
tends to infinity. Second, we prove an equidistribution result for the values
of $\mathfrak{L}(f\otimes \chi,s)$ by establishing analytic properties of the
above limit function. Third, we study the limit of the harmonic average
$\mathrm{Avg}^h\_f \psi(L(f, s)),$ when $f$ runs through the set of primitive
cusp forms of given weight $k$ and level $N\to \infty.$ Most of the results are
obtained conditionally on the Generalized Riemann Hypothesis for
$L(f\otimes\chi, s).$
",0,0,1,0,0,0
579,Learning Graph Representations by Dendrograms,"  Hierarchical graph clustering is a common technique to reveal the multi-scale
structure of complex networks. We propose a novel metric for assessing the
quality of a hierarchical clustering. This metric reflects the ability to
reconstruct the graph from the dendrogram, which encodes the hierarchy. The
optimal representation of the graph defines a class of reducible linkages
leading to regular dendrograms by greedy agglomerative clustering.
",1,0,0,1,0,0
580,Slow and Long-ranged Dynamical Heterogeneities in Dissipative Fluids,"  A two-dimensional bidisperse granular fluid is shown to exhibit pronounced
long-ranged dynamical heterogeneities as dynamical arrest is approached. Here
we focus on the most direct approach to study these heterogeneities: we
identify clusters of slow particles and determine their size, $N_c$, and their
radius of gyration, $R_G$. We show that $N_c\propto R_G^{d_f}$, providing
direct evidence that the most immobile particles arrange in fractal objects
with a fractal dimension, $d_f$, that is observed to increase with packing
fraction $\phi$. The cluster size distribution obeys scaling, approaching an
algebraic decay in the limit of structural arrest, i.e., $\phi\to\phi_c$.
Alternatively, dynamical heterogeneities are analyzed via the four-point
structure factor $S_4(q,t)$ and the dynamical susceptibility $\chi_4(t)$.
$S_4(q,t)$ is shown to obey scaling in the full range of packing fractions,
$0.6\leq\phi\leq 0.805$, and to become increasingly long-ranged as
$\phi\to\phi_c$. Finite size scaling of $\chi_4(t)$ provides a consistency
check for the previously analyzed divergences of $\chi_4(t)\propto
(\phi-\phi_c)^{-\gamma_{\chi}}$ and the correlation length $\xi\propto
(\phi-\phi_c)^{-\gamma_{\xi}}$. We check the robustness of our results with
respect to our definition of mobility. The divergences and the scaling for
$\phi\to\phi_c$ suggest a non-equilibrium glass transition which seems
qualitatively independent of the coefficient of restitution.
",0,1,0,0,0,0
581,A Globally Linearly Convergent Method for Pointwise Quadratically Supportable Convex-Concave Saddle Point Problems,"  We study the \emph{Proximal Alternating Predictor-Corrector} (PAPC) algorithm
introduced recently by Drori, Sabach and Teboulle to solve nonsmooth structured
convex-concave saddle point problems consisting of the sum of a smooth convex
function, a finite collection of nonsmooth convex functions and bilinear terms.
We introduce the notion of pointwise quadratic supportability, which is a
relaxation of a standard strong convexity assumption and allows us to show that
the primal sequence is R-linearly convergent to an optimal solution and the
primal-dual sequence is globally Q-linearly convergent. We illustrate the
proposed method on total variation denoising problems and on locally adaptive
estimation in signal/image deconvolution and denoising with multiresolution
statistical constraints.
",0,0,1,0,0,0
582,Stability of casein micelles cross-linked with genipin: a physicochemical study as a function of pH,"  Chemical or enzymatic cross-linking of casein micelles (CMs) increases their
stability against dissociating agents. In this paper, a comparative study of
stability between native CMs and CMs cross-linked with genipin (CMs-GP) as a
function of pH is described. Stability to temperature and ethanol were
investigated in the pH range 2.0-7.0. The size and the charge
($\zeta$-potential) of the particles were determined by dynamic light
scattering. Native CMs precipitated below pH 5.5, CMs-GP precipitated from pH
3.5 to 4.5, whereas no precipitation was observed at pH 2.0-3.0 or pH 4.5-7.0.
The isoelectric point of CMs-GP was determined to be pH 3.7. Highest stability
against heat and ethanol was observed for CMs-GP at pH 2, where visible
coagulation was determined only after 800 s at 140 $^\circ$C or 87.5% (v/v) of
ethanol. These results confirmed the hypothesis that cross-linking by GP
increased the stability of CMs.
",0,1,0,0,0,0
583,Kites and Residuated Lattices,"  We investigate a construction of an integral residuated lattice starting from
an integral residuated lattice and two sets with an injective mapping from one
set into the second one. The resulting algebra has a shape of a Chinese cascade
kite, therefore, we call this algebra simply a kite. We describe subdirectly
irreducible kites and we classify them. We show that the variety of integral
residuated lattices generated by kites is generated by all finite-dimensional
kites. In particular, we describe some homomorphisms among kites.
",0,0,1,0,0,0
584,TED Talk Recommender Using Speech Transcripts,"  Nowadays, online video platforms mostly recommend related videos by analyzing
user-driven data such as viewing patterns, rather than the content of the
videos. However, content is more important than any other element when videos
aim to deliver knowledge. Therefore, we have developed a web application which
recommends related TED lecture videos to the users, considering the content of
the videos from the transcripts. TED Talk Recommender constructs a network for
recommending videos that are similar content-wise and providing a user
interface.
",1,0,0,0,0,0
585,Attitude Control of the Asteroid Origins Satellite 1 (AOSAT 1),"  Exploration of asteroids and small-bodies can provide valuable insight into
the origins of the solar system, into the origins of Earth and the origins of
the building blocks of life. However, the low-gravity and unknown surface
conditions of asteroids presents a daunting challenge for surface exploration,
manipulation and for resource processing. This has resulted in the loss of
several landers or shortened missions. Fundamental studies are required to
obtain better readings of the material surface properties and physical models
of these small bodies. The Asteroid Origins Satellite 1 (AOSAT 1) is a CubeSat
centrifuge laboratory that spins at up to 4 rpm to simulate the milligravity
conditions of sub 1 km asteroids. Such a laboratory will help to de-risk
development and testing of landing and resource processing technology for
asteroids. Inside the laboratory are crushed meteorites, the remains of
asteroids. The laboratory is equipped with cameras and actuators to perform a
series of science experiments to better understand material properties and
asteroid surface physics. These results will help to improve our physics models
of asteroids. The CubeSat has been designed to be low-cost and contains 3-axis
magnetorquers and a single reaction-wheel to induce spin. In our work, we first
analyze how the attitude control system will de-tumble the spacecraft after
deployment. Further analysis has been conducted to analyze the impact and
stability of the attitude control system to shifting mass (crushed meteorites)
inside the spacecraft as its spinning in its centrifuge mode. AOSAT 1 will be
the first in a series of low-cost CubeSat centrifuges that will be launched
setting the stage for a larger, permanent, on-orbit centrifuge laboratory for
experiments in planetary science, life sciences and manufacturing.
",1,1,0,0,0,0
586,Triplet Network with Attention for Speaker Diarization,"  In automatic speech processing systems, speaker diarization is a crucial
front-end component to separate segments from different speakers. Inspired by
the recent success of deep neural networks (DNNs) in semantic inferencing,
triplet loss-based architectures have been successfully used for this problem.
However, existing work utilizes conventional i-vectors as the input
representation and builds simple fully connected networks for metric learning,
thus not fully leveraging the modeling power of DNN architectures. This paper
investigates the importance of learning effective representations from the
sequences directly in metric learning pipelines for speaker diarization. More
specifically, we propose to employ attention models to learn embeddings and the
metric jointly in an end-to-end fashion. Experiments are conducted on the
CALLHOME conversational speech corpus. The diarization results demonstrate
that, besides providing a unified model, the proposed approach achieves
improved performance when compared against existing approaches.
",0,0,0,1,0,0
587,Dynamics of cracks in disordered materials,"  Predicting when rupture occurs or cracks progress is a major challenge in
numerous elds of industrial, societal and geophysical importance. It remains
largely unsolved: Stress enhancement at cracks and defects, indeed, makes the
macroscale dynamics extremely sensitive to the microscale material disorder.
This results in giant statistical uctuations and non-trivial behaviors upon
upscaling dicult to assess via the continuum approaches of engineering. These
issues are examined here. We will see: How linear elastic fracture mechanics
sidetracks the diculty by reducing the problem to that of the propagation of a
single crack in an eective material free of defects, How slow cracks sometimes
display jerky dynamics, with sudden violent events incompatible with the
previous approach, and how some paradigms of statistical physics can explain
it, How abnormally fast cracks sometimes emerge due to the formation of
microcracks at very small scales.
",0,1,0,0,0,0
588,Optimum Decoder for Multiplicative Spread Spectrum Image Watermarking with Laplacian Modeling,"  This paper investigates the multiplicative spread spectrum watermarking
method for the image. The information bit is spreaded into middle-frequency
Discrete Cosine Transform (DCT) coefficients of each block of an image using a
generated pseudo-random sequence. Unlike the conventional signal modeling, we
suppose that both signal and noise are distributed with Laplacian distribution
because the sample loss of digital media can be better modeled with this
distribution than the Gaussian one. We derive the optimum decoder for the
proposed embedding method thanks to the maximum likelihood decoding scheme. We
also analyze our watermarking system in the presence of noise and provide
analytical evaluations and several simulations. The results show that it has
the suitable performance and transparency required for watermarking
applications.
",1,0,0,0,0,0
589,Adaptive Path-Integral Autoencoder: Representation Learning and Planning for Dynamical Systems,"  We present a representation learning algorithm that learns a low-dimensional
latent dynamical system from high-dimensional \textit{sequential} raw data,
e.g., video. The framework builds upon recent advances in amortized inference
methods that use both an inference network and a refinement procedure to output
samples from a variational distribution given an observation sequence, and
takes advantage of the duality between control and inference to approximately
solve the intractable inference problem using the path integral control
approach. The learned dynamical model can be used to predict and plan the
future states; we also present the efficient planning method that exploits the
learned low-dimensional latent dynamics. Numerical experiments show that the
proposed path-integral control based variational inference method leads to
tighter lower bounds in statistical model learning of sequential data. The
supplementary video: this https URL
",1,0,0,1,0,0
590,Semiblind subgraph reconstruction in Gaussian graphical models,"  Consider a social network where only a few nodes (agents) have meaningful
interactions in the sense that the conditional dependency graph over node
attribute variables (behaviors) is sparse. A company that can only observe the
interactions between its own customers will generally not be able to accurately
estimate its customers' dependency subgraph: it is blinded to any external
interactions of its customers and this blindness creates false edges in its
subgraph. In this paper we address the semiblind scenario where the company has
access to a noisy summary of the complementary subgraph connecting external
agents, e.g., provided by a consolidator. The proposed framework applies to
other applications as well, including field estimation from a network of awake
and sleeping sensors and privacy-constrained information sharing over social
subnetworks. We propose a penalized likelihood approach in the context of a
graph signal obeying a Gaussian graphical models (GGM). We use a convex-concave
iterative optimization algorithm to maximize the penalized likelihood.
",1,0,0,1,0,0
591,Hybrid Collaborative Recommendation via Semi-AutoEncoder,"  In this paper, we present a novel structure, Semi-AutoEncoder, based on
AutoEncoder. We generalize it into a hybrid collaborative filtering model for
rating prediction as well as personalized top-n recommendations. Experimental
results on two real-world datasets demonstrate its state-of-the-art
performances.
",1,0,0,0,0,0
592,The Linear Point: A cleaner cosmological standard ruler,"  We show how a characteristic length scale imprinted in the galaxy two-point
correlation function, dubbed the ""linear point"", can serve as a comoving
cosmological standard ruler. In contrast to the Baryon Acoustic Oscillation
peak location, this scale is constant in redshift and is unaffected by
non-linear effects to within $0.5$ percent precision. We measure the location
of the linear point in the galaxy correlation function of the LOWZ and CMASS
samples from the Twelfth Data Release (DR12) of the Baryon Oscillation
Spectroscopic Survey (BOSS) collaboration. We combine our linear-point
measurement with cosmic-microwave-background constraints from the Planck
satellite to estimate the isotropic-volume distance $D_{V}(z)$, without relying
on a model-template or reconstruction method. We find $D_V(0.32)=1264\pm 28$
Mpc and $D_V(0.57)=2056\pm 22$ Mpc respectively, consistent with the quoted
values from the BOSS collaboration. This remarkable result suggests that all
the distance information contained in the baryon acoustic oscillations can be
conveniently compressed into the single length associated with the linear
point.
",0,1,0,0,0,0
593,Incompressible limit of the Navier-Stokes model with a growth term,"  Starting from isentropic compressible Navier-Stokes equations with growth
term in the continuity equation, we rigorously justify that performing an
incompressible limit one arrives to the two-phase free boundary fluid system.
",0,0,1,0,0,0
594,Stochastic Primal-Dual Method on Riemannian Manifolds with Bounded Sectional Curvature,"  We study a stochastic primal-dual method for constrained optimization over
Riemannian manifolds with bounded sectional curvature. We prove non-asymptotic
convergence to the optimal objective value. More precisely, for the class of
hyperbolic manifolds, we establish a convergence rate that is related to the
sectional curvature lower bound. To prove a convergence rate in terms of
sectional curvature for the elliptic manifolds, we leverage Toponogov's
comparison theorem. In addition, we provide convergence analysis for the
asymptotically elliptic manifolds, where the sectional curvature at each given
point on manifold is locally bounded from below by the distance function. We
demonstrate the performance of the primal-dual algorithm on the sphere for the
non-negative principle component analysis (PCA). In particular, under the
non-negativity constraint on the principle component and for the symmetric
spiked covariance model, we empirically show that the primal-dual approach
outperforms the spectral method. We also examine the performance of the
primal-dual method for the anchored synchronization from partial noisy
measurements of relative rotations on the Lie group SO(3). Lastly, we show that
the primal-dual algorithm can be applied to the weighted MAX-CUT problem under
constraints on the admissible cut. Specifically, we propose different
approximation algorithms for the weighted MAX-CUT problem based on optimizing a
function on the manifold of direct products of the unit spheres as well as the
manifold of direct products of the rotation groups.
",0,0,1,0,0,0
595,Nudging the particle filter,"  We investigate a new sampling scheme aimed at improving the performance of
particle filters whenever (a) there is a significant mismatch between the
assumed model dynamics and the actual system, or (b) the posterior probability
tends to concentrate in relatively small regions of the state space. The
proposed scheme pushes some particles towards specific regions where the
likelihood is expected to be high, an operation known as nudging in the
geophysics literature. We re-interpret nudging in a form applicable to any
particle filtering scheme, as it does not involve any changes in the rest of
the algorithm. Since the particles are modified, but the importance weights do
not account for this modification, the use of nudging leads to additional bias
in the resulting estimators. However, we prove analytically that nudged
particle filters can still attain asymptotic convergence with the same error
rates as conventional particle methods. Simple analysis also yields an
alternative interpretation of the nudging operation that explains its
robustness to model errors. Finally, we show numerical results that illustrate
the improvements that can be attained using the proposed scheme. In particular,
we present nonlinear tracking examples with synthetic data and a model
inference example using real-world financial data.
",0,0,0,1,0,0
596,Quantum oscillations and a non-trivial Berry phase in the noncentrosymmetric superconductor BiPd,"  We report the measurements of de Haas-van Alphen (dHvA) oscillations in the
noncentrosymmetric superconductor BiPd. Several pieces of a complex multi-sheet
Fermi surface are identified, including a small pocket (frequency 40 T) which
is three dimensional and anisotropic. From the temperature dependence of the
amplitude of the oscillations, the cyclotron effective mass is ($0.18$ $\pm$
0.1) $m_e$. Further analysis showed a non-trivial $\pi$-Berry phase is
associated with the 40 T pocket, which strongly supports the presence of
topological states in bulk BiPd and may result in topological superconductivity
due to the proximity coupling to other bands.
",0,1,0,0,0,0
597,ExSIS: Extended Sure Independence Screening for Ultrahigh-dimensional Linear Models,"  Statistical inference can be computationally prohibitive in
ultrahigh-dimensional linear models. Correlation-based variable screening, in
which one leverages marginal correlations for removal of irrelevant variables
from the model prior to statistical inference, can be used to overcome this
challenge. Prior works on correlation-based variable screening either impose
strong statistical priors on the linear model or assume specific post-screening
inference methods. This paper first extends the analysis of correlation-based
variable screening to arbitrary linear models and post-screening inference
techniques. In particular, ($i$) it shows that a condition---termed the
screening condition---is sufficient for successful correlation-based screening
of linear models, and ($ii$) it provides insights into the dependence of
marginal correlation-based screening on different problem parameters. Numerical
experiments confirm that these insights are not mere artifacts of analysis;
rather, they are reflective of the challenges associated with marginal
correlation-based variable screening. Second, the paper explicitly derives the
screening condition for two families of linear models, namely, sub-Gaussian
linear models and arbitrary (random or deterministic) linear models. In the
process, it establishes that---under appropriate conditions---it is possible to
reduce the dimension of an ultrahigh-dimensional, arbitrary linear model to
almost the sample size even when the number of active variables scales almost
linearly with the sample size.
",0,0,1,1,0,0
598,Unveiling ADP-binding sites and channels in respiratory complexes: Validation of Murburn concept as a holistic explanation for oxidative phosphorylation,"  Mitochondrial oxidative phosphorylation (mOxPhos) makes ATP, the energy
currency of life. Chemiosmosis, a proton centric mechanism, advocates that
Complex V harnesses a transmembrane potential (TMP) for ATP synthesis. This
perception of cellular respiration requires oxygen to stay tethered at Complex
IV (an association inhibited by cyanide) and diffusible reactive oxygen species
(DROS) are considered wasteful and toxic products. With new mechanistic
insights on heme and flavin enzymes, an oxygen or DROS centric explanation
(called murburn concept) was recently proposed for mOxPhos. In the new
mechanism, TMP is not directly harnessed, protons are a rate limiting reactant
and DROS within matrix serve as the chemical coupling agents that directly link
NADH oxidation with ATP synthesis. Herein, we report multiple ADP binding sites
and solvent accessible DROS channels in respiratory proteins, which validate
the oxygen or DROS centric power generation (ATP synthesis) system in mOxPhos.
Since cyanide's heme binding Kd is high (mM), low doses (uM) of cyanide is
lethal because cyanide disrupts DROS dynamics in mOxPhos. The critical study
also provides comprehensive arguments against Mitchell's and Boyer's
explanations and extensive support for murburn concept based holistic
perspectives for mOxPhos.
",0,0,0,0,1,0
599,Banach synaptic algebras,"  Using a representation theorem of Erik Alfsen, Frederic Schultz, and Erling
Stormer for special JB-algebras, we prove that a synaptic algebra is norm
complete (i.e., Banach) if and only if it is isomorphic to the self-adjoint
part of a Rickart C*-algebra. Also, we give conditions on a Banach synaptic
algebra that are equivalent to the condition that it is isomorphic to the
self-adjoint part of an AW*-algebra. Moreover, we study some relationships
between synaptic algebras and so-called generalized Hermitian algebras.
",0,0,1,0,0,0
600,"Pressure tuning of structure, superconductivity and novel magnetic order in the Ce-underdoped electron-doped cuprate T'-Pr_1.3-xLa_0.7Ce_xCuO_4 (x = 0.1)","  High-pressure neutron powder diffraction, muon-spin rotation and
magnetization studies of the structural, magnetic and the superconducting
properties of the Ce-underdoped superconducting (SC) electron-doped cuprate
system T'-Pr_1.3-xLa_0.7Ce_xCuO_4 with x = 0.1 are reported. A strong reduction
of the lattice constants a and c is observed under pressure. However, no
indication of any pressure induced phase transition from T' to T structure is
observed up to the maximum applied pressure of p = 11 GPa. Large and non-linear
increase of the short-range magnetic order temperature T_so in
T'-Pr_1.3-xLa_0.7Ce_xCuO_4 (x = 0.1) was observed under pressure.
Simultaneously pressure causes a non-linear decrease of the SC transition
temperature T_c. All these experiments establish the short-range magnetic order
as an intrinsic and a new competing phase in SC T'-Pr_1.2La_0.7Ce_0.1CuO_4. The
observed pressure effects may be interpreted in terms of the improved nesting
conditions through the reduction of the in-plane and out-of-plane lattice
constants upon hydrostatic pressure.
",0,1,0,0,0,0
601,Regrasping by Fixtureless Fixturing,"  This paper presents a fixturing strategy for regrasping that does not require
a physical fixture. To regrasp an object in a gripper, a robot pushes the
object against external contact/s in the environment such that the external
contact keeps the object stationary while the fingers slide over the object. We
call this manipulation technique fixtureless fixturing. Exploiting the
mechanics of pushing, we characterize a convex polyhedral set of pushes that
results in fixtureless fixturing. These pushes are robust against uncertainty
in the object inertia, grasping force, and the friction at the contacts. We
propose a sampling-based planner that uses the sets of robust pushes to rapidly
build a tree of reachable grasps. A path in this tree is a pushing strategy,
possibly involving pushes from different sides, to regrasp the object. We
demonstrate the experimental validity and robustness of the proposed
manipulation technique with different regrasp examples on a manipulation
platform. Such a fast and flexible regrasp planner facilitates versatile and
flexible automation solutions.
",1,0,0,0,0,0
602,Subset Labeled LDA for Large-Scale Multi-Label Classification,"  Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard
unsupervised Latent Dirichlet Allocation (LDA) algorithm, to address
multi-label learning tasks. Previous work has shown it to perform in par with
other state-of-the-art multi-label methods. Nonetheless, with increasing label
sets sizes LLDA encounters scalability issues. In this work, we introduce
Subset LLDA, a simple variant of the standard LLDA algorithm, that not only can
effectively scale up to problems with hundreds of thousands of labels but also
improves over the LLDA state-of-the-art. We conduct extensive experiments on
eight data sets, with label sets sizes ranging from hundreds to hundreds of
thousands, comparing our proposed algorithm with the previously proposed LLDA
algorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme
multi-label classification. The results show a steady advantage of our method
over the other LLDA algorithms and competitive results compared to the extreme
multi-label classification algorithms.
",0,0,0,1,0,0
603,A Hybrid Approach to Video Source Identification,"  Multimedia Forensics allows to determine whether videos or images have been
captured with the same device, and thus, eventually, by the same person.
Currently, the most promising technology to achieve this task, exploits the
unique traces left by the camera sensor into the visual content. Anyway, image
and video source identification are still treated separately from one another.
This approach is limited and anachronistic if we consider that most of the
visual media are today acquired using smartphones, that capture both images and
videos. In this paper we overcome this limitation by exploring a new approach
that allows to synergistically exploit images and videos to study the device
from which they both come. Indeed, we prove it is possible to identify the
source of a digital video by exploiting a reference sensor pattern noise
generated from still images taken by the same device of the query video. The
proposed method provides comparable or even better performance, when compared
to the current video identification strategies, where a reference pattern is
estimated from video frames. We also show how this strategy can be effective
even in case of in-camera digitally stabilized videos, where a non-stabilized
reference is not available, by solving some state-of-the-art limitations. We
explore a possible direct application of this result, that is social media
profile linking, i.e. discovering relationships between two or more social
media profiles by comparing the visual contents - images or videos - shared
therein.
",1,0,0,0,0,0
604,Asymptotics of ABC,"  We present an informal review of recent work on the asymptotics of
Approximate Bayesian Computation (ABC). In particular we focus on how does the
ABC posterior, or point estimates obtained by ABC, behave in the limit as we
have more data? The results we review show that ABC can perform well in terms
of point estimation, but standard implementations will over-estimate the
uncertainty about the parameters. If we use the regression correction of
Beaumont et al. then ABC can also accurately quantify this uncertainty. The
theoretical results also have practical implications for how to implement ABC.
",0,0,1,1,0,0
605,Learning Sparse Polymatrix Games in Polynomial Time and Sample Complexity,"  We consider the problem of learning sparse polymatrix games from observations
of strategic interactions. We show that a polynomial time method based on
$\ell_{1,2}$-group regularized logistic regression recovers a game, whose Nash
equilibria are the $\epsilon$-Nash equilibria of the game from which the data
was generated (true game), in $\mathcal{O}(m^4 d^4 \log (pd))$ samples of
strategy profiles --- where $m$ is the maximum number of pure strategies of a
player, $p$ is the number of players, and $d$ is the maximum degree of the game
graph. Under slightly more stringent separability conditions on the payoff
matrices of the true game, we show that our method learns a game with the exact
same Nash equilibria as the true game. We also show that $\Omega(d \log (pm))$
samples are necessary for any method to consistently recover a game, with the
same Nash-equilibria as the true game, from observations of strategic
interactions. We verify our theoretical results through simulation experiments.
",1,0,0,0,0,0
606,Superposition solutions to the extended KdV equation for water surface waves,"  The KdV equation can be derived in the shallow water limit of the Euler
equations. Over the last few decades, this equation has been extended to
include higher order effects. Although this equation has only one conservation
law, exact periodic and solitonic solutions exist. Khare and Saxena
\cite{KhSa,KhSa14,KhSa15} demonstrated the possibility of generating new exact
solutions by combining known ones for several fundamental equations (e.g.,
Korteweg - de Vries, Nonlinear Schr??dinger). Here we find that this
construction can be repeated for higher order, non-integrable extensions of
these equations. Contrary to many statements in the literature, there seems to
be no correlation between integrability and the number of nonlinear one
variable wave solutions.
",0,1,0,0,0,0
607,Boosting the Actor with Dual Critic,"  This paper proposes a new actor-critic-style algorithm called Dual
Actor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian
dual form of the Bellman optimality equation, which can be viewed as a
two-player game between the actor and a critic-like function, which is named as
dual critic. Compared to its actor-critic relatives, Dual-AC has the desired
property that the actor and dual critic are updated cooperatively to optimize
the same objective function, providing a more transparent way for learning the
critic that is directly related to the objective function of the actor. We then
provide a concrete algorithm that can effectively solve the minimax
optimization problem, using techniques of multi-step bootstrapping, path
regularization, and stochastic dual ascent algorithm. We demonstrate that the
proposed algorithm achieves the state-of-the-art performances across several
benchmarks.
",1,0,0,0,0,0
608,Counting Dominating Sets of Graphs,"  Counting dominating sets in a graph $G$ is closely related to the
neighborhood complex of $G$. We exploit this relation to prove that the number
of dominating sets $d(G)$ of a graph is determined by the number of complete
bipartite subgraphs of its complement. More precisely, we state the following.
Let $G$ be a simple graph of order $n$ such that its complement has exactly
$a(G)$ subgraphs isomorphic to $K_{2p,2q}$ and exactly $b(G)$ subgraphs
isomorphic to $K_{2p+1,2q+1}$. Then $d(G) = 2^n -1 + 2[a(G)-b(G)]$. We also
show some new relations between the domination polynomial and the neighborhood
polynomial of a graph.
",0,0,1,0,0,0
609,High SNR Consistent Compressive Sensing,"  High signal to noise ratio (SNR) consistency of model selection criteria in
linear regression models has attracted a lot of attention recently. However,
most of the existing literature on high SNR consistency deals with model order
selection. Further, the limited literature available on the high SNR
consistency of subset selection procedures (SSPs) is applicable to linear
regression with full rank measurement matrices only. Hence, the performance of
SSPs used in underdetermined linear models (a.k.a compressive sensing (CS)
algorithms) at high SNR is largely unknown. This paper fills this gap by
deriving necessary and sufficient conditions for the high SNR consistency of
popular CS algorithms like $l_0$-minimization, basis pursuit de-noising or
LASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions
analytically establish the high SNR inconsistency of CS algorithms when used
with the tuning parameters discussed in literature. Novel tuning parameters
with SNR adaptations are developed using the sufficient conditions and the
choice of SNR adaptations are discussed analytically using convergence rate
analysis. CS algorithms with the proposed tuning parameters are numerically
shown to be high SNR consistent and outperform existing tuning parameters in
the moderate to high SNR regime.
",1,0,0,1,0,0
610,Communication Reducing Algorithms for Distributed Hierarchical N-Body Problems with Boundary Distributions,"  Reduction of communication and efficient partitioning are key issues for
achieving scalability in hierarchical $N$-Body algorithms like FMM. In the
present work, we propose four independent strategies to improve partitioning
and reduce communication. First of all, we show that the conventional wisdom of
using space-filling curve partitioning may not work well for boundary integral
problems, which constitute about 50% of FMM's application user base. We propose
an alternative method which modifies orthogonal recursive bisection to solve
the cell-partition misalignment that has kept it from scaling previously.
Secondly, we optimize the granularity of communication to find the optimal
balance between a bulk-synchronous collective communication of the local
essential tree and an RDMA per task per cell. Finally, we take the dynamic
sparse data exchange proposed by Hoefler et al. and extend it to a hierarchical
sparse data exchange, which is demonstrated at scale to be faster than the MPI
library's MPI_Alltoallv that is commonly used.
",1,0,0,0,0,0
611,Traffic Surveillance Camera Calibration by 3D Model Bounding Box Alignment for Accurate Vehicle Speed Measurement,"  In this paper, we focus on fully automatic traffic surveillance camera
calibration, which we use for speed measurement of passing vehicles. We improve
over a recent state-of-the-art camera calibration method for traffic
surveillance based on two detected vanishing points. More importantly, we
propose a novel automatic scene scale inference method. The method is based on
matching bounding boxes of rendered 3D models of vehicles with detected
bounding boxes in the image. The proposed method can be used from arbitrary
viewpoints, since it has no constraints on camera placement. We evaluate our
method on the recent comprehensive dataset for speed measurement BrnoCompSpeed.
Experiments show that our automatic camera calibration method by detection of
two vanishing points reduces error by 50% (mean distance ratio error reduced
from 0.18 to 0.09) compared to the previous state-of-the-art method. We also
show that our scene scale inference method is more precise, outperforming both
state-of-the-art automatic calibration method for speed measurement (error
reduction by 86% -- 7.98km/h to 1.10km/h) and manual calibration (error
reduction by 19% -- 1.35km/h to 1.10km/h). We also present qualitative results
of the proposed automatic camera calibration method on video sequences obtained
from real surveillance cameras in various places, and under different lighting
conditions (night, dawn, day).
",1,0,0,0,0,0
612,Technical Report for Real-Time Certified Probabilistic Pedestrian Forecasting,"  The success of autonomous systems will depend upon their ability to safely
navigate human-centric environments. This motivates the need for a real-time,
probabilistic forecasting algorithm for pedestrians, cyclists, and other agents
since these predictions will form a necessary step in assessing the risk of any
action. This paper presents a novel approach to probabilistic forecasting for
pedestrians based on weighted sums of ordinary differential equations that are
learned from historical trajectory information within a fixed scene. The
resulting algorithm is embarrassingly parallel and is able to work at real-time
speeds using a naive Python implementation. The quality of predicted locations
of agents generated by the proposed algorithm is validated on a variety of
examples and considerably higher than existing state of the art approaches over
long time horizons.
",1,0,1,0,0,0
613,Distance Measure Machines,"  This paper presents a distance-based discriminative framework for learning
with probability distributions. Instead of using kernel mean embeddings or
generalized radial basis kernels, we introduce embeddings based on
dissimilarity of distributions to some reference distributions denoted as
templates. Our framework extends the theory of similarity of Balcan et al.
(2008) to the population distribution case and we show that, for some learning
problems, some dissimilarity on distribution achieves low-error linear decision
functions with high probability. Our key result is to prove that the theory
also holds for empirical distributions. Algorithmically, the proposed approach
consists in computing a mapping based on pairwise dissimilarity where learning
a linear decision function is amenable. Our experimental results show that the
Wasserstein distance embedding performs better than kernel mean embeddings and
computing Wasserstein distance is far more tractable than estimating pairwise
Kullback-Leibler divergence of empirical distributions.
",0,0,0,1,0,0
614,DSBGK Method to Incorporate the CLL Reflection Model and to Simulate Gas Mixtures,"  Molecular reflections on usual wall surfaces can be statistically described
by the Maxwell diffuse reflection model, which has been successfully applied in
the DSBGK simulations. We develop the DSBGK algorithm to implement the
Cercignani-Lampis-Lord (CLL) reflection model, which is widely applied to
polished surfaces and used particularly in modeling space shuttles to predict
the heat and force loads exerted by the high-speed flows around the surfaces.
We also extend the DSBGK method to simulate gas mixtures and high contrast of
number densities of different components can be handled at a cost of memory
usage much lower than that needed by the DSMC simulations because the average
numbers of simulated molecules of different components per cell can be equal in
the DSBGK simulations.
",0,1,0,0,0,0
615,"Tropical formulae for summation over a part of SL(2, Z)","  Let $f(a,b,c,d)=\sqrt{a^2+b^2}+\sqrt{c^2+d^2}-\sqrt{(a+c)^2+(b+d)^2}$, let
$(a,b,c,d)$ stand for $a,b,c,d\in\mathbb Z_{\geq 0}$ such that $ad-bc=1$.
Define \begin{equation} \label{eq_main} F(s) = \sum_{(a,b,c,d)} f(a,b,c,d)^s.
\end{equation} In other words, we consider the sum of the powers of the
triangle inequality defects for the lattice parallelograms (in the first
quadrant) of area one.
We prove that $F(s)$ converges when $s>1/2$ and diverges at $s=1/2$. We also
prove $$\sum\limits_{\substack{(a,b,c,d),\\ 1\leq a\leq b, 1\leq c\leq d}}
\frac{1}{(a+b)^2(c+d)^2(a+b+c+d)^2} = 1/24,$$ and show a general method to
obtain such formulae. The method comes from the consideration of the tropical
analogue of the caustic curves, whose moduli give a complete set of continuous
invariants on the space of convex domains.
",0,0,1,0,0,0
616,Efficient sampling of conditioned Markov jump processes,"  We consider the task of generating draws from a Markov jump process (MJP)
between two time points at which the process is known. Resulting draws are
typically termed bridges and the generation of such bridges plays a key role in
simulation-based inference algorithms for MJPs. The problem is challenging due
to the intractability of the conditioned process, necessitating the use of
computationally intensive methods such as weighted resampling or Markov chain
Monte Carlo. An efficient implementation of such schemes requires an
approximation of the intractable conditioned hazard/propensity function that is
both cheap and accurate. In this paper, we review some existing approaches to
this problem before outlining our novel contribution. Essentially, we leverage
the tractability of a Gaussian approximation of the MJP and suggest a
computationally efficient implementation of the resulting conditioned hazard
approximation. We compare and contrast our approach with existing methods using
three examples.
",0,0,0,1,0,0
617,Holography and thermalization in optical pump-probe spectroscopy,"  Using holography, we model experiments in which a 2+1D strange metal is
pumped by a laser pulse into a highly excited state, after which the time
evolution of the optical conductivity is probed. We consider a finite-density
state with mildly broken translation invariance and excite it by oscillating
electric field pulses. At zero density, the optical conductivity would assume
its thermalized value immediately after the pumping has ended. At finite
density, pulses with significant DC components give rise to slow exponential
relaxation, governed by a vector quasinormal mode. In contrast, for
high-frequency pulses the amplitude of the quasinormal mode is strongly
suppressed, so that the optical conductivity assumes its thermalized value
effectively instantaneously. This surprising prediction may provide a stimulus
for taking up the challenge to realize these experiments in the laboratory.
Such experiments would test a crucial open question faced by applied
holography: Are its predictions artefacts of the large $N$ limit or do they
enjoy sufficient UV independence to hold at least qualitatively in real-world
systems?
",0,1,0,0,0,0
618,On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis,"  In this paper, we study random subsampling of Gaussian process regression,
one of the simplest approximation baselines, from a theoretical perspective.
Although subsampling discards a large part of training data, we show provable
guarantees on the accuracy of the predictive mean/variance and its
generalization ability. For analysis, we consider embedding kernel matrices
into graphons, which encapsulate the difference of the sample size and enables
us to evaluate the approximation and generalization errors in a unified manner.
The experimental results show that the subsampling approximation achieves a
better trade-off regarding accuracy and runtime than the Nystr??m and random
Fourier expansion methods.
",1,0,0,1,0,0
619,Representing Hybrid Automata by Action Language Modulo Theories,"  Both hybrid automata and action languages are formalisms for describing the
evolution of dynamic systems. This paper establishes a formal relationship
between them. We show how to succinctly represent hybrid automata in an action
language which in turn is defined as a high-level notation for answer set
programming modulo theories (ASPMT) --- an extension of answer set programs to
the first-order level similar to the way satisfiability modulo theories (SMT)
extends propositional satisfiability (SAT). We first show how to represent
linear hybrid automata with convex invariants by an action language modulo
theories. A further translation into SMT allows for computing them using SMT
solvers that support arithmetic over reals. Next, we extend the representation
to the general class of non-linear hybrid automata allowing even non-convex
invariants. We represent them by an action language modulo ODE (Ordinary
Differential Equations), which can be compiled into satisfiability modulo ODE.
We developed a prototype system cplus2aspmt based on these translations, which
allows for a succinct representation of hybrid transition systems that can be
computed effectively by the state-of-the-art SMT solver dReal.
",1,0,0,0,0,0
620,An enthalpy-based multiple-relaxation-time lattice Boltzmann method for solid-liquid phase change heat transfer in metal foams,"  In this paper, an enthalpy-based multiple-relaxation-time (MRT) lattice
Boltzmann (LB) method is developed for solid-liquid phase change heat transfer
in metal foams under local thermal non-equilibrium (LTNE) condition. The
enthalpy-based MRT-LB method consists of three different MRT-LB models: one for
flow field based on the generalized non-Darcy model, and the other two for
phase change material (PCM) and metal foam temperature fields described by the
LTNE model. The moving solid-liquid phase interface is implicitly tracked
through the liquid fraction, which is simultaneously obtained when the energy
equations of PCM and metal foam are solved. The present method has several
distinctive features. First, as compared with previous studies, the present
method avoids the iteration procedure, thus it retains the inherent merits of
the standard LB method and is superior over the iteration method in terms of
accuracy and computational efficiency. Second, a volumetric LB scheme instead
of the bounce-back scheme is employed to realize the no-slip velocity condition
in the interface and solid phase regions, which is consistent with the actual
situation. Last but not least, the MRT collision model is employed, and with
additional degrees of freedom, it has the ability to reduce the numerical
diffusion across phase interface induced by solid-liquid phase change.
Numerical tests demonstrate that the present method can be served as an
accurate and efficient numerical tool for studying metal foam enhanced
solid-liquid phase change heat transfer in latent heat storage. Finally,
comparisons and discussions are made to offer useful information for practical
applications of the present method.
",0,1,0,0,0,0
621,Birth of a subaqueous barchan dune,"  Barchan dunes are crescentic shape dunes with horns pointing downstream. The
present paper reports the formation of subaqueous barchan dunes from initially
conical heaps in a rectangular channel. Because the most unique feature of a
barchan dune is its horns, we associate the timescale for the appearance of
horns to the formation of a barchan dune. A granular heap initially conical was
placed on the bottom wall of a closed conduit and it was entrained by a water
flow in turbulent regime. After a certain time, horns appear and grow, until an
equilibrium length is reached. Our results show the existence of the timescales
$0.5t_c$ and $2.5t_c$ for the appearance and equilibrium of horns,
respectively, where $t_c$ is a characteristic time that scales with the grains
diameter, gravity acceleration, densities of the fluid and grains, and shear
and threshold velocities.
",0,1,0,0,0,0
622,Uncoupled isotonic regression via minimum Wasserstein deconvolution,"  Isotonic regression is a standard problem in shape-constrained estimation
where the goal is to estimate an unknown nondecreasing regression function $f$
from independent pairs $(x_i, y_i)$ where $\mathbb{E}[y_i]=f(x_i), i=1, \ldots
n$. While this problem is well understood both statistically and
computationally, much less is known about its uncoupled counterpart where one
is given only the unordered sets $\{x_1, \ldots, x_n\}$ and $\{y_1, \ldots,
y_n\}$. In this work, we leverage tools from optimal transport theory to derive
minimax rates under weak moments conditions on $y_i$ and to give an efficient
algorithm achieving optimal rates. Both upper and lower bounds employ
moment-matching arguments that are also pertinent to learning mixtures of
distributions and deconvolution.
",0,0,0,1,0,0
623,Failure of Smooth Pasting Principle and Nonexistence of Equilibrium Stopping Rules under Time-Inconsistency,"  This paper considers a time-inconsistent stopping problem in which the
inconsistency arises from non-constant time preference rates. We show that the
smooth pasting principle, the main approach that has been used to construct
explicit solutions for conventional time-consistent optimal stopping problems,
may fail under time-inconsistency. Specifically, we prove that the smooth
pasting principle solves a time-inconsistent problem within the intra-personal
game theoretic framework if and only if a certain inequality on the model
primitives is satisfied. We show that the violation of this inequality can
happen even for very simple non-exponential discount functions. Moreover, we
demonstrate that the stopping problem does not admit any intra-personal
equilibrium whenever the smooth pasting principle fails. The ""negative"" results
in this paper caution blindly extending the classical approaches for
time-consistent stopping problems to their time-inconsistent counterparts.
",0,0,0,0,0,1
624,Perils of Zero-Interaction Security in the Internet of Things,"  The Internet of Things (IoT) demands authentication systems which can provide
both security and usability. Recent research utilizes the rich sensing
capabilities of smart devices to build security schemes operating without human
interaction, such as zero-interaction pairing (ZIP) and zero-interaction
authentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and
reported promising results. However, those schemes were often evaluated under
conditions which do not reflect realistic IoT scenarios. In addition, drawing
any comparison among the existing schemes is impossible due to the lack of a
common public dataset and unavailability of scheme implementations.
In this paper, we address these challenges by conducting the first
large-scale comparative study of ZIP and ZIA schemes, carried out under
realistic conditions. We collect and release the most comprehensive dataset in
the domain to date, containing over 4250 hours of audio recordings and 1
billion sensor readings from three different scenarios, and evaluate five
state-of-the-art schemes based on these data. Our study reveals that the
effectiveness of the existing proposals is highly dependent on the scenario
they are used in. In particular, we show that these schemes are subject to
error rates between 0.6% and 52.8%.
",1,0,0,0,0,0
625,"Coarse-grained simulation of auxetic, two-dimensional crystal dynamics","  The increasing number of protein-based metamaterials demands reliable and
efficient methods to study the physicochemical properties they may display. In
this regard, we develop a simulation strategy based on Molecular Dynamics (MD)
that addresses the geometric degrees of freedom of an auxetic two-dimensional
protein crystal. This model consists of a network of impenetrable rigid squares
linked through massless rigid rods, thus featuring a large number of both
holonomic and nonholonomic constraints. Our MD methodology is optimized to
study highly constrained systems and allows for the simulation of long-time
dynamics with reasonably large timesteps. The data extracted from the
simulations shows a persistent motional interdependence among the protein
subunits in the crystal. We characterize the dynamical correlations featured by
these subunits and identify two regimes characterized by their locality or
nonlocality, depending on the geometric parameters of the crystal. From the
same data, we also calculate the Poisson\rq{}s (longitudinal to axial strain)
ratio of the crystal, and learn that, due to holonomic constraints (rigidness
of the rod links), the crystal remains auxetic even after significant changes
in the original geometry. The nonholonomic ones (collisions between subunits)
increase the number of inhomogeneous deformations of the crystal, thus driving
it away from an isotropic response. Our work provides the first simulation of
the dynamics of protein crystals and offers insights into promising mechanical
properties afforded by these materials.
",0,1,0,0,0,0
626,Core2Vec: A core-preserving feature learning framework for networks,"  Recent advances in the field of network representation learning are mostly
attributed to the application of the skip-gram model in the context of graphs.
State-of-the-art analogues of skip-gram model in graphs define a notion of
neighbourhood and aim to find the vector representation for a node, which
maximizes the likelihood of preserving this neighborhood.
In this paper, we take a drastic departure from the existing notion of
neighbourhood of a node by utilizing the idea of coreness. More specifically,
we utilize the well-established idea that nodes with similar core numbers play
equivalent roles in the network and hence induce a novel and an organic notion
of neighbourhood. Based on this idea, we propose core2vec, a new algorithmic
framework for learning low dimensional continuous feature mapping for a node.
Consequently, the nodes having similar core numbers are relatively closer in
the vector space that we learn.
We further demonstrate the effectiveness of core2vec by comparing word
similarity scores obtained by our method where the node representations are
drawn from standard word association graphs against scores computed by other
state-of-the-art network representation techniques like node2vec, DeepWalk and
LINE. Our results always outperform these existing methods
",1,0,0,0,0,0
627,Predicting wind pressures around circular cylinders using machine learning techniques,"  Numerous studies have been carried out to measure wind pressures around
circular cylinders since the early 20th century due to its engineering
significance. Consequently, a large amount of wind pressure data sets have
accumulated, which presents an excellent opportunity for using machine learning
(ML) techniques to train models to predict wind pressures around circular
cylinders. Wind pressures around smooth circular cylinders are a function of
mainly the Reynolds number (Re), turbulence intensity (Ti) of the incident
wind, and circumferential angle of the cylinder. Considering these three
parameters as the inputs, this study trained two ML models to predict mean and
fluctuating pressures respectively. Three machine learning algorithms including
decision tree regressor, random forest, and gradient boosting regression trees
(GBRT) were tested. The GBRT models exhibited the best performance for
predicting both mean and fluctuating pressures, and they are capable of making
accurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to
15%. It is believed that the GBRT models provide very efficient and economical
alternative to traditional wind tunnel tests and computational fluid dynamic
simulations for determining wind pressures around smooth circular cylinders
within the studied Re and Ti range.
",1,0,0,1,0,0
628,Randomly coloring simple hypergraphs with fewer colors,"  We study the problem of constructing a (near) uniform random proper
$q$-coloring of a simple $k$-uniform hypergraph with $n$ vertices and maximum
degree $\Delta$. (Proper in that no edge is mono-colored and simple in that two
edges have maximum intersection of size one). We show that if $q\geq
\max\{C_k\log n,500k^3\Delta^{1/(k-1)}\}$ then the Glauber Dynamics will become
close to uniform in $O(n\log n)$ time, given a random (improper) start. This
improves on the results in Frieze and Melsted [5].
",1,0,0,0,0,0
629,"Contributed Discussion to Uncertainty Quantification for the Horseshoe by St??phanie van der Pas, Botond Szab?? and Aad van der Vaart","  We begin by introducing the main ideas of the paper under discussion. We
discuss some interesting issues regarding adaptive component-wise credible
intervals. We then briefly touch upon the concepts of self-similarity and
excessive bias restriction. This is then followed by some comments on the
extensive simulation study carried out in the paper.
",0,0,1,1,0,0
630,Channel masking for multivariate time series shapelets,"  Time series shapelets are discriminative sub-sequences and their similarity
to time series can be used for time series classification. Initial shapelet
extraction algorithms searched shapelets by complete enumeration of all
possible data sub-sequences. Research on shapelets for univariate time series
proposed a mechanism called shapelet learning which parameterizes the shapelets
and learns them jointly with a prediction model in an optimization procedure.
Trivial extension of this method to multivariate time series does not yield
very good results due to the presence of noisy channels which lead to
overfitting. In this paper we propose a shapelet learning scheme for
multivariate time series in which we introduce channel masks to discount noisy
channels and serve as an implicit regularization.
",1,0,0,0,0,0
631,Spin mediated enhanced negative magnetoresistance in Ni80Fe20 and p-silicon bilayer,"  In this work, we present an experimental study of spin mediated enhanced
negative magnetoresistance in Ni80Fe20 (50 nm)/p-Si (350 nm) bilayer. The
resistance measurement shows a reduction of ~2.5% for the bilayer specimen as
compared to 1.3% for Ni80Fe20 (50 nm) on oxide specimen for an out-of-plane
applied magnetic field of 3T. In the Ni80Fe20-only film, the negative
magnetoresistance behavior is attributed to anisotropic magnetoresistance. We
propose that spin polarization due to spin-Hall effect is the underlying cause
of the enhanced negative magnetoresistance observed in the bilayer. Silicon has
weak spin orbit coupling so spin Hall magnetoresistance measurement is not
feasible. We use V2{\omega} and V3{\omega} measurement as a function of
magnetic field and angular rotation of magnetic field in direction normal to
electric current to elucidate the spin-Hall effect. The angular rotation of
magnetic field shows a sinusoidal behavior for both V2{\omega} and V3{\omega},
which is attributed to the spin phonon interactions resulting from the
spin-Hall effect mediated spin polarization. We propose that the spin
polarization leads to a decrease in hole-phonon scattering resulting in
enhanced negative magnetoresistance.
",0,1,0,0,0,0
632,On the Limitations of Representing Functions on Sets,"  Recent work on the representation of functions on sets has considered the use
of summation in a latent space to enforce permutation invariance. In
particular, it has been conjectured that the dimension of this latent space may
remain fixed as the cardinality of the sets under consideration increases.
However, we demonstrate that the analysis leading to this conjecture requires
mappings which are highly discontinuous and argue that this is only of limited
practical use. Motivated by this observation, we prove that an implementation
of this model via continuous mappings (as provided by e.g. neural networks or
Gaussian processes) actually imposes a constraint on the dimensionality of the
latent space. Practical universal function representation for set inputs can
only be achieved with a latent dimension at least the size of the maximum
number of input elements.
",1,0,0,1,0,0
633,Learning Models from Data with Measurement Error: Tackling Underreporting,"  Measurement error in observational datasets can lead to systematic bias in
inferences based on these datasets. As studies based on observational data are
increasingly used to inform decisions with real-world impact, it is critical
that we develop a robust set of techniques for analyzing and adjusting for
these biases. In this paper we present a method for estimating the distribution
of an outcome given a binary exposure that is subject to underreporting. Our
method is based on a missing data view of the measurement error problem, where
the true exposure is treated as a latent variable that is marginalized out of a
joint model. We prove three different conditions under which the outcome
distribution can still be identified from data containing only error-prone
observations of the exposure. We demonstrate this method on synthetic data and
analyze its sensitivity to near violations of the identifiability conditions.
Finally, we use this method to estimate the effects of maternal smoking and
opioid use during pregnancy on childhood obesity, two import problems from
public health. Using the proposed method, we estimate these effects using only
subject-reported drug use data and substantially refine the range of estimates
generated by a sensitivity analysis-based approach. Further, the estimates
produced by our method are consistent with existing literature on both the
effects of maternal smoking and the rate at which subjects underreport smoking.
",1,0,0,1,0,0
634,A simple introduction to Karmarkar's Algorithm for Linear Programming,"  An extremely simple, description of Karmarkar's algorithm with very few
technical terms is given.
",1,0,0,0,0,0
635,Magneto-inductive Passive Relaying in Arbitrarily Arranged Networks,"  We consider a wireless sensor network that uses inductive near-field coupling
for wireless powering or communication, or for both. The severely limited range
of an inductively coupled source-destination pair can be improved using
resonant relay devices, which are purely passive in nature. Utilization of such
magneto-inductive relays has only been studied for regular network topologies,
allowing simplified assumptions on the mutual antenna couplings. In this work
we present an analysis of magneto-inductive passive relaying in arbitrarily
arranged networks. We find that the resulting channel has characteristics
similar to multipath fading: the channel power gain is governed by a
non-coherent sum of phasors, resulting in increased frequency selectivity. We
propose and study two strategies to increase the channel power gain of random
relay networks: i) deactivation of individual relays by open-circuit switching
and ii) frequency tuning. The presented results show that both methods improve
the utilization of available passive relays, leading to reliable and
significant performance gains.
",1,0,0,0,0,0
636,Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert Spaces,"  Transfer operators such as the Perron--Frobenius or Koopman operator play an
important role in the global analysis of complex dynamical systems. The
eigenfunctions of these operators can be used to detect metastable sets, to
project the dynamics onto the dominant slow processes, or to separate
superimposed signals. We extend transfer operator theory to reproducing kernel
Hilbert spaces and show that these operators are related to Hilbert space
representations of conditional distributions, known as conditional mean
embeddings in the machine learning community. Moreover, numerical methods to
compute empirical estimates of these embeddings are akin to data-driven methods
for the approximation of transfer operators such as extended dynamic mode
decomposition and its variants. One main benefit of the presented kernel-based
approaches is that these methods can be applied to any domain where a
similarity measure given by a kernel is available. We illustrate the results
with the aid of guiding examples and highlight potential applications in
molecular dynamics as well as video and text data analysis.
",1,0,0,1,0,0
637,Asymptotics of the bound state induced by $??$-interaction supported on a weakly deformed plane,"  In this paper we consider the three-dimensional Schr??dinger operator with
a $\delta$-interaction of strength $\alpha > 0$ supported on an unbounded
surface parametrized by the mapping $\mathbb{R}^2\ni x\mapsto (x,\beta f(x))$,
where $\beta \in [0,\infty)$ and $f\colon \mathbb{R}^2\rightarrow\mathbb{R}$,
$f\not\equiv 0$, is a $C^2$-smooth, compactly supported function. The surface
supporting the interaction can be viewed as a local deformation of the plane.
It is known that the essential spectrum of this Schr??dinger operator
coincides with $[-\frac14\alpha^2,+\infty)$. We prove that for all sufficiently
small $\beta > 0$ its discrete spectrum is non-empty and consists of a unique
simple eigenvalue. Moreover, we obtain an asymptotic expansion of this
eigenvalue in the limit $\beta \rightarrow 0+$. In particular, this eigenvalue
tends to $-\frac14\alpha^2$ exponentially fast as $\beta\rightarrow 0+$.
",0,0,1,0,0,0
638,An?­lise comparativa de pesquisas de origens e destinos: uma abordagem baseada em Redes Complexas,"  In this paper, a comparative study was conducted between complex networks
representing origin and destination survey data. Similarities were found
between the characteristics of the networks of Brazilian cities with networks
of foreign cities. Power laws were found in the distributions of edge weights
and this scale - free behavior can occur due to the economic characteristics of
the cities.
",1,0,0,0,0,0
639,Inverse Kinematics for Control of Tensegrity Soft Robots: Existence and Optimality of Solutions,"  Tension-network (`tensegrity') robots encounter many control challenges as
articulated soft robots, due to the structures' high-dimensional nonlinear
dynamics. Control approaches have been developed which use the inverse
kinematics of tensegrity structures, either for open-loop control or as
equilibrium inputs for closed-loop controllers. However, current formulations
of the tensegrity inverse kinematics problem are limited in robotics
applications: first, they can lead to higher than needed cable tensions, and
second, may lack solutions when applied to robots with high node-to-cable
ratios. This work provides progress in both directions. To address the first
limitation, the objective function for the inverse kinematics optimization
problem is modified to produce cable tensions as low or lower than before, thus
reducing the load on the robots' motors. For the second, a reformulation of the
static equilibrium constraint is proposed, which produces solutions independent
of the number of nodes within each rigid body. Simulation results using the
second reformulation on a specific tensegrity spine robot show reasonable
open-loop control results, whereas the previous formulation could not produce
any solution.
",1,0,0,0,0,0
640,Smallest eigenvalue density for regular or fixed-trace complex Wishart-Laguerre ensemble and entanglement in coupled kicked tops,"  The statistical behaviour of the smallest eigenvalue has important
implications for systems which can be modeled using a Wishart-Laguerre
ensemble, the regular one or the fixed trace one. For example, the density of
the smallest eigenvalue of the Wishart-Laguerre ensemble plays a crucial role
in characterizing multiple channel telecommunication systems. Similarly, in the
quantum entanglement problem, the smallest eigenvalue of the fixed trace
ensemble carries information regarding the nature of entanglement.
For real Wishart-Laguerre matrices, there exists an elegant recurrence scheme
suggested by Edelman to directly obtain the exact expression for the smallest
eigenvalue density. In the case of complex Wishart-Laguerre matrices, for
finding exact and explicit expressions for the smallest eigenvalue density,
existing results based on determinants become impractical when the determinants
involve large-size matrices. In this work, we derive a recurrence scheme for
the complex case which is analogous to that of Edelman's for the real case.
This is used to obtain exact results for the smallest eigenvalue density for
both the regular, and the fixed trace complex Wishart-Laguerre ensembles. We
validate our analytical results using Monte Carlo simulations. We also study
scaled Wishart-Laguerre ensemble and investigate its efficacy in approximating
the fixed-trace ensemble. Eventually, we apply our result for the fixed-trace
ensemble to investigate the behaviour of the smallest eigenvalue in the
paradigmatic system of coupled kicked tops.
",0,1,1,1,0,0
641,Development of probabilistic dam breach model using Bayesian inference,"  Dam breach models are commonly used to predict outflow hydrographs of
potentially failing dams and are key ingredients for evaluating flood risk. In
this paper a new dam breach modeling framework is introduced that shall improve
the reliability of hydrograph predictions of homogeneous earthen embankment
dams. Striving for a small number of parameters, the simplified physics-based
model describes the processes of failing embankment dams by breach enlargement,
driven by progressive surface erosion. Therein the erosion rate of dam material
is modeled by empirical sediment transport formulations. Embedding the model
into a Bayesian multilevel framework allows for quantitative analysis of
different categories of uncertainties. To this end, data available in
literature of observed peak discharge and final breach width of historical dam
failures was used to perform model inversion by applying Markov Chain Monte
Carlo simulation. Prior knowledge is mainly based on non-informative
distribution functions. The resulting posterior distribution shows that the
main source of uncertainty is a correlated subset of parameters, consisting of
the residual error term and the epistemic term quantifying the breach erosion
rate. The prediction intervals of peak discharge and final breach width are
congruent with values known from literature. To finally predict the outflow
hydrograph for real case applications, an alternative residual model was
formulated that assumes perfect data and a perfect model. The fully
probabilistic fashion of hydrograph prediction has the potential to improve the
adequate risk management of downstream flooding.
",0,0,0,1,0,0
642,Large Magellanic Cloud Near-Infrared Synoptic Survey. V. Period-Luminosity Relations of Miras,"  We study the near-infrared properties of 690 Mira candidates in the central
region of the Large Magellanic Cloud, based on time-series observations at
JHKs. We use densely-sampled I-band observations from the OGLE project to
generate template light curves in the near infrared and derive robust mean
magnitudes at those wavelengths. We obtain near-infrared Period-Luminosity
relations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We
study the Period-Luminosity-Color relations and the color excesses of
Carbon-rich Miras, which show evidence for a substantially different reddening
law.
",0,0,0,1,0,0
643,One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network,"  There is an inherent need for autonomous cars, drones, and other robots to
have a notion of how their environment behaves and to anticipate changes in the
near future. In this work, we focus on anticipating future appearance given the
current frame of a video. Existing work focuses on either predicting the future
appearance as the next frame of a video, or predicting future motion as optical
flow or motion trajectories starting from a single video frame. This work
stretches the ability of CNNs (Convolutional Neural Networks) to predict an
anticipation of appearance at an arbitrarily given future time, not necessarily
the next video frame. We condition our predicted future appearance on a
continuous time variable that allows us to anticipate future frames at a given
temporal distance, directly from the input video frame. We show that CNNs can
learn an intrinsic representation of typical appearance changes over time and
successfully generate realistic predictions at a deliberate time difference in
the near future.
",1,0,0,0,0,0
644,Deep Multi-User Reinforcement Learning for Distributed Dynamic Spectrum Access,"  We consider the problem of dynamic spectrum access for network utility
maximization in multichannel wireless networks. The shared bandwidth is divided
into K orthogonal channels. In the beginning of each time slot, each user
selects a channel and transmits a packet with a certain transmission
probability. After each time slot, each user that has transmitted a packet
receives a local observation indicating whether its packet was successfully
delivered or not (i.e., ACK signal). The objective is a multi-user strategy for
accessing the spectrum that maximizes a certain network utility in a
distributed manner without online coordination or message exchanges between
users. Obtaining an optimal solution for the spectrum access problem is
computationally expensive in general due to the large state space and partial
observability of the states. To tackle this problem, we develop a novel
distributed dynamic spectrum access algorithm based on deep multi-user
reinforcement leaning. Specifically, at each time slot, each user maps its
current state to spectrum access actions based on a trained deep-Q network used
to maximize the objective function. Game theoretic analysis of the system
dynamics is developed for establishing design principles for the implementation
of the algorithm. Experimental results demonstrate strong performance of the
algorithm.
",1,0,0,0,0,0
645,Myopic Bayesian Design of Experiments via Posterior Sampling and Probabilistic Programming,"  We design a new myopic strategy for a wide class of sequential design of
experiment (DOE) problems, where the goal is to collect data in order to to
fulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling
(MPS), is inspired by the classical posterior (Thompson) sampling algorithm for
multi-armed bandits and leverages the flexibility of probabilistic programming
and approximate Bayesian inference to address a broad set of problems.
Empirically, this general-purpose strategy is competitive with more specialised
methods in a wide array of DOE tasks, and more importantly, enables addressing
complex DOE goals where no existing method seems applicable. On the theoretical
side, we leverage ideas from adaptive submodularity and reinforcement learning
to derive conditions under which MPS achieves sublinear regret against natural
benchmark policies.
",0,0,0,1,0,0
646,Data-Driven Sparse Structure Selection for Deep Neural Networks,"  Deep convolutional neural networks have liberated its extraordinary power on
various tasks. However, it is still very challenging to deploy state-of-the-art
models into real-world applications due to their high computational complexity.
How can we design a compact and effective network without massive experiments
and expert knowledge? In this paper, we propose a simple and effective
framework to learn and prune deep models in an end-to-end manner. In our
framework, a new type of parameter -- scaling factor is first introduced to
scale the outputs of specific structures, such as neurons, groups or residual
blocks. Then we add sparsity regularizations on these factors, and solve this
optimization problem by a modified stochastic Accelerated Proximal Gradient
(APG) method. By forcing some of the factors to zero, we can safely remove the
corresponding structures, thus prune the unimportant parts of a CNN. Comparing
with other structure selection methods that may need thousands of trials or
iterative fine-tuning, our method is trained fully end-to-end in one training
pass without bells and whistles. We evaluate our method, Sparse Structure
Selection with several state-of-the-art CNNs, and demonstrate very promising
results with adaptive depth and width selection.
",1,0,0,0,0,0
647,Scaling laws and bounds for the turbulent G.O. Roberts dynamo,"  Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both
with and without a significant mean field are obtained. Exact bounds are
derived for the total energy which conform with the Kolmogorov phenomenology of
turbulence. Best fits to numerical data show the same functional dependences as
the inequalities obtained from optimum theory.
",0,1,0,0,0,0
648,Control Strategies for the Fokker-Planck Equation,"  Using a projection-based decoupling of the Fokker-Planck equation, control
strategies that allow to speed up the convergence to the stationary
distribution are investigated. By means of an operator theoretic framework for
a bilinear control system, two different feedback control laws are proposed.
Projected Riccati and Lyapunov equations are derived and properties of the
associated solutions are given. The well-posedness of the closed loop systems
is shown and local and global stabilization results, respectively, are
obtained. An essential tool in the construction of the controls is the choice
of appropriate control shape functions. Results for a two dimensional double
well potential illustrate the theoretical findings in a numerical setup.
",0,0,1,0,0,0
649,On Popov's formula involving the Von Mangoldt function,"  We offer a generalization of a formula of Popov involving the Von Mangoldt
function. Some commentary on its relation to other results in analytic number
theory is mentioned as well as an analogue involving the m$\ddot{o}$bius
function.
",0,0,1,0,0,0
650,On fibering compact manifold over the circle,"  In this paper, we show that any compact manifold that carries a
SL(n;R)-foliation is fibered on the circle S^1.
",0,0,1,0,0,0
651,Phonon-Induced Topological Transition to a Type-II Weyl Semimetal,"  Given the importance of crystal symmetry for the emergence of topological
quantum states, we have studied, as exemplified in NbNiTe2, the interplay of
crystal symmetry, atomic displacements (lattice vibration), band degeneracy,
and band topology. For NbNiTe2 structure in space group 53 (Pmna) - having an
inversion center arising from two glide planes and one mirror plane with a
2-fold rotation and screw axis - a full gap opening exists between two band
manifolds near the Fermi energy. Upon atomic displacements by optical phonons,
the symmetry lowers to space group 28 (Pma2), eliminating one glide plane along
c, the associated rotation and screw axis, and the inversion center. As a
result, twenty Weyl points emerge, including four type-II Weyl points in the
G-X direction at the boundary between a pair of adjacent electron and hole
bands. Thus, optical phonons may offer control of the transition to a Weyl
fermion state.
",0,1,0,0,0,0
652,Multi-hop assortativities for networks classification,"  Several social, medical, engineering and biological challenges rely on
discovering the functionality of networks from their structure and node
metadata, when it is available. For example, in chemoinformatics one might want
to detect whether a molecule is toxic based on structure and atomic types, or
discover the research field of a scientific collaboration network. Existing
techniques rely on counting or measuring structural patterns that are known to
show large variations from network to network, such as the number of triangles,
or the assortativity of node metadata. We introduce the concept of multi-hop
assortativity, that captures the similarity of the nodes situated at the
extremities of a randomly selected path of a given length. We show that
multi-hop assortativity unifies various existing concepts and offers a
versatile family of 'fingerprints' to characterize networks. These fingerprints
allow in turn to recover the functionalities of a network, with the help of the
machine learning toolbox. Our method is evaluated empirically on established
social and chemoinformatic network benchmarks. Results reveal that our
assortativity based features are competitive providing highly accurate results
often outperforming state of the art methods for the network classification
task.
",1,0,0,1,0,0
653,A Bayesian Nonparametrics based Robust Particle Filter Algorithm,"  This paper is concerned with the online estimation of a nonlinear dynamic
system from a series of noisy measurements. The focus is on cases wherein
outliers are present in-between normal noises. We assume that the outliers
follow an unknown generating mechanism which deviates from that of normal
noises, and then model the outliers using a Bayesian nonparametric model called
Dirichlet process mixture (DPM). A sequential particle-based algorithm is
derived for posterior inference for the outlier model as well as the state of
the system to be estimated. The resulting algorithm is termed DPM based robust
PF (DPM-RPF). The nonparametric feature makes this algorithm allow the data to
""speak for itself"" to determine the complexity and structure of the outlier
model. Simulation results show that it performs remarkably better than two
state-of-the-art methods especially when outliers appear frequently along time.
",0,0,0,1,0,0
654,Highly accurate model for prediction of lung nodule malignancy with CT scans,"  Computed tomography (CT) examinations are commonly used to predict lung
nodule malignancy in patients, which are shown to improve noninvasive early
diagnosis of lung cancer. It remains challenging for computational approaches
to achieve performance comparable to experienced radiologists. Here we present
NoduleX, a systematic approach to predict lung nodule malignancy from CT data,
based on deep learning convolutional neural networks (CNN). For training and
validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.
All nodules were identified and classified by four experienced thoracic
radiologists who participated in the LIDC project. NoduleX achieves high
accuracy for nodule malignancy classification, with an AUC of ~0.99. This is
commensurate with the analysis of the dataset by experienced radiologists. Our
approach, NoduleX, provides an effective framework for highly accurate nodule
malignancy prediction with the model trained on a large patient population. Our
results are replicable with software available at
this http URL.
",0,0,0,1,1,0
655,Rotating Rayleigh-Taylor turbulence,"  The turbulent Rayleigh--Taylor system in a rotating reference frame is
investigated by direct numerical simulations within the Oberbeck-Boussinesq
approximation. On the basis of theoretical arguments, supported by our
simulations, we show that the Rossby number decreases in time, and therefore
the Coriolis force becomes more important as the system evolves and produces
many effects on Rayleigh--Taylor turbulence. We find that rotation reduces the
intensity of turbulent velocity fluctuations and therefore the growth rate of
the temperature mixing layer. Moreover, in presence of rotation the conversion
of potential energy into turbulent kinetic energy is found to be less effective
and the efficiency of the heat transfer is reduced. Finally, during the
evolution of the mixing layer we observe the development of a
cyclone-anticyclone asymmetry.
",0,1,0,0,0,0
656,Evolutionary dynamics of N-person Hawk-Dove games,"  In the animal world, the competition between individuals belonging to
different species for a resource often requires the cooperation of several
individuals in groups. This paper proposes a generalization of the Hawk-Dove
Game for an arbitrary number of agents: the N-person Hawk-Dove Game. In this
model, doves exemplify the cooperative behavior without intraspecies conflict,
while hawks represent the aggressive behavior. In the absence of hawks, doves
share the resource equally and avoid conflict, but having hawks around lead to
doves escaping without fighting. Conversely, hawks fight for the resource at
the cost of getting injured. Nevertheless, if doves are present in sufficient
number to expel the hawks, they can aggregate to protect the resource, and thus
avoid being plundered by hawks. We derive and numerically solve an exact
equation for the evolution of the system in both finite and infinite well-mixed
populations, finding the conditions for stable coexistence between both
species. Furthermore, by varying the different parameters, we found a scenario
of bifurcations that leads the system from dominating hawks and coexistence to
bi-stability, multiple interior equilibria and dominating doves.
",0,1,0,0,0,0
657,A global model for predicting the arrival of imported dengue infections,"  With approximately half of the world's population at risk of contracting
dengue, this mosquito-borne disease is of global concern. International
travellers significantly contribute to dengue's rapid and large-scale spread by
importing the disease from endemic into non-endemic countries. To prevent
future outbreaks and dengue from establishing in non-endemic countries,
knowledge about the arrival time and location of infected travellers is
crucial. We propose a network model that predicts the monthly number of dengue
infected air passengers arriving at any given airport. We consider
international air travel volumes, monthly dengue incidence rates and temporal
infection dynamics. Our findings shed light onto dengue importation routes and
reveal country-specific reporting rates that have been until now largely
unknown.
",1,0,0,0,1,0
658,Contextually Customized Video Summaries via Natural Language,"  The best summary of a long video differs among different people due to its
highly subjective nature. Even for the same person, the best summary may change
with time or mood. In this paper, we introduce the task of generating
customized video summaries through simple text. First, we train a deep
architecture to effectively learn semantic embeddings of video frames by
leveraging the abundance of image-caption data via a progressive and residual
manner. Given a user-specific text description, our algorithm is able to select
semantically relevant video segments and produce a temporally aligned video
summary. In order to evaluate our textually customized video summaries, we
conduct experimental comparison with baseline methods that utilize ground-truth
information. Despite the challenging baselines, our method still manages to
show comparable or even exceeding performance. We also show that our method is
able to generate semantically diverse video summaries by only utilizing the
learned visual embeddings.
",1,0,0,0,0,0
659,Observation of surface plasmon polaritons in 2D electron gas of surface electron accumulation in InN nanostructures,"  Recently, heavily doped semiconductors are emerging as an alternate for low
loss plasmonic materials. InN, belonging to the group III nitrides, possesses
the unique property of surface electron accumulation (SEA) which provides two
dimensional electron gas (2DEG) system. In this report, we demonstrated the
surface plasmon properties of InN nanoparticles originating from SEA using the
real space mapping of the surface plasmon fields for the first time. The SEA is
confirmed by Raman studies which are further corroborated by photoluminescence
and photoemission spectroscopic studies. The frequency of 2DEG corresponding to
SEA is found to be in the THz region. The periodic fringes are observed in the
near-field scanning optical microscopic images of InN nanostructures. The
observed fringes are attributed to the interference of propagated and back
reflected surface plasmon polaritons (SPPs). The observation of SPPs is solely
attributed to the 2DEG corresponding to the SEA of InN. In addition, resonance
kind of behavior with the enhancement of the near-field intensity is observed
in the near-field images of InN nanostructures. Observation of SPPs indicates
that InN with SEA can be a promising THz plasmonic material for the light
confinement.
",0,1,0,0,0,0
660,Asymmetric Mach-Zehnder atom interferometers,"  It is shown that using beam splitters with non-equal wave vectors results in
a new recoil diagram which is qualitatively different from the well-known
diagram associated with the Mach-Zehnder atom interferometer. We predict a new
asymmetric Mach-Zehnder atom interferometer (AMZAI) and study it when one uses
a Raman beam splitter. The main feature is that the phase of AMZAI contains a
quantum part proportional to the recoil frequency. A response sensitive only to
the quantum phase was found. A new technique to measure the recoil frequency
and fine structure constant is proposed and studied outside of the Raman-Nath
approximation.
",0,1,0,0,0,0
661,Partial Information Stochastic Differential Games for Backward Stochastic Systems Driven By L??vy Processes,"  In this paper, we consider a partial information two-person zero-sum
stochastic differential game problem where the system is governed by a backward
stochastic differential equation driven by Teugels martingales associated with
a L??vy process and an independent Brownian motion. One sufficient (a
verification theorem) and one necessary conditions for the existence of optimal
controls are proved. To illustrate the general results, a linear quadratic
stochastic differential game problem is discussed.
",0,0,1,0,0,0
662,Inter-Session Modeling for Session-Based Recommendation,"  In recent years, research has been done on applying Recurrent Neural Networks
(RNNs) as recommender systems. Results have been promising, especially in the
session-based setting where RNNs have been shown to outperform state-of-the-art
models. In many of these experiments, the RNN could potentially improve the
recommendations by utilizing information about the user's past sessions, in
addition to its own interactions in the current session. A problem for
session-based recommendation, is how to produce accurate recommendations at the
start of a session, before the system has learned much about the user's current
interests. We propose a novel approach that extends a RNN recommender to be
able to process the user's recent sessions, in order to improve
recommendations. This is done by using a second RNN to learn from recent
sessions, and predict the user's interest in the current session. By feeding
this information to the original RNN, it is able to improve its
recommendations. Our experiments on two different datasets show that the
proposed approach can significantly improve recommendations throughout the
sessions, compared to a single RNN working only on the current session. The
proposed model especially improves recommendations at the start of sessions,
and is therefore able to deal with the cold start problem within sessions.
",1,0,0,0,0,0
663,Multiscale Modeling of Shock Wave Localization in Porous Energetic Material,"  Shock wave interactions with defects, such as pores, are known to play a key
role in the chemical initiation of energetic materials. The shock response of
hexanitrostilbene is studied through a combination of large scale reactive
molecular dynamics and mesoscale hydrodynamic simulations. In order to extend
our simulation capability at the mesoscale to include weak shock conditions (<
6 GPa), atomistic simulations of pore collapse are used to define a strain rate
dependent strength model. Comparing these simulation methods allows us to
impose physically-reasonable constraints on the mesoscale model parameters. In
doing so, we have been able to study shock waves interacting with pores as a
function of this viscoplastic material response. We find that the pore collapse
behavior of weak shocks is characteristically different to that of strong
shocks.
",0,1,0,0,0,0
664,Cryptoasset Factor Models,"  We propose factor models for the cross-section of daily cryptoasset returns
and provide source code for data downloads, computing risk factors and
backtesting them out-of-sample. In ""cryptoassets"" we include all
cryptocurrencies and a host of various other digital assets (coins and tokens)
for which exchange market data is available. Based on our empirical analysis,
we identify the leading factor that appears to strongly contribute into daily
cryptoasset returns. Our results suggest that cross-sectional statistical
arbitrage trading may be possible for cryptoassets subject to efficient
executions and shorting.
",0,0,0,0,0,1
665,Multi-dimensional Graph Fourier Transform,"  Many signals on Cartesian product graphs appear in the real world, such as
digital images, sensor observation time series, and movie ratings on Netflix.
These signals are ""multi-dimensional"" and have directional characteristics
along each factor graph. However, the existing graph Fourier transform does not
distinguish these directions, and assigns 1-D spectra to signals on product
graphs. Further, these spectra are often multi-valued at some frequencies. Our
main result is a multi-dimensional graph Fourier transform that solves such
problems associated with the conventional GFT. Using algebraic properties of
Cartesian products, the proposed transform rearranges 1-D spectra obtained by
the conventional GFT into the multi-dimensional frequency domain, of which each
dimension represents a directional frequency along each factor graph. Thus, the
multi-dimensional graph Fourier transform enables directional frequency
analysis, in addition to frequency analysis with the conventional GFT.
Moreover, this rearrangement resolves the multi-valuedness of spectra in some
cases. The multi-dimensional graph Fourier transform is a foundation of novel
filterings and stationarities that utilize dimensional information of graph
signals, which are also discussed in this study. The proposed methods are
applicable to a wide variety of data that can be regarded as signals on
Cartesian product graphs. This study also notes that multivariate graph signals
can be regarded as 2-D univariate graph signals. This correspondence provides
natural definitions of the multivariate graph Fourier transform and the
multivariate stationarity based on their 2-D univariate versions.
",1,0,0,1,0,0
666,Criteria for the Application of Double Exponential Transformation,"  The double exponential formula was introduced for calculating definite
integrals with singular point oscillation functions and Fourier-integrals. The
double exponential transformation is not only useful for numerical computations
but it is also used in different methods of Sinc theory. In this paper we use
double exponential transformation for calculating particular improper
integrals. By improving integral estimates having singular final points. By
comparison between double exponential transformations and single exponential
transformations it is proved that the error margin of double exponential
transformations is smaller. Finally Fourier-integral and double exponential
transformations are discussed.
",0,0,1,0,0,0
667,Ultra-high strain in epitaxial silicon carbide nanostructures utilizing residual stress amplification,"  Strain engineering has attracted great attention, particularly for epitaxial
films grown on a different substrate. Residual strains of SiC have been widely
employed to form ultra-high frequency and high Q factor resonators. However, to
date the highest residual strain of SiC was reported to be limited to
approximately 0.6%. Large strains induced into SiC could lead to several
interesting physical phenomena, as well as significant improvement of resonant
frequencies. We report an unprecedented nano strain-amplifier structure with an
ultra-high residual strain up to 8% utilizing the natural residual stress
between epitaxial 3C SiC and Si. In addition, the applied strain can be tuned
by changing the dimensions of the amplifier structure. The possibility of
introducing such a controllable and ultra-high strain will open the door to
investigating the physics of SiC in large strain regimes, and the development
of ultra sensitive mechanical sensors.
",0,1,0,0,0,0
668,The Diverse Club: The Integrative Core of Complex Networks,"  A complex system can be represented and analyzed as a network, where nodes
represent the units of the network and edges represent connections between
those units. For example, a brain network represents neurons as nodes and axons
between neurons as edges. In many networks, some nodes have a
disproportionately high number of edges. These nodes also have many edges
between each other, and are referred to as the rich club. In many different
networks, the nodes of this club are assumed to support global network
integration. However, another set of nodes potentially exhibits a connectivity
structure that is more advantageous to global network integration. Here, in a
myriad of different biological and man-made networks, we discover the diverse
club--a set of nodes that have edges diversely distributed across the network.
The diverse club exhibits, to a greater extent than the rich club, properties
consistent with an integrative network function--these nodes are more highly
interconnected and their edges are more critical for efficient global
integration. Moreover, we present a generative evolutionary network model that
produces networks with a diverse club but not a rich club, thus demonstrating
that these two clubs potentially evolved via distinct selection pressures.
Given the variety of different networks that we analyzed--the c. elegans, the
macaque brain, the human brain, the United States power grid, and global air
traffic--the diverse club appears to be ubiquitous in complex networks. These
results warrant the distinction and analysis of two critical clubs of nodes in
all complex systems.
",0,1,0,0,0,0
669,Bayesian Semisupervised Learning with Deep Generative Models,"  Neural network based generative models with discriminative components are a
powerful approach for semi-supervised learning. However, these techniques a)
cannot account for model uncertainty in the estimation of the model's
discriminative component and b) lack flexibility to capture complex stochastic
patterns in the label generation process. To avoid these problems, we first
propose to use a discriminative component with stochastic inputs for increased
noise flexibility. We show how an efficient Gibbs sampling procedure can
marginalize the stochastic inputs when inferring missing labels in this model.
Following this, we extend the discriminative component to be fully Bayesian and
produce estimates of uncertainty in its parameter values. This opens the door
for semi-supervised Bayesian active learning.
",0,0,0,1,0,0
670,Robust Detection of Covariate-Treatment Interactions in Clinical Trials,"  Detection of interactions between treatment effects and patient descriptors
in clinical trials is critical for optimizing the drug development process. The
increasing volume of data accumulated in clinical trials provides a unique
opportunity to discover new biomarkers and further the goal of personalized
medicine, but it also requires innovative robust biomarker detection methods
capable of detecting non-linear, and sometimes weak, signals. We propose a set
of novel univariate statistical tests, based on the theory of random walks,
which are able to capture non-linear and non-monotonic covariate-treatment
interactions. We also propose a novel combined test, which leverages the power
of all of our proposed univariate tests into a single general-case tool. We
present results for both synthetic trials as well as real-world clinical
trials, where we compare our method with state-of-the-art techniques and
demonstrate the utility and robustness of our approach.
",0,0,0,1,0,0
671,The Future of RICH Detectors through the Light of the LHCb RICH,"  The limitations in performance of the present RICH system in the LHCb
experiment are given by the natural chromatic dispersion of the gaseous
Cherenkov radiator, the aberrations of the optical system and the pixel size of
the photon detectors. Moreover, the overall PID performance can be affected by
high detector occupancy as the pattern recognition becomes more difficult with
high particle multiplicities. This paper shows a way to improve performance by
systematically addressing each of the previously mentioned limitations. These
ideas are applied in the present and future upgrade phases of the LHCb
experiment. Although applied to specific circumstances, they are used as a
paradigm on what is achievable in the development and realisation of high
precision RICH detectors.
",0,1,0,0,0,0
672,Stability of Valuations: Higher Rational Rank,"  Given a klt singularity $x\in (X, D)$, we show that a quasi-monomial
valuation $v$ with a finitely generated associated graded ring is the minimizer
of the normalized volume function $\widehat{\rm vol}_{(X,D),x}$, if and only if
$v$ induces a degeneration to a K-semistable log Fano cone singularity.
Moreover, such a minimizer is unique among all quasi-monomial valuations up to
rescaling. As a consequence, we prove that for a klt singularity $x\in X$ on
the Gromov-Hausdorff limit of K??hler-Einstein Fano manifolds, the
intermediate K-semistable cone associated to its metric tangent cone is
uniquely determined by the algebraic structure of $x\in X$, hence confirming a
conjecture by Donaldson-Sun.
",0,0,1,0,0,0
673,Higgs mode and its decay in a two dimensional antiferromagnet,"  Condensed-matter analogs of the Higgs boson in particle physics allow
insights into its behavior in different symmetries and dimensionalities.
Evidence for the Higgs mode has been reported in a number of different
settings, including ultracold atomic gases, disordered superconductors, and
dimerized quantum magnets. However, decay processes of the Higgs mode (which
are eminently important in particle physics) have not yet been studied in
condensed matter due to the lack of a suitable material system coupled to a
direct experimental probe. A quantitative understanding of these processes is
particularly important for low-dimensional systems where the Higgs mode decays
rapidly and has remained elusive to most experimental probes. Here, we discover
and study the Higgs mode in a two-dimensional antiferromagnet using
spin-polarized inelastic neutron scattering. Our spin-wave spectra of
Ca$_2$RuO$_4$ directly reveal a well-defined, dispersive Higgs mode, which
quickly decays into transverse Goldstone modes at the antiferromagnetic
ordering wavevector. Through a complete mapping of the transverse modes in the
reciprocal space, we uniquely specify the minimal model Hamiltonian and
describe the decay process. We thus establish a novel condensed matter platform
for research on the dynamics of the Higgs mode.
",0,1,0,0,0,0
674,Robust and Efficient Boosting Method using the Conditional Risk,"  Well-known for its simplicity and effectiveness in classification, AdaBoost,
however, suffers from overfitting when class-conditional distributions have
significant overlap. Moreover, it is very sensitive to noise that appears in
the labels. This article tackles the above limitations simultaneously via
optimizing a modified loss function (i.e., the conditional risk). The proposed
approach has the following two advantages. (1) It is able to directly take into
account label uncertainty with an associated label confidence. (2) It
introduces a ""trustworthiness"" measure on training samples via the Bayesian
risk rule, and hence the resulting classifier tends to have finite sample
performance that is superior to that of the original AdaBoost when there is a
large overlap between class conditional distributions. Theoretical properties
of the proposed method are investigated. Extensive experimental results using
synthetic data and real-world data sets from UCI machine learning repository
are provided. The empirical study shows the high competitiveness of the
proposed method in predication accuracy and robustness when compared with the
original AdaBoost and several existing robust AdaBoost algorithms.
",0,0,0,1,0,0
675,High Dimensional Robust Estimation of Sparse Models via Trimmed Hard Thresholding,"  We study the problem of sparsity constrained $M$-estimation with arbitrary
corruptions to both {\em explanatory and response} variables in the
high-dimensional regime, where the number of variables $d$ is larger than the
sample size $n$. Our main contribution is a highly efficient gradient-based
optimization algorithm that we call Trimmed Hard Thresholding -- a robust
variant of Iterative Hard Thresholding (IHT) by using trimmed mean in gradient
computations. Our algorithm can deal with a wide class of sparsity constrained
$M$-estimation problems, and we can tolerate a nearly dimension independent
fraction of arbitrarily corrupted samples. More specifically, when the
corrupted fraction satisfies $\epsilon \lesssim {1} /\left({\sqrt{k} \log
(nd)}\right)$, where $k$ is the sparsity of the parameter, we obtain accurate
estimation and model selection guarantees with optimal sample complexity.
Furthermore, we extend our algorithm to sparse Gaussian graphical model
(precision matrix) estimation via a neighborhood selection approach. We
demonstrate the effectiveness of robust estimation in sparse linear, logistic
regression, and sparse precision matrix estimation on synthetic and real-world
US equities data.
",1,0,1,1,0,0
676,Stop talking to me -- a communication-avoiding ADER-DG realisation,"  We present a communication- and data-sensitive formulation of ADER-DG for
hyperbolic differential equation systems. Sensitive here has multiple flavours:
First, the formulation reduces the persistent memory footprint. This reduces
pressure on the memory subsystem. Second, the formulation realises the
underlying predictor-corrector scheme with single-touch semantics, i.e., each
degree of freedom is read on average only once per time step from the main
memory. This reduces communication through the memory controllers. Third, the
formulation breaks up the tight coupling of the explicit time stepping's
algorithmic steps to mesh traversals. This averages out data access peaks.
Different operations and algorithmic steps are ran on different grid entities.
Finally, the formulation hides distributed memory data transfer behind the
computation aligned with the mesh traversal. This reduces pressure on the
machine interconnects. All techniques applied by our formulation are elaborated
by means of a rigorous task formalism. They break up ADER-DG's tight causal
coupling of compute steps and can be generalised to other predictor-corrector
schemes.
",1,0,0,0,0,0
677,The effect of prior probabilities on quantification and propagation of imprecise probabilities resulting from small datasets,"  This paper outlines a methodology for Bayesian multimodel uncertainty
quantification (UQ) and propagation and presents an investigation into the
effect of prior probabilities on the resulting uncertainties. The UQ
methodology is adapted from the information-theoretic method previously
presented by the authors (Zhang and Shields, 2018) to a fully Bayesian
construction that enables greater flexibility in quantifying uncertainty in
probability model form. Being Bayesian in nature and rooted in UQ from small
datasets, prior probabilities in both probability model form and model
parameters are shown to have a significant impact on quantified uncertainties
and, consequently, on the uncertainties propagated through a physics-based
model. These effects are specifically investigated for a simplified plate
buckling problem with uncertainties in material properties derived from a small
number of experiments using noninformative priors and priors derived from past
studies of varying appropriateness. It is illustrated that prior probabilities
can have a significant impact on multimodel UQ for small datasets and
inappropriate (but seemingly reasonable) priors may even have lingering effects
that bias probabilities even for large datasets. When applied to uncertainty
propagation, this may result in probability bounds on response quantities that
do not include the true probabilities.
",0,0,0,1,0,0
678,Hierarchical loss for classification,"  Failing to distinguish between a sheepdog and a skyscraper should be worse
and penalized more than failing to distinguish between a sheepdog and a poodle;
after all, sheepdogs and poodles are both breeds of dogs. However, existing
metrics of failure (so-called ""loss"" or ""win"") used in textual or visual
classification/recognition via neural networks seldom view a sheepdog as more
similar to a poodle than to a skyscraper. We define a metric that, inter alia,
can penalize failure to distinguish between a sheepdog and a skyscraper more
than failure to distinguish between a sheepdog and a poodle. Unlike previously
employed possibilities, this metric is based on an ultrametric tree associated
with any given tree organization into a semantically meaningful hierarchy of a
classifier's classes.
",1,0,0,1,0,0
679,"An efficient data structure for counting all linear extensions of a poset, calculating its jump number, and the likes","  Achieving the goals in the title (and others) relies on a cardinality-wise
scanning of the ideals of the poset. Specifically, the relevant numbers
attached to the k+1 element ideals are inferred from the corresponding numbers
of the k-element (order) ideals. Crucial in all of this is a compressed
representation (using wildcards) of the ideal lattice. The whole scheme invites
distributed computation.
",1,0,0,0,0,0
680,Perception-in-the-Loop Adversarial Examples,"  We present a scalable, black box, perception-in-the-loop technique to find
adversarial examples for deep neural network classifiers. Black box means that
our procedure only has input-output access to the classifier, and not to the
internal structure, parameters, or intermediate confidence values.
Perception-in-the-loop means that the notion of proximity between inputs can be
directly queried from human participants rather than an arbitrarily chosen
metric. Our technique is based on covariance matrix adaptation evolution
strategy (CMA-ES), a black box optimization approach. CMA-ES explores the
search space iteratively in a black box manner, by generating populations of
candidates according to a distribution, choosing the best candidates according
to a cost function, and updating the posterior distribution to favor the best
candidates. We run CMA-ES using human participants to provide the fitness
function, using the insight that the choice of best candidates in CMA-ES can be
naturally modeled as a perception task: pick the top $k$ inputs perceptually
closest to a fixed input. We empirically demonstrate that finding adversarial
examples is feasible using small populations and few iterations. We compare the
performance of CMA-ES on the MNIST benchmark with other black-box approaches
using $L_p$ norms as a cost function, and show that it performs favorably both
in terms of success in finding adversarial examples and in minimizing the
distance between the original and the adversarial input. In experiments on the
MNIST, CIFAR10, and GTSRB benchmarks, we demonstrate that CMA-ES can find
perceptually similar adversarial inputs with a small number of iterations and
small population sizes when using perception-in-the-loop. Finally, we show that
networks trained specifically to be robust against $L_\infty$ norm can still be
susceptible to perceptually similar adversarial examples.
",1,0,0,1,0,0
681,Deep Fluids: A Generative Network for Parameterized Fluid Simulations,"  This paper presents a novel generative model to synthesize fluid simulations
from a set of reduced parameters. A convolutional neural network is trained on
a collection of discrete, parameterizable fluid simulation velocity fields. Due
to the capability of deep learning architectures to learn representative
features of the data, our generative model is able to accurately approximate
the training data set, while providing plausible interpolated in-betweens. The
proposed generative model is optimized for fluids by a novel loss function that
guarantees divergence-free velocity fields at all times. In addition, we
demonstrate that we can handle complex parameterizations in reduced spaces, and
advance simulations in time by integrating in the latent space with a second
network. Our method models a wide variety of fluid behaviors, thus enabling
applications such as fast construction of simulations, interpolation of fluids
with different parameters, time re-sampling, latent space simulations, and
compression of fluid simulation data. Reconstructed velocity fields are
generated up to 700x faster than traditional CPU solvers, while achieving
compression rates of over 1300x.
",0,0,0,1,0,0
682,Bias Reduction in Instrumental Variable Estimation through First-Stage Shrinkage,"  The two-stage least-squares (2SLS) estimator is known to be biased when its
first-stage fit is poor. I show that better first-stage prediction can
alleviate this bias. In a two-stage linear regression model with Normal noise,
I consider shrinkage in the estimation of the first-stage instrumental variable
coefficients. For at least four instrumental variables and a single endogenous
regressor, I establish that the standard 2SLS estimator is dominated with
respect to bias. The dominating IV estimator applies James-Stein type shrinkage
in a first-stage high-dimensional Normal-means problem followed by a
control-function approach in the second stage. It preserves invariances of the
structural instrumental variable equations.
",0,0,1,1,0,0
683,An Unsupervised Learning Classifier with Competitive Error Performance,"  An unsupervised learning classification model is described. It achieves
classification error probability competitive with that of popular supervised
learning classifiers such as SVM or kNN. The model is based on the incremental
execution of small step shift and rotation operations upon selected
discriminative hyperplanes at the arrival of input samples. When applied, in
conjunction with a selected feature extractor, to a subset of the ImageNet
dataset benchmark, it yields 6.2 % Top 3 probability of error; this exceeds by
merely about 2 % the result achieved by (supervised) k-Nearest Neighbor, both
using same feature extractor. This result may also be contrasted with popular
unsupervised learning schemes such as k-Means which is shown to be practically
useless on same dataset.
",0,0,0,1,0,0
684,Exploring the predictability of range-based volatility estimators using RNNs,"  We investigate the predictability of several range-based stock volatility
estimators, and compare them to the standard close-to-close estimator which is
most commonly acknowledged as the volatility. The patterns of volatility
changes are analyzed using LSTM recurrent neural networks, which are a state of
the art method of sequence learning. We implement the analysis on all current
constituents of the Dow Jones Industrial Average index, and report averaged
evaluation results. We find that changes in the values of range-based
estimators are more predictable than that of the estimator using daily closing
values only.
",0,0,0,1,0,1
685,Mean squared displacement and sinuosity of three-dimensional random search movements,"  Correlated random walks (CRW) have been used for a long time as a null model
for animal's random search movement in two dimensions (2D). An increasing
number of studies focus on animals' movement in three dimensions (3D), but the
key properties of CRW, such as the way the mean squared displacement is related
to the path length, are well known only in 1D and 2D. In this paper I derive
such properties for 3D CRW, in a consistent way with the expression of these
properties in 2D. This should allow 3D CRW to act as a null model when
analyzing actual 3D movements similarly to what is done in 2D
",0,0,0,0,1,0
686,Context-Aware Pedestrian Motion Prediction In Urban Intersections,"  This paper presents a novel context-based approach for pedestrian motion
prediction in crowded, urban intersections, with the additional flexibility of
prediction in similar, but new, environments. Previously, Chen et. al. combined
Markovian-based and clustering-based approaches to learn motion primitives in a
grid-based world and subsequently predict pedestrian trajectories by modeling
the transition between learned primitives as a Gaussian Process (GP). This work
extends that prior approach by incorporating semantic features from the
environment (relative distance to curbside and status of pedestrian traffic
lights) in the GP formulation for more accurate predictions of pedestrian
trajectories over the same timescale. We evaluate the new approach on
real-world data collected using one of the vehicles in the MIT Mobility On
Demand fleet. The results show 12.5% improvement in prediction accuracy and a
2.65 times reduction in Area Under the Curve (AUC), which is used as a metric
to quantify the span of predicted set of trajectories, such that a lower AUC
corresponds to a higher level of confidence in the future direction of
pedestrian motion.
",1,0,0,1,0,0
687,EnergyNet: Energy-based Adaptive Structural Learning of Artificial Neural Network Architectures,"  We present E NERGY N ET , a new framework for analyzing and building
artificial neural network architectures. Our approach adaptively learns the
structure of the networks in an unsupervised manner. The methodology is based
upon the theoretical guarantees of the energy function of restricted Boltzmann
machines (RBM) of infinite number of nodes. We present experimental results to
show that the final network adapts to the complexity of a given problem.
",1,0,0,0,0,0
688,Local Algorithms for Hierarchical Dense Subgraph Discovery,"  Finding the dense regions of a graph and relations among them is a
fundamental problem in network analysis. Core and truss decompositions reveal
dense subgraphs with hierarchical relations. The incremental nature of
algorithms for computing these decompositions and the need for global
information at each step of the algorithm hinders scalable parallelization and
approximations since the densest regions are not revealed until the end. In a
previous work, Lu et al. proposed to iteratively compute the $h$-indices of
neighbor vertex degrees to obtain the core numbers and prove that the
convergence is obtained after a finite number of iterations. This work
generalizes the iterative $h$-index computation for truss decomposition as well
as nucleus decomposition which leverages higher-order structures to generalize
core and truss decompositions. In addition, we prove convergence bounds on the
number of iterations. We present a framework of local algorithms to obtain the
core, truss, and nucleus decompositions. Our algorithms are local, parallel,
offer high scalability, and enable approximations to explore time and quality
trade-offs. Our shared-memory implementation verifies the efficiency,
scalability, and effectiveness of our local algorithms on real-world networks.
",1,0,0,0,0,0
689,Robust Gesture-Based Communication for Underwater Human-Robot Interaction in the context of Search and Rescue Diver Missions,"  We propose a robust gesture-based communication pipeline for divers to
instruct an Autonomous Underwater Vehicle (AUV) to assist them in performing
high-risk tasks and helping in case of emergency. A gesture communication
language (CADDIAN) is developed, based on consolidated and standardized diver
gestures, including an alphabet, syntax and semantics, ensuring a logical
consistency. A hierarchical classification approach is introduced for hand
gesture recognition based on stereo imagery and multi-descriptor aggregation to
specifically cope with underwater image artifacts, e.g. light backscatter or
color attenuation. Once the classification task is finished, a syntax check is
performed to filter out invalid command sequences sent by the diver or
generated by errors in the classifier. Throughout this process, the diver
receives constant feedback from an underwater tablet to acknowledge or abort
the mission at any time. The objective is to prevent the AUV from executing
unnecessary, infeasible or potentially harmful motions. Experimental results
under different environmental conditions in archaeological exploration and
bridge inspection applications show that the system performs well in the field.
",1,0,0,0,0,0
690,High-$T_\textrm {C}$ superconductivity in Cs$_3$C$_{60}$ compounds governed by local Cs-C$_{60}$ Coulomb interactions,"  Unique among alkali-doped $\textit {A}$$_3$C$_{60}$ fullerene compounds, the
A15 and fcc forms of Cs$_3$C$_{60}$ exhibit superconducting states varying
under hydrostatic pressure with highest transition temperatures at $T_\textrm
{C}$$^\textrm {meas}$ = 38.3 and 35.2 K, respectively. Herein it is argued that
these two compounds under pressure represent the optimal materials of the
$\textit {A}$$_3$C$_{60}$ family, and that the C$_{60}$-associated
superconductivity is mediated through Coulombic interactions with charges on
the alkalis. A derivation of the interlayer Coulombic pairing model of
high-$T_\textrm {C}$ superconductivity employing non-planar geometry is
introduced, generalizing the picture of two interacting layers to an
interaction between charge reservoirs located on the C$_{60}$ and alkali ions.
The optimal transition temperature follows the algebraic expression, $T_\textrm
{C0}$ = (12.474 nm$^2$ K)/$\ell$${\zeta}$, where $\ell$ relates to the mean
spacing between interacting surface charges on the C$_{60}$ and ${\zeta}$ is
the average radial distance between the C$_{60}$ surface and the neighboring Cs
ions. Values of $T_\textrm {C0}$ for the measured cation stoichiometries of
Cs$_{3-\textrm{x}}$C$_{60}$ with x $\approx$ 0 are found to be 38.19 and 36.88
K for the A15 and fcc forms, respectively, with the dichotomy in transition
temperature reflecting the larger ${\zeta}$ and structural disorder in the fcc
form. In the A15 form, modeled interacting charges and Coulomb potential
e$^2$/${\zeta}$ are shown to agree quantitatively with findings from
nuclear-spin relaxation and mid-infrared optical conductivity. In the fcc form,
suppression of $T_\textrm {C}$$^\textrm {meas}$ below $T_\textrm {C0}$ is
ascribed to native structural disorder. Phononic effects in conjunction with
Coulombic pairing are discussed.
",0,1,0,0,0,0
691,Analysis and mitigation of interface losses in trenched superconducting coplanar waveguide resonators,"  Improving the performance of superconducting qubits and resonators generally
results from a combination of materials and fabrication process improvements
and design modifications that reduce device sensitivity to residual losses. One
instance of this approach is to use trenching into the device substrate in
combination with superconductors and dielectrics with low intrinsic losses to
improve quality factors and coherence times. Here we demonstrate titanium
nitride coplanar waveguide resonators with mean quality factors exceeding two
million and controlled trenching reaching 2.2 $\mu$m into the silicon
substrate. Additionally, we measure sets of resonators with a range of sizes
and trench depths and compare these results with finite-element simulations to
demonstrate quantitative agreement with a model of interface dielectric loss.
We then apply this analysis to determine the extent to which trenching can
improve resonator performance.
",0,1,0,0,0,0
692,Recent Operation of the FNAL Magnetron $H^{-}$ Ion Source,"  This paper will detail changes in the operational paradigm of the Fermi
National Accelerator Laboratory (FNAL) magnetron $H^{-}$ ion source due to
upgrades in the accelerator system. Prior to November of 2012 the $H^{-}$ ions
for High Energy Physics (HEP) experiments were extracted at ~18 keV vertically
downward into a 90 degree bending magnet and accelerated through a
Cockcroft-Walton accelerating column to 750 keV. Following the upgrade in the
fall of 2012 the $H^{-}$ ions are now directly extracted from a magnetron at 35
keV and accelerated to 750 keV by a Radio Frequency Quadrupole (RFQ). This
change in extraction energy as well as the orientation of the ion source
required not only a redesign of the ion source, but an updated understanding of
its operation at these new values. Discussed in detail are the changes to the
ion source timing, arc discharge current, hydrogen gas pressure, and cesium
delivery system that were needed to maintain consistent operation at >99%
uptime for HEP, with an increased ion source lifetime of over 9 months.
",0,1,0,0,0,0
693,A Ball Breaking Away from a Fluid,"  We consider the withdrawal of a ball from a fluid reservoir to understand the
longevity of the connection between that ball and the fluid it breaks away
from, at intermediate Reynolds numbers. Scaling arguments based on the
processes observed as the ball interacts with the fluid surface were applied to
the `pinch-off time', when the ball breaks its connection with the fluid from
which it has been withdrawn, measured experimentally. At the lowest Reynolds
numbers tested, pinch-off occurs in a `surface seal' close to the reservoir
surface, where at larger Reynolds numbers pinch-off occurs in an `ejecta seal'
close to the ball. Our scaling analysis shows that the connection between ball
and fluid is controlled by the fluid film draining from the ball as it
continues to be winched away from the fluid reservoir. The draining flow itself
depends on the amount of fluid coating the ball on exit from the reservoir. We
consider the possibilities that this coating was created through: a surface
tension driven Landau Levitch Derjaguin wetting of the surface; a
visco-inertial quick coating; or alternatively through the inertia of the fluid
moving with the ball through the reservoir. We show that although the pinch-off
mechanism is controlled by viscosity, the coating mechanism is governed by a
different length and timescale, dictated by the inertial added mass of the ball
when submersed.
",0,1,0,0,0,0
694,Unveiling the internal entanglement structure of the Kondo singlet,"  We disentangle all the individual degrees of freedom in the quantum impurity
problem to deconstruct the Kondo singlet, both in real and energy space, by
studying the contribution of each individual free electron eigenstate. This is
a problem of two spins coupled to a bath, where the bath is formed by the
remaining conduction electrons. Being a mixed state, we resort to the
""concurrence"" to quantify entanglement. We identify ""projected natural
orbitals"" that allow us to individualize a single-particle electronic wave
function that is responsible of more than $90\%$ of the impurity screening. In
the weak coupling regime, the impurity is entangled to an electron at the Fermi
level, while in the strong coupling regime, the impurity counterintuitively
entangles mostly with the high energy electrons and disentangles completely
from the low-energy states carving a ""hole"" around the Fermi level. This
enables one to use concurrence as a pseudo order parameter to compute the
characteristic ""size"" of the Kondo cloud, beyond which electrons are are weakly
correlated to the impurity and are dominated by the physics of the boundary.
",0,1,0,0,0,0
695,A parallel orbital-updating based plane-wave basis method for electronic structure calculations,"  Motivated by the recently proposed parallel orbital-updating approach in real
space method, we propose a parallel orbital-updating based plane-wave basis
method for electronic structure calculations, for solving the corresponding
eigenvalue problems. In addition, we propose two new modified parallel
orbital-updating methods. Compared to the traditional plane-wave methods, our
methods allow for two-level parallelization, which is particularly interesting
for large scale parallelization. Numerical experiments show that these new
methods are more reliable and efficient for large scale calculations on modern
supercomputers
",0,1,1,0,0,0
696,Dynamics of the multi-soliton waves in the sine-Gordon model with two identical point impurities,"  The particular type of four-kink multi-solitons (or quadrons) adiabatic
dynamics of the sine-Gordon equation in a model with two identical point
attracting impurities has been studied. This model can be used for describing
magnetization localized waves in multilayer ferromagnet. The quadrons structure
and properties has been numerically investigated. The cases of both large and
small distances between impurities has been viewed. The dependence of the
localized in impurity region nonlinear high-amplitude waves frequencies on the
distance between the impurities has been found. For an analytical description
of two bound localized on impurities nonlinear waves dynamics, using
perturbation theory, the system of differential equations for harmonic
oscillators with elastic link has been found. The analytical model
qualitatively describes the results of the sine-Gordon equation numerical
simulation.
",0,1,0,0,0,0
697,Clarifying the Hubble constant tension with a Bayesian hierarchical model of the local distance ladder,"  Estimates of the Hubble constant, $H_0$, from the distance ladder and the
cosmic microwave background (CMB) differ at the $\sim$3-$\sigma$ level,
indicating a potential issue with the standard $\Lambda$CDM cosmology.
Interpreting this tension correctly requires a model comparison calculation
depending on not only the traditional `$n$-$\sigma$' mismatch but also the
tails of the likelihoods. Determining the form of the tails of the local $H_0$
likelihood is impossible with the standard Gaussian least-squares
approximation, as it requires using non-Gaussian distributions to faithfully
represent anchor likelihoods and model outliers in the Cepheid and supernova
(SN) populations, and simultaneous fitting of the full distance-ladder dataset
to correctly propagate uncertainties. We have developed a Bayesian hierarchical
model that describes the full distance ladder, from nearby geometric anchors
through Cepheids to Hubble-Flow SNe. This model does not rely on any
distributions being Gaussian, allowing outliers to be modeled and obviating the
need for arbitrary data cuts. Sampling from the $\sim$3000-parameter joint
posterior using Hamiltonian Monte Carlo, we find $H_0$ = (72.72 $\pm$ 1.67)
${\rm km\,s^{-1}\,Mpc^{-1}}$ when applied to the outlier-cleaned Riess et al.
(2016) data, and ($73.15 \pm 1.78$) ${\rm km\,s^{-1}\,Mpc^{-1}}$ with SN
outliers reintroduced. Our high-fidelity sampling of the low-$H_0$ tail of the
distance-ladder likelihood allows us to apply Bayesian model comparison to
assess the evidence for deviation from $\Lambda$CDM. We set up this comparison
to yield a lower limit on the odds of the underlying model being $\Lambda$CDM
given the distance-ladder and Planck XIII (2016) CMB data. The odds against
$\Lambda$CDM are at worst 10:1 or 7:1, depending on whether the SNe outliers
are cut or modeled, or 60:1 if an approximation to the Planck Int. XLVI (2016)
likelihood is used.
",0,1,0,0,0,0
698,Multi-User Multi-Armed Bandits for Uncoordinated Spectrum Access,"  A multi-user multi-armed bandit (MAB) framework is used to develop algorithms
for uncoordinated spectrum access. The number of users is assumed to be unknown
to each user. A stochastic setting is first considered, where the rewards on a
channel are the same for each user. In contrast to prior work, it is assumed
that the number of users can possibly exceed the number of channels, and that
rewards can be non-zero even under collisions. The proposed algorithm consists
of an estimation phase and an allocation phase. It is shown that if every user
adopts the algorithm, the system wide regret is constant with time with high
probability. The regret guarantees hold for any number of users and channels,
in particular, even when the number of users is less than the number of
channels. Next, an adversarial multi-user MAB framework is considered, where
the rewards on the channels are user-dependent. It is assumed that the number
of users is less than the number of channels, and that the users receive zero
reward on collision. The proposed algorithm combines the Exp3.P algorithm
developed in prior work for single user adversarial bandits with a collision
resolution mechanism to achieve sub-linear regret. It is shown that if every
user employs the proposed algorithm, the system wide regret is of the order
$O(T^\frac{3}{4})$ over a horizon of time $T$. The algorithms in both
stochastic and adversarial scenarios are extended to the dynamic case where the
number of users in the system evolves over time and are shown to lead to
sub-linear regret.
",0,0,0,1,0,0
699,A Comparative Analysis of Contact Models in Trajectory Optimization for Manipulation,"  In this paper, we analyze the effects of contact models on contact-implicit
trajectory optimization for manipulation. We consider three different
approaches: (1) a contact model that is based on complementarity constraints,
(2) a smooth contact model, and our proposed method (3) a variable smooth
contact model. We compare these models in simulation in terms of physical
accuracy, quality of motions, and computation time. In each case, the
optimization process is initialized by setting all torque variables to zero,
namely, without a meaningful initial guess. For simulations, we consider a
pushing task with varying complexity for a 7 degrees-of-freedom robot arm. Our
results demonstrate that the optimization based on the proposed variable smooth
contact model provides a good trade-off between the physical fidelity and
quality of motions at the cost of increased computation time.
",1,0,0,0,0,0
700,Computationally Efficient Measures of Internal Neuron Importance,"  The challenge of assigning importance to individual neurons in a network is
of interest when interpreting deep learning models. In recent work, Dhamdhere
et al. proposed Total Conductance, a ""natural refinement of Integrated
Gradients"" for attributing importance to internal neurons. Unfortunately, the
authors found that calculating conductance in tensorflow required the addition
of several custom gradient operators and did not scale well. In this work, we
show that the formula for Total Conductance is mathematically equivalent to
Path Integrated Gradients computed on a hidden layer in the network. We provide
a scalable implementation of Total Conductance using standard tensorflow
gradient operators that we call Neuron Integrated Gradients. We compare Neuron
Integrated Gradients to DeepLIFT, a pre-existing computationally efficient
approach that is applicable to calculating internal neuron importance. We find
that DeepLIFT produces strong empirical results and is faster to compute, but
because it lacks the theoretical properties of Neuron Integrated Gradients, it
may not always be preferred in practice. Colab notebook reproducing results:
this http URL
",0,0,0,1,0,0
701,Mutual Interpretability of Robinson Arithmetic and Adjunctive Set Theory with Extensionality,"  An elementary rheory of concatenation is introduced and used to establish
mutual interpretability of Robinson arithmetic, Minimal Predicative Set Theory,
the quantifier-free part of Kirby's finitary set theory, and Adjunctive Set
Theory, with or without extensionality.
",0,0,1,0,0,0
702,Coordination of Dynamic Software Components with JavaBIP,"  JavaBIP allows the coordination of software components by clearly separating
the functional and coordination aspects of the system behavior. JavaBIP
implements the principles of the BIP component framework rooted in rigorous
operational semantics. Recent work both on BIP and JavaBIP allows the
coordination of static components defined prior to system deployment, i.e., the
architecture of the coordinated system is fixed in terms of its component
instances. Nevertheless, modern systems, often make use of components that can
register and deregister dynamically during system execution. In this paper, we
present an extension of JavaBIP that can handle this type of dynamicity. We use
first-order interaction logic to define synchronization constraints based on
component types. Additionally, we use directed graphs with edge coloring to
model dependencies among components that determine the validity of an online
system. We present the software architecture of our implementation, provide and
discuss performance evaluation results.
",1,0,0,0,0,0
703,Is It Safe to Uplift This Patch? An Empirical Study on Mozilla Firefox,"  In rapid release development processes, patches that fix critical issues, or
implement high-value features are often promoted directly from the development
channel to a stabilization channel, potentially skipping one or more
stabilization channels. This practice is called patch uplift. Patch uplift is
risky, because patches that are rushed through the stabilization phase can end
up introducing regressions in the code. This paper examines patch uplift
operations at Mozilla, with the aim to identify the characteristics of uplifted
patches that introduce regressions. Through statistical and manual analyses, we
quantitatively and qualitatively investigate the reasons behind patch uplift
decisions and the characteristics of uplifted patches that introduced
regressions. Additionally, we interviewed three Mozilla release managers to
understand organizational factors that affect patch uplift decisions and
outcomes. Results show that most patches are uplifted because of a wrong
functionality or a crash. Uplifted patches that lead to faults tend to have
larger patch size, and most of the faults are due to semantic or memory errors
in the patches. Also, release managers are more inclined to accept patch uplift
requests that concern certain specific components, and-or that are submitted by
certain specific developers.
",1,0,0,0,0,0
704,Gamorithm,"  Examining games from a fresh perspective we present the idea of game-inspired
and game-based algorithms, dubbed ""gamorithms"".
",1,0,0,0,0,0
705,Common change point estimation in panel data from the least squares and maximum likelihood viewpoints,"  We establish the convergence rates and asymptotic distributions of the common
break change-point estimators, obtained by least squares and maximum likelihood
in panel data models and compare their asymptotic variances. Our model
assumptions accommodate a variety of commonly encountered probability
distributions and, in particular, models of particular interest in econometrics
beyond the commonly analyzed Gaussian model, including the zero-inflated
Poisson model for count data, and the probit and tobit models. We also provide
novel results for time dependent data in the signal-plus-noise model, with
emphasis on a wide array of noise processes, including Gaussian process,
MA$(\infty)$ and $m$-dependent processes. The obtained results show that
maximum likelihood estimation requires a stronger signal-to-noise model
identifiability condition compared to its least squares counterpart. Finally,
since there are three different asymptotic regimes that depend on the behavior
of the norm difference of the model parameters before and after the change
point, which cannot be realistically assumed to be known, we develop a novel
data driven adaptive procedure that provides valid confidence intervals for the
common break, without requiring a priori knowledge of the asymptotic regime the
problem falls in.
",0,0,1,1,0,0
706,Outage analysis in two-way communication with RF energy harvesting relay and co-channel interference,"  The study of relays with the scope of energy-harvesting (EH) looks
interesting as a means of enabling sustainable, wireless communication without
the need to recharge or replace the battery driving the relays. However,
reliability of such communication systems becomes an important design challenge
when such relays scavenge energy from the information bearing RF signals
received from the source, using the technique of simultaneous wireless
information and power transfer (SWIPT). To this aim, this work studies
bidirectional communication in a decode-and-forward (DF) relay assisted
cooperative wireless network in presence of co-channel interference (CCI). In
order to quantify the reliability of the bidirectional communication systems, a
closed form expression for the outage probability of the system is derived for
both power splitting (PS) and time switching (TS) mode of operation of the
relay. Simulation results are used to validate the accuracy of our analytical
results and illustrate the dependence of the outage probability on various
system parameters, like PS factor, TS factor, and distance of the relay from
both the users. Results of performance comparison between PS relaying (PSR) and
TS relaying (TSR) schemes are also presented. Besides, simulation results are
also used to illustrate the spectral-efficiency and the energy-efficiency of
the proposed system. The results show that, both in terms of spectral
efficiency and the energy-efficiency, the two-way communication system in
presence of moderate CCI power, performs better than the similar system without
CCI. Additionally, it is also found that PSR is superior to TSR protocol in
terms of peak energy-efficiency.
",1,0,0,0,0,0
707,KGAN: How to Break The Minimax Game in GAN,"  Generative Adversarial Networks (GANs) were intuitively and attractively
explained under the perspective of game theory, wherein two involving parties
are a discriminator and a generator. In this game, the task of the
discriminator is to discriminate the real and generated (i.e., fake) data,
whilst the task of the generator is to generate the fake data that maximally
confuses the discriminator. In this paper, we propose a new viewpoint for GANs,
which is termed as the minimizing general loss viewpoint. This viewpoint shows
a connection between the general loss of a classification problem regarding a
convex loss function and a f-divergence between the true and fake data
distributions. Mathematically, we proposed a setting for the classification
problem of the true and fake data, wherein we can prove that the general loss
of this classification problem is exactly the negative f-divergence for a
certain convex function f. This allows us to interpret the problem of learning
the generator for dismissing the f-divergence between the true and fake data
distributions as that of maximizing the general loss which is equivalent to the
min-max problem in GAN if the Logistic loss is used in the classification
problem. However, this viewpoint strengthens GANs in two ways. First, it allows
us to employ any convex loss function for the discriminator. Second, it
suggests that rather than limiting ourselves in NN-based discriminators, we can
alternatively utilize other powerful families. Bearing this viewpoint, we then
propose using the kernel-based family for discriminators. This family has two
appealing features: i) a powerful capacity in classifying non-linear nature
data and ii) being convex in the feature space. Using the convexity of this
family, we can further develop Fenchel duality to equivalently transform the
max-min problem to the max-max dual problem.
",1,0,0,1,0,0
708,Computing representation matrices for the action of Frobenius to cohomology groups,"  This paper is concerned with the computation of representation matrices for
the action of Frobenius to the cohomology groups of algebraic varieties.
Specifically we shall give an algorithm to compute the matrices for arbitrary
algebraic varieties with defining equations over perfect fields of positive
characteristic, and estimate its complexity. Moreover, we propose a specific
efficient method, which works for complete intersections.
",1,0,1,0,0,0
709,The solitary g-mode frequencies in early B-type stars,"  We present possible explanations of pulsations in early B-type main sequence
stars which arise purely from the excitation of gravity modes. There are three
stars with this type of oscillations detected from the BRITE light curves:
$\kappa$ Cen, a Car, $\kappa$ Vel. We show that by changing metallicity or the
opacity profile it is possible in some models to dump pressure modes keeping
gravity modes unstable. Other possible scenario involves pulsations of a lower
mass companion.
",0,1,0,0,0,0
710,Composition of Credal Sets via Polyhedral Geometry,"  Recently introduced composition operator for credal sets is an analogy of
such operators in probability, possibility, evidence and valuation-based
systems theories. It was designed to construct multidimensional models (in the
framework of credal sets) from a system of low- dimensional credal sets. In
this paper we study its potential from the computational point of view
utilizing methods of polyhedral geometry.
",1,0,0,0,0,0
711,The microarchitecture of a multi-threaded RISC-V compliant processing core family for IoT end-nodes,"  Internet-of-Things end-nodes demand low power processing platforms
characterized by heterogeneous dedicated units, controlled by a processor core
running concurrent control threads. Such architecture scheme fits one of the
main target application domain of the RISC-V instruction set. We present an
open-source processing core compliant with RISC-V on the software side and with
the popular Pulpino processor platform on the hardware side, while supporting
interleaved multi-threading for IoT applications. The latter feature is a novel
contribution in this application domain. We report details about the
microarchitecture design along with performance data.
",1,0,0,0,0,0
712,A Recent Survey on the Applications of Genetic Programming in Image Processing,"  During the last two decades, Genetic Programming (GP) has been largely used
to tackle optimization, classification, and automatic features selection
related tasks. The widespread use of GP is mainly due to its flexible and
comprehensible tree-type structure. Similarly, research is also gaining
momentum in the field of Image Processing (IP) because of its promising results
over wide areas of applications ranging from medical IP to multispectral
imaging. IP is mainly involved in applications such as computer vision, pattern
recognition, image compression, storage and transmission, and medical
diagnostics. This prevailing nature of images and their associated algorithm
i.e complexities gave an impetus to the exploration of GP. GP has thus been
used in different ways for IP since its inception. Many interesting GP
techniques have been developed and employed in the field of IP. To give the
research community an extensive view of these techniques, this paper presents
the diverse applications of GP in IP and provides useful resources for further
research. Also, comparison of different parameters used in ten different
applications of IP are summarized in tabular form. Moreover, analysis of
different parameters used in IP related tasks is carried-out to save the time
needed in future for evaluating the parameters of GP. As more advancement is
made in GP methodologies, its success in solving complex tasks not only related
to IP but also in other fields will increase. Additionally, guidelines are
provided for applying GP in IP related tasks, pros and cons of GP techniques
are discussed, and some future directions are also set.
",1,0,0,0,0,0
713,Optimal Input Placement in Lattice Graphs,"  The control of dynamical, networked systems continues to receive much
attention across the engineering and scientific research fields. Of particular
interest is the proper way to determine which nodes of the network should
receive external control inputs in order to effectively and efficiently control
portions of the network. Published methods to accomplish this task either find
a minimal set of driver nodes to guarantee controllability or a larger set of
driver nodes which optimizes some control metric. Here, we investigate the
control of lattice systems which provides analytical insight into the
relationship between network structure and controllability. First we derive a
closed form expression for the individual elements of the controllability
Gramian of infinite lattice systems. Second, we focus on nearest neighbor
lattices for which the distance between nodes appears in the expression for the
controllability Gramian. We show that common control energy metrics scale
exponentially with respect to the maximum distance between a driver node and a
target node.
",1,0,0,0,0,0
714,Construction of constant mean curvature n-noids using the DPW method,"  We construct constant mean curvature surfaces in euclidean space with genus
zero and n ends asymptotic to Delaunay surfaces using the DPW method.
",0,0,1,0,0,0
715,"Universality in numerical computation with random data. Case studies, analytic results and some speculations","  We discuss various universality aspects of numerical computations using
standard algorithms. These aspects include empirical observations and rigorous
results. We also make various speculations about computation in a broader
sense.
",0,1,1,0,0,0
716,The X-ray reflection spectrum of the radio-loud quasar 4C 74.26,"  The relativistic jets created by some active galactic nuclei are important
agents of AGN feedback. In spite of this, our understanding of what produces
these jets is still incomplete. X-ray observations, which can probe the
processes operating in the central regions in immediate vicinity of the
supermassive black hole, the presumed jet launching point, are potentially
particularly valuable in illuminating the jet formation process. Here, we
present the hard X-ray NuSTAR observations of the radio-loud quasar 4C 74.26 in
a joint analysis with quasi-simultaneous, soft X-ray Swift observations. Our
spectral analysis reveals a high-energy cut-off of 183$_{-35}^{+51}$ keV and
confirms the presence of ionized reflection in the source. From the average
spectrum we detect that the accretion disk is mildly recessed with an inner
radius of $R_\mathrm{in}=4-180\,R_\mathrm{g}$. However, no significant
evolution of the inner radius is seen during the three months covered by our
NuSTAR campaign. This lack of variation could mean that the jet formation in
this radio-loud quasar differs from what is observed in broad-line radio
galaxies.
",0,1,0,0,0,0
717,Tree Structured Synthesis of Gaussian Trees,"  A new synthesis scheme is proposed to effectively generate a random vector
with prescribed joint density that induces a (latent) Gaussian tree structure.
The quality of synthesis is measured by total variation distance between the
synthesized and desired statistics. The proposed layered and successive
encoding scheme relies on the learned structure of tree to use minimal number
of common random variables to synthesize the desired density. We characterize
the achievable rate region for the rate tuples of multi-layer latent Gaussian
tree, through which the number of bits needed to simulate such Gaussian joint
density are determined. The random sources used in our algorithm are the latent
variables at the top layer of tree, the additive independent Gaussian noises,
and the Bernoulli sign inputs that capture the ambiguity of correlation signs
between the variables.
",1,0,0,0,0,0
718,Information Retrieval and Recommendation System for Astronomical Observatories,"  We present a machine learning based information retrieval system for
astronomical observatories that tries to address user defined queries related
to an instrument. In the modern instrumentation scenario where heterogeneous
systems and talents are simultaneously at work, the ability to supply with the
right information helps speeding up the detector maintenance operations.
Enhancing the detector uptime leads to increased coincidence observation and
improves the likelihood for the detection of astrophysical signals. Besides,
such efforts will efficiently disseminate technical knowledge to a wider
audience and will help the ongoing efforts to build upcoming detectors like the
LIGO-India etc even at the design phase to foresee possible challenges. The
proposed method analyses existing documented efforts at the site to
intelligently group together related information to a query and to present it
on-line to the user. The user in response can further go into interesting links
and find already developed solutions or probable ways to address the present
situation optimally. A web application that incorporates the above idea has
been implemented and tested for LIGO Livingston, LIGO Hanford and Virgo
observatories.
",0,1,0,0,0,0
719,Database Learning: Toward a Database that Becomes Smarter Every Time,"  In today's databases, previous query answers rarely benefit answering future
queries. For the first time, to the best of our knowledge, we change this
paradigm in an approximate query processing (AQP) context. We make the
following observation: the answer to each query reveals some degree of
knowledge about the answer to another query because their answers stem from the
same underlying distribution that has produced the entire dataset. Exploiting
and refining this knowledge should allow us to answer queries more
analytically, rather than by reading enormous amounts of raw data. Also,
processing more queries should continuously enhance our knowledge of the
underlying distribution, and hence lead to increasingly faster response times
for future queries.
We call this novel idea---learning from past query answers---Database
Learning. We exploit the principle of maximum entropy to produce answers, which
are in expectation guaranteed to be more accurate than existing sample-based
approximations. Empowered by this idea, we build a query engine on top of Spark
SQL, called Verdict. We conduct extensive experiments on real-world query
traces from a large customer of a major database vendor. Our results
demonstrate that Verdict supports 73.7% of these queries, speeding them up by
up to 23.0x for the same accuracy level compared to existing AQP systems.
",1,0,0,0,0,0
720,A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models,"  Beam search is a desirable choice of test-time decoding algorithm for neural
sequence models because it potentially avoids search errors made by simpler
greedy methods. However, typical cross entropy training procedures for these
models do not directly consider the behaviour of the final decoding method. As
a result, for cross-entropy trained models, beam decoding can sometimes yield
reduced test performance when compared with greedy decoding. In order to train
models that can more effectively make use of beam search, we propose a new
training procedure that focuses on the final loss metric (e.g. Hamming loss)
evaluated on the output of beam search. While well-defined, this ""direct loss""
objective is itself discontinuous and thus difficult to optimize. Hence, in our
approach, we form a sub-differentiable surrogate objective by introducing a
novel continuous approximation of the beam search decoding procedure. In
experiments, we show that optimizing this new training objective yields
substantially better results on two sequence tasks (Named Entity Recognition
and CCG Supertagging) when compared with both cross entropy trained greedy
decoding and cross entropy trained beam decoding baselines.
",1,0,0,0,0,0
721,Measurably entire functions and their growth,"  In 1997 B. Weiss introduced the notion of measurably entire functions and
proved that they exist on every arbitrary free C- action defined on standard
probability space. In the same paper he asked about the minimal possible growth
of measurably entire functions. In this work we show that for every arbitrary
free C- action defined on a standard probability space there exists a
measurably entire function whose growth does not exceed exp (exp[log^p |z|])
for any p > 3. This complements a recent result by Buhovski, Gl?¬cksam,
Logunov, and Sodin (arXiv:1703.08101) who showed that such functions cannot
grow slower than exp (exp[log^p |z|]) for any p < 2.
",0,0,1,0,0,0
722,"Graph Theoretical Models of Closed n-Dimensional Manifolds: Digital Models of a Moebius Strip, a Torus, a Projective Plane a Klein Bottle and n-Dimensional Spheres","  In this paper, we show how to construct graph theoretical models of
n-dimensional continuous objects and manifolds. These models retain topological
properties of their continuous counterparts. An LCL collection of n-cells in
Euclidean space is introduced and investigated. If an LCL collection of n-cells
is a cover of a continuous n-dimensional manifold then the intersection graph
of this cover is a digital closed n-dimensional manifold with the same topology
as its continuous counterpart. As an example, we prove that the digital model
of a continuous n-dimensional sphere is a digital n-sphere with at least 2n+2
points, the digital model of a continuous projective plane is a digital
projective plane with at least eleven points, the digital model of a continuous
Klein bottle is the digital Klein bottle with at least sixteen points, the
digital model of a continuous torus is the digital torus with at least sixteen
points and the digital model of a continuous Moebius band is the digital
Moebius band with at least twelve points.
",1,0,1,0,0,0
723,Completely $p$-primitive binary quadratic forms,"  Let $f(x,y)=ax^2+bxy+cy^2$ be a binary quadratic form with integer
coefficients. For a prime $p$ not dividing the discriminant of $f$, we say $f$
is completely $p$-primitive if for any non-zero integer $N$, the diophantine
equation $f(x,y)=N$ has always an integer solution $(x,y)=(m,n)$ with
$(m,n,p)=1$ whenever it has an integer solution. In this article, we study
various properties of completely $p$-primitive binary quadratic forms. In
particular, we give a necessary and sufficient condition for a definite binary
quadratic form $f$ to be completely $p$-primitive.
",0,0,1,0,0,0
724,Analysis of Multivariate Data and Repeated Measures Designs with the R Package MANOVA.RM,"  The numerical availability of statistical inference methods for a modern and
robust analysis of longitudinal- and multivariate data in factorial experiments
is an essential element in research and education. While existing approaches
that rely on specific distributional assumptions of the data (multivariate
normality and/or characteristic covariance matrices) are implemented in
statistical software packages, there is a need for user-friendly software that
can be used for the analysis of data that do not fulfill the aforementioned
assumptions and provide accurate p-value and confidence interval estimates.
Therefore, newly developed statistical methods for the analysis of repeated
measures designs and multivariate data that neither assume multivariate
normality nor specific covariance matrices have been implemented in the freely
available R-package MANOVA.RM. The package is equipped with a graphical user
interface for plausible applications in academia and other educational purpose.
Several motivating examples illustrate the application of the methods.
",0,0,0,1,0,0
725,Multiple Illumination Phaseless Super-Resolution (MIPS) with Applications To Phaseless DOA Estimation and Diffraction Imaging,"  Phaseless super-resolution is the problem of recovering an unknown signal
from measurements of the magnitudes of the low frequency Fourier transform of
the signal. This problem arises in applications where measuring the phase, and
making high-frequency measurements, are either too costly or altogether
infeasible. The problem is especially challenging because it combines the
difficult problems of phase retrieval and classical super-resolution
",1,0,1,0,0,0
726,Local Marchenko-Pastur Law for Random Bipartite Graphs,"  This paper is the first chapter of three of the author's undergraduate
thesis. We study the random matrix ensemble of covariance matrices arising from
random $(d_b, d_w)$-regular bipartite graphs on a set of $M$ black vertices and
$N$ white vertices, for $d_b \gg \log^4 N$. We simultaneously prove that the
Green's functions of these covariance matrices and the adjacency matrices of
the underlying graphs agree with the corresponding limiting law (e.g.
Marchenko-Pastur law for covariance matrices) down to the optimal scale. This
is an improvement from the previously known mesoscopic results. We obtain
eigenvector delocalization for the covariance matrix ensemble as consequence,
as well as a weak rigidity estimate.
",0,0,1,1,0,0
727,Photoelectron Yields of Scintillation Counters with Embedded Wavelength-Shifting Fibers Read Out With Silicon Photomultipliers,"  Photoelectron yields of extruded scintillation counters with titanium dioxide
coating and embedded wavelength shifting fibers read out by silicon
photomultipliers have been measured at the Fermilab Test Beam Facility using
120\,GeV protons. The yields were measured as a function of transverse,
longitudinal, and angular positions for a variety of scintillator compositions
and reflective coating mixtures, fiber diameters, and photosensor sizes. Timing
performance was also studied. These studies were carried out by the Cosmic Ray
Veto Group of the Mu2e collaboration as part of their R\&D program.
",0,1,0,0,0,0
728,"Precise measurement of hyperfine structure in the $ \rm {3\,S_{1/2}} $ state of $ \rm{^7Li} $","  We report a precise measurement of hyperfine structure in the $ \rm
{3\,S_{1/2}} $ state of the odd isotope of Li, namely $ \rm {^7Li} $. The state
is excited from the ground $ \rm {2\,S_{1/2}} $ state (which has the same
parity) using two single-photon transitions via the intermediate $ \rm
{2\,P_{3/2}} $ state. The value of the hyperfine constant we measure is $ A =
93.095(52)$ MHz, which resolves two discrepant values reported in the
literature measured using other techniques. Our value is also consistent with
theoretical calculations.
",0,1,0,0,0,0
729,Reconstructing global fields from dynamics in the abelianized Galois group,"  We study a dynamical system induced by the Artin reciprocity map for a global
field. We translate the conjugacy of such dynamical systems into various
arithmetical properties that are equivalent to field isomorphism, relating it
to anabelian geometry.
",0,0,1,0,0,0
730,Algebraic models of the Euclidean plane,"  We introduce a new invariant, the real (logarithmic)-Kodaira dimension, that
allows to distinguish smooth real algebraic surfaces up to birational
diffeomorphism. As an application, we construct infinite families of smooth
rational real algebraic surfaces with trivial homology groups, whose real loci
are diffeomorphic to $\mathbb{R}^2$, but which are pairwise not birationally
diffeomorphic. There are thus infinitely many non-trivial models of the
euclidean plane, contrary to the compact case.
",0,0,1,0,0,0
731,Decentralization of Multiagent Policies by Learning What to Communicate,"  Effective communication is required for teams of robots to solve
sophisticated collaborative tasks. In practice it is typical for both the
encoding and semantics of communication to be manually defined by an expert;
this is true regardless of whether the behaviors themselves are bespoke,
optimization based, or learned. We present an agent architecture and training
methodology using neural networks to learn task-oriented communication
semantics based on the example of a communication-unaware expert policy. A
perimeter defense game illustrates the system's ability to handle dynamically
changing numbers of agents and its graceful degradation in performance as
communication constraints are tightened or the expert's observability
assumptions are broken.
",1,0,0,0,0,0
732,The tumbling rotational state of 1I/`Oumuamua,"  The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a
planetesimal born in another planetary system. This interloper exhibits a
variable colour within a range that is broadly consistent with local small
bodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited
Kuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an
axial ratio exceeding 5:1. Rotation period estimates are inconsistent and
varied, with reported values between 6.9 and 8.3 hours. Here we analyse all
available optical photometry reported to date. No single rotation period can
explain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be
in an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or
tumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135
and 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the
available data are insufficient to uniquely constrain the true frequencies and
shape. Assuming a body that responds to NPA rotation in a similar manner to
Solar System asteroids and comets, the timescale to damp 1I/'Oumuamua's
tumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling
within its parent planetary system, and will remain tumbling well after it has
left ours.
",0,1,0,0,0,0
733,Homology theory formulas for generalized Riemann-Hurwitz and generalized monoidal transformations,"  In the context of orientable circuits and subcomplexes of these as
representing certain singular spaces, we consider characteristic class formulas
generalizing those classical results as seen for the Riemann-Hurwitz formula
for regulating the topology of branched covering maps and that for monoidal
transformations which include the standard blowing-up process. Here the results
are presented as cap product pairings, which will be elements of a suitable
homology theory, rather than characteristic numbers as would be the case when
taking Kronecker products once Poincar?? duality is defined. We further
consider possible applications and examples including branched covering maps,
singular varieties involving virtual tangent bundles, the
Chern-Schwartz-MacPherson class, the homology L-class, generalized signature,
and the cohomology signature class.
",0,0,1,0,0,0
734,Video and Accelerometer-Based Motion Analysis for Automated Surgical Skills Assessment,"  Purpose: Basic surgical skills of suturing and knot tying are an essential
part of medical training. Having an automated system for surgical skills
assessment could help save experts time and improve training efficiency. There
have been some recent attempts at automated surgical skills assessment using
either video analysis or acceleration data. In this paper, we present a novel
approach for automated assessment of OSATS based surgical skills and provide an
analysis of different features on multi-modal data (video and accelerometer
data). Methods: We conduct the largest study, to the best of our knowledge, for
basic surgical skills assessment on a dataset that contained video and
accelerometer data for suturing and knot-tying tasks. We introduce ""entropy
based"" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy
(XApEn), which quantify the amount of predictability and regularity of
fluctuations in time-series data. The proposed features are compared to
existing methods of Sequential Motion Texture (SMT), Discrete Cosine Transform
(DCT) and Discrete Fourier Transform (DFT), for surgical skills assessment.
Results: We report average performance of different features across all
applicable OSATS criteria for suturing and knot tying tasks. Our analysis shows
that the proposed entropy based features out-perform previous state-of-the-art
methods using video data. For accelerometer data, our method performs better
for suturing only. We also show that fusion of video and acceleration features
can improve overall performance with the proposed entropy features achieving
highest accuracy. Conclusions: Automated surgical skills assessment can be
achieved with high accuracy using the proposed entropy features. Such a system
can significantly improve the efficiency of surgical training in medical
schools and teaching hospitals.
",1,0,0,0,0,0
735,A Theory of Complex Stochastic Systems with Two Types of Counteracting Entities,"  Many complex systems share two characteristics: 1) they are stochastic in
nature, and 2) they are characterized by a large number of factors. At the same
time, various natural complex systems appear to have two types of intertwined
constituents that exhibit counteracting effects on their equilibrium. In this
study, we employ these few characteristics to lay the groundwork for analyzing
such complex systems. The equilibrium point of these systems is generally
studied either through the kinetic notion of equilibrium or its energetic
notion, but not both. We postulate that these systems attempt to regulate the
state vector of their constituents such that both the kinetic and the energetic
notions of equilibrium are met. Based on this postulate, we prove: 1) the
existence of a point such that the kinetic notion of equilibrium is met for the
less abundant constituents and, at the same time, the state vector of more
abundant entities is regulated to minimize the energetic notion of equilibrium;
2) the effect of unboundedly increasing less (more) abundant constituents
stabilizes (destabilizes) the system; and 3) the (unrestricted) equilibrium of
the system is the point at which the number of stabilizing and destabilizing
entities increase unboundedly with the same rate.
",0,1,0,0,0,0
736,Mixing of odd- and even-frequency pairings in strongly correlated electron systems under magnetic field,"  Even- and odd-frequency superconductivity coexist due to broken time-reversal
symmetry under magnetic field. In order to describe this mixing, we extend the
linearized Eliashberg equation for the spin and charge fluctuation mechanism in
strongly correlated electron systems. We apply this extended Eliashberg
equation to the odd-frequency superconductivity on a quasi-one-dimensional
isosceles triangular lattice under in-plane magnetic field and examine the
effect of the even-frequency component.
",0,1,0,0,0,0
737,Stability of the Poincar?? bundle,"  Let X be an irreducible smooth projective curve, of genus at least two, over
an algebraically closed field k. Let $\mathcal{M}^d_G$ denote the moduli stack
of principal G-bundles over X of fixed topological type $d \in \pi_1(G)$, where
G is any almost simple affine algebraic group over k. We prove that the
universal bundle over $X \times \mathcal{M}^d_G$ is stable with respect to any
polarization on $X \times \mathcal{M}^d_G$. A similar result is proved for the
Poincar?? adjoint bundle over $X \times M_G^{d, rs}$, where $M_G^{d, rs}$ is
the coarse moduli space of regularly stable principal G-bundles over X of fixed
topological type d.
",0,0,1,0,0,0
738,Existence results for primitive elements in cubic and quartic extensions of a finite field,"  With $\Fq$ the finite field of $q$ elements, we investigate the following
question. If $\gamma$ generates $\Fqn$ over $\Fq$ and $\beta$ is a non-zero
element of $\Fqn$, is there always an $a \in \Fq$ such that $\beta(\gamma + a)$
is a primitive element? We resolve this case when $n=3$, thereby proving a
conjecture by Cohen. We also improve substantially on what is known when $n=4$.
",0,0,1,0,0,0
739,Morse geodesics in torsion groups,"  In this paper we exhibit Morse geodesics, often called ""hyperbolic
directions"", in infinite unbounded torsion groups. The groups studied are
lacunary hyperbolic groups and constructed using graded small cancellation
conditions. In all previously known examples, Morse geodesics were found in
groups which also contained Morse elements, infinite order elements whose
cyclic subgroup gives a Morse quasi-geodesic. Our result presents the first
example of a group which contains Morse geodesics but no Morse elements. In
fact, we show that there is an isometrically embedded $7$-regular tree inside
such groups where every infinite, simple path is a Morse geodesic.
",0,0,1,0,0,0
740,Global and local thermometry schemes in coupled quantum systems,"  We study the ultimate bounds on the estimation of temperature for an
interacting quantum system. We consider two coupled bosonic modes that are
assumed to be thermal and using quantum estimation theory establish the role
the Hamiltonian parameters play in thermometry. We show that in the case of a
conserved particle number the interaction between the modes leads to a decrease
in the overall sensitivity to temperature, while interestingly, if particle
exchange is allowed with the thermal bath the converse is true. We explain this
dichotomy by examining the energy spectra. Finally, we devise experimentally
implementable thermometry schemes that rely only on locally accessible
information from the total system, showing that almost Heisenberg limited
precision can still be achieved, and we address the (im)possibility for
multiparameter estimation in the system.
",0,1,0,0,0,0
741,The statistical significance filter leads to overconfident expectations of replicability,"  We show that publishing results using the statistical significance
filter---publishing only when the p-value is less than 0.05---leads to a
vicious cycle of overoptimistic expectation of the replicability of results.
First, we show analytically that when true statistical power is relatively low,
computing power based on statistically significant results will lead to
overestimates of power. Then, we present a case study using 10 experimental
comparisons drawn from a recently published meta-analysis in psycholinguistics
(J??ger et al., 2017). We show that the statistically significant results
yield an illusion of replicability. This illusion holds even if the researcher
doesn't conduct any formal power analysis but just uses statistical
significance to informally assess robustness (i.e., replicability) of results.
",0,0,1,1,0,0
742,A levitated nanoparticle as a classical two-level atom,"  The center-of-mass motion of a single optically levitated nanoparticle
resembles three uncoupled harmonic oscillators. We show how a suitable
modulation of the optical trapping potential can give rise to a coupling
between two of these oscillators, such that their dynamics are governed by a
classical equation of motion that resembles the Schr??dinger equation for a
two-level system. Based on experimental data, we illustrate the dynamics of
this parametrically coupled system both in the frequency and in the time
domain. We discuss the limitations and differences of the mechanical analogue
in comparison to a true quantum mechanical system.
",0,1,0,0,0,0
743,Simulating and Reconstructing Neurodynamics with Epsilon-Automata Applied to Electroencephalography (EEG) Microstate Sequences,"  We introduce new techniques to the analysis of neural spatiotemporal dynamics
via applying $\epsilon$-machine reconstruction to electroencephalography (EEG)
microstate sequences. Microstates are short duration quasi-stable states of the
dynamically changing electrical field topographies recorded via an array of
electrodes from the human scalp, and cluster into four canonical classes. The
sequence of microstates observed under particular conditions can be considered
an information source with unknown underlying structure. $\epsilon$-machines
are discrete dynamical system automata with state-dependent probabilities on
different future observations (in this case the next measured EEG microstate).
They artificially reproduce underlying structure in an optimally predictive
manner as generative models exhibiting dynamics emulating the behaviour of the
source. Here we present experiments using both simulations and empirical data
supporting the value of associating these discrete dynamical systems with
mental states (e.g. mind-wandering, focused attention, etc.) and with clinical
populations. The neurodynamics of mental states and clinical populations can
then be further characterized by properties of these dynamical systems,
including: i) statistical complexity (determined by the number of states of the
corresponding $\epsilon$-automaton); ii) entropy rate; iii) characteristic
sequence patterning (syntax, probabilistic grammars); iv) duration, persistence
and stability of dynamical patterns; and v) algebraic measures such as
Krohn-Rhodes complexity or holonomy length of the decompositions of these. The
potential applications include the characterization of mental states in
neurodynamic terms for mental health diagnostics, well-being interventions,
human-machine interface, and others on both subject-specific and
group/population-level.
",1,1,0,0,0,0
744,The LCES HIRES/Keck Precision Radial Velocity Exoplanet Survey,"  We describe a 20-year survey carried out by the Lick-Carnegie Exoplanet
Survey Team (LCES), using precision radial velocities from HIRES on the Keck-I
telescope to find and characterize extrasolar planetary systems orbiting nearby
F, G, K, and M dwarf stars. We provide here 60,949 precision radial velocities
for 1,624 stars contained in that survey. We tabulate a list of 357 significant
periodic signals that are of constant period and phase, and not coincident in
period and/or phase with stellar activity indices. These signals are thus
strongly suggestive of barycentric reflex motion of the star induced by one or
more candidate exoplanets in Keplerian motion about the host star. Of these
signals, 225 have already been published as planet claims, 60 are classified as
significant unpublished planet candidates that await photometric follow-up to
rule out activity-related causes, and 54 are also unpublished, but are
classified as ""significant"" signals that require confirmation by additional
data before rising to classification as planet candidates. Of particular
interest is our detection of a candidate planet with a minimum mass of 3.9
Earth masses and an orbital period of 9.9 days orbiting Lalande 21185, the
fourth-closest main sequence star to the Sun. For each of our exoplanetary
candidate signals, we provide the period and semi-amplitude of the Keplerian
orbital fit, and a likelihood ratio estimate of its statistical significance.
We also tabulate 18 Keplerian-like signals that we classify as likely arising
from stellar activity.
",0,1,0,0,0,0
745,Artificial Intelligence Based Malware Analysis,"  Artificial intelligence methods have often been applied to perform specific
functions or tasks in the cyber-defense realm. However, as adversary methods
become more complex and difficult to divine, piecemeal efforts to understand
cyber-attacks, and malware-based attacks in particular, are not providing
sufficient means for malware analysts to understand the past, present and
future characteristics of malware.
In this paper, we present the Malware Analysis and Attributed using Genetic
Information (MAAGI) system. The underlying idea behind the MAAGI system is that
there are strong similarities between malware behavior and biological organism
behavior, and applying biologically inspired methods to corpora of malware can
help analysts better understand the ecosystem of malware attacks. Due to the
sophistication of the malware and the analysis, the MAAGI system relies heavily
on artificial intelligence techniques to provide this capability. It has
already yielded promising results over its development life, and will hopefully
inspire more integration between the artificial intelligence and cyber--defense
communities.
",1,0,0,0,0,0
746,Bounded height in families of dynamical systems,"  Let a and b be algebraic numbers such that exactly one of a and b is an
algebraic integer, and let f_t(z):=z^2+t be a family of polynomials
parametrized by t. We prove that the set of all algebraic numbers t for which
there exist positive integers m and n such that f_t^m(a)=f_t^n(b) has bounded
Weil height. This is a special case of a more general result supporting a new
bounded height conjecture in dynamics. Our results fit into the general setting
of the principle of unlikely intersections in arithmetic dynamics.
",0,0,1,0,0,0
747,Input-to-State Stability of a Clamped-Free Damped String in the Presence of Distributed and Boundary Disturbances,"  This note establishes the input-to-state stability (ISS) property for a
clamped-free damped string with respect to distributed and boundary
disturbances. While efficient methods for establishing ISS properties for
distributed parameter systems with respect to distributed disturbances have
been developed during the last decades, establishing ISS properties with
respect to boundary disturbances remains challenging. One of the well-known
methods for well-posedness analysis of systems with boundary inputs is to use
an adequate lifting operator, which transfers the boundary disturbance to a
distributed one. However, the resulting distributed disturbance involves time
derivatives of the boundary perturbation. Thus, the subsequent ISS estimate
depends on its amplitude, and may not be expressed in the strict form of ISS
properties. To solve this problem, we show for a clamped-free damped string
equation that the projection of the original system trajectories in an adequate
Riesz basis can be used to establish the desired ISS property.
",1,0,0,0,0,0
748,A New Achievable Rate Region for Multiple-Access Channel with States,"  The problem of reliable communication over the multiple-access channel (MAC)
with states is investigated. We propose a new coding scheme for this problem
which uses quasi-group codes (QGC). We derive a new computable single-letter
characterization of the achievable rate region. As an example, we investigate
the problem of doubly-dirty MAC with modulo-$4$ addition. It is shown that the
sum-rate $R_1+R_2=1$ bits per channel use is achievable using the new scheme.
Whereas, the natural extension of the Gel'fand-Pinsker scheme, sum-rates
greater than $0.32$ are not achievable.
",1,0,0,0,0,0
749,Reassessing Graphene Absorption and Emission Spectroscopy,"  We present a new paradigm for understanding optical absorption and hot
electron dynamics experiments in graphene. Our analysis pivots on assigning
proper importance to phonon assisted indirect processes and bleaching of direct
processes. We show indirect processes figure in the excess absorption in the UV
region. Experiments which were thought to indicate ultrafast relaxation of
electrons and holes, reaching a thermal distribution from an extremely
non-thermal one in under 5-10 fs, instead are explained by the nascent electron
and hole distributions produced by indirect transitions. These need no
relaxation or ad-hoc energy removal to agree with the observed emission spectra
and fast pulsed absorption spectra. The fast emission following pulsed
absorption is dominated by phonon assisted processes, which vastly outnumber
direct ones and are always available, connecting any electron with any hole any
time. Calculations are given, including explicitly calculating the magnitude of
indirect processes, supporting these views.
",0,1,0,0,0,0
750,A deep generative model for single-cell RNA sequencing with application to detecting differentially expressed genes,"  We propose a probabilistic model for interpreting gene expression levels that
are observed through single-cell RNA sequencing. In the model, each cell has a
low-dimensional latent representation. Additional latent variables account for
technical effects that may erroneously set some observations of gene expression
levels to zero. Conditional distributions are specified by neural networks,
giving the proposed model enough flexibility to fit the data well. We use
variational inference and stochastic optimization to approximate the posterior
distribution. The inference procedure scales to over one million cells, whereas
competing algorithms do not. Even for smaller datasets, for several tasks, the
proposed procedure outperforms state-of-the-art methods like ZIFA and
ZINB-WaVE. We also extend our framework to take into account batch effects and
other confounding factors and propose a natural Bayesian hypothesis framework
for differential expression that outperforms tradition DESeq2.
",1,0,0,1,0,0
751,Analytic heating rate of neutron star merger ejecta derived from Fermi's theory of beta decay,"  Macronovae (kilonovae) that arise in binary neutron star mergers are powered
by radioactive beta decay of hundreds of $r$-process nuclides. We derive, using
Fermi's theory of beta decay, an analytic estimate of the nuclear heating rate.
We show that the heating rate evolves as a power law ranging between $t^{-6/5}$
to $t^{-4/3}$. The overall magnitude of the heating rate is determined by the
mean values of nuclear quantities, e.g., the nuclear matrix elements of beta
decay. These values are specified by using nuclear experimental data. We
discuss the role of higher order beta transitions and the robustness of the
power law. The robust and simple form of the heating rate suggests that
observations of the late-time bolometric light curve $\propto t^{-\frac{4}{3}}$
would be a direct evidence of a $r$-process driven macronova. Such observations
could also enable us to estimate the total amount of $r$-process nuclei
produced in the merger.
",0,1,0,0,0,0
752,Molecular Simulation of Caloric Properties of Fluids Modelled by Force Fields with Intramolecular Contributions: Application to Heat Capacities,"  The calculation of caloric properties such as heat capacity, Joule-Thomson
coefficients and the speed of sound by classical force-field-based molecular
simulation methodology has received scant attention in the literature,
particularly for systems composed of complex molecules whose force fields (FFs)
are characterized by a combination of intramolecular and intermolecular terms
(referred to herein as ""flexible FFs""). The calculation of a thermodynamic
property for a system whose molecules are described by such a FF involves the
calculation of the residual property prior to its addition to the corresponding
ideal-gas (IG) property, the latter of which is separately calculated, either
using thermochemical compilations or nowadays accurate quantum mechanical
calculations. Although the simulation of a volumetric residual property
proceeds by simply replacing the intermolecular FF in the rigid molecule case
by the total (intramolecular plus intermolecular) FF, this is not the case for
a caloric property. We discuss the methodology required in performing such
calculations, and focus on the example of the molar heat capacity at constant
pressure, $c_P$, one of the most important caloric properties. We also consider
three approximations for the calculation procedure, and illustrate their
consequences for the examples of the relatively simple molecule 2-propanol,
${\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\rm HO(CH_2)_2NH_2}$, an
important fluid used in carbon capture.
",0,1,0,0,0,0
753,On well-posedness of a velocity-vorticity formulation of the Navier-Stokes equations with no-slip boundary conditions,"  We study well-posedness of a velocity-vorticity formulation of the
Navier--Stokes equations, supplemented with no-slip velocity boundary
conditions, a no-penetration vorticity boundary condition, along with a natural
vorticity boundary condition depending on a pressure functional. In the
stationary case we prove existence and uniqueness of a suitable weak solution
to the system under a small data condition. The topic of the paper is driven by
recent developments of vorticity based numerical methods for the Navier--Stokes
equations.
",0,0,1,0,0,0
754,Generative Adversarial Networks for Electronic Health Records: A Framework for Exploring and Evaluating Methods for Predicting Drug-Induced Laboratory Test Trajectories,"  Generative Adversarial Networks (GANs) represent a promising class of
generative networks that combine neural networks with game theory. From
generating realistic images and videos to assisting musical creation, GANs are
transforming many fields of arts and sciences. However, their application to
healthcare has not been fully realized, more specifically in generating
electronic health records (EHR) data. In this paper, we propose a framework for
exploring the value of GANs in the context of continuous laboratory time series
data. We devise an unsupervised evaluation method that measures the predictive
power of synthetic laboratory test time series. Further, we show that when it
comes to predicting the impact of drug exposure on laboratory test data,
incorporating representation learning of the training cohorts prior to training
GAN models is beneficial.
",1,0,0,1,0,0
755,Properties of Kinetic Transition Networks for Atomic Clusters and Glassy Solids,"  A database of minima and transition states corresponds to a network where the
minima represent nodes and the transition states correspond to edges between
the pairs of minima they connect via steepest-descent paths. Here we construct
networks for small clusters bound by the Morse potential for a selection of
physically relevant parameters, in two and three dimensions. The properties of
these unweighted and undirected networks are analysed to examine two features:
whether they are small-world, where the shortest path between nodes involves
only a small number or edges; and whether they are scale-free, having a degree
distribution that follows a power law. Small-world character is present, but
statistical tests show that a power law is not a good fit, so the networks are
not scale-free. These results for clusters are compared with the corresponding
properties for the molecular and atomic structural glass formers
ortho-terphenyl and binary Lennard-Jones. These glassy systems do not show
small-world properties, suggesting that such behaviour is linked to the
structure-seeking landscapes of the Morse clusters.
",0,1,0,1,0,0
756,Asynchronous Byzantine Machine Learning (the case of SGD),"  Asynchronous distributed machine learning solutions have proven very
effective so far, but always assuming perfectly functioning workers. In
practice, some of the workers can however exhibit Byzantine behavior, caused by
hardware failures, software bugs, corrupt data, or even malicious attacks. We
introduce \emph{Kardam}, the first distributed asynchronous stochastic gradient
descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of
two complementary components: a filtering and a dampening component. The first
is scalar-based and ensures resilience against $\frac{1}{3}$ Byzantine workers.
Essentially, this filter leverages the Lipschitzness of cost functions and acts
as a self-stabilizer against Byzantine workers that would attempt to corrupt
the progress of SGD. The dampening component bounds the convergence rate by
adjusting to stale information through a generic gradient weighting scheme. We
prove that Kardam guarantees almost sure convergence in the presence of
asynchrony and Byzantine behavior, and we derive its convergence rate. We
evaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead
with respect to non Byzantine-resilient solutions. We empirically show that
Kardam does not introduce additional noise to the learning procedure but does
induce a slowdown (the cost of Byzantine resilience) that we both theoretically
and empirically show to be less than $f/n$, where $f$ is the number of
Byzantine failures tolerated and $n$ the total number of workers.
Interestingly, we also empirically observe that the dampening component is
interesting in its own right for it enables to build an SGD algorithm that
outperforms alternative staleness-aware asynchronous competitors in
environments with honest workers.
",0,0,0,1,0,0
757,A Recorded Debating Dataset,"  This paper describes an English audio and textual dataset of debating
speeches, a unique resource for the growing research field of computational
argumentation and debating technologies. We detail the process of speech
recording by professional debaters, the transcription of the speeches with an
Automatic Speech Recognition (ASR) system, their consequent automatic
processing to produce a text that is more ""NLP-friendly"", and in parallel --
the manual transcription of the speeches in order to produce gold-standard
""reference"" transcripts. We release 60 speeches on various controversial
topics, each in five formats corresponding to the different stages in the
production of the data. The intention is to allow utilizing this resource for
multiple research purposes, be it the addition of in-domain training data for a
debate-specific ASR system, or applying argumentation mining on either noisy or
clean debate transcripts. We intend to make further releases of this data in
the future.
",1,0,0,0,0,0
758,Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos,"  Deep learning has been demonstrated to achieve excellent results for image
classification and object detection. However, the impact of deep learning on
video analysis (e.g. action detection and recognition) has been limited due to
complexity of video data and lack of annotations. Previous convolutional neural
networks (CNN) based video action detection approaches usually consist of two
major steps: frame-level action proposal detection and association of proposals
across frames. Also, these methods employ two-stream CNN framework to handle
spatial and temporal feature separately. In this paper, we propose an
end-to-end deep network called Tube Convolutional Neural Network (T-CNN) for
action detection in videos. The proposed architecture is a unified network that
is able to recognize and localize action based on 3D convolution features. A
video is first divided into equal length clips and for each clip a set of tube
proposals are generated next based on 3D Convolutional Network (ConvNet)
features. Finally, the tube proposals of different clips are linked together
employing network flow and spatio-temporal action detection is performed using
these linked video proposals. Extensive experiments on several video datasets
demonstrate the superior performance of T-CNN for classifying and localizing
actions in both trimmed and untrimmed videos compared to state-of-the-arts.
",1,0,0,0,0,0
759,Distances and Isomorphism between Networks and the Stability of Network Invariants,"  We develop the theoretical foundations of a network distance that has
recently been applied to various subfields of topological data analysis, namely
persistent homology and hierarchical clustering. While this network distance
has previously appeared in the context of finite networks, we extend the
setting to that of compact networks. The main challenge in this new setting is
the lack of an easy notion of sampling from compact networks; we solve this
problem in the process of obtaining our results. The generality of our setting
means that we automatically establish results for exotic objects such as
directed metric spaces and Finsler manifolds. We identify readily computable
network invariants and establish their quantitative stability under this
network distance. We also discuss the computational complexity involved in
precisely computing this distance, and develop easily-computable lower bounds
by using the identified invariants. By constructing a wide range of explicit
examples, we show that these lower bounds are effective in distinguishing
between networks. Finally, we provide a simple algorithm that computes a lower
bound on the distance between two networks in polynomial time and illustrate
our metric and invariant constructions on a database of random networks and a
database of simulated hippocampal networks.
",1,0,1,0,0,0
760,"Expropriations, Property Confiscations and New Offshore Entities: Evidence from the Panama Papers","  Using the Panama Papers, we show that the beginning of media reporting on
expropriations and property confiscations in a country increases the
probability that offshore entities are incorporated by agents from the same
country in the same month. This result is robust to the use of country-year
fixed effects and the exclusion of tax havens. Further analysis shows that the
effect is driven by countries with non-corrupt and effective governments, which
supports the notion that offshore entities are incorporated when reasonably
well-intended and well-functioning governments become more serious about
fighting organized crime by confiscating proceeds of crime.
",0,0,0,0,0,1
761,Revealing Hidden Potentials of the q-Space Signal in Breast Cancer,"  Mammography screening for early detection of breast lesions currently suffers
from high amounts of false positive findings, which result in unnecessary
invasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many
of these false-positive findings prior to biopsy. Current approaches estimate
tissue properties by means of quantitative parameters taken from generative,
biophysical models fit to the q-space encoded signal under certain assumptions
regarding noise and spatial homogeneity. This process is prone to fitting
instability and partial information loss due to model simplicity. We reveal
unexplored potentials of the signal by integrating all data processing
components into a convolutional neural network (CNN) architecture that is
designed to propagate clinical target information down to the raw input images.
This approach enables simultaneous and target-specific optimization of image
normalization, signal exploitation, global representation learning and
classification. Using a multicentric data set of 222 patients, we demonstrate
that our approach significantly improves clinical decision making with respect
to the current state of the art.
",1,0,0,0,0,0
762,Concave Flow on Small Depth Directed Networks,"  Small depth networks arise in a variety of network related applications,
often in the form of maximum flow and maximum weighted matching. Recent works
have generalized such methods to include costs arising from concave functions.
In this paper we give an algorithm that takes a depth $D$ network and strictly
increasing concave weight functions of flows on the edges and computes a $(1 -
\epsilon)$-approximation to the maximum weight flow in time $mD \epsilon^{-1}$
times an overhead that is logarithmic in the various numerical parameters
related to the magnitudes of gradients and capacities.
Our approach is based on extending the scaling algorithm for approximate
maximum weighted matchings by [Duan-Pettie JACM`14] to the setting of small
depth networks, and then generalizing it to concave functions. In this more
restricted setting of linear weights in the range $[w_{\min}, w_{\max}]$, it
produces a $(1 - \epsilon)$-approximation in time $O(mD \epsilon^{-1} \log(
w_{\max} /w_{\min}))$. The algorithm combines a variety of tools and provides a
unified approach towards several problems involving small depth networks.
",1,0,0,0,0,0
763,Knowledge Management Strategies and Processes in Agile Software Development: A Systematic Literature Review,"  Knowledge-intensive companies that adopt Agile Software Development (ASD)
relay on efficient implementation of Knowledge Management (KM) strategies to
promotes different Knowledge Processes (KPs) to gain competitive advantage.
This study aims to explore how companies that adopt ASD implement KM strategies
utilizing practices that promote the KPs in the different organizational
layers. Through a systematic literature review, we analyzed 32 primary studies,
selected by automated search and snowballing in the extant literature. To
analyze the data, we applied narrative synthesis. Most of the identified KM
practices implement personalization strategies (81 %), supported by
codification (19 %). Our review shows that the primary studies do not report KM
practices in the strategic layer and two of them in the product portfolio
layer; on the other hand, in the project layer, the studies report 33 practices
that implement personalization strategy, and seven practices that implement
codification. KM strategies in ASD promote mainly the knowledge transfer
process with practices that stimulate social interaction to share tacit
knowledge in the project layer. As a result of using informal communication, a
significant amount of knowledge can be lost or not properly transferred to
other individuals and, instead of propagating the knowledge, it remains inside
a few individuals minds.
",1,0,0,0,0,0
764,A supernova at 50 pc: Effects on the Earth's atmosphere and biota,"  Recent 60Fe results have suggested that the estimated distances of supernovae
in the last few million years should be reduced from 100 pc to 50 pc. Two
events or series of events are suggested, one about 2.7 million years to 1.7
million years ago, and another may at 6.5 to 8.7 million years ago. We ask what
effects such supernovae are expected to have on the terrestrial atmosphere and
biota. Assuming that the Local Bubble was formed before the event being
considered, and that the supernova and the Earth were both inside a weak,
disordered magnetic field at that time, TeV-PeV cosmic rays at Earth will
increase by a factor of a few hundred. Tropospheric ionization will increase
proportionately, and the overall muon radiation load on terrestrial organisms
will increase by a factor of 150. All return to pre-burst levels within 10kyr.
In the case of an ordered magnetic field, effects depend strongly on the field
orientation. The upper bound in this case is with a largely coherent field
aligned along the line of sight to the supernova, in which case TeV-PeV cosmic
ray flux increases are 10^4; in the case of a transverse field they are below
current levels. We suggest a substantial increase in the extended effects of
supernovae on Earth and in the lethal distance estimate; more work is
needed.This paper is an explicit followup to Thomas et al. (2016). We also here
provide more detail on the computational procedures used in both works.
",0,1,0,0,0,0
765,Surface tension of flowing soap films,"  The surface tension of flowing soap films is measured with respect to the
film thickness and the concentration of soap solution. We perform this
measurement by measuring the curvature of the nylon wires that bound the soap
film channel and use the measured curvature to parametrize the relation between
the surface tension and the tension of the wire. We find the surface tension of
our soap films increases when the film is relatively thin or made of soap
solution of low concentration, otherwise it approaches an asymptotic value 30
mN/m. A simple adsorption model with only two parameters describes our
observations reasonably well. With our measurements, we are also able to
measure Gibbs elasticity for our soap film.
",0,1,0,0,0,0
766,Plugo: a VLC Systematic Perspective of Large-scale Indoor Localization,"  Indoor localization based on Visible Light Communication (VLC) has been in
favor with both the academia and industry for years. In this paper, we present
a prototyping photodiode-based VLC system towards large-scale localization.
Specially, we give in-depth analysis of the design constraints and
considerations for large-scale indoor localization research. After that we
identify the key enablers for such systems: 1) distributed architecture, 2)
one-way communication, and 3) random multiple access. Accordingly, we propose
Plugo -- a photodiode-based VLC system conforming to the aforementioned
criteria. We present a compact design of the VLC-compatible LED bulbs featuring
plug-and-go use-cases. The basic framed slotted Additive Links On-line Hawaii
Area (ALOHA) is exploited to achieve random multiple access over the shared
optical medium. We show its effectiveness in beacon broadcasting by
experiments, and further demonstrate its scalability to large-scale scenarios
through simulations. Finally, preliminary localization experiments are
conducted using fingerprinting-based methods in a customized testbed, achieving
an average accuracy of 0.14 m along with a 90-percentile accuracy of 0.33 m.
",1,0,0,0,0,0
767,Deep Convolutional Networks as shallow Gaussian Processes,"  We show that the output of a (residual) convolutional neural network (CNN)
with an appropriate prior over the weights and biases is a Gaussian process
(GP) in the limit of infinitely many convolutional filters, extending similar
results for dense networks. For a CNN, the equivalent kernel can be computed
exactly and, unlike ""deep kernels"", has very few parameters: only the
hyperparameters of the original CNN. Further, we show that this kernel has two
properties that allow it to be computed efficiently; the cost of evaluating the
kernel for a pair of images is similar to a single forward pass through the
original CNN with only one filter per layer. The kernel equivalent to a
32-layer ResNet obtains 0.84% classification error on MNIST, a new record for
GPs with a comparable number of parameters.
",0,0,0,1,0,0
768,Dynamic Watermarking for General LTI Systems,"  Detecting attacks in control systems is an important aspect of designing
secure and resilient control systems. Recently, a dynamic watermarking approach
was proposed for detecting malicious sensor attacks for SISO LTI systems with
partial state observations and MIMO LTI systems with a full rank input matrix
and full state observations; however, these previous approaches cannot be
applied to general LTI systems that are MIMO and have partial state
observations. This paper designs a dynamic watermarking approach for detecting
malicious sensor attacks for general LTI systems, and we provide a new set of
asymptotic and statistical tests. We prove these tests can detect attacks that
follow a specified attack model (more general than replay attacks), and we also
show that these tests simplify to existing tests when the system is SISO or has
full rank input matrix and full state observations. The benefit of our approach
is demonstrated with a simulation analysis of detecting sensor attacks in
autonomous vehicles. Our approach can distinguish between sensor attacks and
wind disturbance (through an internal model principle framework), whereas
improperly designed tests cannot distinguish between sensor attacks and wind
disturbance.
",1,0,1,0,0,0
769,FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers,"  For people with visual impairments, tactile graphics are an important means
to learn and explore information. However, raised line tactile graphics created
with traditional materials such as embossing are static. While available
refreshable displays can dynamically change the content, they are still too
expensive for many users, and are limited in size. These factors limit
wide-spread adoption and the representation of large graphics or data sets. In
this paper, we present FluxMaker, an inexpensive scalable system that renders
dynamic information on top of static tactile graphics with movable tactile
markers. These dynamic tactile markers can be easily reconfigured and used to
annotate static raised line tactile graphics, including maps, graphs, and
diagrams. We developed a hardware prototype that actuates magnetic tactile
markers driven by low-cost and scalable electromagnetic coil arrays, which can
be fabricated with standard printed circuit board manufacturing. We evaluate
our prototype with six participants with visual impairments and found positive
results across four application areas: location finding or navigating on
tactile maps, data analysis, and physicalization, feature identification for
tactile graphics, and drawing support. The user study confirms advantages in
application domains such as education and data exploration.
",1,0,0,0,0,0
770,Regularising Non-linear Models Using Feature Side-information,"  Very often features come with their own vectorial descriptions which provide
detailed information about their properties. We refer to these vectorial
descriptions as feature side-information. In the standard learning scenario,
input is represented as a vector of features and the feature side-information
is most often ignored or used only for feature selection prior to model
fitting. We believe that feature side-information which carries information
about features intrinsic property will help improve model prediction if used in
a proper way during learning process. In this paper, we propose a framework
that allows for the incorporation of the feature side-information during the
learning of very general model families to improve the prediction performance.
We control the structures of the learned models so that they reflect features
similarities as these are defined on the basis of the side-information. We
perform experiments on a number of benchmark datasets which show significant
predictive performance gains, over a number of baselines, as a result of the
exploitation of the side-information.
",1,0,0,1,0,0
771,On Diamond's $L^1$ criterion for asymptotic density of Beurling generalized integers,"  We give a short proof of the $L^{1}$ criterion for Beurling generalized
integers to have a positive asymptotic density. We actually prove the existence
of density under a weaker hypothesis. We also discuss related sufficient
conditions for the estimate $m(x)=\sum_{n_{k}\leq x} \mu(n_k)/n_k=o(1)$, with
$\mu$ the Beurling analog of the Moebius function.
",0,0,1,0,0,0
772,"""i have a feeling trump will win.................."": Forecasting Winners and Losers from User Predictions on Twitter","  Social media users often make explicit predictions about upcoming events.
Such statements vary in the degree of certainty the author expresses toward the
outcome:""Leonardo DiCaprio will win Best Actor"" vs. ""Leonardo DiCaprio may win""
or ""No way Leonardo wins!"". Can popular beliefs on social media predict who
will win? To answer this question, we build a corpus of tweets annotated for
veridicality on which we train a log-linear classifier that detects positive
veridicality with high precision. We then forecast uncertain outcomes using the
wisdom of crowds, by aggregating users' explicit predictions. Our method for
forecasting winners is fully automated, relying only on a set of contenders as
input. It requires no training data of past outcomes and outperforms sentiment
and tweet volume baselines on a broad range of contest prediction tasks. We
further demonstrate how our approach can be used to measure the reliability of
individual accounts' predictions and retrospectively identify surprise
outcomes.
",1,0,0,0,0,0
773,SLAM-Assisted Coverage Path Planning for Indoor LiDAR Mapping Systems,"  Applications involving autonomous navigation and planning of mobile agents
can benefit greatly by employing online Simultaneous Localization and Mapping
(SLAM) techniques, however, their proper implementation still warrants an
efficient amalgamation with any offline path planning method that may be used
for the particular application. In this paper, such a case of amalgamation is
considered for a LiDAR-based indoor mapping system which presents itself as a
2D coverage path planning problem implemented along with online SLAM. This
paper shows how classic offline Coverage Path Planning (CPP) can be altered for
use with online SLAM by proposing two modifications: (i) performing convex
decomposition of the polygonal coverage area to allow for an arbitrary choice
of an initial point while still tracing the shortest coverage path and (ii)
using a new approach to stitch together the different cells within the
polygonal area to form a continuous coverage path. Furthermore, an alteration
to the SLAM operation to suit the coverage path planning strategy is also made
that evaluates navigation errors in terms of an area coverage cost function.
The implementation results show how the combination of the two modified offline
and online planning strategies allow for an improvement in the total area
coverage by the mapping system - the modification thus presents an approach for
modifying offline and online navigation strategies for robust operation.
",1,0,0,0,0,0
774,Low energy bands and transport properties of chromium arsenide,"  We apply a method that combines the tight-binding approximation and the
Lowdin down-folding procedure to evaluate the electronic band structure of the
newly discovered pressure-induced superconductor CrAs. By integrating out all
low-lying arsenic degrees of freedom, we derive an effective Hamiltonian model
describing the Cr d bands near the Fermi level. We calculate and make
predictions for the energy spectra, the Fermi surface, the density of states
and transport and magnetic properties of this compound. Our results are
consistent with local-density approximation calculations as well as they show
good agreement with available experimental data for resistivity and Cr magnetic
moment.
",0,1,0,0,0,0
775,High Dynamic Range Imaging Technology,"  In this lecture note, we describe high dynamic range (HDR) imaging systems;
such systems are able to represent luminances of much larger brightness and,
typically, also a larger range of colors than conventional standard dynamic
range (SDR) imaging systems. The larger luminance range greatly improve the
overall quality of visual content, making it appears much more realistic and
appealing to observers. HDR is one of the key technologies of the future
imaging pipeline, which will change the way the digital visual content is
represented and manipulated today.
",1,0,0,0,0,0
776,Classification of pro-$p$ PD$^2$ pairs and the pro-$p$ curve complex,"  We classify pro-$p$ Poincar?? duality pairs in dimension two. We then use
this classification to build a pro-$p$ analogue of the curve complex and
establish its basic properties. We conclude with some statements concerning
separability properties of the mapping class group.
",0,0,1,0,0,0
777,A Survey on Content-Aware Video Analysis for Sports,"  Sports data analysis is becoming increasingly large-scale, diversified, and
shared, but difficulty persists in rapidly accessing the most crucial
information. Previous surveys have focused on the methodologies of sports video
analysis from the spatiotemporal viewpoint instead of a content-based
viewpoint, and few of these studies have considered semantics. This study
develops a deeper interpretation of content-aware sports video analysis by
examining the insight offered by research into the structure of content under
different scenarios. On the basis of this insight, we provide an overview of
the themes particularly relevant to the research on content-aware systems for
broadcast sports. Specifically, we focus on the video content analysis
techniques applied in sportscasts over the past decade from the perspectives of
fundamentals and general review, a content hierarchical model, and trends and
challenges. Content-aware analysis methods are discussed with respect to
object-, event-, and context-oriented groups. In each group, the gap between
sensation and content excitement must be bridged using proper strategies. In
this regard, a content-aware approach is required to determine user demands.
Finally, the paper summarizes the future trends and challenges for sports video
analysis. We believe that our findings can advance the field of research on
content-aware video analysis for broadcast sports.
",1,0,0,0,0,0
778,Recursive Multikernel Filters Exploiting Nonlinear Temporal Structure,"  In kernel methods, temporal information on the data is commonly included by
using time-delayed embeddings as inputs. Recently, an alternative formulation
was proposed by defining a gamma-filter explicitly in a reproducing kernel
Hilbert space, giving rise to a complex model where multiple kernels operate on
different temporal combinations of the input signal. In the original
formulation, the kernels are then simply combined to obtain a single kernel
matrix (for instance by averaging), which provides computational benefits but
discards important information on the temporal structure of the signal.
Inspired by works on multiple kernel learning, we overcome this drawback by
considering the different kernels separately. We propose an efficient strategy
to adaptively combine and select these kernels during the training phase. The
resulting batch and online algorithms automatically learn to process highly
nonlinear temporal information extracted from the input signal, which is
implicitly encoded in the kernel values. We evaluate our proposal on several
artificial and real tasks, showing that it can outperform classical approaches
both in batch and online settings.
",1,0,0,1,0,0
779,Gaussian-Dirichlet Posterior Dominance in Sequential Learning,"  We consider the problem of sequential learning from categorical observations
bounded in [0,1]. We establish an ordering between the Dirichlet posterior over
categorical outcomes and a Gaussian posterior under observations with N(0,1)
noise. We establish that, conditioned upon identical data with at least two
observations, the posterior mean of the categorical distribution will always
second-order stochastically dominate the posterior mean of the Gaussian
distribution. These results provide a useful tool for the analysis of
sequential learning under categorical outcomes.
",0,0,1,1,0,0
780,Learning Combinations of Sigmoids Through Gradient Estimation,"  We develop a new approach to learn the parameters of regression models with
hidden variables. In a nutshell, we estimate the gradient of the regression
function at a set of random points, and cluster the estimated gradients. The
centers of the clusters are used as estimates for the parameters of hidden
units. We justify this approach by studying a toy model, whereby the regression
function is a linear combination of sigmoids. We prove that indeed the
estimated gradients concentrate around the parameter vectors of the hidden
units, and provide non-asymptotic bounds on the number of required samples. To
the best of our knowledge, no comparable guarantees have been proven for linear
combinations of sigmoids.
",1,0,0,1,0,0
781,Fine Structure and Lifetime of Dark Excitons in Transition Metal Dichalcogenide Monolayers,"  The intricate interplay between optically dark and bright excitons governs
the light-matter interaction in transition metal dichalcogenide monolayers. We
have performed a detailed investigation of the ""spin-forbidden"" dark excitons
in WSe2 monolayers by optical spectroscopy in an out-of-plane magnetic field
Bz. In agreement with the theoretical predictions deduced from group theory
analysis, magneto-photoluminescence experiments reveal a zero field splitting
$\delta=0.6 \pm 0.1$ meV between two dark exciton states. The low energy state
being strictly dipole forbidden (perfectly dark) at Bz=0 while the upper state
is partially coupled to light with z polarization (""grey"" exciton). The first
determination of the dark neutral exciton lifetime $\tau_D$ in a transition
metal dichalcogenide monolayer is obtained by time-resolved photoluminescence.
We measure $\tau_D \sim 110 \pm 10$ ps for the grey exciton state, i.e. two
orders of magnitude longer than the radiative lifetime of the bright neutral
exciton at T=12 K.
",0,1,0,0,0,0
782,On bifibrations of model categories,"  In this article, we develop a notion of Quillen bifibration which combines
the two notions of Grothendieck bifibration and of Quillen model structure. In
particular, given a bifibration $p:\mathcal E\to\mathcal B$, we describe when a
family of model structures on the fibers $\mathcal E_A$ and on the basis
category $\mathcal B$ combines into a model structure on the total category
$\mathcal E$, such that the functor $p$ preserves cofibrations, fibrations and
weak equivalences. Using this Grothendieck construction for model structures,
we revisit the traditional definition of Reedy model structures, and possible
generalizations, and exhibit their bifibrational nature.
",1,0,1,0,0,0
783,Canonical sine and cosine Transforms For Integrable Boehmians,"  In this paper we define canonical sine and cosine transform, convolution
operations, prove convolution theorems in space of integrable functions on real
space. Further, obtain some results require to construct the spaces of
integrable Boehmians then extend this canonical sine and canonical cosine
transforms to space of integrable Boehmians and obtain their properties.
",0,0,1,0,0,0
784,Deep Learning Sparse Ternary Projections for Compressed Sensing of Images,"  Compressed sensing (CS) is a sampling theory that allows reconstruction of
sparse (or compressible) signals from an incomplete number of measurements,
using of a sensing mechanism implemented by an appropriate projection matrix.
The CS theory is based on random Gaussian projection matrices, which satisfy
recovery guarantees with high probability; however, sparse ternary {0, -1, +1}
projections are more suitable for hardware implementation. In this paper, we
present a deep learning approach to obtain very sparse ternary projections for
compressed sensing. Our deep learning architecture jointly learns a pair of a
projection matrix and a reconstruction operator in an end-to-end fashion. The
experimental results on real images demonstrate the effectiveness of the
proposed approach compared to state-of-the-art methods, with significant
advantage in terms of complexity.
",1,0,0,1,0,0
785,Learning a Predictive Model for Music Using PULSE,"  Predictive models for music are studied by researchers of algorithmic
composition, the cognitive sciences and machine learning. They serve as base
models for composition, can simulate human prediction and provide a
multidisciplinary application domain for learning algorithms. A particularly
well established and constantly advanced subtask is the prediction of
monophonic melodies. As melodies typically involve non-Markovian dependencies
their prediction requires a capable learning algorithm. In this thesis, I apply
the recent feature discovery and learning method PULSE to the realm of symbolic
music modeling. PULSE is comprised of a feature generating operation and
L1-regularized optimization. These are used to iteratively expand and cull the
feature set, effectively exploring feature spaces that are too large for common
feature selection approaches. I design a general Python framework for PULSE,
propose task-optimized feature generating operations and various
music-theoretically motivated features that are evaluated on a standard corpus
of monophonic folk and chorale melodies. The proposed method significantly
outperforms comparable state-of-the-art models. I further discuss the free
parameters of the learning algorithm and analyze the feature composition of the
learned models. The models learned by PULSE afford an easy inspection and are
musicologically interpreted for the first time.
",1,0,0,0,0,0
786,Learning Nonlinear Brain Dynamics: van der Pol Meets LSTM,"  Many real-world data sets, especially in biology, are produced by highly
multivariate and nonlinear complex dynamical systems. In this paper, we focus
on brain imaging data, including both calcium imaging and functional MRI data.
Standard vector-autoregressive models are limited by their linearity
assumptions, while nonlinear general-purpose, large-scale temporal models, such
as LSTM networks, typically require large amounts of training data, not always
readily available in biological applications; furthermore, such models have
limited interpretability. We introduce here a novel approach for learning a
nonlinear differential equation model aimed at capturing brain dynamics.
Specifically, we propose a variable-projection optimization approach to
estimate the parameters of the multivariate (coupled) van der Pol oscillator,
and demonstrate that such a model can accurately represent nonlinear dynamics
of the brain data. Furthermore, in order to improve the predictive accuracy
when forecasting future brain-activity time series, we use this analytical
model as an unlimited source of simulated data for pretraining LSTM; such
model-specific data augmentation approach consistently improves LSTM
performance on both calcium and fMRI imaging data.
",0,0,0,1,1,0
787,Neural networks for topology optimization,"  In this research, we propose a deep learning based approach for speeding up
the topology optimization methods. The problem we seek to solve is the layout
problem. The main novelty of this work is to state the problem as an image
segmentation task. We leverage the power of deep learning methods as the
efficient pixel-wise image labeling technique to perform the topology
optimization. We introduce convolutional encoder-decoder architecture and the
overall approach of solving the above-described problem with high performance.
The conducted experiments demonstrate the significant acceleration of the
optimization process. The proposed approach has excellent generalization
properties. We demonstrate the ability of the application of the proposed model
to other problems. The successful results, as well as the drawbacks of the
current method, are discussed.
",1,0,0,0,0,0
788,Abstract Interpretation with Unfoldings,"  We present and evaluate a technique for computing path-sensitive interference
conditions during abstract interpretation of concurrent programs. In lieu of
fixed point computation, we use prime event structures to compactly represent
causal dependence and interference between sequences of transformers. Our main
contribution is an unfolding algorithm that uses a new notion of independence
to avoid redundant transformer application, thread-local fixed points to reduce
the size of the unfolding, and a novel cutoff criterion based on subsumption to
guarantee termination of the analysis. Our experiments show that the abstract
unfolding produces an order of magnitude fewer false alarms than a mature
abstract interpreter, while being several orders of magnitude faster than
solver-based tools that have the same precision.
",1,0,0,0,0,0
789,Perturbation theory for cosmologies with non-linear structure,"  The next generation of cosmological surveys will operate over unprecedented
scales, and will therefore provide exciting new opportunities for testing
general relativity. The standard method for modelling the structures that these
surveys will observe is to use cosmological perturbation theory for linear
structures on horizon-sized scales, and Newtonian gravity for non-linear
structures on much smaller scales. We propose a two-parameter formalism that
generalizes this approach, thereby allowing interactions between large and
small scales to be studied in a self-consistent and well-defined way. This uses
both post-Newtonian gravity and cosmological perturbation theory, and can be
used to model realistic cosmological scenarios including matter, radiation and
a cosmological constant. We find that the resulting field equations can be
written as a hierarchical set of perturbation equations. At leading-order,
these equations allow us to recover a standard set of Friedmann equations, as
well as a Newton-Poisson equation for the inhomogeneous part of the Newtonian
energy density in an expanding background. For the perturbations in the
large-scale cosmology, however, we find that the field equations are sourced by
both non-linear and mode-mixing terms, due to the existence of small-scale
structures. These extra terms should be expected to give rise to new
gravitational effects, through the mixing of gravitational modes on small and
large scales - effects that are beyond the scope of standard linear
cosmological perturbation theory. We expect our formalism to be useful for
accurately modelling gravitational physics in universes that contain non-linear
structures, and for investigating the effects of non-linear gravity in the era
of ultra-large-scale surveys.
",0,1,0,0,0,0
790,"Heads or tails in zero gravity: an example of a classical contextual ""measurement""","  Playing the game of heads or tails in zero gravity demonstrates that there
exists a contextual ""measurement"" in classical mechanics. When the coin is
flipped, its orientation is a continuous variable. However, the ""measurement""
that occurs when the coin is caught by clapping two hands together gives a
discrete value (heads or tails) that depends on the context (orientation of the
hands). It is then shown that there is a strong analogy with the spin
measurement of the Stern-Gerlach experiment, and in particular with Stern and
Gerlach's sequential measurements. Finally, we clarify the analogy by recalling
how the de Broglie-Bohm interpretation simply explains the spin ""measurement"".
",0,1,0,0,0,0
791,Amortized Inference Regularization,"  The variational autoencoder (VAE) is a popular model for density estimation
and representation learning. Canonically, the variational principle suggests to
prefer an expressive inference model so that the variational approximation is
accurate. However, it is often overlooked that an overly-expressive inference
model can be detrimental to the test set performance of both the amortized
posterior approximator and, more importantly, the generative density estimator.
In this paper, we leverage the fact that VAEs rely on amortized inference and
propose techniques for amortized inference regularization (AIR) that control
the smoothness of the inference model. We demonstrate that, by applying AIR, it
is possible to improve VAE generalization on both inference and generative
performance. Our paper challenges the belief that amortized inference is simply
a mechanism for approximating maximum likelihood training and illustrates that
regularization of the amortization family provides a new direction for
understanding and improving generalization in VAEs.
",0,0,0,1,0,0
792,Phase Synchronization on Spacially Embeded Duplex Networks with Total Cost Constraint,"  Synchronization on multiplex networks have attracted increasing attention in
the past few years. We investigate collective behaviors of Kuramoto oscillators
on single layer and duplex spacial networks with total cost restriction, which
was introduced by Li et. al [Li G., Reis S. D., Moreira A. A., Havlin S.,
Stanley H. E. and Jr A. J., {\it Phys. Rev. Lett.} 104, 018701 (2010)] and
termed as the Li network afterwards. In the Li network model, with the increase
of its spacial exponent, the network's structure will vary from the random type
to the small-world one, and finally to the regular lattice.We first explore how
the spacial exponent influences the synchronizability of Kuramoto oscillators
on single layer Li networks and find that the closer the Li network is to a
regular lattice, the more difficult for it to evolve into synchronization. Then
we investigate synchronizability of duplex Li networks and find that the
existence of inter-layer interaction can greatly enhance inter-layer and global
synchronizability. When the inter-layer coupling strength is larger than a
certain critical value, whatever the intra-layer coupling strength is, the
inter-layer synchronization will always occur. Furthermore, on single layer Li
networks, nodes with larger degrees more easily reach global synchronization,
while on duplex Li networks, this phenomenon becomes much less obvious.
Finally, we study the impact of inter-link density on global synchronization
and obtain that sparse inter-links can lead to the emergence of global
synchronization for duplex Li networks just as dense inter-links do. In a word,
inter-layer interaction plays a vital role in determining synchronizability for
duplex spacial networks with total cost constraint.
",0,1,0,0,0,0
793,Linear complexity of Legendre-polynomial quotients,"  We continue to investigate binary sequence $(f_u)$ over $\{0,1\}$ defined by
$(-1)^{f_u}=\left(\frac{(u^w-u^{wp})/p}{p}\right)$ for integers $u\ge 0$, where
$\left(\frac{\cdot}{p}\right)$ is the Legendre symbol and we restrict
$\left(\frac{0}{p}\right)=1$. In an earlier work, the linear complexity of
$(f_u)$ was determined for $w=p-1$ under the assumption of $2^{p-1}\not\equiv 1
\pmod {p^2}$. In this work, we give possible values on the linear complexity of
$(f_u)$ for all $1\le w<p-1$ under the same conditions. We also state that the
case of larger $w(\geq p)$ can be reduced to that of $0\leq w\leq p-1$.
",1,0,1,0,0,0
794,The Time Dimension of Science: Connecting the Past to the Future,"  A central question in science of science concerns how time affects citations.
Despite the long-standing interests and its broad impact, we lack systematic
answers to this simple yet fundamental question. By reviewing and classifying
prior studies for the past 50 years, we find a significant lack of consensus in
the literature, primarily due to the coexistence of retrospective and
prospective approaches to measuring citation age distributions. These two
approaches have been pursued in parallel, lacking any known connections between
the two. Here we developed a new theoretical framework that not only allows us
to connect the two approaches through precise mathematical relationships, it
also helps us reconcile the interplay between temporal decay of citations and
the growth of science, helping us uncover new functional forms characterizing
citation age distributions. We find retrospective distribution follows a
lognormal distribution with exponential cutoff, while prospective distribution
is governed by the interplay between a lognormal distribution and the growth in
the number of references. Most interestingly, the two approaches can be
connected once rescaled by the growth of publications and citations. We further
validate our framework using both large-scale citation datasets and analytical
models capturing citation dynamics. Together this paper presents a
comprehensive analysis of the time dimension of science, representing a new
empirical and theoretical basis for all future studies in this area.
",1,1,0,0,0,0
795,Excited states of defect lines in silicon: A first-principles study based on hydrogen cluster analogues,"  Excited states of a single donor in bulk silicon have previously been studied
extensively based on effective mass theory. However, a proper theoretical
description of the excited states of a donor cluster is still scarce. Here we
study the excitations of lines of defects within a single-valley spherical band
approximation, thus mapping the problem to a scaled hydrogen atom array. A
series of detailed full configuration-interaction and time-dependent hybrid
density-functional theory calculations have been performed to understand linear
clusters of up to 10 donors. Our studies illustrate the generic features of
their excited states, addressing the competition between formation of
inter-donor ionic states and intra-donor atomic excited states. At short
inter-donor distances, excited states of donor molecules are dominant, at
intermediate distances ionic states play an important role, and at long
distances the intra-donor excitations are predominant as expected. The
calculations presented here emphasise the importance of correlations between
donor electrons, and are thus complementary to other recent approaches that
include effective mass anisotropy and multi-valley effects. The exchange
splittings between relevant excited states have also been estimated for a donor
pair and for a three-donor arrays; the splittings are much larger than those in
the ground state in the range of donor separations between 10 and 20 nm. This
establishes a solid theoretical basis for the use of excited-state exchange
interactions for controllable quantum gate operations in silicon.
",0,1,0,0,0,0
796,Neutral Carbon Emission in luminous infrared galaxies The \CI\ Lines as Total Molecular Gas Tracers,"  We present a statistical study on the [C I]($^{3} \rm P_{1} \rightarrow {\rm
^3 P}_{0}$), [C I] ($^{3} \rm P_{2} \rightarrow {\rm ^3 P}_{1}$) lines
(hereafter [C I] (1$-$0) and [C I] (2$-$1), respectively) and the CO (1$-$0)
line for a sample of (ultra)luminous infrared galaxies [(U)LIRGs]. We explore
the correlations between the luminosities of CO (1$-$0) and [C I] lines, and
find that $L'_\mathrm{CO(1-0)}$ correlates almost linearly with both $L'_
\mathrm{[CI](1-0)}$ and $L'_\mathrm{[CI](2-1)}$, suggesting that [C I] lines
can trace total molecular gas mass at least for (U)LIRGs. We also investigate
the dependence of $L'_\mathrm{[CI](1-0)}$/$L'_\mathrm{CO(1-0)}$,
$L'_\mathrm{[CI](2-1)}$/$L'_\mathrm{CO(1-0)}$ and
$L'_\mathrm{[CI](2-1)}$/$L'_\mathrm{[CI](1-0)}$ on the far-infrared color of
60-to-100 $\mu$m, and find non-correlation, a weak correlation and a modest
correlation, respectively. Under the assumption that these two carbon
transitions are optically thin, we further calculate the [C I] line excitation
temperatures, atomic carbon masses, and the mean [C I] line flux-to-H$_2$ mass
conversion factors for our sample. The resulting $\mathrm{H_2}$ masses using
these [C I]-based conversion factors roughly agree with those derived from
$L'_\mathrm{CO(1-0)}$ and CO-to-H$_2$ conversion factor.
",0,1,0,0,0,0
797,Kernel Approximation Methods for Speech Recognition,"  We study large-scale kernel methods for acoustic modeling in speech
recognition and compare their performance to deep neural networks (DNNs). We
perform experiments on four speech recognition datasets, including the TIMIT
and Broadcast News benchmark tasks, and compare these two types of models on
frame-level performance metrics (accuracy, cross-entropy), as well as on
recognition metrics (word/character error rate). In order to scale kernel
methods to these large datasets, we use the random Fourier feature method of
Rahimi and Recht (2007). We propose two novel techniques for improving the
performance of kernel acoustic models. First, in order to reduce the number of
random features required by kernel models, we propose a simple but effective
method for feature selection. The method is able to explore a large number of
non-linear features while maintaining a compact model more efficiently than
existing approaches. Second, we present a number of frame-level metrics which
correlate very strongly with recognition performance when computed on the
heldout set; we take advantage of these correlations by monitoring these
metrics during training in order to decide when to stop learning. This
technique can noticeably improve the recognition performance of both DNN and
kernel models, while narrowing the gap between them. Additionally, we show that
the linear bottleneck method of Sainath et al. (2013) improves the performance
of our kernel models significantly, in addition to speeding up training and
making the models more compact. Together, these three methods dramatically
improve the performance of kernel acoustic models, making their performance
comparable to DNNs on the tasks we explored.
",1,0,0,1,0,0
798,Composition Factors of Tensor Products of Symmetric Powers,"  We determine the composition factors of the tensor product $S(E)\otimes S(E)$
of two copies of the symmetric algebra of the natural module $E$ of a general
linear group over an algebraically closed field of positive characteristic. Our
main result may be regarded as a substantial generalisation of the tensor
product theorem of Krop and Sullivan, on composition factors of $S(E)$. We
earlier answered the question of which polynomially injective modules are
infinitesimally injective in terms of the ""divisibility index"". We are now able
to give an explicit description of the divisibility index for polynomial
modules for general linear groups of degree at most $3$.
",0,0,1,0,0,0
799,Stochastic Ratcheting on a Funneled Energy Landscape is Necessary for Highly Efficient Contractility of Actomyosin Force Dipoles,"  Current understanding of how contractility emerges in disordered actomyosin
networks of non-muscle cells is still largely based on the intuition derived
from earlier works on muscle contractility. This view, however, largely
overlooks the free energy gain following passive cross-linker binding, which,
even in the absence of active fluctuations, provides a thermodynamic drive
towards highly overlapping filamentous states. In this work, we shed light on
this phenomenon, showing that passive cross-linkers, when considered in the
context of two anti-parallel filaments, generate noticeable contractile forces.
However, as binding free energy of cross-linkers is increased, a sharp onset of
kinetic arrest follows, greatly diminishing effectiveness of this contractility
mechanism, allowing the network to contract only with weakly resisting tensions
at its boundary. We have carried out stochastic simulations elucidating this
mechanism, followed by a mean-field treatment that predicts how contractile
forces asymptotically scale at small and large binding energies, respectively.
Furthermore, when considering an active contractile filament pair, based on
non-muscle myosin II, we found that the non-processive nature of these motors
leads to highly inefficient force generation, due to recoil slippage of the
overlap during periods when the motor is dissociated. However, we discovered
that passive cross-linkers can serve as a structural ratchet during these
unbound motor time spans, resulting in vast force amplification. Our results
shed light on the non-equilibrium effects of transiently binding proteins in
biological active matter, as observed in the non-muscle actin cytoskeleton,
showing that highly efficient contractile force dipoles result from synergy of
passive cross-linker and active motor dynamics, via a ratcheting mechanism on a
funneled energy landscape.
",0,1,0,0,0,0
800,Mitigation of Phase Noise in Massive MIMO Systems: A Rate-Splitting Approach,"  This work encompasses Rate-Splitting (RS), providing significant benefits in
multi-user settings in the context of huge degrees of freedom promised by
massive Multiple-Input Multiple-Output (MIMO). However, the requirement of
massive MIMO for cost-efficient implementation makes them more prone to
hardware imperfections such as phase noise (PN). As a result, we focus on a
realistic broadcast channel with a large number of antennas and hampered by the
unavoidable PN. Moreover, we employ the RS transmission strategy, and we show
its robustness against PN, since the sum-rate does not saturate at high
signal-to-noise ratio (SNR). Although, the analytical results are obtained by
means of the deterministic equivalent analysis, they coincide with simulation
results even for finite system dimensions.
",1,0,0,0,0,0
801,Tangle-tree duality in abstract separation systems,"  We prove a general width duality theorem for combinatorial structures with
well-defined notions of cohesion and separation. These might be graphs and
matroids, but can be much more general or quite different. The theorem asserts
a duality between the existence of high cohesiveness somewhere local and a
global overall tree structure.
We describe cohesive substructures in a unified way in the format of tangles:
as orientations of low-order separations satisfying certain consistency axioms.
These axioms can be expressed without reference to the underlying structure,
such as a graph or matroid, but just in terms of the poset of the separations
themselves. This makes it possible to identify tangles, and apply our
tangle-tree duality theorem, in very diverse settings.
Our result implies all the classical duality theorems for width parameters in
graph minor theory, such as path-width, tree-width, branch-width or rank-width.
It yields new, tangle-type, duality theorems for tree-width and path-width. It
implies the existence of width parameters dual to cohesive substructures such
as $k$-blocks, edge-tangles, or given subsets of tangles, for which no width
duality theorems were previously known.
Abstract separation systems can be found also in structures quite unlike
graphs and matroids. For example, our theorem can be applied to image analysis
by capturing the regions of an image as tangles of separations defined as
natural partitions of its set of pixels. It can be applied in big data contexts
by capturing clusters as tangles. It can be applied in the social sciences,
e.g. by capturing as tangles the few typical mindsets of individuals found by a
survey. It could also be applied in pure mathematics, e.g. to separations of
compact manifolds.
",0,0,1,0,0,0
802,Attention-Based Guided Structured Sparsity of Deep Neural Networks,"  Network pruning is aimed at imposing sparsity in a neural network
architecture by increasing the portion of zero-valued weights for reducing its
size regarding energy-efficiency consideration and increasing evaluation speed.
In most of the conducted research efforts, the sparsity is enforced for network
pruning without any attention to the internal network characteristics such as
unbalanced outputs of the neurons or more specifically the distribution of the
weights and outputs of the neurons. That may cause severe accuracy drop due to
uncontrolled sparsity. In this work, we propose an attention mechanism that
simultaneously controls the sparsity intensity and supervised network pruning
by keeping important information bottlenecks of the network to be active. On
CIFAR-10, the proposed method outperforms the best baseline method by 6% and
reduced the accuracy drop by 2.6x at the same level of sparsity.
",0,0,0,1,0,0
803,A Graph Model with Indirect Co-location Links,"  Graph models are widely used to analyse diffusion processes embedded in
social contacts and to develop applications. A range of graph models are
available to replicate the underlying social structures and dynamics
realistically. However, most of the current graph models can only consider
concurrent interactions among individuals in the co-located interaction
networks. However, they do not account for indirect interactions that can
transmit spreading items to individuals who visit the same locations at
different times but within a certain time limit. The diffusion phenomena
occurring through direct and indirect interactions is called same place
different time (SPDT) diffusion. This paper introduces a model to synthesize
co-located interaction graphs capturing both direct interactions, where
individuals meet at a location, and indirect interactions, where individuals
visit the same location at different times within a set timeframe. We analyze
60 million location updates made by 2 million users from a social networking
application to characterize the graph properties, including the space-time
correlations and its time evolving characteristics, such as bursty or ongoing
behaviors. The generated synthetic graph reproduces diffusion dynamics of a
realistic contact graph, and reduces the prediction error by up to 82% when
compare to other contact graph models demonstrating its potential for
forecasting epidemic spread.
",1,0,0,0,0,0
804,Essentially No Barriers in Neural Network Energy Landscape,"  Training neural networks involves finding minima of a high-dimensional
non-convex loss function. Knowledge of the structure of this energy landscape
is sparse. Relaxing from linear interpolations, we construct continuous paths
between minima of recent neural network architectures on CIFAR10 and CIFAR100.
Surprisingly, the paths are essentially flat in both the training and test
landscapes. This implies that neural networks have enough capacity for
structural changes, or that these changes are small between minima. Also, each
minimum has at least one vanishing Hessian eigenvalue in addition to those
resulting from trivial invariance.
",0,0,0,1,0,0
805,Homogenization of nonlinear elliptic systems in nonreflexive Musielak-Orlicz spaces,"  We study the homogenization process for families of strongly nonlinear
elliptic systems with the homogeneous Dirichlet boundary conditions. The growth
and the coercivity of the elliptic operator is assumed to be indicated by a
general inhomogeneous anisotropic $N-$function, which may be possibly also
dependent on the spatial variable, i.e., the homogenization process will change
the characteristic function spaces at each step. Such a problem is well known
and there exists many positive results for the function satisfying $\Delta_2$
and $\nabla_2$ conditions an being in addition H??lder continuous with
respect to the spatial variable. We shall show that cases these conditions can
be neglected and will deal with a rather general problem in general function
space setting.
",0,0,1,0,0,0
806,A Generalization of Permanent Inequalities and Applications in Counting and Optimization,"  A polynomial $p\in\mathbb{R}[z_1,\dots,z_n]$ is real stable if it has no
roots in the upper-half complex plane. Gurvits's permanent inequality gives a
lower bound on the coefficient of the $z_1z_2\dots z_n$ monomial of a real
stable polynomial $p$ with nonnegative coefficients. This fundamental
inequality has been used to attack several counting and optimization problems.
Here, we study a more general question: Given a stable multilinear polynomial
$p$ with nonnegative coefficients and a set of monomials $S$, we show that if
the polynomial obtained by summing up all monomials in $S$ is real stable, then
we can lowerbound the sum of coefficients of monomials of $p$ that are in $S$.
We also prove generalizations of this theorem to (real stable) polynomials that
are not multilinear. We use our theorem to give a new proof of Schrijver's
inequality on the number of perfect matchings of a regular bipartite graph,
generalize a recent result of Nikolov and Singh, and give deterministic
polynomial time approximation algorithms for several counting problems.
",1,0,1,0,0,0
807,Proceedings Fifth International Workshop on Verification and Program Transformation,"  This volume contains the proceedings of the Fifth International Workshop on
Verification and Program Transformation (VPT 2017). The workshop took place in
Uppsala, Sweden, on April 29th, 2017, affiliated with the European Joint
Conferences on Theory and Practice of Software (ETAPS). The aim of the VPT
workshop series is to provide a forum where people from the areas of program
transformation and program verification can fruitfully exchange ideas and gain
a deeper understanding of the interactions between those two fields. Seven
papers were presented at the workshop. Additionally, three invited talks were
given by Javier Esparza (Technische Universit??t M?¬nchen, Germany), Manuel
Hermenegildo (IMDEA Software Institute, Madrid, Spain), and Alexey Khoroshilov
(Linux Verification Center, ISPRAS, Moscow, Russia).
",1,0,0,0,0,0
808,H-infinity Filtering for Cloud-Aided Semi-active Suspension with Delayed Information,"  This chapter presents an H-infinity filtering framework for cloud-aided
semiactive suspension system with time-varying delays. In this system, road
profile information is downloaded from a cloud database to facilitate onboard
estimation of suspension states. Time-varying data transmission delays are
considered and assumed to be bounded. A quarter-car linear suspension model is
used and an H-infinity filter is designed with both onboard sensor measurements
and delayed road profile information from the cloud. The filter design
procedure is designed based on linear matrix inequalities (LMIs). Numerical
simulation results are reported that illustrates the fusion of cloud-based and
on-board information that can be achieved in Vehicleto- Cloud-to-Vehicle
(V2C2V) implementation.
",1,0,0,0,0,0
809,Deep Spatio-Temporal Random Fields for Efficient Video Segmentation,"  In this work we introduce a time- and memory-efficient method for structured
prediction that couples neuron decisions across both space at time. We show
that we are able to perform exact and efficient inference on a densely
connected spatio-temporal graph by capitalizing on recent advances on deep
Gaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)
efficient, (b) has a unique global minimum, and (c) can be trained end-to-end
alongside contemporary deep networks for video understanding. We experiment
with multiple connectivity patterns in the temporal domain, and present
empirical improvements over strong baselines on the tasks of both semantic and
instance segmentation of videos.
",0,0,0,1,0,0
810,A Coherent vorticity preserving eddy viscosity correction for Large-Eddy Simulation,"  This paper introduces a new approach to Large-Eddy Simulation (LES) where
subgrid-scale (SGS) dissipation is applied proportionally to the degree of
local spectral broadening, hence mitigated or deactivated in regions dominated
by large-scale and/or laminar vortical motion. The proposed Coherent vorticity
preserving (CvP) LES methodology is based on the evaluation of the ratio of the
test-filtered to resolved (or grid-filtered) enstrophy $\sigma$. Values of
$\sigma$ close to 1 indicate low sub-test-filter turbulent activity, justifying
local deactivation of the SGS dissipation. The intensity of the SGS dissipation
is progressively increased for $\sigma < 1$ which corresponds to a small-scale
spectral broadening. The SGS dissipation is then fully activated in developed
turbulence characterized by $\sigma \le \sigma_{eq}$, where the value
$\sigma_{eq}$ is derived assuming a Kolmogorov spectrum. The proposed approach
can be applied to any eddy-viscosity model, is algorithmically simple and
computationally inexpensive. LES of Taylor-Green vortex breakdown demonstrates
that the CvP methodology improves the performance of traditional, non-dynamic
dissipative SGS models, capturing the peak of total turbulent kinetic energy
dissipation during transition. Similar accuracy is obtained by adopting
Germano's dynamic procedure albeit at more than twice the computational
overhead. A CvP-LES of a pair of unstable periodic helical vortices is shown to
predict accurately the experimentally observed growth rate using coarse
resolutions. The ability of the CvP methodology to dynamically sort the
coherent, large-scale motion from the smaller, broadband scales during
transition is demonstrated via flow visualizations. LES of compressible channel
are carried out and show a good match with a reference DNS.
",1,1,0,0,0,0
811,A Universal Marginalizer for Amortized Inference in Generative Models,"  We consider the problem of inference in a causal generative model where the
set of available observations differs between data instances. We show how
combining samples drawn from the graphical model with an appropriate masking
function makes it possible to train a single neural network to approximate all
the corresponding conditional marginal distributions and thus amortize the cost
of inference. We further demonstrate that the efficiency of importance sampling
may be improved by basing proposals on the output of the neural network. We
also outline how the same network can be used to generate samples from an
approximate joint posterior via a chain decomposition of the graph.
",1,0,0,1,0,0
812,Approximating the Backbone in the Weighted Maximum Satisfiability Problem,"  The weighted Maximum Satisfiability problem (weighted MAX-SAT) is a NP-hard
problem with numerous applications arising in artificial intelligence. As an
efficient tool for heuristic design, the backbone has been applied to
heuristics design for many NP-hard problems. In this paper, we investigated the
computational complexity for retrieving the backbone in weighted MAX-SAT and
developed a new algorithm for solving this problem. We showed that it is
intractable to retrieve the full backbone under the assumption that . Moreover,
it is intractable to retrieve a fixed fraction of the backbone as well. And
then we presented a backbone guided local search (BGLS) with Walksat operator
for weighted MAX-SAT. BGLS consists of two phases: the first phase samples the
backbone information from local optima and the backbone phase conducts local
search under the guideline of backbone. Extensive experimental results on the
benchmark showed that BGLS outperforms the existing heuristics in both solution
quality and runtime.
",1,0,0,0,0,0
813,Fulde-Ferrell-Larkin-Ovchinnikov state in spin-orbit-coupled superconductors,"  We show that in the presence of magnetic field, two superconducting phases
with the center-of-mass momentum of Cooper pair parallel to the magnetic field
are induced in spin-orbit-coupled superconductor Li$_2$Pd$_3$B. Specifically,
at small magnetic field, the center-of-mass momentum is induced due to the
energy-spectrum distortion and no unpairing region with vanishing singlet
correlation appears. We refer to this superconducting state as the drift-BCS
state. By further increasing the magnetic field, the superconducting state
falls into the Fulde-Ferrell-Larkin-Ovchinnikov state with the emergence of the
unpairing regions. The observed abrupt enhancement of the center-of-mass
momenta and suppression on the order parameters during the crossover indicate
the first-order phase transition. Enhanced Pauli limit and hence enlarged
magnetic-field regime of the Fulde-Ferrell-Larkin-Ovchinnikov state, due to the
spin-flip terms of the spin-orbit coupling, are revealed. We also address the
triplet correlations induced by the spin-orbit coupling, and show that the
Cooper-pair spin polarizations, generated by the magnetic field and
center-of-mass momentum with the triplet correlations, exhibit totally
different magnetic-field dependences between the drift-BCS and
Fulde-Ferrell-Larkin-Ovchinnikov states.
",0,1,0,0,0,0
814,New type integral inequalities for convex functions with applications II,"  We have recently established some integral inequalities for convex functions
via the Hermite-Hadamard's inequalities. In continuation here, we also
establish some interesting new integral inequalities for convex functions via
the Hermite--Hadamard's inequalities and Jensen's integral inequality. Useful
applications involving special means are also included.
",0,0,1,0,0,0
815,Geometric Insights into Support Vector Machine Behavior using the KKT Conditions,"  The support vector machine (SVM) is a powerful and widely used classification
algorithm. This paper uses the Karush-Kuhn-Tucker conditions to provide
rigorous mathematical proof for new insights into the behavior of SVM. These
insights provide perhaps unexpected relationships between SVM and two other
linear classifiers: the mean difference and the maximal data piling direction.
For example, we show that in many cases SVM can be viewed as a cropped version
of these classifiers. By carefully exploring these connections we show how SVM
tuning behavior is affected by characteristics including: balanced vs.
unbalanced classes, low vs. high dimension, separable vs. non-separable data.
These results provide further insights into tuning SVM via cross-validation by
explaining observed pathological behavior and motivating improved
cross-validation methodology. Finally, we also provide new results on the
geometry of complete data piling directions in high dimensional space.
",0,0,0,1,0,0
816,Smart Grids Data Analysis: A Systematic Mapping Study,"  Data analytics and data science play a significant role in nowadays society.
In the context of Smart Grids (SG), the collection of vast amounts of data has
seen the emergence of a plethora of data analysis approaches. In this paper, we
conduct a Systematic Mapping Study (SMS) aimed at getting insights about
different facets of SG data analysis: application sub-domains (e.g., power load
control), aspects covered (e.g., forecasting), used techniques (e.g.,
clustering), tool-support, research methods (e.g., experiments/simulations),
replicability/reproducibility of research. The final goal is to provide a view
of the current status of research. Overall, we found that each sub-domain has
its peculiarities in terms of techniques, approaches and research methodologies
applied. Simulations and experiments play a crucial role in many areas. The
replicability of studies is limited concerning the provided implemented
algorithms, and to a lower extent due to the usage of private datasets.
",1,0,0,0,0,0
817,"Physical problem solving: Joint planning with symbolic, geometric, and dynamic constraints","  In this paper, we present a new task that investigates how people interact
with and make judgments about towers of blocks. In Experiment~1, participants
in the lab solved a series of problems in which they had to re-configure three
blocks from an initial to a final configuration. We recorded whether they used
one hand or two hands to do so. In Experiment~2, we asked participants online
to judge whether they think the person in the lab used one or two hands. The
results revealed a close correspondence between participants' actions in the
lab, and the mental simulations of participants online. To explain
participants' actions and mental simulations, we develop a model that plans
over a symbolic representation of the situation, executes the plan using a
geometric solver, and checks the plan's feasibility by taking into account the
physical constraints of the scene. Our model explains participants' actions and
judgments to a high degree of quantitative accuracy.
",1,0,0,1,0,0
818,Complex Networks: from Classical to Quantum,"  Recent progress in applying complex network theory to problems faced in
quantum information and computation has resulted in a beneficial crossover
between two fields. Complex network methods have successfully been used to
characterize quantum walk and transport models, entangled communication
networks, graph-theoretic models of emergent space-time and in detecting
mesoscale structure in quantum systems. Information physics is setting the
stage for a theory of complex and networked systems with quantum
information-inspired methods appearing in complex network science, including
information-theoretic distance and correlation measures for network
characterization. Novel quantum induced effects have been predicted in random
graphs---where edges represent entangled links---and quantum computer
algorithms have recently been proposed to offer super-polynomial enhancement
for several network and graph theoretic problems. Here we review the results at
the cutting edge, pinpointing the similarities and reconciling the differences
found in the series of results at the intersection of these two fields.
",1,1,0,0,0,0
819,On the K-theory of C*-algebras for substitution tilings (a pedestrian version),"  Under suitable conditions, a substitution tiling gives rise to a Smale space,
from which three equivalence relations can be constructed, namely the stable,
unstable, and asymptotic equivalence relations. We denote with $S$, $U$, and
$A$ their corresponding $C^*$-algebras in the sense of Renault. In this article
we show that the $K$-theories of $S$ and $U$ can be computed from the
cohomology and homology of a single cochain complex with connecting maps for
tilings of the line and of the plane. Moreover, we provide formulas to compute
the $K$-theory for these three $C^*$-algebras. Furthermore, we show that the
$K$-theory groups for tilings of dimension 1 are always torsion free. For
tilings of dimension 2, only $K_0(U)$ and $K_1(S)$ can contain torsion.
",0,0,1,0,0,0
820,The Tutte embedding of the mated-CRT map converges to Liouville quantum gravity,"  We prove that the Tutte embeddings (a.k.a. harmonic/embeddings) of certain
random planar maps converge to $\gamma$-Liouville quantum gravity
($\gamma$-LQG). Specifically, we treat mated-CRT maps, which are discretized
matings of correlated continuum random trees, and $\gamma$ ranges from $0$ to
$2$ as one varies the correlation parameter. We also show that the associated
space-filling path on the embedded map converges to space-filling
SLE$_{\kappa}$ for $\kappa =16/\gamma^2$ (in the annealed sense) and that
simple random walk on the embedded map converges to Brownian motion (in the
quenched sense). Our arguments also yield analogous statements for the Smith
(square tiling) embedding of the mated-CRT map.
This work constitutes the first proof that a discrete conformal embedding of
a random planar map converges to LQG. Many more such statements have been
conjectured. Since the mated-CRT map can be viewed as a coarse-grained
approximation to other random planar maps (the UIPT, tree-weighted maps,
bipolar-oriented maps, etc.), our results indicate a potential approach for
proving that embeddings of these maps converge to LQG as well.
To prove the main result, we establish several (independently interesting)
theorems about LQG surfaces decorated by space-filling SLE. There is a natural
way to use the SLE curve to divide the plane into `cells' corresponding to
vertices of the mated-CRT map. We study the law of the shape of the
origin-containing cell, in particular proving moments for the ratio of its
squared diameter to its area. We also give bounds on the degree of the
origin-containing cell and establish a form of ergodicity for the entire
configuration. Ultimately, we use these properties to show (using a general
theorem proved in a separate paper) that random walk on these cells converges
to a time change of Brownian motion, which in turn leads to the Tutte embedding
result.
",0,0,1,0,0,0
821,On the Computation of the Shannon Capacity of a Discrete Channel with Noise,"  Muroga [M52] showed how to express the Shannon channel capacity of a discrete
channel with noise [S49] as an explicit function of the transition
probabilities. His method accommodates channels with any finite number of input
symbols, any finite number of output symbols and any transition probability
matrix. Silverman [S55] carried out Muroga's method in the special case of a
binary channel (and went on to analyse ""cascades"" of several such binary
channels).
This article is a note on the resulting formula for the capacity C(a, c) of a
single binary channel. We aim to clarify some of the arguments and correct a
small error. In service of this aim, we first formulate several of Shannon's
definitions and proofs in terms of discrete measure-theoretic probability
theory. We provide an alternate proof to Silverman's, of the feasibility of the
optimal input distribution for a binary channel. For convenience, we also
express C(a, c) in a single expression explicitly dependent on a and c only,
which Silverman stopped short of doing.
",1,0,0,0,0,0
822,Gorenstein homological properties of tensor rings,"  Let $R$ be a two-sided noetherian ring and $M$ be a nilpotent $R$-bimodule,
which is finitely generated on both sides. We study Gorenstein homological
properties of the tensor ring $T_R(M)$. Under certain conditions, the ring $R$
is Gorenstein if and only if so is $T_R(M)$. We characterize Gorenstein
projective $T_R(M)$-modules in terms of $R$-modules.
",0,0,1,0,0,0
823,Compositional Human Pose Regression,"  Regression based methods are not performing as well as detection based
methods for human pose estimation. A central problem is that the structural
information in the pose is not well exploited in the previous regression
methods. In this work, we propose a structure-aware regression approach. It
adopts a reparameterized pose representation using bones instead of joints. It
exploits the joint connection structure to define a compositional loss function
that encodes the long range interactions in the pose. It is simple, effective,
and general for both 2D and 3D pose estimation in a unified setting.
Comprehensive evaluation validates the effectiveness of our approach. It
significantly advances the state-of-the-art on Human3.6M and is competitive
with state-of-the-art results on MPII.
",1,0,0,0,0,0
824,Stochastic functional differential equations and sensitivity to their initial path,"  We consider systems with memory represented by stochastic functional
differential equations. Substantially, these are stochastic differential
equations with coefficients depending on the past history of the process
itself. Such coefficients are hence defined on a functional space. Models with
memory appear in many applications ranging from biology to finance. Here we
consider the results of some evaluations based on these models (e.g. the prices
of some financial products) and the risks connected to the choice of these
models. In particular we focus on the impact of the initial condition on the
evaluations. This problem is known as the analysis of sensitivity to the
initial condition and, in the terminology of finance, it is referred to as the
Delta. In this work the initial condition is represented by the relevant past
history of the stochastic functional differential equation. This naturally
leads to the redesign of the definition of Delta. We suggest to define it as a
functional directional derivative, this is a natural choice. For this we study
a representation formula which allows for its computation without requiring
that the evaluation functional is differentiable. This feature is particularly
relevant for applications. Our formula is achieved by studying an appropriate
relationship between Malliavin derivative and functional directional
derivative. For this we introduce the technique of {\it randomisation of the
initial condition}.
",0,0,1,0,0,0
825,Remarks on Inner Functions and Optimal Approximants,"  We discuss the concept of inner function in reproducing kernel Hilbert spaces
with an orthogonal basis of monomials and examine connections between inner
functions and optimal polynomial approximants to $1/f$, where $f$ is a function
in the space. We revisit some classical examples from this perspective, and
show how a construction of Shapiro and Shields can be modified to produce inner
functions.
",0,0,1,0,0,0
826,A Stochastic Model for File Lifetime and Security in Data Center Networks,"  Data center networks are an important infrastructure in various applications
of modern information technologies. Note that each data center always has a
finite lifetime, thus once a data center fails, then it will lose all its
storage files and useful information. For this, it is necessary to replicate
and copy each important file into other data centers such that this file can
increase its lifetime of staying in a data center network. In this paper, we
describe a large-scale data center network with a file d-threshold policy,
which is to replicate each important file into at most d-1 other data centers
such that this file can maintain in the data center network under a given level
of data security in the long-term. To this end, we develop three relevant
Markov processes to propose two effective methods for assessing the file
lifetime and data security. By using the RG-factorizations, we show that the
two methods are used to be able to more effectively evaluate the file lifetime
of large-scale data center networks. We hope the methodology and results given
in this paper are applicable in the file lifetime study of more general data
center networks with replication mechanism.
",1,0,0,0,0,0
827,Asymptotic Confidence Regions for High-dimensional Structured Sparsity,"  In the setting of high-dimensional linear regression models, we propose two
frameworks for constructing pointwise and group confidence sets for penalized
estimators which incorporate prior knowledge about the organization of the
non-zero coefficients. This is done by desparsifying the estimator as in van de
Geer et al. [18] and van de Geer and Stucky [17], then using an appropriate
estimator for the precision matrix $\Theta$. In order to estimate the precision
matrix a corresponding structured matrix norm penalty has to be introduced.
After normalization the result is an asymptotic pivot.
The asymptotic behavior is studied and simulations are added to study the
differences between the two schemes.
",0,0,1,1,0,0
828,Building a Structured Query Engine,"  Finding patterns in data and being able to retrieve information from those
patterns is an important task in Information retrieval. Complex search
requirements which are not fulfilled by simple string matching and require
exploring certain patterns in data demand a better query engine that can
support searching via structured queries. In this article, we built a
structured query engine which supports searching data through structured
queries on the lines of ElasticSearch. We will show how we achieved real time
indexing and retrieving of data through a RESTful API and how complex queries
can be created and processed using efficient data structures we created for
storing the data in structured way. Finally, we will conclude with an example
of movie recommendation system built on top of this query engine.
",1,0,0,0,0,0
829,A Theoretical Perspective of Solving Phaseless Compressed Sensing via Its Nonconvex Relaxation,"  As a natural extension of compressive sensing and the requirement of some
practical problems, Phaseless Compressed Sensing (PCS) has been introduced and
studied recently. Many theoretical results have been obtained for PCS with the
aid of its convex relaxation. Motivated by successful applications of nonconvex
relaxed methods for solving compressive sensing, in this paper, we try to
investigate PCS via its nonconvex relaxation. Specifically, we relax PCS in the
real context by the corresponding $\ell_p$-minimization with $p\in (0,1)$. We
show that there exists a constant $p^\ast\in (0,1]$ such that for any fixed
$p\in(0, p^\ast)$, every optimal solution to the $\ell_p$-minimization also
solves the concerned problem; and derive an expression of such a constant
$p^\ast$ by making use of the known data and the sparsity level of the
concerned problem. These provide a theoretical basis for solving this class of
problems via the corresponding $\ell_p$-minimization.
",0,0,1,0,0,0
830,A Comprehensive Study of Ly$?ñ$ Emission in the High-redshift Galaxy Population,"  We present an exhaustive census of Lyman alpha (Ly$\alpha$) emission in the
general galaxy population at $3<z<4.6$. We use the Michigan/Magellan Fiber
System (M2FS) spectrograph to study a stellar mass (M$_*$) selected sample of
625 galaxies homogeneously distributed in the range
$7.6<\log{\mbox{M$_*$/M$_{\odot}$}}<10.6$. Our sample is selected from the
3D-HST/CANDELS survey, which provides the complementary data to estimate
Ly$\alpha$ equivalent widths ($W_{Ly\alpha}$) and escape fractions ($f_{esc}$)
for our galaxies. We find both quantities to anti-correlate with M$_*$,
star-formation rate (SFR), UV luminosity, and UV slope ($\beta$). We then model
the $W_{Ly\alpha}$ distribution as a function of M$_{UV}$ and $\beta$ using a
Bayesian approach. Based on our model and matching the properties of typical
Lyman break galaxy (LBG) selections, we conclude that the $W_{Ly\alpha}$
distribution in such samples is heavily dependent on the limiting M$_{UV}$ of
the survey. Regarding narrowband surveys, we find their $W_{Ly\alpha}$
selections to bias samples toward low M$_*$, while their line-flux limitations
preferentially leave out low-SFR galaxies. We can also use our model to predict
the fraction of Ly$\alpha$-emitting LBGs at $4\leqslant z\leqslant 7$. We show
that reported drops in the Ly$\alpha$ fraction at $z\geqslant6$, usually
attributed to the rapidly increasing neutral gas fraction of the universe, can
also be explained by survey M$_{UV}$ incompleteness. This result does not
dismiss reionization occurring at $z\sim7$, but highlights that current data is
not inconsistent with this process taking place at $z>7$.
",0,1,0,0,0,0
831,Optimal Timing of Decisions: A General Theory Based on Continuation Values,"  Building on insights of Jovanovic (1982) and subsequent authors, we develop a
comprehensive theory of optimal timing of decisions based around continuation
value functions and operators that act on them. Optimality results are provided
under general settings, with bounded or unbounded reward functions. This
approach has several intrinsic advantages that we exploit in developing the
theory. One is that continuation value functions are smoother than value
functions, allowing for sharper analysis of optimal policies and more efficient
computation. Another is that, for a range of problems, the continuation value
function exists in a lower dimensional space than the value function,
mitigating the curse of dimensionality. In one typical experiment, this reduces
the computation time from over a week to less than three minutes.
",0,0,1,0,0,0
832,OSIRIS-REx Contamination Control Strategy and Implementation,"  OSIRIS-REx will return pristine samples of carbonaceous asteroid Bennu. This
article describes how pristine was defined based on expectations of Bennu and
on a realistic understanding of what is achievable with a constrained schedule
and budget, and how that definition flowed to requirements and implementation.
To return a pristine sample, the OSIRIS- REx spacecraft sampling hardware was
maintained at level 100 A/2 and <180 ng/cm2 of amino acids and hydrazine on the
sampler head through precision cleaning, control of materials, and vigilance.
Contamination is further characterized via witness material exposed to the
spacecraft assembly and testing environment as well as in space. This
characterization provided knowledge of the expected background and will be used
in conjunction with archived spacecraft components for comparison with the
samples when they are delivered to Earth for analysis. Most of all, the
cleanliness of the OSIRIS-REx spacecraft was achieved through communication
among scientists, engineers, managers, and technicians.
",0,1,0,0,0,0
833,Linear Stochastic Approximation: Constant Step-Size and Iterate Averaging,"  We consider $d$-dimensional linear stochastic approximation algorithms (LSAs)
with a constant step-size and the so called Polyak-Ruppert (PR) averaging of
iterates. LSAs are widely applied in machine learning and reinforcement
learning (RL), where the aim is to compute an appropriate $\theta_{*} \in
\mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$
updates per iteration. In this paper, we are motivated by the problem (in RL)
of policy evaluation from experience replay using the \emph{temporal
difference} (TD) class of learning algorithms that are also LSAs. For LSAs with
a constant step-size, and PR averaging, we provide bounds for the mean squared
error (MSE) after $t$ iterations. We assume that data is \iid with finite
variance (underlying distribution being $P$) and that the expected dynamics is
Hurwitz. For a given LSA with PR averaging, and data distribution $P$
satisfying the said assumptions, we show that there exists a range of constant
step-sizes such that its MSE decays as $O(\frac{1}{t})$.
We examine the conditions under which a constant step-size can be chosen
uniformly for a class of data distributions $\mathcal{P}$, and show that not
all data distributions `admit' such a uniform constant step-size. We also
suggest a heuristic step-size tuning algorithm to choose a constant step-size
of a given LSA for a given data distribution $P$. We compare our results with
related work and also discuss the implication of our results in the context of
TD algorithms that are LSAs.
",1,0,0,1,0,0
834,Curse of Heterogeneity: Computational Barriers in Sparse Mixture Models and Phase Retrieval,"  We study the fundamental tradeoffs between statistical accuracy and
computational tractability in the analysis of high dimensional heterogeneous
data. As examples, we study sparse Gaussian mixture model, mixture of sparse
linear regressions, and sparse phase retrieval model. For these models, we
exploit an oracle-based computational model to establish conjecture-free
computationally feasible minimax lower bounds, which quantify the minimum
signal strength required for the existence of any algorithm that is both
computationally tractable and statistically accurate. Our analysis shows that
there exist significant gaps between computationally feasible minimax risks and
classical ones. These gaps quantify the statistical price we must pay to
achieve computational tractability in the presence of data heterogeneity. Our
results cover the problems of detection, estimation, support recovery, and
clustering, and moreover, resolve several conjectures of Azizyan et al. (2013,
2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our
results reveal a new but counter-intuitive phenomenon in heterogeneous data
analysis that more data might lead to less computation complexity.
",0,0,0,1,0,0
835,Modality Attention for End-to-End Audio-visual Speech Recognition,"  Audio-visual speech recognition (AVSR) system is thought to be one of the
most promising solutions for robust speech recognition, especially in noisy
environment. In this paper, we propose a novel multimodal attention based
method for audio-visual speech recognition which could automatically learn the
fused representation from both modalities based on their importance. Our method
is realized using state-of-the-art sequence-to-sequence (Seq2seq)
architectures. Experimental results show that relative improvements from 2% up
to 36% over the auditory modality alone are obtained depending on the different
signal-to-noise-ratio (SNR). Compared to the traditional feature concatenation
methods, our proposed approach can achieve better recognition performance under
both clean and noisy conditions. We believe modality attention based end-to-end
method can be easily generalized to other multimodal tasks with correlated
information.
",1,0,0,0,0,0
836,Affine Rough Models,"  The goal of this survey article is to explain and elucidate the affine
structure of recent models appearing in the rough volatility literature, and
show how it leads to exponential-affine transform formulas.
",0,0,0,0,0,1
837,When flux standards go wild: white dwarfs in the age of Kepler,"  White dwarf stars have been used as flux standards for decades, thanks to
their staid simplicity. We have empirically tested their photometric stability
by analyzing the light curves of 398 high-probability candidates and
spectroscopically confirmed white dwarfs observed during the original Kepler
mission and later with K2 Campaigns 0-8. We find that the vast majority (>97
per cent) of non-pulsating and apparently isolated white dwarfs are stable to
better than 1 per cent in the Kepler bandpass on 1-hr to 10-d timescales,
confirming that these stellar remnants are useful flux standards. From the
cases that do exhibit significant variability, we caution that binarity,
magnetism, and pulsations are three important attributes to rule out when
establishing white dwarfs as flux standards, especially those hotter than
30,000 K.
",0,1,0,0,0,0
838,Multi-view Low-rank Sparse Subspace Clustering,"  Most existing approaches address multi-view subspace clustering problem by
constructing the affinity matrix on each view separately and afterwards propose
how to extend spectral clustering algorithm to handle multi-view data. This
paper presents an approach to multi-view subspace clustering that learns a
joint subspace representation by constructing affinity matrix shared among all
views. Relying on the importance of both low-rank and sparsity constraints in
the construction of the affinity matrix, we introduce the objective that
balances between the agreement across different views, while at the same time
encourages sparsity and low-rankness of the solution. Related low-rank and
sparsity constrained optimization problem is for each view solved using the
alternating direction method of multipliers. Furthermore, we extend our
approach to cluster data drawn from nonlinear subspaces by solving the
corresponding problem in a reproducing kernel Hilbert space. The proposed
algorithm outperforms state-of-the-art multi-view subspace clustering
algorithms on one synthetic and four real-world datasets.
",1,0,0,1,0,0
839,"Single-trial P300 Classification using PCA with LDA, QDA and Neural Networks","  The P300 event-related potential (ERP), evoked in scalp-recorded
electroencephalography (EEG) by external stimuli, has proven to be a reliable
response for controlling a BCI. The P300 component of an event related
potential is thus widely used in brain-computer interfaces to translate the
subjects' intent by mere thoughts into commands to control artificial devices.
The main challenge in the classification of P300 trials in
electroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of
the P300 response. To overcome the low SNR of individual trials, it is common
practice to average together many consecutive trials, which effectively
diminishes the random noise. Unfortunately, when more repeated trials are
required for applications such as the P300 speller, the communication rate is
greatly reduced. This has resulted in a need for better methods to improve
single-trial classification accuracy of P300 response. In this work, we use
Principal Component Analysis (PCA) as a preprocessing method and use Linear
Discriminant Analysis (LDA)and neural networks for classification. The results
show that a combination of PCA with these methods provided as high as 13\%
accuracy gain for single-trial classification while using only 3 to 4 principal
components.
",1,0,0,1,0,0
840,Stable representations of posets,"  The purpose of this paper is to study stable representations of partially
ordered sets (posets) and compare it to the well known theory for quivers. In
particular, we prove that every indecomposable representation of a poset of
finite type is stable with respect to some weight and construct that weight
explicitly in terms of the dimension vector. We show that if a poset is
primitive then Coxeter transformations preserve stable representations. When
the base field is the field of complex numbers we establish the connection
between the polystable representations and the unitary $\chi$-representations
of posets. This connection explains the similarity of the results obtained in
the series of papers.
",0,0,1,0,0,0
841,Automatic segmentation of trees in dynamic outdoor environments,"  Segmentation in dynamic outdoor environments can be difficult when the
illumination levels and other aspects of the scene cannot be controlled.
Specifically in orchard and vineyard automation contexts, a background material
is often used to shield a camera's field of view from other rows of crops. In
this paper, we describe a method that uses superpixels to determine low texture
regions of the image that correspond to the background material, and then show
how this information can be integrated with the color distribution of the image
to compute optimal segmentation parameters to segment objects of interest.
Quantitative and qualitative experiments demonstrate the suitability of this
approach for dynamic outdoor environments, specifically for tree reconstruction
and apple flower detection applications.
",1,0,0,0,0,0
842,A Practical Bandit Method with Advantages in Neural Network Tuning,"  Stochastic bandit algorithms can be used for challenging non-convex
optimization problems. Hyperparameter tuning of neural networks is particularly
challenging, necessitating new approaches. To this end, we present a method
that adaptively partitions the combined space of hyperparameters, context, and
training resources (e.g., total number of training iterations). By adaptively
partitioning the space, the algorithm is able to focus on the portions of the
hyperparameter search space that are most relevant in a practical way. By
including the resources in the combined space, the method tends to use fewer
training resources overall. Our experiments show that this method can surpass
state-of-the-art methods in tuning neural networks on benchmark datasets. In
some cases, our implementations can achieve the same levels of accuracy on
benchmark datasets as existing state-of-the-art approaches while saving over
50% of our computational resources (e.g. time, training iterations).
",1,0,0,1,0,0
843,Dynamic Security Analysis of Power Systems by a Sampling-Based Algorithm,"  Dynamic security analysis is an important problem of power systems on
ensuring safe operation and stable power supply even when certain faults occur.
No matter such faults are caused by vulnerabilities of system components,
physical attacks, or cyber-attacks that are more related to cyber-security,
they eventually affect the physical stability of a power system. Examples of
the loss of physical stability include the Northeast blackout of 2003 in North
America and the 2015 system-wide blackout in Ukraine. The nonlinear hybrid
nature, that is, nonlinear continuous dynamics integrated with discrete
switching, and the high degree of freedom property of power system dynamics
make it challenging to conduct the dynamic security analysis. In this paper, we
use the hybrid automaton model to describe the dynamics of a power system and
mainly deal with the index-1 differential-algebraic equation models regarding
the continuous dynamics in different discrete states. The analysis problem is
formulated as a reachability problem of the associated hybrid model. A
sampling-based algorithm is then proposed by integrating modeling and
randomized simulation of the hybrid dynamics to search for a feasible execution
connecting an initial state of the post-fault system and a target set in the
desired operation mode. The proposed method enables the use of existing power
system simulators for the synthesis of discrete switching and control
strategies through randomized simulation. The effectiveness and performance of
the proposed approach are demonstrated with an application to the dynamic
security analysis of the New England 39-bus benchmark power system exhibiting
hybrid dynamics. In addition to evaluating the dynamic security, the proposed
method searches for a feasible strategy to ensure the dynamic security of the
system in face of disruptions.
",1,0,0,0,0,0
844,Stochastic and Chance-Constrained Conic Distribution System Expansion Planning Using Bilinear Benders Decomposition,"  Second order conic programming (SOCP) has been used to model various
applications in power systems, such as operation and expansion planning. In
this paper, we present a two-stage stochastic mixed integer SOCP (MISOCP) model
for the distribution system expansion planning problem that considers
uncertainty and also captures the nonlinear AC power flow. To avoid costly
investment plans due to some extreme scenarios, we further present a
chance-constrained variant that could lead to cost-effective solutions. To
address the computational challenge, we extend the basic Benders decomposition
method and develop a bilinear variant to compute stochastic and
chance-constrained MISOCP formulations. A set of numerical experiments is
performed to illustrate the performance of our models and computational
methods. In particular, results show that our Benders decomposition algorithms
drastically outperform a professional MISOCP solver in handling stochastic
scenarios by orders of magnitude.
",0,0,1,0,0,0
845,Multi-Hop Extensions of Energy-Efficient Wireless Sensor Network Time Synchronization,"  We present the multi-hop extensions of the recently proposed energy-efficient
time synchronization scheme for wireless sensor networks, which is based on the
asynchronous source clock frequency recovery and reversed two-way message
exchanges. We consider two hierarchical extensions based on packet relaying and
time-translating gateways, respectively, and analyze their performance with
respect to the number of layers and the delay variations through simulations.
The simulation results demonstrate that the time synchronization performance of
the packet relaying, which has lower complexity, is close to that of
time-translating gateways.
",1,0,0,0,0,0
846,Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content,"  Instructional labs are widely seen as a unique, albeit expensive, way to
teach scientific content. We measured the effectiveness of introductory lab
courses at achieving this educational goal across nine different lab courses at
three very different institutions. These institutions and courses encompassed a
broad range of student populations and instructional styles. The nine courses
studied had two key things in common: the labs aimed to reinforce the content
presented in lectures, and the labs were optional. By comparing the performance
of students who did and did not take the labs (with careful normalization for
selection effects), we found universally and precisely no added value to
learning from taking the labs as measured by course exam performance. This work
should motivate institutions and departments to reexamine the goals and conduct
of their lab courses, given their resource-intensive nature. We show why these
results make sense when looking at the comparative mental processes of students
involved in research and instructional labs, and offer alternative goals and
instructional approaches that would make lab courses more educationally
valuable.
",0,1,0,0,0,0
847,Smoothed Noise and Mexican Hat Coupling Produce Pattern in a Stochastic Neural Field,"  The formation of pattern in biological systems may be modeled by a set of
reaction-diffusion equations. A diffusion-type coupling operator biologically
significant in neuroscience is a difference of Gaussian functions (Mexican Hat
operator) used as a spatial-convolution kernel. We are interested in the
difference among behaviors of \emph{stochastic} neural field equations, namely
space-time stochastic differential-integral equations, and similar
deterministic ones. We explore, quantitatively, how the parameters of our model
that measure the shape of the coupling kernel, coupling strength, and aspects
of the spatially-smoothed space-time noise, control the pattern in the
resulting evolving random field. We find that a spatial pattern that is damped
in time in a deterministic system may be sustained and amplified by
stochasticity, most strikingly at an optimal spatio-temporal noise level. In
addition, we find that spatially-smoothed noise alone causes pattern formation
even without spatial coupling.
",0,0,0,0,1,0
848,NeuroNER: an easy-to-use program for named-entity recognition based on neural networks,"  Named-entity recognition (NER) aims at identifying entities of interest in a
text. Artificial neural networks (ANNs) have recently been shown to outperform
existing NER systems. However, ANNs remain challenging to use for non-expert
users. In this paper, we present NeuroNER, an easy-to-use named-entity
recognition tool based on ANNs. Users can annotate entities using a graphical
web-based user interface (BRAT): the annotations are then used to train an ANN,
which in turn predict entities' locations and categories in new texts. NeuroNER
makes this annotation-training-prediction flow smooth and accessible to anyone.
",1,0,0,1,0,0
849,Macro diversity in Cellular Networks with Random Blockages,"  Blocking objects (blockages) between a transmitter and receiver cause
wireless communication links to transition from line-of-sight (LOS) to
non-line-of-sight (NLOS) propagation, which can greatly reduce the received
power, particularly at higher frequencies such as millimeter wave (mmWave). We
consider a cellular network in which a mobile user attempts to connect to two
or more base stations (BSs) simultaneously, to increase the probability of at
least one LOS link, which is a form of macrodiversity. We develop a framework
for determining the LOS probability as a function of the number of BSs, when
taking into account the correlation between blockages: for example, a single
blockage close to the device -- including the user's own body -- could block
multiple BSs. We consider the impact of the size of blocking objects on the
system reliability probability and show that macrodiversity gains are higher
when the blocking objects are small. We also show that the BS density must
scale as the square of the blockage density to maintain a given level of
reliability.
",1,0,1,0,0,0
850,Opinion dynamics model based on cognitive biases,"  We present an introduction to a novel model of an individual and group
opinion dynamics, taking into account different ways in which different sources
of information are filtered due to cognitive biases. The agent based model,
using Bayesian updating of the individual belief distribution, is based on the
recent psychology work by Dan Kahan. Open nature of the model allows to study
the effects of both static and time-dependent biases and information processing
filters. In particular, the paper compares the effects of two important
psychological mechanisms: the confirmation bias and the politically motivated
reasoning. Depending on the effectiveness of the information filtering (agent
bias), the agents confronted with an objective information source may either
reach a consensus based on the truth, or remain divided despite the evidence.
In general, the model might provide an understanding into the increasingly
polarized modern societies, especially as it allows mixing of different types
of filters: psychological, social, and algorithmic.
",1,1,0,0,0,0
851,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,"  Convolutional Neural Networks (CNNs) are commonly thought to recognise
objects by learning increasingly complex representations of object shapes. Some
recent studies suggest a more important role of image textures. We here put
these conflicting hypotheses to a quantitative test by evaluating CNNs and
human observers on images with a texture-shape cue conflict. We show that
ImageNet-trained CNNs are strongly biased towards recognising textures rather
than shapes, which is in stark contrast to human behavioural evidence and
reveals fundamentally different classification strategies. We then demonstrate
that the same standard architecture (ResNet-50) that learns a texture-based
representation on ImageNet is able to learn a shape-based representation
instead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet.
This provides a much better fit for human behavioural performance in our
well-controlled psychophysical lab setting (nine experiments totalling 48,560
psychophysical trials across 97 observers) and comes with a number of
unexpected emergent benefits such as improved object detection performance and
previously unseen robustness towards a wide range of image distortions,
highlighting advantages of a shape-based representation.
",0,0,0,0,1,0
852,Explicit evaluation of harmonic sums,"  In this paper, we obtain some formulae for harmonic sums, alternating
harmonic sums and Stirling number sums by using the method of integral
representations of series. As applications of these formulae, we give explicit
formula of several quadratic and cubic Euler sums through zeta values and
linear sums. Furthermore, some relationships between harmonic numbers and
Stirling numbers of the first kind are established.
",0,0,1,0,0,0
853,Complete Cyclic Proof Systems for Inductive Entailments,"  In this paper we develop cyclic proof systems for the problem of inclusion
between the least sets of models of mutually recursive predicates, when the
ground constraints in the inductive definitions belong to the quantifier-free
fragments of (i) First Order Logic with the canonical Herbrand interpretation
and (ii) Separation Logic, respectively. Inspired by classical
automata-theoretic techniques of proving language inclusion between tree
automata, we give a small set of inference rules, that are proved to be sound
and complete, under certain semantic restrictions, involving the set of
constraints in the inductive system. Moreover, we investigate the decidability
and computational complexity of these restrictions for all the logical
fragments considered and provide a proof search semi-algorithm that becomes a
decision procedure for the entailment problem, for those systems that fulfill
the restrictions.
",1,0,0,0,0,0
854,HybridNet: Classification and Reconstruction Cooperation for Semi-Supervised Learning,"  In this paper, we introduce a new model for leveraging unlabeled data to
improve generalization performances of image classifiers: a two-branch
encoder-decoder architecture called HybridNet. The first branch receives
supervision signal and is dedicated to the extraction of invariant
class-related representations. The second branch is fully unsupervised and
dedicated to model information discarded by the first branch to reconstruct
input data. To further support the expected behavior of our model, we propose
an original training objective. It favors stability in the discriminative
branch and complementarity between the learned representations in the two
branches. HybridNet is able to outperform state-of-the-art results on CIFAR-10,
SVHN and STL-10 in various semi-supervised settings. In addition,
visualizations and ablation studies validate our contributions and the behavior
of the model on both CIFAR-10 and STL-10 datasets.
",0,0,0,1,0,0
855,Learning Distributed Representations of Texts and Entities from Knowledge Base,"  We describe a neural network model that jointly learns distributed
representations of texts and knowledge base (KB) entities. Given a text in the
KB, we train our proposed model to predict entities that are relevant to the
text. Our model is designed to be generic with the ability to address various
NLP tasks with ease. We train the model using a large corpus of texts and their
entity annotations extracted from Wikipedia. We evaluated the model on three
important NLP tasks (i.e., sentence textual similarity, entity linking, and
factoid question answering) involving both unsupervised and supervised
settings. As a result, we achieved state-of-the-art results on all three of
these tasks. Our code and trained models are publicly available for further
academic research.
",1,0,0,0,0,0
856,Moduli Spaces of Unordered $n\ge5$ Points on the Riemann Sphere and Their Singularities,"  For $n\ge5$, it is well known that the moduli space $\mathfrak{M_{0,\:n}}$ of
unordered $n$ points on the Riemann sphere is a quotient space of the Zariski
open set $K_n$ of $\mathbb C^{n-3}$ by an $S_n$ action. The stabilizers of this
$S_n$ action at certain points of this Zariski open set $K_n$ correspond to the
groups fixing the sets of $n$ points on the Riemann sphere. Let $\alpha$ be a
subset of $n$ distinct points on the Riemann sphere. We call the group of all
linear fractional transformations leaving $\alpha$ invariant the stabilizer of
$\alpha$, which is finite by observation. For each non-trivial finite subgroup
$G$ of the group ${\rm PSL}(2,{\Bbb C})$ of linear fractional transformations,
we give the necessary and sufficient condition for finite subsets of the
Riemann sphere under which the stabilizers of them are conjugate to $G$. We
also prove that there does exist some finite subset of the Riemann sphere whose
stabilizer coincides with $G$. Next we obtain the irreducible decompositions of
the representations of the stabilizers on the tangent spaces at the
singularities of $\mathfrak{M_{0,\:n}}$. At last, on $\mathfrak{M_{0,\:5}}$ and
$\mathfrak{M_{0,\:6}}$, we work out explicitly the singularities and the
representations of their stabilizers on the tangent spaces at them.
",0,0,1,0,0,0
857,Multipath Error Correction in Radio Interferometric Positioning Systems,"  The radio interferometric positioning system (RIPS) is an accurate node
localization method featuring a novel phase-based ranging process. Multipath is
the limiting error source for RIPS in ground-deployed scenarios or indoor
applications. There are four distinct channels involved in the ranging process
for RIPS. Multipath reflections affect both the phase and amplitude of the
ranging signal for each channel. By exploiting untapped amplitude information,
we put forward a scheme to estimate each channel's multipath profile, which is
then subsequently used to correct corresponding errors in phase measurements.
Simulations show that such a scheme is very effective in reducing multipath
phase errors, which are essentially brought down to the level of receiver noise
under moderate multipath conditions. It is further demonstrated that ranging
errors in RIPS are also greatly reduced via the proposed scheme.
",1,0,0,0,0,0
858,An evaluation homomorphism for quantum toroidal gl(n) algebras,"  We present an affine analog of the evaluation map for quantum groups. Namely
we introduce a surjective homomorphism from the quantum toroidal gl(n) algebra
to the quantum affine gl(n) algebra completed with respect to the homogeneous
grading. We give a brief discussion of evaluation modules.
",0,0,1,0,0,0
859,"A Framework for Time-Consistent, Risk-Sensitive Model Predictive Control: Theory and Algorithms","  In this paper we present a framework for risk-sensitive model predictive
control (MPC) of linear systems affected by stochastic multiplicative
uncertainty. Our key innovation is to consider a time-consistent, dynamic risk
evaluation of the cumulative cost as the objective function to be minimized.
This framework is axiomatically justified in terms of time-consistency of risk
assessments, is amenable to dynamic optimization, and is unifying in the sense
that it captures a full range of risk preferences from risk-neutral (i.e.,
expectation) to worst case. Within this framework, we propose and analyze an
online risk-sensitive MPC algorithm that is provably stabilizing. Furthermore,
by exploiting the dual representation of time-consistent, dynamic risk
measures, we cast the computation of the MPC control law as a convex
optimization problem amenable to real-time implementation. Simulation results
are presented and discussed.
",1,0,1,0,0,0
860,Monitoring Telluric Absorption with CAMAL,"  Ground-based astronomical observations may be limited by telluric water vapor
absorption, which is highly variable in time and significantly complicates both
spectroscopy and photometry in the near-infrared (NIR). To achieve the
sensitivity required to detect Earth-sized exoplanets in the NIR, simultaneous
monitoring of precipitable water vapor (PWV) becomes necessary to mitigate the
impact of variable telluric lines on radial velocity measurements and transit
light curves. To address this issue, we present the Camera for the Automatic
Monitoring of Atmospheric Lines (CAMAL), a stand-alone, inexpensive six-inch
aperture telescope dedicated to measuring PWV at the Fred Lawrence Whipple
Observatory on Mount Hopkins. CAMAL utilizes three narrowband NIR filters to
trace the amount of atmospheric water vapor affecting simultaneous observations
with the MINiature Exoplanet Radial Velocity Array (MINERVA) and MINERVA-Red
telescopes. Here we present the current design of CAMAL, discuss our data
analysis methods, and show results from 11 nights of PWV measurements taken
with CAMAL. For seven nights of data, we have independent PWV measurements
extracted from high-resolution stellar spectra taken with the Tillinghast
Reflector Echelle Spectrometer (TRES) also located on Mount Hopkins. We use the
TRES spectra to calibrate the CAMAL absolute PWV scale. Comparisons between
CAMAL and TRES PWV estimates show excellent agreement, matching to within 1 mm
over a 10 mm range in PWV. Analysis of CAMAL's photometric precision propagates
to PWV measurements precise to better than 0.5 mm in dry (PWV < 4 mm)
conditions. We also find that CAMAL-derived PWVs are highly correlated with
those from a GPS-based water vapor monitor located approximately 90 km away at
Kitt Peak National Observatory, with a root mean square PWV difference of 0.8
mm.
",0,1,0,0,0,0
861,Common Knowledge in a Logic of Gossips,"  Gossip protocols aim at arriving, by means of point-to-point or group
communications, at a situation in which all the agents know each other secrets.
Recently a number of authors studied distributed epistemic gossip protocols.
These protocols use as guards formulas from a simple epistemic logic, which
makes their analysis and verification substantially easier.
We study here common knowledge in the context of such a logic. First, we
analyze when it can be reduced to iterated knowledge. Then we show that the
semantics and truth for formulas without nested common knowledge operator are
decidable. This implies that implementability, partial correctness and
termination of distributed epistemic gossip protocols that use non-nested
common knowledge operator is decidable, as well. Given that common knowledge is
equivalent to an infinite conjunction of nested knowledge, these results are
non-trivial generalizations of the corresponding decidability results for the
original epistemic logic, established in (Apt & Wojtczak, 2016).
K. R. Apt & D. Wojtczak (2016): On Decidability of a Logic of Gossips. In
Proc. of JELIA 2016, pp. 18-33, doi:10.1007/ 978-3-319-48758-8_2.
",1,0,0,0,0,0
862,Network-theoretic approach to sparsified discrete vortex dynamics,"  We examine discrete vortex dynamics in two-dimensional flow through a
network-theoretic approach. The interaction of the vortices is represented with
a graph, which allows the use of network-theoretic approaches to identify key
vortex-to-vortex interactions. We employ sparsification techniques on these
graph representations based on spectral theory for constructing sparsified
models and evaluating the dynamics of vortices in the sparsified setup.
Identification of vortex structures based on graph sparsification and sparse
vortex dynamics are illustrated through an example of point-vortex clusters
interacting amongst themselves. We also evaluate the performance of
sparsification with increasing number of point vortices. The
sparsified-dynamics model developed with spectral graph theory requires reduced
number of vortex-to-vortex interactions but agrees well with the full nonlinear
dynamics. Furthermore, the sparsified model derived from the sparse graphs
conserves the invariants of discrete vortex dynamics. We highlight the
similarities and differences between the present sparsified-dynamics model and
the reduced-order models.
",0,1,0,0,0,0
863,B??cklund Transformation and Quasi-Integrable Deformation of Mixed Fermi-Pasta-Ulam and Frenkel-Kontorova Models,"  In this paper we study a non-linear partial differential equation (PDE),
proposed by N. Kudryashov [arXiv:1611.06813v1[nlin.SI]], using continuum limit
approximation of mixed Fermi-Pasta-Ulam and Frenkel-Kontorova Models. This
generalized semi-discrete equation can be considered as a model for the
description of non-linear dislocation waves in crystal lattice and the
corresponding continuous system can be called mixed generalized potential KdV
and sine-Gordon equation. We obtain the B??cklund transformation of this
equation in Riccati form in inverse method. We further study the
quasi-integrable deformation of this model.
",0,1,1,0,0,0
864,Data-Mining Textual Responses to Uncover Misconception Patterns,"  An important, yet largely unstudied, problem in student data analysis is to
detect misconceptions from students' responses to open-response questions.
Misconception detection enables instructors to deliver more targeted feedback
on the misconceptions exhibited by many students in their class, thus improving
the quality of instruction. In this paper, we propose a new natural language
processing-based framework to detect the common misconceptions among students'
textual responses to short-answer questions. We propose a probabilistic model
for students' textual responses involving misconceptions and experimentally
validate it on a real-world student-response dataset. Experimental results show
that our proposed framework excels at classifying whether a response exhibits
one or more misconceptions. More importantly, it can also automatically detect
the common misconceptions exhibited across responses from multiple students to
multiple questions; this property is especially important at large scale, since
instructors will no longer need to manually specify all possible misconceptions
that students might exhibit.
",1,0,0,1,0,0
865,Symmetries of handlebodies and their fixed points: Dihedral extended Schottky groups,"  A Schottky structure on a handlebody $M$ of genus $g$ is provided by a
Schottky group of rank $g$. A symmetry (an orientation-reversing involution) of
$M$ is known to have at most $(g+1)$ connected components of fixed points. Each
of these components is either a point or a compact bordered surface (either
orientable or not) whose boundary is contained in the border of $M$. In this
paper, we derive sharp upper bounds for the total number of connected
components of the sets of fixed points of given two or three symmetries of $M$.
In order to obtain such an upper bound, we obtain a geometrical structure
description of those extended Kleinian groups $K$ containing a Schottky group
$\Gamma$ as finite index normal subgroup so that $K/\Gamma$ is a dihedral group
(called dihedral Schottky groups). Our upper bounds turn out to be different to
the corresponding ones at the level of closed Riemann surfaces. In contrast to
the case of Riemann surfaces, we observe that $M$ cannot have two different
maximal symmetries.
",0,0,1,0,0,0
866,Speaker Diarization using Deep Recurrent Convolutional Neural Networks for Speaker Embeddings,"  In this paper we propose a new method of speaker diarization that employs a
deep learning architecture to learn speaker embeddings. In contrast to the
traditional approaches that build their speaker embeddings using manually
hand-crafted spectral features, we propose to train for this purpose a
recurrent convolutional neural network applied directly on magnitude
spectrograms. To compare our approach with the state of the art, we collect and
release for the public an additional dataset of over 6 hours of fully annotated
broadcast material. The results of our evaluation on the new dataset and three
other benchmark datasets show that our proposed method significantly
outperforms the competitors and reduces diarization error rate by a large
margin of over 30% with respect to the baseline.
",1,0,0,0,0,0
867,A Robust Multi-Batch L-BFGS Method for Machine Learning,"  This paper describes an implementation of the L-BFGS method designed to deal
with two adversarial situations. The first occurs in distributed computing
environments where some of the computational nodes devoted to the evaluation of
the function and gradient are unable to return results on time. A similar
challenge occurs in a multi-batch approach in which the data points used to
compute function and gradients are purposely changed at each iteration to
accelerate the learning process. Difficulties arise because L-BFGS employs
gradient differences to update the Hessian approximations, and when these
gradients are computed using different data points the updating process can be
unstable. This paper shows how to perform stable quasi-Newton updating in the
multi-batch setting, studies the convergence properties for both convex and
nonconvex functions, and illustrates the behavior of the algorithm in a
distributed computing platform on binary classification logistic regression and
neural network training problems that arise in machine learning.
",1,0,1,1,0,0
868,Network of vertically c-oriented prism shaped InN nanowalls grown on c-GaN/sapphire template by chemical vapor deposition technique,"  Networks of vertically c-oriented prism shaped InN nanowalls, are grown on
c-GaN/sapphire templates using a CVD technique, where pure indium and ammonia
are used as metal and nitrogen precursors. A systematic study of the growth,
structural and electronic properties of these samples shows a preferential
growth of the islands along [11-20] and [0001] directions leading to the
formation of such a network structure, where the vertically [0001] oriented
tapered walls are laterally align along one of the three [11-20] directions.
Inclined facets of these walls are identified as r-planes [(1-102)-planes] of
wurtzite InN. Onset of absorption for these samples is observed to be higher
than the band gap of InN suggesting a high background carrier concentration in
this material. Study of the valence band edge through XPS indicates the
formation of positive depletion regions below the r-plane side facets of the
walls. This is in contrast with the observation for c-plane InN epilayers,
where electron accumulation is often reported below the top surface.
",0,1,0,0,0,0
869,Enhanced version of AdaBoostM1 with J48 Tree learning method,"  Machine Learning focuses on the construction and study of systems that can
learn from data. This is connected with the classification problem, which
usually is what Machine Learning algorithms are designed to solve. When a
machine learning method is used by people with no special expertise in machine
learning, it is important that the method be robust in classification, in the
sense that reasonable performance is obtained with minimal tuning of the
problem at hand. Algorithms are evaluated based on how robust they can classify
the given data. In this paper, we propose a quantifiable measure of robustness,
and describe a particular learning method that is robust according to this
measure in the context of classification problem. We proposed Adaptive Boosting
(AdaBoostM1) with J48(C4.5 tree) as a base learner with tuning weight threshold
(P) and number of iterations (I) for boosting algorithm. To benchmark the
performance, we used the baseline classifier, AdaBoostM1 with Decision Stump as
base learner without tuning parameters. By tuning parameters and using J48 as
base learner, we are able to reduce the overall average error rate ratio
(errorC/errorNB) from 2.4 to 0.9 for development sets of data and 2.1 to 1.2
for evaluation sets of data.
",0,0,0,1,0,0
870,A Game of Tax Evasion: evidences from an agent-based model,"  This paper presents a simple agent-based model of an economic system,
populated by agents playing different games according to their different view
about social cohesion and tax payment. After a first set of simulations,
correctly replicating results of existing literature, a wider analysis is
presented in order to study the effects of a dynamic-adaptation rule, in which
citizens may possibly decide to modify their individual tax compliance
according to individual criteria, such as, the strength of their ethical
commitment, the satisfaction gained by consumption of the public good and the
perceived opinion of neighbors. Results show the presence of thresholds levels
in the composition of society - between taxpayers and evaders - which explain
the extent of damages deriving from tax evasion.
",0,0,0,0,0,1
871,Variability response functions for statically determinate beams with arbitrary nonlinear constitutive laws,"  The variability response function (VRF) is generalized to statically
determinate Euler Bernoulli beams with arbitrary stress-strain laws following
Cauchy elastic behavior. The VRF is a Green's function that maps the spectral
density function (SDF) of a statistically homogeneous random field describing
the correlation structure of input uncertainty to the variance of a response
quantity. The appeal of such Green's functions is that the variance can be
determined for any correlation structure by a trivial computation of a
convolution integral. The method introduced in this work derives VRFs in closed
form for arbitrary nonlinear Cauchy-elastic constitutive laws and is
demonstrated through three examples. It is shown why and how higher order
spectra of the random field affect the response variance for nonlinear
constitutive laws. In the general sense, the VRF for a statically determinate
beam is found to be a matrix kernel whose inner product by a matrix of higher
order SDFs and statistical moments is integrated to give the response variance.
The resulting VRF matrix is unique regardless of the random field's marginal
probability density function (PDF) and SDFs.
",0,1,0,0,0,0
872,Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,"  We show that training a deep network using batch normalization is equivalent
to approximate inference in Bayesian models. We further demonstrate that this
finding allows us to make meaningful estimates of the model uncertainty using
conventional architectures, without modifications to the network or the
training procedure. Our approach is thoroughly validated by measuring the
quality of uncertainty in a series of empirical experiments on different tasks.
It outperforms baselines with strong statistical significance, and displays
competitive performance with recent Bayesian approaches.
",0,0,0,1,0,0
873,Multi-Antenna Coded Caching,"  In this paper we consider a single-cell downlink scenario where a
multiple-antenna base station delivers contents to multiple cache-enabled user
terminals. Based on the multicasting opportunities provided by the so-called
Coded Caching technique, we investigate three delivery approaches. Our baseline
scheme employs the coded caching technique on top of max-min fair multicasting.
The second one consists of a joint design of Zero-Forcing (ZF) and coded
caching, where the coded chunks are formed in the signal domain (complex
field). The third scheme is similar to the second one with the difference that
the coded chunks are formed in the data domain (finite field). We derive
closed-form rate expressions where our results suggest that the latter two
schemes surpass the first one in terms of Degrees of Freedom (DoF). However, at
the intermediate SNR regime forming coded chunks in the signal domain results
in power loss, and will deteriorate throughput of the second scheme. The main
message of our paper is that the schemes performing well in terms of DoF may
not be directly appropriate for intermediate SNR regimes, and modified schemes
should be employed.
",1,0,1,0,0,0
874,Convolved subsampling estimation with applications to block bootstrap,"  The block bootstrap approximates sampling distributions from dependent data
by resampling data blocks. A fundamental problem is establishing its
consistency for the distribution of a sample mean, as a prototypical statistic.
We use a structural relationship with subsampling to characterize the bootstrap
in a new and general manner. While subsampling and block bootstrap differ, the
block bootstrap distribution of a sample mean equals that of a $k$-fold
self-convolution of a subsampling distribution. Motivated by this, we provide
simple necessary and sufficient conditions for a convolved subsampling
estimator to produce a normal limit that matches the target of bootstrap
estimation. These conditions may be linked to consistency properties of an
original subsampling distribution, which are often obtainable under minimal
assumptions. Through several examples, the results are shown to validate the
block bootstrap for means under significantly weakened assumptions in many
existing (and some new) dependence settings, which also addresses a standing
conjecture of Politis, Romano and Wolf(1999). Beyond sample means, the
convolved subsampling estimator may not match the block bootstrap, but instead
provides a hybrid-resampling estimator of interest in its own right. For
general statistics with normal limits, results also establish the consistency
of convolved subsampling under minimal dependence conditions, including
non-stationarity.
",0,0,1,1,0,0
875,Computing Simple Multiple Zeros of Polynomial Systems,"  Given a polynomial system f associated with a simple multiple zero x of
multiplicity {\mu}, we give a computable lower bound on the minimal distance
between the simple multiple zero x and other zeros of f. If x is only given
with limited accuracy, we propose a numerical criterion that f is certified to
have {\mu} zeros (counting multiplicities) in a small ball around x.
Furthermore, for simple double zeros and simple triple zeros whose Jacobian is
of normalized form, we define modified Newton iterations and prove the
quantified quadratic convergence when the starting point is close to the exact
simple multiple zero. For simple multiple zeros of arbitrary multiplicity whose
Jacobian matrix may not have a normalized form, we perform unitary
transformations and modified Newton iterations, and prove its non-quantified
quadratic convergence and its quantified convergence for simple triple zeros.
",0,0,1,0,0,0
876,Latent Geometry and Memorization in Generative Models,"  It can be difficult to tell whether a trained generative model has learned to
generate novel examples or has simply memorized a specific set of outputs. In
published work, it is common to attempt to address this visually, for example
by displaying a generated example and its nearest neighbor(s) in the training
set (in, for example, the L2 metric). As any generative model induces a
probability density on its output domain, we propose studying this density
directly. We first study the geometry of the latent representation and
generator, relate this to the output density, and then develop techniques to
compute and inspect the output density. As an application, we demonstrate that
""memorization"" tends to a density made of delta functions concentrated on the
memorized examples. We note that without first understanding the geometry, the
measurement would be essentially impossible to make.
",1,0,0,1,0,0
877,Refracting Metasurfaces without Spurious Diffraction,"  Refraction represents one of the most fundamental operations that may be
performed by a metasurface. However, simple phasegradient metasurface designs
suffer from restricted angular deflection due to spurious diffraction orders.
It has been recently shown, using a circuit-based approach, that refraction
without spurious diffraction, or diffraction-free, can fortunately be achieved
by a transverse metasurface exhibiting either loss-gain or bianisotropy. Here,
we rederive these conditions using a medium-based - and hence more insightfull
- approach based on Generalized Sheet Transition Conditions (GSTCs) and surface
susceptibility tensors, and experimentally demonstrate two diffraction-free
refractive metasurfaces that are essentially lossless, passive, bianisotropic
and reciprocal.
",0,1,0,0,0,0
878,Accurate Motion Estimation through Random Sample Aggregated Consensus,"  We reconsider the classic problem of estimating accurately a 2D
transformation from point matches between images containing outliers. RANSAC
discriminates outliers by randomly generating minimalistic sampled hypotheses
and verifying their consensus over the input data. Its response is based on the
single hypothesis that obtained the largest inlier support. In this article we
show that the resulting accuracy can be improved by aggregating all generated
hypotheses. This yields RANSAAC, a framework that improves systematically over
RANSAC and its state-of-the-art variants by statistically aggregating
hypotheses. To this end, we introduce a simple strategy that allows to rapidly
average 2D transformations, leading to an almost negligible extra computational
cost. We give practical applications on projective transforms and
homography+distortion models and demonstrate a significant performance gain in
both cases.
",1,0,0,0,0,0
879,Landau Collision Integral Solver with Adaptive Mesh Refinement on Emerging Architectures,"  The Landau collision integral is an accurate model for the small-angle
dominated Coulomb collisions in fusion plasmas. We investigate a high order
accurate, fully conservative, finite element discretization of the nonlinear
multi-species Landau integral with adaptive mesh refinement using the PETSc
library (www.mcs.anl.gov/petsc). We develop algorithms and techniques to
efficiently utilize emerging architectures with an approach that minimizes
memory usage and movement and is suitable for vector processing. The Landau
collision integral is vectorized with Intel AVX-512 intrinsics and the solver
sustains as much as 22% of the theoretical peak flop rate of the Second
Generation Intel Xeon Phi, Knights Landing, processor.
",1,0,0,0,0,0
880,A Topologist's View of Kinematic Maps and Manipulation Complexity,"  In this paper we combine a survey of the most important topological
properties of kinematic maps that appear in robotics, with the exposition of
some basic results regarding the topological complexity of a map. In
particular, we discuss mechanical devices that consist of rigid parts connected
by joints and show how the geometry of the joints determines the forward
kinematic map that relates the configuration of joints with the pose of the
end-effector of the device. We explain how to compute the dimension of the
joint space and describe topological obstructions for a kinematic map to be a
fibration or to admit a continuous section. In the second part of the paper we
define the complexity of a continuous map and show how the concept can be
viewed as a measure of the difficulty to find a robust manipulation plan for a
given mechanical device. We also derive some basic estimates for the complexity
and relate it to the degree of instability of a manipulation plan.
",1,0,1,0,0,0
881,Faster Tensor Canonicalization,"  The Butler-Portugal algorithm for obtaining the canonical form of a tensor
expression with respect to slot symmetries and dummy-index renaming suffers, in
certain cases with a high degree of symmetry, from $O(n!)$ explosion in both
computation time and memory. We present a modified algorithm which alleviates
this problem in the most common cases---tensor expressions with subsets of
indices which are totally symmetric or totally antisymmetric---in polynomial
time. We also present an implementation of the label-renaming mechanism which
improves upon that of the original Butler-Portugal algorithm, thus providing a
significant speed increase for the average case as well as the highly-symmetric
special case. The worst-case behavior remains $O(n!)$, although it occurs in
more limited situations unlikely to appear in actual computations. We comment
on possible strategies to take if the nature of a computation should make these
situations more likely.
",1,0,0,0,0,0
882,Mass Preconditioning for the Exact One-Flavor Action in Lattice QCD with Domain-Wall Fermion,"  The mass-preconditioning (MP) technique has become a standard tool to enhance
the efficiency of the hybrid Monte-Carlo simulation (HMC) of lattice QCD with
dynamical quarks, for 2-flavors QCD with degenerate quark masses, as well as
its extension to the case of one-flavor by taking the square-root of the
fermion determinant of 2-flavors with degenerate masses. However, for lattice
QCD with domain-wall fermion, the fermion determinant of any single fermion
flavor can be expressed as a functional integral with an exact pseudofermion
action $ \phi^\dagger H^{-1} \phi $, where $ H^{-1} $ is a positive-definite
Hermitian operator without taking square-root, and with the chiral structure
\cite{Chen:2014hyy}. Consequently, the mass-preconditioning for the exact
one-flavor action (EOFA) does not necessarily follow the conventional (old) MP
pattern. In this paper, we present a new mass-preconditioning for the EOFA,
which is more efficient than the old MP which we have used in Refs.
\cite{Chen:2014hyy,Chen:2014bbc}. We perform numerical tests in lattice QCD
with $ N_f = 1 $ and $ N_f = 1+1+1+1 $ optimal domain-wall quarks, with one
mass-preconditioner applied to one of the exact one-flavor actions, and we find
that the efficiency of the new MP is more than 20\% higher than that of the old
MP.
",0,1,0,0,0,0
883,Finite-time scaling at the Anderson transition for vibrations in solids,"  A model in which a three-dimensional elastic medium is represented by a
network of identical masses connected by springs of random strengths and
allowed to vibrate only along a selected axis of the reference frame, exhibits
an Anderson localization transition. To study this transition, we assume that
the dynamical matrix of the network is given by a product of a sparse random
matrix with real, independent, Gaussian-distributed non-zero entries and its
transpose. A finite-time scaling analysis of system's response to an initial
excitation allows us to estimate the critical parameters of the localization
transition. The critical exponent is found to be $\nu = 1.57 \pm 0.02$ in
agreement with previous studies of Anderson transition belonging to the
three-dimensional orthogonal universality class.
",0,1,0,0,0,0
884,Universal and shape dependent features of surface superconductivity,"  We analyze the response of a type II superconducting wire to an external
magnetic field parallel to it in the framework of Ginzburg-Landau theory. We
focus on the surface superconductivity regime of applied field between the
second and third critical values, where the superconducting state survives only
close to the sample's boundary. Our first finding is that, in first
approximation, the shape of the boundary plays no role in determining the
density of superconducting electrons. A second order term is however isolated,
directly proportional to the mean curvature of the boundary. This demonstrates
that points of higher boundary curvature (counted inwards) attract
superconducting electrons.
",0,1,1,0,0,0
885,Weighted Low Rank Approximation for Background Estimation Problems,"  Classical principal component analysis (PCA) is not robust to the presence of
sparse outliers in the data. The use of the $\ell_1$ norm in the Robust PCA
(RPCA) method successfully eliminates the weakness of PCA in separating the
sparse outliers. In this paper, by sticking a simple weight to the Frobenius
norm, we propose a weighted low rank (WLR) method to avoid the often
computationally expensive algorithms relying on the $\ell_1$ norm. As a proof
of concept, a background estimation model has been presented and compared with
two $\ell_1$ norm minimization algorithms. We illustrate that as long as a
simple weight matrix is inferred from the data, one can use the weighted
Frobenius norm and achieve the same or better performance.
",0,0,1,0,0,0
886,Controlling Chiral Domain Walls in Antiferromagnets Using Spin-Wave Helicity,"  In antiferromagnets, the Dzyaloshinskii-Moriya interaction lifts the
degeneracy of left- and right-circularly polarized spin waves. This
relativistic coupling increases the efficiency of spin-wave-induced domain wall
motion and leads to higher drift velocities. We show that in biaxial
antiferromagnets, the spin-wave helicity controls both the direction and
magnitude of the magnonic force on chiral domain walls. By contrast, in
uniaxial antiferromagnets, the magnonic force is propulsive with a helicity
dependent strength.
",0,1,0,0,0,0
887,The Spatial Shape of Avalanches,"  In disordered elastic systems, driven by displacing a parabolic confining
potential adiabatically slowly, all advance of the system is in bursts, termed
avalanches. Avalanches have a finite extension in time, which is much smaller
than the waiting-time between them. Avalanches also have a finite extension
$\ell$ in space, i.e. only a part of the interface of size $\ell$ moves during
an avalanche. Here we study their spatial shape $\left< S(x)\right>_{\ell}$
given $\ell$, as well as its fluctuations encoded in the second cumulant
$\left< S^{2}(x)\right>_{\ell}^{\rm c}$. We establish scaling relations
governing the behavior close to the boundary. We then give analytic results for
the Brownian force model, in which the microscopic disorder for each degree of
freedom is a random walk. Finally, we confirm these results with numerical
simulations. To do this properly we elucidate the influence of discretization
effects, which also confirms the assumptions entering into the scaling ansatz.
This allows us to reach the scaling limit already for avalanches of moderate
size. We find excellent agreement for the universal shape, its fluctuations,
including all amplitudes.
",0,1,0,0,0,0
888,Multiple Topological Electronic Phases in Superconductor MoC,"  The search for a superconductor with non-s-wave pairing is important not only
for understanding unconventional mechanisms of superconductivity but also for
finding new types of quasiparticles such as Majorana bound states. Materials
with both topological band structure and superconductivity are promising
candidates as $p+ip$ superconducting states can be generated through pairing
the spin-polarized topological surface states. In this work, the electronic and
phonon properties of the superconductor molybdenum carbide (MoC) are studied
with first-principles methods. Our calculations show that nontrivial band
topology and superconductivity coexist in both structural phases of MoC,
namely, the cubic $\alpha$ and hexagonal $\gamma$ phases. The $\alpha$ phase is
a strong topological insulator and the $\gamma$ phase is a topological nodal
line semimetal with drumhead surface states. In addition, hole doping can
stabilize the crystal structure of the $\alpha$ phase and elevate the
transition temperature in the $\gamma$ phase. Therefore, MoC in different
structural forms can be a practical material platform for studying topological
superconductivity and elusive Majorana fermions.
",0,1,0,0,0,0
889,Extension complexity of stable set polytopes of bipartite graphs,"  The extension complexity $\mathsf{xc}(P)$ of a polytope $P$ is the minimum
number of facets of a polytope that affinely projects to $P$. Let $G$ be a
bipartite graph with $n$ vertices, $m$ edges, and no isolated vertices. Let
$\mathsf{STAB}(G)$ be the convex hull of the stable sets of $G$. It is easy to
see that $n \leqslant \mathsf{xc} (\mathsf{STAB}(G)) \leqslant n+m$. We improve
both of these bounds. For the upper bound, we show that $\mathsf{xc}
(\mathsf{STAB}(G))$ is $O(\frac{n^2}{\log n})$, which is an improvement when
$G$ has quadratically many edges. For the lower bound, we prove that
$\mathsf{xc} (\mathsf{STAB}(G))$ is $\Omega(n \log n)$ when $G$ is the
incidence graph of a finite projective plane. We also provide examples of
$3$-regular bipartite graphs $G$ such that the edge vs stable set matrix of $G$
has a fooling set of size $|E(G)|$.
",1,0,0,0,0,0
890,Performance Scaling Law for Multi-Cell Multi-User Massive MIMO,"  This work provides a comprehensive scaling law based performance analysis for
multi-cell multi-user massive multiple-input-multiple-output (MIMO) downlink
systems. Imperfect channel state information (CSI), pilot contamination, and
channel spatial correlation are all considered. First, a sum- rate lower bound
is derived by exploiting the asymptotically deterministic property of the
received signal power, while keeping the random nature of other components in
the signal-to-interference-plus-noise-ratio (SINR) intact. Via a general
scaling model on important network parameters, including the number of users,
the channel training energy and the data transmission power, with respect to
the number of base station antennas, the asymptotic scaling law of the
effective SINR is obtained, which reveals quantitatively the tradeoff of the
network parameters. More importantly, pilot contamination and pilot
contamination elimination (PCE) are considered in the analytical framework. In
addition, the applicability of the derived asymptotic scaling law in practical
systems with large but finite antenna numbers are discussed. Finally,
sufficient conditions on the parameter scalings for the SINR to be
asymptotically deterministic in the sense of mean square convergence are
provided, which covers existing results on such analysis as special cases and
shows the effect of PCE explicitly.
",1,0,0,0,0,0
891,On the Status of the Measurement Problem: Recalling the Relativistic Transactional Interpretation,"  In view of a resurgence of concern about the measurement problem, it is
pointed out that the Relativistic Transactional Interpretation (RTI) remedies
issues previously considered as drawbacks or refutations of the original TI.
Specifically, once one takes into account relativistic processes that are not
representable at the non-relativistic level (such as particle creation and
annihilation, and virtual propagation), absorption is quantitatively defined in
unambiguous physical terms. In addition, specifics of the relativistic
transactional model demonstrate that the Maudlin `contingent absorber'
challenge to the original TI cannot even be mounted: basic features of
established relativistic field theories (in particular, the asymmetry between
field sources and the bosonic fields, and the fact that slow-moving bound
states, such as atoms, are not offer waves) dictate that the `slow-moving offer
wave' required for the challenge scenario cannot exist. It is concluded that
issues previously considered obstacles for TI are no longer legitimately viewed
as such, and that reconsideration of the transactional picture is warranted in
connection with solving the measurement problem.
",0,1,0,0,0,0
892,Pressure-tuning of bond-directional exchange interactions and magnetic frustration in hyperhoneycomb iridate $?ý$-$\mathrm{Li_2IrO_3}$,"  We explore the response of Ir $5d$ orbitals to pressure in
$\beta$-$\mathrm{Li_2IrO_3}$, a hyperhoneycomb iridate in proximity to a Kitaev
quantum spin liquid (QSL) ground state. X-ray absorption spectroscopy reveals a
reconstruction of the electronic ground state below 2 GPa, the same pressure
range where x-ray magnetic circular dichroism shows an apparent collapse of
magnetic order. The electronic reconstruction, which manifests a reduction in
the effective spin-orbit (SO) interaction in $5d$ orbitals, pushes
$\beta$-$\mathrm{Li_2IrO_3}$ further away from the pure $J_{\rm eff}=1/2$
limit. Although lattice symmetry is preserved across the electronic transition,
x-ray diffraction shows a highly anisotropic compression of the hyperhoneycomb
lattice which affects the balance of bond-directional Ir-Ir exchange
interactions driven by spin-orbit coupling at Ir sites. An enhancement of
symmetric anisotropic exchange over Kitaev and Heisenberg exchange interactions
seen in theoretical calculations that use precisely this anisotropic Ir-Ir bond
compression provides one possible route to realization of a QSL state in this
hyperhoneycomb iridate at high pressures.
",0,1,0,0,0,0
893,On perpetuities with gamma-like tails,"  An infinite convergent sum of independent and identically distributed random
variables discounted by a multiplicative random walk is called perpetuity,
because of a possible actuarial application. We give three disjoint groups of
sufficient conditions which ensure that the distribution right tail of a
perpetuity $\mathbb{P}\{X>x\}$ is asymptotic to $ax^ce^{-bx}$ as $x\to\infty$
for some $a,b>0$ and $c\in\mathbb{R}$. Our results complement those of Denisov
and Zwart [J. Appl. Probab. 44 (2007), 1031--1046]. As an auxiliary tool we
provide criteria for the finiteness of the one-sided exponential moments of
perpetuities. Several examples are given in which the distributions of
perpetuities are explicitly identified.
",0,0,1,0,0,0
894,Computationally Efficient Estimation of the Spectral Gap of a Markov Chain,"  We consider the problem of estimating from sample paths the absolute spectral
gap $\gamma_*$ of a reversible, irreducible and aperiodic Markov chain
$(X_t)_{t \in \mathbb{N}}$ over a finite state $\Omega$. We propose the ${\tt
UCPI}$ (Upper Confidence Power Iteration) algorithm for this problem, a
low-complexity algorithm which estimates the spectral gap in time ${\cal O}(n)$
and memory space ${\cal O}((\ln n)^2)$ given $n$ samples. This is in stark
contrast with most known methods which require at least memory space ${\cal
O}(|\Omega|)$, so that they cannot be applied to large state spaces.
Furthermore, ${\tt UCPI}$ is amenable to parallel implementation.
",0,0,0,1,0,0
895,Mixed Precision Training of Convolutional Neural Networks using Integer Operations,"  The state-of-the-art (SOTA) for mixed precision training is dominated by
variants of low precision floating point operations, and in particular, FP16
accumulating into FP32 Micikevicius et al. (2017). On the other hand, while a
lot of research has also happened in the domain of low and mixed-precision
Integer training, these works either present results for non-SOTA networks (for
instance only AlexNet for ImageNet-1K), or relatively small datasets (like
CIFAR-10). In this work, we train state-of-the-art visual understanding neural
networks on the ImageNet-1K dataset, with Integer operations on General Purpose
(GP) hardware. In particular, we focus on Integer Fused-Multiply-and-Accumulate
(FMA) operations which take two pairs of INT16 operands and accumulate results
into an INT32 output.We propose a shared exponent representation of tensors and
develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network
operations. The nuances of developing an efficient integer convolution kernel
is examined, including methods to handle overflow of the INT32 accumulator. We
implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and
these networks achieve or exceed SOTA accuracy within the same number of
iterations as their FP32 counterparts without any change in hyper-parameters
and with a 1.8X improvement in end-to-end training throughput. To the best of
our knowledge these results represent the first INT16 training results on GP
hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported
accuracy using half-precision
",1,0,0,0,0,0
896,Performance of a small size telescope (SST-1M) camera for gamma-ray astronomy with the Cherenkov Telescope Array,"  The foreseen implementations of the Small Size Telescopes (SST) in CTA will
provide unique insights into the highest energy gamma rays offering fundamental
means to discover and under- stand the sources populating the Galaxy and our
local neighborhood. Aiming at such a goal, the SST-1M is one of the three
different implementations that are being prototyped and tested for CTA. SST-1M
is a Davies-Cotton single mirror telescope equipped with a unique camera
technology based on SiPMs with demonstrated advantages over classical
photomultipliers in terms of duty-cycle. In this contribution, we describe the
telescope components, the camera, and the trigger and readout system. The
results of the commissioning of the camera using a dedicated test setup are
then presented. The performances of the camera first prototype in terms of
expected trigger rates and trigger efficiencies for different night-sky
background conditions are presented, and the camera response is compared to
end-to-end simulations.
",0,1,0,0,0,0
897,Bounded solutions for a class of Hamiltonian systems,"  We obtain bounded for all $t$ solutions of ordinary differential equations as
limits of the solutions of the corresponding Dirichlet problems on $(-L,L)$,
with $L \rightarrow \infty$. We derive a priori estimates for the Dirichlet
problems, allowing passage to the limit, via a diagonal sequence. This approach
carries over to the PDE case.
",0,0,1,0,0,0
898,Cosmological perturbation effects on gravitational-wave luminosity distance estimates,"  Waveforms of gravitational waves provide information about a variety of
parameters for the binary system merging. However, standard calculations have
been performed assuming a FLRW universe with no perturbations. In reality this
assumption should be dropped: we show that the inclusion of cosmological
perturbations translates into corrections to the estimate of astrophysical
parameters derived for the merging binary systems. We compute corrections to
the estimate of the luminosity distance due to velocity, volume, lensing and
gravitational potential effects. Our results show that the amplitude of the
corrections will be negligible for current instruments, mildly important for
experiments like the planned DECIGO, and very important for future ones such as
the Big Bang Observer.
",0,1,0,0,0,0
899,Biocompatible Writing of Data into DNA,"  A simple DNA-based data storage scheme is demonstrated in which information
is written using ""addressing"" oligonucleotides. In contrast to other methods
that allow arbitrary code to be stored, the resulting DNA is suitable for
downstream enzymatic and biological processing. This capability is crucial for
DNA computers, and may allow for a diverse array of computational operations to
be carried out using this DNA. Although here we use gel-based methods for
information readout, we also propose more advanced methods involving
protein/DNA complexes and atomic force microscopy/nano-pore schemes for data
readout.
",1,1,0,0,0,0
900,Faster Clustering via Non-Backtracking Random Walks,"  This paper presents VEC-NBT, a variation on the unsupervised graph clustering
technique VEC, which improves upon the performance of the original algorithm
significantly for sparse graphs. VEC employs a novel application of the
state-of-the-art word2vec model to embed a graph in Euclidean space via random
walks on the nodes of the graph. In VEC-NBT, we modify the original algorithm
to use a non-backtracking random walk instead of the normal backtracking random
walk used in VEC. We introduce a modification to a non-backtracking random
walk, which we call a begrudgingly-backtracking random walk, and show
empirically that using this model of random walks for VEC-NBT requires shorter
walks on the graph to obtain results with comparable or greater accuracy than
VEC, especially for sparser graphs.
",1,0,0,1,0,0
901,Nanostructured complex oxides as a route towards thermal behavior in artificial spin ice systems,"  We have used soft x-ray photoemission electron microscopy to image the
magnetization of single domain La$_{0.7}$Sr$_{0.3}$MnO$_{3}$ nano-islands
arranged in geometrically frustrated configurations such as square ice and
kagome ice geometries. Upon thermal randomization, ensembles of nano-islands
with strong inter-island magnetic coupling relax towards low-energy
configurations. Statistical analysis shows that the likelihood of ensembles
falling into low-energy configurations depends strongly on the annealing
temperature. Annealing to just below the Curie temperature of the ferromagnetic
film (T$_{C}$ = 338 K) allows for a much greater probability of achieving low
energy configurations as compared to annealing above the Curie temperature. At
this thermally active temperature of 325 K, the ensemble of ferromagnetic
nano-islands explore their energy landscape over time and eventually transition
to lower energy states as compared to the frozen-in configurations obtained
upon cooling from above the Curie temperature. Thus, this materials system
allows for a facile method to systematically study thermal evolution of
artificial spin ice arrays of nano-islands at temperatures modestly above room
temperature.
",0,1,0,0,0,0
902,Finding Submodularity Hidden in Symmetric Difference,"  A set function $f$ on a finite set $V$ is submodular if $f(X) + f(Y) \geq f(X
\cup Y) + f(X \cap Y)$ for any pair $X, Y \subseteq V$. The symmetric
difference transformation (SD-transformation) of $f$ by a canonical set $S
\subseteq V$ is a set function $g$ given by $g(X) = f(X \vartriangle S)$ for $X
\subseteq V$,where $X \vartriangle S = (X \setminus S) \cup (S \setminus X)$
denotes the symmetric difference between $X$ and $S$. Submodularity and
SD-transformations are regarded as the counterparts of convexity and affine
transformations in a discrete space, respectively. However, submodularity is
not preserved under SD-transformations, in contrast to the fact that convexity
is invariant under affine transformations. This paper presents a
characterization of SD-stransformations preserving submodularity. Then, we are
concerned with the problem of discovering a canonical set $S$, given the
SD-transformation $g$ of a submodular function $f$ by $S$, provided that $g(X)$
is given by a function value oracle. A submodular function $f$ on $V$ is said
to be strict if $f(X) + f(Y) > f(X \cup Y) + f(X \cap Y)$ holds whenever both
$X \setminus Y$ and $Y \setminus X$ are nonempty. We show that the problem is
solved by using ${\rm O}(|V|)$ oracle calls when $f$ is strictly submodular,
although it requires exponentially many oracle calls in general.
",1,0,0,0,0,0
903,On the Consistency of Quick Shift,"  Quick Shift is a popular mode-seeking and clustering algorithm. We present
finite sample statistical consistency guarantees for Quick Shift on mode and
cluster recovery under mild distributional assumptions. We then apply our
results to construct a consistent modal regression algorithm.
",1,0,0,1,0,0
904,Adaptive Quantization for Deep Neural Network,"  In recent years Deep Neural Networks (DNNs) have been rapidly developed in
various applications, together with increasingly complex architectures. The
performance gain of these DNNs generally comes with high computational costs
and large memory consumption, which may not be affordable for mobile platforms.
Deep model quantization can be used for reducing the computation and memory
costs of DNNs, and deploying complex DNNs on mobile equipment. In this work, we
propose an optimization framework for deep model quantization. First, we
propose a measurement to estimate the effect of parameter quantization errors
in individual layers on the overall model prediction accuracy. Then, we propose
an optimization process based on this measurement for finding optimal
quantization bit-width for each layer. This is the first work that
theoretically analyse the relationship between parameter quantization errors of
individual layers and model accuracy. Our new quantization algorithm
outperforms previous quantization optimization methods, and achieves 20-40%
higher compression rate compared to equal bit-width quantization at the same
model prediction accuracy.
",1,0,0,1,0,0
905,A Simple Convolutional Generative Network for Next Item Recommendation,"  Convolutional Neural Networks (CNNs) have been recently introduced in the
domain of session-based next item recommendation. An ordered collection of past
items the user has interacted with in a session (or sequence) are embedded into
a 2-dimensional latent matrix, and treated as an image. The convolution and
pooling operations are then applied to the mapped item embeddings. In this
paper, we first examine the typical session-based CNN recommender and show that
both the generative model and network architecture are suboptimal when modeling
long-range dependencies in the item sequence. To address the issues, we
introduce a simple, but very effective generative model that is capable of
learning high-level representation from both short- and long-range item
dependencies. The network architecture of the proposed model is formed of a
stack of \emph{holed} convolutional layers, which can efficiently increase the
receptive fields without relying on the pooling operation. Another contribution
is the effective use of residual block structure in recommender systems, which
can ease the optimization for much deeper networks. The proposed generative
model attains state-of-the-art accuracy with less training time in the next
item recommendation task. It accordingly can be used as a powerful
recommendation baseline to beat in future, especially when there are long
sequences of user feedback.
",0,0,0,1,0,0
906,High-temperature terahertz optical diode effect without magnetic order in polar FeZnMo$_3$O$_8$,"  We present a terahertz spectroscopic study of polar ferrimagnet
FeZnMo$_3$O$_8$. Our main finding is a giant high-temperature optical diode
effect, or nonreciprocal directional dichroism, where the transmitted light
intensity in one direction is over 100 times lower than intensity transmitted
in the opposite direction. The effect takes place in the paramagnetic phase
with no long-range magnetic order in the crystal, which contrasts sharply with
all existing reports of the terahertz optical diode effect in other
magnetoelectric materials, where the long-range magnetic ordering is a
necessary prerequisite. In \fzmo, the effect occurs resonantly with a strong
magnetic dipole active transition centered at 1.27 THz and assigned as electron
spin resonance between the eigenstates of the single-ion anisotropy
Hamiltonian. We propose that the optical diode effect in paramagnetic
FeZnMo$_3$O$_8$ is driven by signle-ion terms in magnetoelectric free energy.
",0,1,0,0,0,0
907,Character Networks and Book Genre Classification,"  We compare the social character networks of biographical, legendary and
fictional texts, in search for marks of genre differentiation. We examine the
degree distribution of character appearance and find a power law that does not
depend on the literary genre or historical content. We also analyze local and
global complex networks measures, in particular, correlation plots between the
recently introduced Lobby (or Hirsh $H(1)$) index and Degree, Betweenness and
Closeness centralities. Assortativity plots, which previous literature claims
to separate fictional from real social networks, were also studied. We've found
no relevant differences in the books for these network measures and we give a
plausible explanation why the previous assortativity result is not correct.
",1,1,0,0,0,0
908,Phase diagrams of Bose-Hubbard model and antiferromagnetic spin-1/2 models on a honeycomb lattice,"  Motivated by the recent experimental realization of the Haldane model by
ultracold fermions in an optical lattice, we investigate phase diagrams of the
hard-core Bose-Hubbard model on a honeycomb lattice. This model is closely
related with a spin-1/2 antiferromagnetic (AF) quantum spin model.
Nearest-neighbor (NN) hopping amplitude is positive and it prefers an AF
configurations of phases of Bose-Einstein condensates. On the other hand, an
amplitude of the next-NN hopping depends on an angle variable as in the Haldane
model. Phase diagrams are obtained by means of an extended path-integral
Monte-Carlo simulations. Besides the AF state, a 120$^o$-order state, there
appear other phases including a Bose metal in which no long-range orders exist.
",0,1,0,0,0,0
909,Forecasting the Impact of Stellar Activity on Transiting Exoplanet Spectra,"  Exoplanet host star activity, in the form of unocculted star spots or
faculae, alters the observed transmission and emission spectra of the
exoplanet. This effect can be exacerbated when combining data from different
epochs if the stellar photosphere varies between observations due to activity.
redHere we present a method to characterize and correct for relative changes
due to stellar activity by exploiting multi-epoch ($\ge$2 visits/transits)
observations to place them in a consistent reference frame. Using measurements
from portions of the planet's orbit where negligible planet transmission or
emission can be assumed, we determine changes to the stellar spectral
amplitude. With the analytical methods described here, we predict the impact of
stellar variability on transit observations. Supplementing these forecasts with
Kepler-measured stellar variabilities for F-, G-, K-, and M-dwarfs, and
predicted transit precisions by JWST's NIRISS, NIRCam, and MIRI, we conclude
that stellar activity does not impact infrared transiting exoplanet
observations of most presently-known or predicted TESS targets by current or
near-future platforms, such as JWST.
",0,1,0,0,0,0
910,A Domain Specific Language for Performance Portable Molecular Dynamics Algorithms,"  Developers of Molecular Dynamics (MD) codes face significant challenges when
adapting existing simulation packages to new hardware. In a continuously
diversifying hardware landscape it becomes increasingly difficult for
scientists to be experts both in their own domain (physics/chemistry/biology)
and specialists in the low level parallelisation and optimisation of their
codes. To address this challenge, we describe a ""Separation of Concerns""
approach for the development of parallel and optimised MD codes: the science
specialist writes code at a high abstraction level in a domain specific
language (DSL), which is then translated into efficient computer code by a
scientific programmer. In a related context, an abstraction for the solution of
partial differential equations with grid based methods has recently been
implemented in the (Py)OP2 library. Inspired by this approach, we develop a
Python code generation system for molecular dynamics simulations on different
parallel architectures, including massively parallel distributed memory systems
and GPUs. We demonstrate the efficiency of the auto-generated code by studying
its performance and scalability on different hardware and compare it to other
state-of-the-art simulation packages. With growing data volumes the extraction
of physically meaningful information from the simulation becomes increasingly
challenging and requires equally efficient implementations. A particular
advantage of our approach is the easy expression of such analysis algorithms.
We consider two popular methods for deducing the crystalline structure of a
material from the local environment of each atom, show how they can be
expressed in our abstraction and implement them in the code generation
framework.
",1,1,0,0,0,0
911,CMB anisotropies at all orders: the non-linear Sachs-Wolfe formula,"  We obtain the non-linear generalization of the Sachs-Wolfe + integrated
Sachs-Wolfe (ISW) formula describing the CMB temperature anisotropies. Our
formula is valid at all orders in perturbation theory, is also valid in all
gauges and includes scalar, vector and tensor modes. A direct consequence of
our results is that the maps of the logarithmic temperature anisotropies are
much cleaner than the usual CMB maps, because they automatically remove many
secondary anisotropies. This can for instance, facilitate the search for
primordial non-Gaussianity in future works. It also disentangles the non-linear
ISW from other effects. Finally, we provide a method which can iteratively be
used to obtain the lensing solution at the desired order.
",0,1,0,0,0,0
912,On a binary system of Prendiville: The cubic case,"  We prove sharp decoupling inequalities for a class of two dimensional
non-degenerate surfaces in R^5, introduced by Prendiville. As a consequence, we
obtain sharp bounds on the number of integer solutions of the Diophantine
systems associated with these surfaces.
",0,0,1,0,0,0
913,Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation,"  In this paper, we introduce a simple, yet powerful pipeline for medical image
segmentation that combines Fully Convolutional Networks (FCNs) with Fully
Convolutional Residual Networks (FC-ResNets). We propose and examine a design
that takes particular advantage of recent advances in the understanding of both
Convolutional Neural Networks as well as ResNets. Our approach focuses upon the
importance of a trainable pre-processing when using FC-ResNets and we show that
a low-capacity FCN model can serve as a pre-processor to normalize medical
input data. In our image segmentation pipeline, we use FCNs to obtain
normalized images, which are then iteratively refined by means of a FC-ResNet
to generate a segmentation prediction. As in other fully convolutional
approaches, our pipeline can be used off-the-shelf on different image
modalities. We show that using this pipeline, we exhibit state-of-the-art
performance on the challenging Electron Microscopy benchmark, when compared to
other 2D methods. We improve segmentation results on CT images of liver
lesions, when contrasting with standard FCN methods. Moreover, when applying
our 2D pipeline on a challenging 3D MRI prostate segmentation challenge we
reach results that are competitive even when compared to 3D methods. The
obtained results illustrate the strong potential and versatility of the
pipeline by achieving highly accurate results on multi-modality images from
different anatomical regions and organs.
",1,0,0,0,0,0
914,Strong isomorphism in Marinatto-Weber type quantum games,"  Our purpose is to focus attention on a new criterion for quantum schemes by
bringing together the notions of quantum game and game isomorphism. A quantum
game scheme is required to generate the classical game as a special case. Now,
given a quantum game scheme and two isomorphic classical games, we additionally
require the resulting quantum games to be isomorphic as well. We show how this
isomorphism condition influences the players' strategy sets. We are concerned
with the Marinatto-Weber type quantum game scheme and the strong isomorphism
between games in strategic form.
",1,0,0,0,0,0
915,Riemannian stochastic variance reduced gradient,"  Stochastic variance reduction algorithms have recently become popular for
minimizing the average of a large but finite number of loss functions. In this
paper, we propose a novel Riemannian extension of the Euclidean stochastic
variance reduced gradient algorithm (R-SVRG) to a manifold search space. The
key challenges of averaging, adding, and subtracting multiple gradients are
addressed with retraction and vector transport. We present a global convergence
analysis of the proposed algorithm with a decay step size and a local
convergence rate analysis under a fixed step size under some natural
assumptions. The proposed algorithm is applied to problems on the Grassmann
manifold, such as principal component analysis, low-rank matrix completion, and
computation of the Karcher mean of subspaces, and outperforms the standard
Riemannian stochastic gradient descent algorithm in each case.
",1,0,1,1,0,0
916,Causal Inference by Stochastic Complexity,"  The algorithmic Markov condition states that the most likely causal direction
between two random variables X and Y can be identified as that direction with
the lowest Kolmogorov complexity. Due to the halting problem, however, this
notion is not computable.
We hence propose to do causal inference by stochastic complexity. That is, we
propose to approximate Kolmogorov complexity via the Minimum Description Length
(MDL) principle, using a score that is mini-max optimal with regard to the
model class under consideration. This means that even in an adversarial
setting, such as when the true distribution is not in this class, we still
obtain the optimal encoding for the data relative to the class.
We instantiate this framework, which we call CISC, for pairs of univariate
discrete variables, using the class of multinomial distributions. Experiments
show that CISC is highly accurate on synthetic, benchmark, as well as
real-world data, outperforming the state of the art by a margin, and scales
extremely well with regard to sample and domain sizes.
",1,0,0,0,0,0
917,Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,"  The recently proposed Temporal Ensembling has achieved state-of-the-art
results in several semi-supervised learning benchmarks. It maintains an
exponential moving average of label predictions on each training example, and
penalizes predictions that are inconsistent with this target. However, because
the targets change only once per epoch, Temporal Ensembling becomes unwieldy
when learning large datasets. To overcome this problem, we propose Mean
Teacher, a method that averages model weights instead of label predictions. As
an additional benefit, Mean Teacher improves test accuracy and enables training
with fewer labels than Temporal Ensembling. Without changing the network
architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250
labels, outperforming Temporal Ensembling trained with 1000 labels. We also
show that a good network architecture is crucial to performance. Combining Mean
Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with
4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels
from 35.24% to 9.11%.
",1,0,0,1,0,0
918,WOMBAT: A Scalable and High Performance Astrophysical MHD Code,"  We present a new code for astrophysical magneto-hydrodynamics specifically
designed and optimized for high performance and scaling on modern and future
supercomputers. We describe a novel hybrid OpenMP/MPI programming model that
emerged from a collaboration between Cray, Inc. and the University of
Minnesota. This design utilizes MPI-RMA optimized for thread scaling, which
allows the code to run extremely efficiently at very high thread counts ideal
for the latest generation of the multi-core and many-core architectures. Such
performance characteristics are needed in the era of ""exascale"" computing. We
describe and demonstrate our high-performance design in detail with the intent
that it may be used as a model for other, future astrophysical codes intended
for applications demanding exceptional performance.
",0,1,0,0,0,0
919,Securing Virtual Network Function Placement with High Availability Guarantees,"  Virtual Network Functions as a Service (VNFaaS) is currently under attentive
study by telecommunications and cloud stakeholders as a promising business and
technical direction consisting of providing network functions as a service on a
cloud (NFV Infrastructure), instead of delivering standalone network
appliances, in order to provide higher scalability and reduce maintenance
costs. However, the functioning of such NFVI hosting the VNFs is fundamental
for all the services and applications running on top of it, forcing to
guarantee a high availability level. Indeed the availability of an VNFaaS
relies on the failure rate of its single components, namely the servers, the
virtualization software, and the communication network. The proper assignment
of the virtual machines implementing network functions to NFVI servers and
their protection is essential to guarantee high availability. We model the High
Availability Virtual Network Function Placement (HA-VNFP) as the problem of
finding the best assignment of virtual machines to servers guaranteeing
protection by replication. We propose a probabilistic approach to measure the
real availability of a system and design both efficient and effective
algorithms that can be used by stakeholders for both online and offline
planning.
",1,0,0,0,0,0
920,The Case for Pyriproxyfen as a Potential Cause for Microcephaly; From Biology to Epidemiology,"  The Zika virus has been found in individual cases but has not been confirmed
as the cause of in the large number of cases of microcephaly in Brazil in
2015-6. Indeed, disparities between the incidence of Zika and microcephaly
across geographic locations has led to questions about the virus's role. Here
we consider whether the insecticide pyriproxyfen used in Brazilian drinking
water might be the primary cause or a cofactor. Pyriproxifen is a juvenile
hormone analog which has been shown to correspond in mammals to a number of fat
soluble regulatory molecules including retinoic acid, a metabolite of vitamin
A, with which it has cross-reactivity and whose application during development
has been shown to cause microcephaly. Methoprene, another juvenile hormone
analog approved as an insecticide in the 1970s has been shown to cause
developmental disorders in mammals. Isotretinoin is another retinoid causing
microcephaly via activation of the retinoid X receptor in developing fetuses.
We review tests of pyriproxyfen by the manufacturer Sumitomo, which actually
found some evidence for this effect, including low brain mass and
arhinencephaly in exposed rat pups. Pyriproxyfen use in Brazil is
unprecedented, never having been applied to a water supply on a large scale.
Claims that its geographical pattern of use rule it out as a cause have not
been documented or confirmed. On the other hand, the very few microcephaly
cases reported in Colombia and the wide discrepancies of incidence in different
states across Brazil despite large numbers of Zika cases undermine the claim
that Zika is the cause. Given this combination of potential molecular
mechanism, toxicological and epidemiological evidence we strongly recommend
that the use of pyriproxyfen in Brazil be suspended until the potential causal
link to microcephaly is investigated further.
",0,1,0,0,0,0
921,A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates,"  This paper considers the problem of decentralized optimization with a
composite objective containing smooth and non-smooth terms. To solve the
problem, a proximal-gradient scheme is studied. Specifically, the smooth and
nonsmooth terms are dealt with by gradient update and proximal update,
respectively. The studied algorithm is closely related to a previous
decentralized optimization algorithm, PG-EXTRA [37], but has a few advantages.
First of all, in our new scheme, agents use uncoordinated step-sizes and the
stable upper bounds on step-sizes are independent from network topology. The
step-sizes depend on local objective functions, and they can be as large as
that of the gradient descent. Secondly, for the special case without non-smooth
terms, linear convergence can be achieved under the strong convexity
assumption. The dependence of the convergence rate on the objective functions
and the network are separated, and the convergence rate of our new scheme is as
good as one of the two convergence rates that match the typical rates for the
general gradient descent and the consensus averaging. We also provide some
numerical experiments to demonstrate the efficacy of the introduced algorithms
and validate our theoretical discoveries.
",0,0,1,1,0,0
922,CODA: Enabling Co-location of Computation and Data for Near-Data Processing,"  Recent studies have demonstrated that near-data processing (NDP) is an
effective technique for improving performance and energy efficiency of
data-intensive workloads. However, leveraging NDP in realistic systems with
multiple memory modules introduces a new challenge. In today's systems, where
no computation occurs in memory modules, the physical address space is
interleaved at a fine granularity among all memory modules to help improve the
utilization of processor-memory interfaces by distributing the memory traffic.
However, this is at odds with efficient use of NDP, which requires careful
placement of data in memory modules such that near-data computations and their
exclusively used data can be localized in individual memory modules, while
distributing shared data among memory modules to reduce hotspots. In order to
address this new challenge, we propose a set of techniques that (1) enable
collections of OS pages to either be fine-grain interleaved among memory
modules (as is done today) or to be placed contiguously on individual memory
modules (as is desirable for NDP private data), and (2) decide whether to
localize or distribute each memory object based on its anticipated access
pattern and steer computations to the memory where the data they access is
located. Our evaluations across a wide range of workloads show that the
proposed mechanism improves performance by 31% and reduces 38% remote data
accesses over a baseline system that cannot exploit computate-data affinity
characteristics.
",1,0,0,0,0,0
923,Generative Models for Spear Phishing Posts on Social Media,"  Historically, machine learning in computer security has prioritized defense:
think intrusion detection systems, malware classification, and botnet traffic
identification. Offense can benefit from data just as well. Social networks,
with their access to extensive personal data, bot-friendly APIs, colloquial
syntax, and prevalence of shortened links, are the perfect venues for spreading
machine-generated malicious content. We aim to discover what capabilities an
adversary might utilize in such a domain. We present a long short-term memory
(LSTM) neural network that learns to socially engineer specific users into
clicking on deceptive URLs. The model is trained with word vector
representations of social media posts, and in order to make a click-through
more likely, it is dynamically seeded with topics extracted from the target's
timeline. We augment the model with clustering to triage high value targets
based on their level of social engagement, and measure success of the LSTM's
phishing expedition using click-rates of IP-tracked links. We achieve state of
the art success rates, tripling those of historic email attack campaigns, and
outperform humans manually performing the same task.
",0,0,0,1,0,0
924,SkipFlow: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring,"  Deep learning has demonstrated tremendous potential for Automatic Text
Scoring (ATS) tasks. In this paper, we describe a new neural architecture that
enhances vanilla neural network models with auxiliary neural coherence
features. Our new method proposes a new \textsc{SkipFlow} mechanism that models
relationships between snapshots of the hidden representations of a long
short-term memory (LSTM) network as it reads. Subsequently, the semantic
relationships between multiple snapshots are used as auxiliary features for
prediction. This has two main benefits. Firstly, essays are typically long
sequences and therefore the memorization capability of the LSTM network may be
insufficient. Implicit access to multiple snapshots can alleviate this problem
by acting as a protection against vanishing gradients. The parameters of the
\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly,
modeling relationships between multiple positions allows our model to learn
features that represent and approximate textual coherence. In our model, we
call this \textit{neural coherence} features. Overall, we present a unified
deep learning architecture that generates neural coherence features as it reads
in an end-to-end fashion. Our approach demonstrates state-of-the-art
performance on the benchmark ASAP dataset, outperforming not only feature
engineering baselines but also other deep learning models.
",1,0,0,0,0,0
925,Automated Speed and Lane Change Decision Making using Deep Reinforcement Learning,"  This paper introduces a method, based on deep reinforcement learning, for
automatically generating a general purpose decision making function. A Deep
Q-Network agent was trained in a simulated environment to handle speed and lane
change decisions for a truck-trailer combination. In a highway driving case, it
is shown that the method produced an agent that matched or surpassed the
performance of a commonly used reference model. To demonstrate the generality
of the method, the exact same algorithm was also tested by training it for an
overtaking case on a road with oncoming traffic. Furthermore, a novel way of
applying a convolutional neural network to high level input that represents
interchangeable objects is also introduced.
",1,0,0,0,0,0
926,Reduction of topological $\mathbb{Z}$ classification in cold atomic systems,"  One of the most challenging problems in correlated topological systems is a
realization of the reduction of topological classification, but very few
experimental platforms have been proposed so far. We here demonstrate that
ultracold dipolar fermions (e.g., $^{167}$Er, $^{161}$Dy, and $^{53}$Cr) loaded
in an optical lattice of two-leg ladder geometry can be the first promising
testbed for the reduction $\mathbb{Z}\to\mathbb{Z}_4$, where solid evidence for
the reduction is available thanks to their high controllability. We further
give a detailed account of how to experimentally access this phenomenon; around
the edges, the destruction of one-particle gapless excitations can be observed
by the local radio frequency spectroscopy, while that of gapless spin
excitations can be observed by a time-dependent spin expectation value of a
superposed state of the ground state and the first excited state. We clarify
that even when the reduction occurs, a gapless edge mode is recovered around a
dislocation, which can be another piece of evidence for the reduction.
",0,1,0,0,0,0
927,Reducing biases on $H_0$ measurements using strong lensing and galaxy dynamics: results from the EAGLE simulation,"  Cosmological parameter constraints from observations of time-delay lenses are
becoming increasingly precise. However, there may be significant bias and
scatter in these measurements due to, among other things, the so-called
mass-sheet degeneracy. To estimate these uncertainties, we analyze strong
lenses from the largest EAGLE hydrodynamical simulation. We apply a mass-sheet
transformation to the radial density profiles of lenses, and by selecting
lenses near isothermality, we find that the bias on H0 can be reduced to 5%
with an intrinsic scatter of 10%, confirming previous results performed on a
different simulation data set. We further investigate whether combining lensing
observables with kinematic constraints helps to minimize this bias. We do not
detect any significant dependence of the bias on lens model parameters or
observational properties of the galaxy, but depending on the source--lens
configuration, a bias may still exist. Cross lenses provide an accurate
estimate of the Hubble constant, while fold (double) lenses tend to be biased
low (high). With kinematic constraints, double lenses show bias and intrinsic
scatter of 6% and 10%, respectively, while quad lenses show bias and intrinsic
scatter of 0.5% and 10%, respectively. For lenses with a reduced $\chi^2 > 1$,
a power-law dependence of the $\chi^2$ on the lens environment (number of
nearby galaxies) is seen. Lastly, we model, in greater detail, the cases of two
double lenses that are significantly biased. We are able to remove the bias,
suggesting that the remaining biases could also be reduced by carefully taking
into account additional sources of systematic uncertainty.
",0,1,0,0,0,0
928,Characterizing the ionospheric current pattern response to southward and northward IMF turnings with dynamical SuperMAG correlation networks,"  We characterize the response of the quiet time (no substorms or storms)
large-scale ionospheric transient equivalent currents to north-south and
south-north IMF turnings by using a dynamical network of ground-based
magnetometers. Canonical correlation between all pairs of SuperMAG magnetometer
stations in the Northern Hemisphere (magnetic latitude (MLAT) 50-82$^{\circ}$)
is used to establish the extent of near-simultaneous magnetic response between
regions of magnetic local time-MLAT. Parameters and maps that describe
spatial-temporal correlation are used to characterize the system and its
response to the turnings aggregated over several hundred events. We find that
regions that experience large increases in correlation post turning coincide
with typical locations of a two-cell convection system and are influenced by
the interplanetary magnetic field $\mathit{B}_{y}$. The time between the
turnings reaching the magnetopause and a network response is found to be
$\sim$8-10 min and correlation in the dayside occurs 2-8 min before that in the
nightside.
",0,1,0,0,0,0
929,On a question of Buchweitz about ranks of syzygies of modules of finite length,"  Let R be a local ring of dimension d. Buchweitz asks if the rank of the d-th
syzygy of a module of finite lengh is greater than or equal to the rank of the
d-th syzygy of the residue field, unless the module has finite projective
dimension. Assuming that R is Gorenstein, we prove that if the question is
affrmative, then R is a hypersurface. If moreover R has dimension two, then we
show that the converse also holds true.
",0,0,1,0,0,0
930,Learning convex bounds for linear quadratic control policy synthesis,"  Learning to make decisions from observed data in dynamic environments remains
a problem of fundamental importance in a number of fields, from artificial
intelligence and robotics, to medicine and finance. This paper concerns the
problem of learning control policies for unknown linear dynamical systems so as
to maximize a quadratic reward function. We present a method to optimize the
expected value of the reward over the posterior distribution of the unknown
system parameters, given data. The algorithm involves sequential convex
programing, and enjoys reliable local convergence and robust stability
guarantees. Numerical simulations and stabilization of a real-world inverted
pendulum are used to demonstrate the approach, with strong performance and
robustness properties observed in both.
",0,0,0,1,0,0
931,General mixed multi-soliton solution to the multi-component Maccari system,"  Based on the KP hierarchy reduction method, the general bright-dark mixed
multi-soliton solution of the multi-component Maccari system is constructed.
The multi-component Maccari system considered comprised of multiple (say $M$)
short-wave components and one long-wave component with all possible
combinations of nonlinearities including all-focusing, all-defocusing and mixed
types. We firstly derive the two-bright-one-dark (2-b-1-d) and
one-bright-two-dark (1-b-2-d) mixed multi-soliton solutions to the
three-component Maccari system in detail. For the interaction between two
solitons, the asymptotic analysis shows that inelastic collision can take place
in a $M$-component Maccari system with $M \geq 3$ only if the bright parts of
the mixed solitons appear at least in two short-wave components. The
energy-exchanging inelastic collision characterized by an intensity
redistribution among the bright parts of the mixed solitons. While the dark
parts of the mixed solitons and the solitons in the long-wave component always
undergo elastic collision which just accompanied by a position shift. In the
end, we extend the corresponding analysis to the $M$-component Maccari system
to obtain its mixed multi-soliton solution. The formula obtained unifies the
all-bright, all-dark and mixed multi-soliton solutions.
",0,1,0,0,0,0
932,Resolution and Relevance Trade-offs in Deep Learning,"  Deep learning has been successfully applied to various tasks, but its
underlying mechanism remains unclear. Neural networks associate similar inputs
in the visible layer to the same state of hidden variables in deep layers. The
fraction of inputs that are associated to the same state is a natural measure
of similarity and is simply related to the cost in bits required to represent
these inputs. The degeneracy of states with the same information cost provides
instead a natural measure of noise and is simply related the entropy of the
frequency of states, that we call relevance. Representations with minimal
noise, at a given level of similarity (resolution), are those that maximise the
relevance. A signature of such efficient representations is that frequency
distributions follow power laws. We show, in extensive numerical experiments,
that deep neural networks extract a hierarchy of efficient representations from
data, because they i) achieve low levels of noise (i.e. high relevance) and ii)
exhibit power law distributions. We also find that the layer that is most
efficient to reliably generate patterns of training data is the one for which
relevance and resolution are traded at the same price, which implies that
frequency distribution follows Zipf's law.
",1,0,0,0,0,0
933,Heisenberg Modules over Quantum 2-tori are metrized quantum vector bundles,"  The modular Gromov-Hausdorff propinquity is a distance on classes of modules
endowed with quantum metric information, in the form of a metric form of a
connection and a left Hilbert module structure. This paper proves that the
family of Heisenberg modules over quantum two tori, when endowed with their
canonical connections, form a family of metrized quantum vector bundles, as a
first step in proving that Heisenberg modules form a continuous family for the
modular Gromov-Hausdorff propinquity.
",0,0,1,0,0,0
934,GraphCombEx: A Software Tool for Exploration of Combinatorial Optimisation Properties of Large Graphs,"  We present a prototype of a software tool for exploration of multiple
combinatorial optimisation problems in large real-world and synthetic complex
networks. Our tool, called GraphCombEx (an acronym of Graph Combinatorial
Explorer), provides a unified framework for scalable computation and
presentation of high-quality suboptimal solutions and bounds for a number of
widely studied combinatorial optimisation problems. Efficient representation
and applicability to large-scale graphs and complex networks are particularly
considered in its design. The problems currently supported include maximum
clique, graph colouring, maximum independent set, minimum vertex clique
covering, minimum dominating set, as well as the longest simple cycle problem.
Suboptimal solutions and intervals for optimal objective values are estimated
using scalable heuristics. The tool is designed with extensibility in mind,
with the view of further problems and both new fast and high-performance
heuristics to be added in the future. GraphCombEx has already been successfully
used as a support tool in a number of recent research studies using
combinatorial optimisation to analyse complex networks, indicating its promise
as a research software tool.
",1,0,0,0,0,0
935,Approches d'analyse distributionnelle pour am??liorer la d??sambigu??sation s??mantique,"  Word sense disambiguation (WSD) improves many Natural Language Processing
(NLP) applications such as Information Retrieval, Machine Translation or
Lexical Simplification. WSD is the ability of determining a word sense among
different ones within a polysemic lexical unit taking into account the context.
The most straightforward approach uses a semantic proximity measure between the
word sense candidates of the target word and those of its context. Such a
method very easily entails a combinatorial explosion. In this paper, we propose
two methods based on distributional analysis which enable to reduce the
exponential complexity without losing the coherence. We present a comparison
between the selection of distributional neighbors and the linearly nearest
neighbors. The figures obtained show that selecting distributional neighbors
leads to better results.
",1,0,0,0,0,0
936,Non-degenerate parametric resonance in tunable superconducting cavity,"  We develop a theory for non-degenerate parametric resonance in a tunable
superconducting cavity. We focus on nonlinear effects that are caused by
nonlinear Josephson elements connected to the cavity. We analyze parametric
amplification in a strong nonlinear regime at the parametric instability
threshold, and calculate maximum gain values. Above the threshold, in the
parametric oscillator regime the linear cavity response diverges at the
oscillator frequency at all pump strengths. We show that this divergence is
related to the continuous degeneracy of the free oscillator state with respect
to the phase. Applying on-resonance input lifts the degeneracy and removes the
divergence. We also investigate the quantum noise squeezing. It is shown that
in the strong amplification regime the noise undergoes four-mode squeezing, and
that in this regime the output signal to noise ratio can significantly exceed
the input value. We also analyze the intermode frequency conversion and
identify parameters at which full conversion is achieved.
",0,1,0,0,0,0
937,Toward Controlled Generation of Text,"  Generic generation and manipulation of text is challenging and has limited
success compared to recent deep generative modeling in visual domain. This
paper aims at generating plausible natural language sentences, whose attributes
are dynamically controlled by learning disentangled latent representations with
designated semantics. We propose a new neural generative model which combines
variational auto-encoders and holistic attribute discriminators for effective
imposition of semantic structures. With differentiable approximation to
discrete text samples, explicit constraints on independent attribute controls,
and efficient collaborative learning of generator and discriminators, our model
learns highly interpretable representations from even only word annotations,
and produces realistic sentences with desired attributes. Quantitative
evaluation validates the accuracy of sentence and attribute generation.
",1,0,0,1,0,0
938,Human peripheral blur is optimal for object recognition,"  Our eyes sample a disproportionately large amount of information at the
centre of gaze with increasingly sparse sampling into the periphery. This
sampling scheme is widely believed to be a wiring constraint whereby high
resolution at the centre is achieved by sacrificing spatial acuity in the
periphery. Here we propose that this sampling scheme may be optimal for object
recognition because the relevant spatial content is dense near an object and
sparse in the surrounding vicinity. We tested this hypothesis by training deep
convolutional neural networks on full-resolution and foveated images. Our main
finding is that networks trained on images with foveated sampling show better
object classification compared to networks trained on full resolution images.
Importantly, blurring images according to the human blur function yielded the
best performance compared to images with shallower or steeper blurring. Taken
together our results suggest that, peripheral blurring in our eyes may have
evolved for optimal object recognition, rather than merely to satisfy wiring
constraints.
",0,0,0,0,1,0
939,Acceleration of Mean Square Distance Calculations with Floating Close Structure in Metadynamics Simulations,"  Molecular dynamics simulates the~movements of atoms. Due to its high cost,
many methods have been developed to ""push the~simulation forward"". One of them,
metadynamics, can hasten the~molecular dynamics with the~help of variables
describing the~simulated process. However, the~evaluation of these variables
can include numerous mean square distance calculations that introduce
substantial computational demands, thus jeopardize the~benefit of the~approach.
Recently, we proposed an~approximative method that significantly reduces
the~number of these distance calculations. Here we evaluate the~performance and
the~scalability on two molecular systems. We assess the~maximal theoretical
speed-up based on the reduction of distance computations and Ahmdal's law and
compare it to the~practical speed-up achieved with our implementation.
",1,0,0,0,0,0
940,Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding,"  Developers increasingly rely on text matching tools to analyze the relation
between natural language words and APIs. However, semantic gaps, namely textual
mismatches between words and APIs, negatively affect these tools. Previous
studies have transformed words or APIs into low-dimensional vectors for
matching; however, inaccurate results were obtained due to the failure of
modeling words and APIs simultaneously. To resolve this problem, two main
challenges are to be addressed: the acquisition of massive words and APIs for
mining and the alignment of words and APIs for modeling. Therefore, this study
proposes Word2API to effectively estimate relatedness of words and APIs.
Word2API collects millions of commonly used words and APIs from code
repositories to address the acquisition challenge. Then, a shuffling strategy
is used to transform related words and APIs into tuples to address the
alignment challenge. Using these tuples, Word2API models words and APIs
simultaneously. Word2API outperforms baselines by 10%-49.6% of relatedness
estimation in terms of precision and NDCG. Word2API is also effective on
solving typical software tasks, e.g., query expansion and API documents
linking. A simple system with Word2API-expanded queries recommends up to 21.4%
more related APIs for developers. Meanwhile, Word2API improves comparison
algorithms by 7.9%-17.4% in linking questions in Question&Answer communities to
API documents.
",1,0,0,0,0,0
941,Feedback optimal controllers for the Heston model,"  We prove the existence of an optimal feedback controller for a stochastic
optimization problem constituted by a variation of the Heston model, where a
stochastic input process is added in order to minimize a given performance
criterion. The stochastic feedback controller is searched by solving a
nonlinear backward parabolic equation for which one proves the existence of a
martingale solution.
",0,0,1,0,0,0
942,User Experience of the CoSTAR System for Instruction of Collaborative Robots,"  How can we enable novice users to create effective task plans for
collaborative robots? Must there be a tradeoff between generalizability and
ease of use? To answer these questions, we conducted a user study with the
CoSTAR system, which integrates perception and reasoning into a Behavior
Tree-based task plan editor. In our study, we ask novice users to perform
simple pick-and-place assembly tasks under varying perception and planning
capabilities. Our study shows that users found Behavior Trees to be an
effective way of specifying task plans. Furthermore, users were also able to
more quickly, effectively, and generally author task plans with the addition of
CoSTAR's planning, perception, and reasoning capabilities. Despite these
improvements, concepts associated with these capabilities were rated by users
as less usable, and our results suggest a direction for further refinement.
",1,0,0,0,0,0
943,Graph complexity and Mahler measure,"  The (torsion) complexity of a finite edge-weighted graph is defined to be the
order of the torsion subgroup of the abelian group presented by its Laplacian
matrix. When G is d-periodic (i.e., G has a free action of the rank-d free
abelian group by graph automorphisms, with finite quotient) the Mahler measure
of its Laplacian determinant polynomial is the growth rate of the complexity of
finite quotients of G. Lehmer's question, an open question about the roots of
monic integral polynomials, is equivalent to a question about the complexity
growth of edge-weighted 1-periodic graphs.
",0,0,1,0,0,0
944,Neutronic Analysis on Potential Accident Tolerant Fuel-Cladding Combination U$_3$Si$_2$-FeCrAl,"  Neutronic performance is investigated for a potential accident tolerant fuel
(ATF),which consists of U$_3$Si$_2$ fuel and FeCrAl cladding. In comparison
with current UO$_2$-Zr system, FeCrAl has a better oxidation resistance but a
larger thermal neutron absorption cross section. U$_3$Si$_2$ has a higher
thermal conductivity and a higher uranium density, which can compensate the
reactivity suppressed by FeCrAl. Based on neutronic investigations, a possible
U$_3$Si$_2$-FeCrAl fuel-cladding systemis taken into consideration. Fundamental
properties of the suggested fuel-cladding combination are investigated in a
fuel assembly.These properties include moderator and fuel temperature
coefficients, control rods worth, radial power distribution (in a fuel rod),
and different void reactivity coefficients. The present work proves that the
new combination has less reactivity variation during its service lifetime.
Although, compared with the current system, it has a little larger deviation on
power distribution and a little less negative temperature coefficient and void
reactivity coefficient and its control rods worth is less important, variations
of these parameters are less important during the service lifetime of fuel.
Hence, U$_3$Si$_2$-FeCrAl system is a potential ATF candidate from a neutronic
view.
",0,1,0,0,0,0
945,Total-positivity preservers,"  We prove that the only entrywise transforms of rectangular matrices which
preserve total positivity or total non-negativity are either constant or
linear. This follows from an extended classification of preservers of these two
properties for matrices of fixed dimension. We also prove that the same
assertions hold upon working only with symmetric matrices; for total-positivity
preservers our proofs proceed through solving two totally positive completion
problems.
",0,0,1,0,0,0
946,A revision of the subtract-with-borrow random number generators,"  The most popular and widely used subtract-with-borrow generator, also known
as RANLUX, is reimplemented as a linear congruential generator using large
integer arithmetic with the modulus size of 576 bits. Modern computers, as well
as the specific structure of the modulus inferred from RANLUX, allow for the
development of a fast modular multiplication -- the core of the procedure. This
was previously believed to be slow and have too high cost in terms of computing
resources. Our tests show a significant gain in generation speed which is
comparable with other fast, high quality random number generators. An
additional feature is the fast skipping of generator states leading to a
seeding scheme which guarantees the uniqueness of random number sequences.
",1,1,0,0,0,0
947,A Macdonald refined topological vertex,"  We consider the refined topological vertex of Iqbal et al, as a function of
two parameters (x, y), and deform it by introducing Macdonald parameters (q,
t), as in the work of Vuletic on plane partitions, to obtain 'a Macdonald
refined topological vertex'. In the limit q -> t, we recover the refined
topological vertex of Iqbal et al. In the limit x -> y, we obtain a
qt-deformation of the topological vertex of Aganagic et al. Copies of the
vertex can be glued to obtain qt-deformed 5D instanton partition functions that
have well-defined 4D limits and, for generic values of (q, t), contain
infinite-towers of poles for every pole in the limit q -> t.
",0,0,1,0,0,0
948,Bias voltage effects on tunneling magnetoresistance in Fe/MgAl${}_2$O${}_4$/Fe(001) junctions: Comparative study with Fe/MgO/Fe(001) junctions,"  We investigate bias voltage effects on the spin-dependent transport
properties of Fe/MgAl${}_2$O${}_4$/Fe(001) magnetic tunneling junctions (MTJs)
by comparing them with those of Fe/MgO/Fe(001) MTJs. By means of the
nonequilibrium Green's function method and the density functional theory, we
calculate bias voltage dependences of magnetoresistance (MR) ratios in both the
MTJs. We find that in both the MTJs, the MR ratio decreases as the bias voltage
increases and finally vanishes at a critical bias voltage $V_{\rm c}$. We also
find that the critical bias voltage $V_{\rm c}$ of the MgAl${}_2$O${}_4$-based
MTJ is clearly larger than that of the MgO-based MTJ. Since the in-plane
lattice constant of the Fe/MgAl${}_2$O${}_4$/Fe(001) supercell is twice that of
the Fe/MgO/Fe(001) one, the Fe electrodes in the MgAl${}_2$O${}_4$-based MTJs
have an identical band structure to that obtained by folding the Fe band
structure of the MgO-based MTJs in the Brillouin zone of the in-plane wave
vector. We show that such a difference in the Fe band structure is the origin
of the difference in the critical bias voltage $V_{\rm c}$ between the
MgAl${}_2$O${}_4$- and MgO-based MTJs.
",0,1,0,0,0,0
949,Listen to Your Face: Inferring Facial Action Units from Audio Channel,"  Extensive efforts have been devoted to recognizing facial action units (AUs).
However, it is still challenging to recognize AUs from spontaneous facial
displays especially when they are accompanied with speech. Different from all
prior work that utilized visual observations for facial AU recognition, this
paper presents a novel approach that recognizes speech-related AUs exclusively
from audio signals based on the fact that facial activities are highly
correlated with voice during speech. Specifically, dynamic and physiological
relationships between AUs and phonemes are modeled through a continuous time
Bayesian network (CTBN); then AU recognition is performed by probabilistic
inference via the CTBN model.
A pilot audiovisual AU-coded database has been constructed to evaluate the
proposed audio-based AU recognition framework. The database consists of a
""clean"" subset with frontal and neutral faces and a challenging subset
collected with large head movements and occlusions. Experimental results on
this database show that the proposed CTBN model achieves promising recognition
performance for 7 speech-related AUs and outperforms the state-of-the-art
visual-based methods especially for those AUs that are activated at low
intensities or ""hardly visible"" in the visual channel. Furthermore, the CTBN
model yields more impressive recognition performance on the challenging subset,
where the visual-based approaches suffer significantly.
",1,0,0,0,0,0
950,Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning,"  We found an easy and quick post-learning method named ""Icing on the Cake"" to
enhance a classification performance in deep learning. The method is that we
train only the final classifier again after an ordinary training is done.
",0,0,0,1,0,0
951,Minimal Effort Back Propagation for Convolutional Neural Networks,"  As traditional neural network consumes a significant amount of computing
resources during back propagation, \citet{Sun2017mePropSB} propose a simple yet
effective technique to alleviate this problem. In this technique, only a small
subset of the full gradients are computed to update the model parameters. In
this paper we extend this technique into the Convolutional Neural Network(CNN)
to reduce calculation in back propagation, and the surprising results verify
its validity in CNN: only 5\% of the gradients are passed back but the model
still achieves the same effect as the traditional CNN, or even better. We also
show that the top-$k$ selection of gradients leads to a sparse calculation in
back propagation, which may bring significant computational benefits for high
computational complexity of convolution operation in CNN.
",1,0,0,1,0,0
952,Estimating Achievable Range of Ground Robots Operating on Single Battery Discharge for Operational Efficacy Amelioration,"  Mobile robots are increasingly being used to assist with active pursuit and
law enforcement. One major limitation for such missions is the resource
(battery) allocated to the robot. Factors like nature and agility of evader,
terrain over which pursuit is being carried out, plausible traversal velocity
and the amount of necessary data to be collected all influence how long the
robot can last in the field and how far it can travel. In this paper, we
develop an analytical model that analyzes the energy utilization for a variety
of components mounted on a robot to estimate the maximum operational range
achievable by the robot operating on a single battery discharge. We categorize
the major consumers of energy as: 1.) ancillary robotic functions such as
computation, communication, sensing etc., and 2.) maneuvering which involves
propulsion, steering etc. Both these consumers draw power from the common power
source but the achievable range is largely affected by the proportion of power
available for maneuvering. For this case study, we performed experiments with
real robots on planar and graded surfaces and evaluated the estimation error
for each case.
",1,0,0,0,0,0
953,Likelihood ratio test for variance components in nonlinear mixed effects models,"  Mixed effects models are widely used to describe heterogeneity in a
population. A crucial issue when adjusting such a model to data consists in
identifying fixed and random effects. From a statistical point of view, it
remains to test the nullity of the variances of a given subset of random
effects. Some authors have proposed to use the likelihood ratio test and have
established its asymptotic distribution in some particular cases. Nevertheless,
to the best of our knowledge, no general variance components testing procedure
has been fully investigated yet. In this paper, we study the likelihood ratio
test properties to test that the variances of a general subset of the random
effects are equal to zero in both linear and nonlinear mixed effects model,
extending the existing results. We prove that the asymptotic distribution of
the test is a chi-bar-square distribution, that is to say a mixture of
chi-square distributions, and we identify the corresponding weights. We
highlight in particular that the limiting distribution depends on the presence
of correlations between the random effects but not on the linear or nonlinear
structure of the mixed effects model. We illustrate the finite sample size
properties of the test procedure through simulation studies and apply the test
procedure to two real datasets of dental growth and of coucal growth.
",0,0,0,1,0,0
954,H??lder continuous solutions of the Monge-Amp??re equation on compact Hermitian manifolds,"  We show that a positive Borel measure of positive finite total mass, on
compact Hermitian manifolds, admits a Holder continuous quasi-plurisubharmonic
solution to the Monge-Ampere equation if and only if it is dominated locally by
Monge-Ampere measures of Holder continuous plurisubharmonic functions.
",0,0,1,0,0,0
955,The beamformer and correlator for the Large European Array for Pulsars,"  The Large European Array for Pulsars combines Europe's largest radio
telescopes to form a tied-array telescope that provides high signal-to-noise
observations of millisecond pulsars (MSPs) with the objective to increase the
sensitivity of detecting low-frequency gravitational waves. As part of this
endeavor we have developed a software correlator and beamformer which enables
the formation of a tied-array beam from the raw voltages from each of
telescopes. We explain the concepts and techniques involved in the process of
adding the raw voltages coherently. We further present the software processing
pipeline that is specifically designed to deal with data from widely spaced,
inhomogeneous radio telescopes and describe the steps involved in preparing,
correlating and creating the tied-array beam. This includes polarization
calibration, bandpass correction, frequency dependent phase correction,
interference mitigation and pulsar gating. A link is provided where the
software can be obtained.
",0,1,0,0,0,0
956,Solving $\ell^p\!$-norm regularization with tensor kernels,"  In this paper, we discuss how a suitable family of tensor kernels can be used
to efficiently solve nonparametric extensions of $\ell^p$ regularized learning
methods. Our main contribution is proposing a fast dual algorithm, and showing
that it allows to solve the problem efficiently. Our results contrast recent
findings suggesting kernel methods cannot be extended beyond Hilbert setting.
Numerical experiments confirm the effectiveness of the method.
",0,0,1,1,0,0
957,Adversarial Phenomenon in the Eyes of Bayesian Deep Learning,"  Deep Learning models are vulnerable to adversarial examples, i.e.\ images
obtained via deliberate imperceptible perturbations, such that the model
misclassifies them with high confidence. However, class confidence by itself is
an incomplete picture of uncertainty. We therefore use principled Bayesian
methods to capture model uncertainty in prediction for observing adversarial
misclassification. We provide an extensive study with different Bayesian neural
networks attacked in both white-box and black-box setups. The behaviour of the
networks for noise, attacks and clean test data is compared. We observe that
Bayesian neural networks are uncertain in their predictions for adversarial
perturbations, a behaviour similar to the one observed for random Gaussian
perturbations. Thus, we conclude that Bayesian neural networks can be
considered for detecting adversarial examples.
",1,0,0,1,0,0
958,Comparative Climates of TRAPPIST-1 planetary system: results from a simple climate-vegetation model,"  The recent discovery of the planetary system hosted by the ultracool dwarf
star TRAPPIST-1 could open new perspectives into the investigation of planetary
climates of Earth-sized exoplanets, their atmospheres and their possible
habitability. In this paper, we use a simple climate-vegetation energy-balance
model to study the climate of the seven TRAPPIST-1 planets and the climate
dependence on the global albedo, on the fraction of vegetation that could cover
their surfaces and on the different greenhouse conditions. The model allows us
to investigate whether liquid water could be maintained on the planetary
surfaces (i.e., by defining a ""surface water zone"") in different planetary
conditions, with or without the presence of greenhouse effect.
It is shown that planet TRAPPIST-1d seems to be the most stable from an
Earth-like perspective, since it resides in the surface water zone for a wide
range of reasonable values of the model parameters. Moreover, according to the
model outer planets (f, g and h) cannot host liquid water on their surfaces,
even for Earth-like conditions, entering a snowball state. Although very
simple, the model allows to extract the main features of the TRAPPIST-1
planetary climates.
",0,1,0,0,0,0
959,Boundedness of $\mathbb{Q}$-Fano varieties with degrees and alpha-invariants bounded from below,"  We show that $\mathbb{Q}$-Fano varieties of fixed dimension with
anti-canonical degrees and alpha-invariants bounded from below form a bounded
family. As a corollary, K-semistable $\mathbb{Q}$-Fano varieties of fixed
dimension with anti-canonical degrees bounded from below form a bounded family.
",0,0,1,0,0,0
960,Width Hierarchies for Quantum and Classical Ordered Binary Decision Diagrams with Repeated Test,"  We consider quantum, nondterministic and probabilistic versions of known
computational model Ordered Read-$k$-times Branching Programs or Ordered Binary
Decision Diagrams with repeated test ($k$-QOBDD, $k$-NOBDD and $k$-POBDD). We
show width hierarchy for complexity classes of Boolean function computed by
these models and discuss relation between different variants of $k$-OBDD.
",1,0,0,0,0,0
961,Complementary views on electron spectra: From Fluctuation Diagnostics to real space correlations,"  We study the relation between the microscopic properties of a many-body
system and the electron spectra, experimentally accessible by photoemission. In
a recent paper [Phys. Rev. Lett. 114, 236402 (2015)], we introduced the
""fluctuation diagnostics"" approach, to extract the dominant wave vector
dependent bosonic fluctuations from the electronic self-energy. Here, we first
reformulate the theory in terms of fermionic modes, to render its connection
with resonance valence bond (RVB) fluctuations more transparent. Secondly, by
using a large-U expansion, where U is the Coulomb interaction, we relate the
fluctuations to real space correlations. Therefore, it becomes possible to
study how electron spectra are related to charge, spin, superconductivity and
RVB-like real space correlations, broadening the analysis of an earlier work
[Phys. Rev. B 89, 245130 (2014)]. This formalism is applied to the pseudogap
physics of the two-dimensional Hubbard model, studied in the dynamical cluster
approximation. We perform calculations for embedded clusters with up to 32
sites, having three inequivalent K-points at the Fermi surface. We find that as
U is increased, correlation functions gradually attain values consistent with
an RVB state. This first happens for correlation functions involving the
antinodal point and gradually spreads to the nodal point along the Fermi
surface. Simultaneously a pseudogap opens up along the Fermi surface. We relate
this to a crossover from a Kondo-like state to an RVB-like localized cluster
state and to the presence of RVB and spin fluctuations. These changes are
caused by a strong momentum dependence in the cluster bath-couplings along the
Fermi surface. We also show, from a more algorithmic perspective, how the
time-consuming calculations in fluctuation diagnostics can be drastically
simplified.
",0,1,0,0,0,0
962,Stall force of a cargo driven by N interacting motor proteins,"  We study a generic one-dimensional model for an intracellular cargo driven by
N motor proteins against an external applied force. The model includes
motor-cargo and motor-motor interactions. The cargo motion is described by an
over-damped Langevin equation, while motor dynamics is specified by hopping
rates which follow a local detailed balance condition with respect to change in
energy per hopping event. Based on this model, we show that the stall force,
the mean external force corresponding to zero mean cargo velocity, is
completely independent of the details of the interactions and is, therefore,
always equal to the sum of the stall forces of the individual motors. This
exact result is arrived on the basis of a simple assumption: the (macroscopic)
state of stall of the cargo is analogous to a state of thermodynamic
equilibrium, and is characterized by vanishing net probability current between
any two microstates, with the latter specified by motor positions relative to
the cargo. The corresponding probability distribution of the microstates under
stall is also determined. These predictions are in complete agreement with
numerical simulations, carried out using specific forms of interaction
potentials.
",0,1,0,0,0,0
963,Dynamics of a Camphoric Acid boat at the air-water interface,"  We report experiments on an agarose gel tablet loaded with camphoric acid
(c-boat) set into self-motion by interfacial tension gradients at the air-water
interface. We observe three distinct modes of c-boat motion: harmonic mode
where the c-boat speed oscillates sinusoidally in time, a steady mode where the
c-boat maintains constant speed, and a relaxation oscillation mode where the
c-boat maintains near-zero speed between sudden jumps in speed and position at
regular time intervals. Whereas all three modes have been separately reported
before in different systems, we show they belong to a common description.
Through control of the air-water surface tension with Sodium Dodecyl Sulfate
(SDS), we experimentally deduce the three self-propulsive modes result from
surface tension difference between Camphoric Acid (CA) and the ambient
surroundings.
",0,1,0,0,0,0
964,Separation of time scales and direct computation of weights in deep neural networks,"  Artificial intelligence is revolutionizing our lives at an ever increasing
pace. At the heart of this revolution is the recent advancements in deep neural
networks (DNN), learning to perform sophisticated, high-level tasks. However,
training DNNs requires massive amounts of data and is very computationally
intensive. Gaining analytical understanding of the solutions found by DNNs can
help us devise more efficient training algorithms, replacing the commonly used
mthod of stochastic gradient descent (SGD). We analyze the dynamics of SGD and
show that, indeed, direct computation of the solutions is possible in many
cases. We show that a high performing setup used in DNNs introduces a
separation of time-scales in the training dynamics, allowing SGD to train
layers from the lowest (closest to input) to the highest. We then show that for
each layer, the distribution of solutions found by SGD can be estimated using a
class-based principal component analysis (PCA) of the layer's input. This
finding allows us to forgo SGD entirely and directly derive the DNN parameters
using this class-based PCA, which can be well estimated using significantly
less data than SGD. We implement these results on image datasets MNIST, CIFAR10
and CIFAR100 and find that, in fact, layers derived using our class-based PCA
perform comparable or superior to neural networks of the same size and
architecture trained using SGD. We also confirm that the class-based PCA often
converges using a fraction of the data required for SGD. Thus, using our method
training time can be reduced both by requiring less training data than SGD, and
by eliminating layers in the costly backpropagation step of the training.
",1,0,0,1,0,0
965,$q$-deformed quadrature operator and optical tomogram,"  In this letter, we define the homodyne $q$-deformed quadrature operator.
Analytic expression for the wavefunctions of $q$-deformed oscillator in the
quadrature basis are found. Furthermore, we compute the explicit analytical
expression for the tomogram of the $q$-deformed coherent states by finding the
eigenstates of the $q$-deformed quadrature operator.
",0,1,1,0,0,0
966,Efficient Graph Edit Distance Computation and Verification via Anchor-aware Lower Bound Estimation,"  Graph edit distance (GED) is an important similarity measure adopted in a
similarity-based analysis between two graphs, and computing GED is a primitive
operator in graph database analysis. Partially due to the NP-hardness, the
existing techniques for computing GED are only able to process very small
graphs with less than 30 vertices. Motivated by this, in this paper we
systematically study the problems of both GED computation, and GED verification
(i.e., verify whether the GED between two graphs is no larger than a user-given
threshold). Firstly, we develop a unified framework that can be instantiated
into either a best-first search approach AStar+ or a depth-first search
approach DFS+. Secondly, we design anchor-aware lower bound estimation
techniques to compute tighter lower bounds for intermediate search states,
which significantly reduce the search spaces of both AStar+ and DFS+. We also
propose efficient techniques to compute the lower bounds. Thirdly, based on our
unified framework, we contrast AStar+ with DFS+ regarding their time and space
complexities, and recommend that AStar+ is better than DFS+ by having a much
smaller search space. Extensive empirical studies validate that AStar+ performs
better than DFS+, and show that our AStar+-BMa approach outperforms the
state-of-the-art technique by more than four orders of magnitude.
",1,0,0,0,0,0
967,Asymptotic measures and links in simplicial complexes,"  We introduce canonical measures on a locally finite simplicial complex $K$
and study their asymptotic behavior under infinitely many barycentric
subdivisions. We also compute the face polynomial of the asymptotic link and
dual block of a simplex in the $d^{th}$ barycentric subdivision $Sd^d(K)$ of
$K$, $d\gg0$. It is almost everywhere constant. When $K$ is finite, we study
the limit face polynomial of $Sd^d(K)$ after F.Brenti-V.Welker and
E.Delucchi-A.Pixton-L.Sabalka.
",0,0,1,0,0,0
968,Usability of Humanly Computable Passwords,"  Reusing passwords across multiple websites is a common practice that
compromises security. Recently, Blum and Vempala have proposed password
strategies to help people calculate, in their heads, passwords for different
sites without dependence on third-party tools or external devices. Thus far,
the security and efficiency of these ""mental algorithms"" has been analyzed only
theoretically. But are such methods usable? We present the first usability
study of humanly computable password strategies, involving a learning phase (to
learn a password strategy), then a rehearsal phase (to login to a few
websites), and multiple follow-up tests. In our user study, with training,
participants were able to calculate a deterministic eight-character password
for an arbitrary new website in under 20 seconds.
",1,0,0,0,0,0
969,Shared urbanism: Big data on accommodation sharing in urban Australia,"  As affordability pressures and tight rental markets in global cities mount,
online shared accommodation sites proliferate. Home sharing arrangements
present dilemmas for planning that aims to improve health and safety standards,
while supporting positives such as the usage of dormant stock and the relieving
of rental pressures on middle/lower income earners. Currently, no formal data
exists on this internationally growing trend. Here, we present a first
quantitative glance on shared accommodation practices across all major urban
centers of Australia enabled via collection and analysis of thousands of online
listings. We examine, countrywide, the spatial and short time scale temporal
characteristics of this market, along with preliminary analysis on rents,
dwelling types and other characteristics. Findings have implications for
housing policy makers and planning practitioners seeking to monitor and respond
to housing policy and affordability pressures in formal and informal housing
markets.
",1,1,0,0,0,0
970,Structural changes in the interbank market across the financial crisis from multiple core-periphery analysis,"  Interbank markets are often characterised in terms of a core-periphery
network structure, with a highly interconnected core of banks holding the
market together, and a periphery of banks connected mostly to the core but not
internally. This paradigm has recently been challenged for short time scales,
where interbank markets seem better characterised by a bipartite structure with
more core-periphery connections than inside the core. Using a novel
core-periphery detection method on the eMID interbank market, we enrich this
picture by showing that the network is actually characterised by multiple
core-periphery pairs. Moreover, a transition from core-periphery to bipartite
structures occurs by shortening the temporal scale of data aggregation. We
further show how the global financial crisis transformed the market, in terms
of composition, multiplicity and internal organisation of core-periphery pairs.
By unveiling such a fine-grained organisation and transformation of the
interbank market, our method can find important applications in the
understanding of how distress can propagate over financial networks.
",0,0,0,0,0,1
971,Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples,"  We present a novel algorithm that uses exact learning and abstraction to
extract a deterministic finite automaton describing the state dynamics of a
given trained RNN. We do this using Angluin's L* algorithm as a learner and the
trained RNN as an oracle. Our technique efficiently extracts accurate automata
from trained RNNs, even when the state vectors are large and require fine
differentiation.
",1,0,0,0,0,0
972,Performance analysis of smart digital signage system based on software-defined IoT and invisible image sensor communication,"  Everything in the world is being connected, and things are becoming
interactive. The future of the interactive world depends on the future Internet
of Things (IoT). Software-defined networking (SDN) technology, a new paradigm
in the networking area, can be useful in creating an IoT because it can handle
interactivity by controlling physical devices, transmission of data among them,
and data acquisition. However, digital signage can be one of the promising
technologies in this era of technology that is progressing toward the
interactive world, connecting users to the IoT network through device-to-device
communication technology. This article illustrates a novel prototype that is
mainly focused on a smart digital signage system comprised of software-defined
IoT (SD-IoT) and invisible image sensor communication technology. We have
proposed an SDN scheme with a view to initiating its flexibility and
compatibility for an IoT network-based smart digital signage system. The idea
of invisible communication can make the users of the technology trendier to it,
and the usage of unused resources such as images and videos can be ensured. In
addition, this communication has paved the way for interactivity between the
user and digital signage, where the digital signage and the camera of a
smartphone can be operated as a transmitter and a receiver, respectively. The
proposed scheme might be applicable to real-world applications because SDN has
the flexibility to adapt with the alteration of network status without any
hardware modifications while displays and smartphones are available everywhere.
A performance analysis of this system showed the advantages of an SD-IoT
network over an Internet protocol-based IoT network considering a queuing
analysis for a dynamic link allocation process in the case of user access to
the IoT network.
",1,0,0,0,0,0
973,What pebbles are made of: Interpretation of the V883 Ori disk,"  Recently, an Atacama Large Millimeter/submillimeter Array (ALMA) observation
of the water snow line in the protoplanetary disk around the FU Orionis star
V883 Ori was reported. The radial variation of the spectral index at
mm-wavelengths around the snow line was interpreted as being due to a pileup of
particles interior to the snow line. However, radial transport of solids in the
outer disk operates on timescales much longer than the typical timescale of an
FU Ori outburst ($10^{1}$--$10^{2}$ yr). Consequently, a steady-state pileup is
unlikely. We argue that it is only necessary to consider water evaporation and
re-coagulation of silicates to explain the recent ALMA observation of V883 Ori
because these processes are short enough to have had their impact since the
outburst. Our model requires the inner disk to have already been optically
thick before the outburst, and our results suggest that the carbon content of
pebbles is low.
",0,1,0,0,0,0
974,Existence and uniqueness of steady weak solutions to the Navier-Stokes equations in $\mathbb{R}^2$,"  The existence of weak solutions to the stationary Navier-Stokes equations in
the whole plane $\mathbb{R}^2$ is proven. This particular geometry was the only
case left open since the work of Leray in 1933. The reason is that due to the
absence of boundaries the local behavior of the solutions cannot be controlled
by the enstrophy in two dimensions. We overcome this difficulty by constructing
approximate weak solutions having a prescribed mean velocity on some given
bounded set. As a corollary, we obtain infinitely many weak solutions in
$\mathbb{R}^2$ parameterized by this mean velocity, which is reminiscent of the
expected convergence of the velocity field at large distances to any prescribed
constant vector field. This explicit parameterization of the weak solutions
allows us to prove a weak-strong uniqueness theorem for small data. The
question of the asymptotic behavior of the weak solutions remains however open,
when the uniqueness theorem doesn't apply.
",0,0,1,0,0,0
975,More on products of Baire spaces,"  New results on the Baire product problem are presented. It is shown that an
arbitrary product of almost locally ccc Baire spaces is Baire; moreover, the
product of a Baire space and a 1st countable space which is $\beta$-unfavorable
in the strong Choquet game is Baire.
",0,0,1,0,0,0
976,Social versus Moral preferences in the Ultimatum Game: A theoretical model and an experiment,"  In the Ultimatum Game (UG) one player, named ""proposer"", has to decide how to
allocate a certain amount of money between herself and a ""responder"". If the
offer is greater than or equal to the responder's minimum acceptable offer
(MAO), then the money is split as proposed, otherwise, neither the proposer nor
the responder get anything. The UG has intrigued generations of behavioral
scientists because people in experiments blatantly violate the equilibrium
predictions that self-interested proposers offer the minimum available non-zero
amount, and self-interested responders accept. Why are these predictions
violated? Previous research has mainly focused on the role of social
preferences. Little is known about the role of general moral preferences for
doing the right thing, preferences that have been shown to play a major role in
other social interactions (e.g., Dictator Game and Prisoner's Dilemma). Here I
develop a theoretical model and an experiment designed to pit social
preferences against moral preferences. I find that, although people recognize
that offering half and rejecting low offers are the morally right things to do,
moral preferences have no causal impact on UG behavior. The experimental data
are indeed well fit by a model according to which: (i) high UG offers are
motivated by inequity aversion and, to a lesser extent, self-interest; (ii)
high MAOs are motivated by inequity aversion.
",0,0,0,0,1,0
977,Moderate Deviation Analysis for Classical-Quantum Channels and Quantum Hypothesis Testing,"  In this work, we study the tradeoffs between the error probabilities of
classical-quantum channels and the blocklength $n$ when the transmission rates
approach the channel capacity at a rate slower than $1/\sqrt{n}$, a research
topic known as moderate deviation analysis. We show that the optimal error
probability vanishes under this rate convergence. Our main technical
contributions are a tight quantum sphere-packing bound, obtained via Chaganty
and Sethuraman's concentration inequality in strong large deviation theory, and
asymptotic expansions of error-exponent functions. Moderate deviation analysis
for quantum hypothesis testing is also established. The converse directly
follows from our channel coding result, while the achievability relies on a
martingale inequality.
",1,0,0,0,0,0
978,A simple recipe for making accurate parametric inference in finite sample,"  Constructing tests or confidence regions that control over the error rates in
the long-run is probably one of the most important problem in statistics. Yet,
the theoretical justification for most methods in statistics is asymptotic. The
bootstrap for example, despite its simplicity and its widespread usage, is an
asymptotic method. There are in general no claim about the exactness of
inferential procedures in finite sample. In this paper, we propose an
alternative to the parametric bootstrap. We setup general conditions to
demonstrate theoretically that accurate inference can be claimed in finite
sample.
",0,0,1,1,0,0
979,GBDT of discrete skew-selfadjoint Dirac systems and explicit solutions of the corresponding non-stationary problems,"  Generalized B??cklund-Darboux transformations (GBDTs) of discrete
skew-selfadjoint Dirac systems have been successfully used for explicit solving
of direct and inverse problems of Weyl-Titchmarsh theory. During explicit
solving of the direct and inverse problems, we considered GBDTs of the trivial
initial systems. However, GBDTs of arbitrary discrete skew-selfadjoint Dirac
systems are important as well and we introduce these transformations in the
present paper. The obtained results are applied to the construction of explicit
solutions of the interesting related non-stationary systems.
",0,0,1,0,0,0
980,Panel collapse and its applications,"  We describe a procedure called panel collapse for replacing a CAT(0) cube
complex $\Psi$ by a ""lower complexity"" CAT(0) cube complex $\Psi_\bullet$
whenever $\Psi$ contains a codimension-$2$ hyperplane that is extremal in one
of the codimension-$1$ hyperplanes containing it. Although $\Psi_\bullet$ is
not in general a subcomplex of $\Psi$, it is a subspace consisting of a
subcomplex together with some cubes that sit inside $\Psi$ ""diagonally"". The
hyperplanes of $\Psi_\bullet$ extend to hyperplanes of $\Psi$. Applying this
procedure, we prove: if a group $G$ acts cocompactly on a CAT(0) cube complex
$\Psi$, then there is a CAT(0) cube complex $\Omega$ so that $G$ acts
cocompactly on $\Omega$ and for each hyperplane $H$ of $\Omega$, the stabiliser
in $G$ of $H$ acts on $H$ essentially.
Using panel collapse, we obtain a new proof of Stallings's theorem on groups
with more than one end. As another illustrative example, we show that panel
collapse applies to the exotic cubulations of free groups constructed by Wise.
Next, we show that the CAT(0) cube complexes constructed by Cashen-Macura can
be collapsed to trees while preserving all of the necessary group actions. (It
also illustrates that our result applies to actions of some non-discrete
groups.) We also discuss possible applications to quasi-isometric rigidity for
certain classes of graphs of free groups with cyclic edge groups. Panel
collapse is also used in forthcoming work of the first-named author and Wilton
to study fixed-point sets of finite subgroups of $\mathrm{Out}(F_n)$ on the
free splitting complex.
",0,0,1,0,0,0
981,Smooth Neighbors on Teacher Graphs for Semi-supervised Learning,"  The recently proposed self-ensembling methods have achieved promising results
in deep semi-supervised learning, which penalize inconsistent predictions of
unlabeled data under different perturbations. However, they only consider
adding perturbations to each single data point, while ignoring the connections
between data samples. In this paper, we propose a novel method, called Smooth
Neighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on
the predictions of the teacher model, i.e., the implicit self-ensemble of
models. Then the graph serves as a similarity measure with respect to which the
representations of ""similar"" neighboring points are learned to be smooth on the
low-dimensional manifold. We achieve state-of-the-art results on
semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for
CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,
the improvements are significant when the labels are fewer. For the
non-augmented MNIST with only 20 labels, the error rate is reduced from
previous 4.81% to 1.36%. Our method also shows robustness to noisy labels.
",1,0,0,1,0,0
982,Detecting Heavy Flows in the SDN Match and Action Model,"  Efficient algorithms and techniques to detect and identify large flows in a
high throughput traffic stream in the SDN match-and-action model are presented.
This is in contrast to previous work that either deviated from the match and
action model by requiring additional switch level capabilities or did not
exploit the SDN data plane. Our construction has two parts; (a) how to sample
in an SDN match and action model, (b) how to detect large flows efficiently and
in a scalable way, in the SDN model.
Our large flow detection methods provide high accuracy and present a good and
practical tradeoff between switch - controller traffic, and the number of
entries required in the switch flow table. Based on different parameters, we
differentiate between heavy flows, elephant flows and bulky flows and present
efficient algorithms to detect flows of the different types.
Additionally, as part of our heavy flow detection scheme, we present sampling
methods to sample packets with arbitrary probability $p$ per packet or per byte
that traverses an SDN switch.
Finally, we show how our algorithms can be adapted to a distributed
monitoring SDN setting with multiple switches, and easily scale with the number
of monitoring switches.
",1,0,0,0,0,0
983,Optimal Resource Allocation with Node and Link Capacity Constraints in Complex Networks,"  With the tremendous increase of the Internet traffic, achieving the best
performance with limited resources is becoming an extremely urgent problem. In
order to address this concern, in this paper, we build an optimization problem
which aims to maximize the total utility of traffic flows with the capacity
constraint of nodes and links in the network. Based on Duality Theory, we
propose an iterative algorithm which adjusts the rates of traffic flows and
capacity of nodes and links simultaneously to maximize the total utility.
Simulation results show that our algorithm performs better than the NUP
algorithm on BA and ER network models, which has shown to get the best
performance so far. Since our research combines the topology information with
capacity constraint, it may give some insights for resource allocation in real
communication networks.
",1,1,0,0,0,0
984,On the complexity of solving a decision problem with flow-depending costs: the case of the IJsselmeer dikes,"  We consider a fundamental integer programming (IP) model for cost-benefit
analysis flood protection through dike building in the Netherlands, due to
Verweij and Zwaneveld.
Experimental analysis with data for the Ijsselmeer lead to integral optimal
solution of the linear programming relaxation of the IP model.
This naturally led to the question of integrality of the polytope associated
with the IP model.
In this paper we first give a negative answer to this question by
establishing non-integrality of the polytope.
Second, we establish natural conditions that guarantee the linear programming
relaxation of the IP model to be integral.
We then test the most recent data on flood probabilities, damage and
investment costs of the IJsselmeer for these conditions.
Third, we show that the IP model can be solved in polynomial time when the
number of dike segments, or the number of feasible barrier heights, are
constant.
",0,0,0,0,0,1
985,Achieving Spectrum Efficient Communication Under Cross-Technology Interference,"  In wireless communication, heterogeneous technologies such as WiFi, ZigBee
and BlueTooth operate in the same ISM band.With the exponential growth in the
number of wireless devices, the ISM band becomes more and more crowded. These
heterogeneous devices have to compete with each other to access spectrum
resources, generating cross-technology interference (CTI). Since CTI may
destroy wireless communication, this field is facing an urgent and challenging
need to investigate spectrum efficiency under CTI. In this paper, we introduce
a novel framework to address this problem from two aspects. On the one hand,
from the perspective of each communication technology itself, we propose novel
channel/link models to capture the channel/link status under CTI. On the other
hand, we investigate spectrum efficiency from the perspective by taking all
heterogeneous technologies as a whole and building crosstechnology
communication among them. The capability of direct communication among
heterogeneous devices brings great opportunities to harmoniously sharing the
spectrum with collaboration rather than competition.
",1,0,0,0,0,0
986,A Galactic Cosmic Ray Electron Intensity Increase of a factor of up to 100 At Energies between 3 and 50 MeV in the Heliosheath between the Termination Shock and the Heliopause Due to Solar Modulation As Measured by Voyager 1,"  We have derived background corrected intensities of 3-50 MeV galactic
electrons observed by Voyager 1 as it passes through the heliosheath from 95 to
122 AU. The overall intensity change of the background corrected data from the
inner to the outer boundary of the heliosheath is a maximum of a factor ~100 at
15 MeV. At lower energies this fractional change becomes less and the corrected
electron spectra in the heliosheath becomes progressively steeper, reaching
values ~ -2.5 for the spectral index just outside of the termination shock. At
higher energies the spectra of electrons has an exponent changing from the
negative LIS spectral index of -1.3 to values approaching zero in the
heliosheath as a result of the solar modulation of the galactic electron
component. The large modulation effects observed below ~100 MV are possible
evidence for enhanced diffusion as part of the modulation process for electrons
in the heliosheath.
",0,1,0,0,0,0
987,Statistical foundations for assessing the difference between the classical and weighted-Gini betas,"  The `beta' is one of the key quantities in the capital asset pricing model
(CAPM). In statistical language, the beta can be viewed as the slope of the
regression line fitted to financial returns on the market against the returns
on the asset under consideration. The insurance counterpart of CAPM, called the
weighted insurance pricing model (WIPM), gives rise to the so-called
weighted-Gini beta. The aforementioned two betas may or may not coincide,
depending on the form of the underlying regression function, and this has
profound implications when designing portfolios and allocating risk capital. To
facilitate these tasks, in this paper we develop large-sample statistical
inference results that, in a straightforward fashion, imply confidence
intervals for, and hypothesis tests about, the equality of the two betas.
",0,0,1,1,0,0
988,Integrating Human-Provided Information Into Belief State Representation Using Dynamic Factorization,"  In partially observed environments, it can be useful for a human to provide
the robot with declarative information that represents probabilistic relational
constraints on properties of objects in the world, augmenting the robot's
sensory observations. For instance, a robot tasked with a search-and-rescue
mission may be informed by the human that two victims are probably in the same
room. An important question arises: how should we represent the robot's
internal knowledge so that this information is correctly processed and combined
with raw sensory information? In this paper, we provide an efficient belief
state representation that dynamically selects an appropriate factoring,
combining aspects of the belief when they are correlated through information
and separating them when they are not. This strategy works in open domains, in
which the set of possible objects is not known in advance, and provides
significant improvements in inference time over a static factoring, leading to
more efficient planning for complex partially observed tasks. We validate our
approach experimentally in two open-domain planning problems: a 2D discrete
gridworld task and a 3D continuous cooking task. A supplementary video can be
found at this http URL.
",1,0,0,0,0,0
989,Characterizing complex networks using Entropy-degree diagrams: unveiling changes in functional brain connectivity induced by Ayahuasca,"  Open problems abound in the theory of complex networks, which has found
successful application to diverse fields of science. With the aim of further
advancing the understanding of the brain's functional connectivity, we propose
to evaluate a network metric which we term the geodesic entropy. This entropy,
in a way that can be made precise, quantifies the Shannon entropy of the
distance distribution to a specific node from all other nodes. Measurements of
geodesic entropy allow for the characterization of the structural information
of a network that takes into account the distinct role of each node into the
network topology. The measurement and characterization of this structural
information has the potential to greatly improve our understanding of sustained
activity and other emergent behaviors in networks, such as self-organized
criticality sometimes seen in such contexts. We apply these concepts and
methods to study the effects of how the psychedelic Ayahuasca affects the
functional connectivity of the human brain. We show that the geodesic entropy
is able to differentiate the functional networks of the human brain in two
different states of consciousness in the resting state: (i) the ordinary waking
state and (ii) a state altered by ingestion of the Ayahuasca. The entropy of
the nodes of brain networks from subjects under the influence of Ayahuasca
diverge significantly from those of the ordinary waking state. The functional
brain networks from subjects in the altered state have, on average, a larger
geodesic entropy compared to the ordinary state. We conclude that geodesic
entropy is a useful tool for analyzing complex networks and discuss how and why
it may bring even further valuable insights into the study of the human brain
and other empirical networks.
",0,0,0,0,1,0
990,On Integral Upper Limits Assuming Power Law Spectra and the Sensitivity in High-Energy Astronomy,"  The high-energy non-thermal universe is dominated by power law-like spectra.
Therefore results in high-energy astronomy are often reported as parameters of
power law fits, or, in the case of a non-detection, as an upper limit assuming
the underlying unseen spectrum behaves as a power law. In this paper I
demonstrate a simple and powerful one-to-one relation of the integral upper
limit in the two dimensional power law parameter space into the spectrum
parameter space and use this method to unravel the so far convoluted question
of the sensitivity of astroparticle telescopes.
",0,1,0,0,0,0
991,Estimates for solutions of Dirac equations and an application to a geometric elliptic-parabolic problem,"  We develop estimates for the solutions and derive existence and uniqueness
results of various local boundary value problems for Dirac equations that
improve all relevant results known in the literature. With these estimates at
hand, we derive a general existence, uniqueness and regularity theorem for
solutions of Dirac equations with such boundary conditions. We also apply these
estimates to a new nonlinear elliptic-parabolic problem, the Dirac-harmonic
heat flow on Riemannian spin manifolds. This problem is motivated by the
supersymmetric nonlinear $\sigma$-model and combines a harmonic heat flow type
equation with a Dirac equation that depends nonlinearly on the flow.
",0,0,1,0,0,0
992,Analysis of Political Party Twitter Accounts' Retweeters During Japan's 2017 Election,"  In modern election campaigns, political parties utilize social media to
advertise their policies and candidates and to communicate to the electorate.
In Japan's latest general election in 2017, the 48th general election for the
Lower House, social media, especially Twitter, was actively used. In this
paper, we analyze the users who retweeted tweets of political parties on
Twitter during the election. Our aim is to clarify what kinds of users are
diffusing (retweeting) tweets of political parties. The results indicate that
the characteristics of retweeters of the largest ruling party (Liberal
Democratic Party of Japan) and the largest opposition party (The Constitutional
Democratic Party of Japan) were similar, even though the retweeters did not
overlap each other. We also found that a particular opposition party (Japanese
Communist Party) had quite different characteristics from other political
parties.
",1,0,0,0,0,0
993,When a triangle is isosceles?,"  In 1840 Jacob Steiner on Christian Rudolf's request proved that a triangle
with two equal bisectors is isosceles. But what about changing the bisectors to
cevians? Cevian is any line segment in a triangle with one endpoint on a vertex
of the triangle and other endpoint on the opposite side. Not for any pairs of
equal cevians the triangle is isosceles. Theorem. If for a triangle ABC there
are equal cevians issuing from A and B, which intersect on the bisector or on
the median of the angle C, then AC=BC (so the triangle ABC is isosceles).
Proposition. Let ABC be an isosceles triangle. Define circle C to be the circle
symmetric relative to AB to the circumscribed circle of the triangle ABC. Then
the locus of intersection points of pairs of equal cevians is the union of the
base AB, the triangle's axis of symmetry, and the circle C.
",0,0,1,0,0,0
994,Anomaly detecting and ranking of the cloud computing platform by multi-view learning,"  Anomaly detecting as an important technical in cloud computing is applied to
support smooth running of the cloud platform. Traditional detecting methods
based on statistic, analysis, etc. lead to the high false-alarm rate due to
non-adaptive and sensitive parameters setting. We presented an online model for
anomaly detecting using machine learning theory. However, most existing methods
based on machine learning linked all features from difference sub-systems into
a long feature vector directly, which is difficult to both exploit the
complement information between sub-systems and ignore multi-view features
enhancing the classification performance. Aiming to this problem, the proposed
method automatic fuses multi-view features and optimize the discriminative
model to enhance the accuracy. This model takes advantage of extreme learning
machine (ELM) to improve detection efficiency. ELM is the single hidden layer
neural network, which is transforming iterative solution the output weights to
solution of linear equations and avoiding the local optimal solution. Moreover,
we rank anomies according to the relationship between samples and the
classification boundary, and then assigning weights for ranked anomalies,
retraining the classification model finally. Our method exploits the complement
information between sub-systems sufficiently, and avoids the influence from
imbalance dataset, therefore, deal with various challenges from the cloud
computing platform. We deploy the privately cloud platform by Openstack,
verifying the proposed model and comparing results to the state-of-the-art
methods with better efficiency and simplicity.
",1,0,0,1,0,0
995,On self-affine sets,"  We survey the dimension theory of self-affine sets for general mathematical
audience. The article is in Finnish.
",0,0,1,0,0,0
996,The effect of phase change on stability of convective flow in a layer of volatile liquid driven by a horizontal temperature gradient,"  Buoyancy-thermocapillary convection in a layer of volatile liquid driven by a
horizontal temperature gradient arises in a variety of situations. Recent
studies have shown that the composition of the gas phase, which is typically a
mixture of vapour and air, has a noticeable effect on the critical Marangoni
number describing the onset of convection as well as on the observed convection
pattern. Specifically, as the total pressure or, equivalently, the average
concentration of air is decreased, the threshold of the instability leading to
the emergence of convective rolls is found to increase rather significantly. We
present a linear stability analysis of the problem which shows that this trend
can be readily understood by considering the transport of heat and vapour
through the gas phase. In particular, we show that transport in the gas phase
has a noticeable effect even at atmospheric conditions, when phase change is
greatly suppressed.
",0,1,0,0,0,0
997,A graph model of message passing processes,"  In the paper we consider a graph model of message passing processes and
present a method verification of message passing processes. The method is
illustrated by an example of a verification of sliding window protocol.
",1,0,0,0,0,0
998,Learning the Sparse and Low Rank PARAFAC Decomposition via the Elastic Net,"  In this article, we derive a Bayesian model to learning the sparse and low
rank PARAFAC decomposition for the observed tensor with missing values via the
elastic net, with property to find the true rank and sparse factor matrix which
is robust to the noise. We formulate efficient block coordinate descent
algorithm and admax stochastic block coordinate descent algorithm to solve it,
which can be used to solve the large scale problem. To choose the appropriate
rank and sparsity in PARAFAC decomposition, we will give a solution path by
gradually increasing the regularization to increase the sparsity and decrease
the rank. When we find the sparse structure of the factor matrix, we can fixed
the sparse structure, using a small to regularization to decreasing the
recovery error, and one can choose the proper decomposition from the solution
path with sufficient sparse factor matrix with low recovery error. We test the
power of our algorithm on the simulation data and real data, which show it is
powerful.
",0,0,1,1,0,0
999,"Holomorphic differentials, thermostats and Anosov flows","  We introduce a new family of thermostat flows on the unit tangent bundle of
an oriented Riemannian $2$-manifold. Suitably reparametrised, these flows
include the geodesic flow of metrics of negative Gauss curvature and the
geodesic flow induced by the Hilbert metric on the quotient surface of
divisible convex sets. We show that the family of flows can be parametrised in
terms of certain weighted holomorphic differentials and investigate their
properties. In particular, we prove that they admit a dominated splitting and
we identify special cases in which the flows are Anosov. In the latter case, we
study when they admit an invariant measure in the Lebesgue class and the
regularity of the weak foliations.
",0,0,1,0,0,0
1000,"Multiplicative slices, relativistic Toda and shifted quantum affine algebras","  We introduce the shifted quantum affine algebras. They map homomorphically
into the quantized $K$-theoretic Coulomb branches of $3d\ {\mathcal N}=4$ SUSY
quiver gauge theories. In type $A$, they are endowed with a coproduct, and they
act on the equivariant $K$-theory of parabolic Laumon spaces. In type $A_1$,
they are closely related to the open relativistic quantum Toda lattice of type
$A$.
",0,0,1,0,0,0
1001,Statistical Latent Space Approach for Mixed Data Modelling and Applications,"  The analysis of mixed data has been raising challenges in statistics and
machine learning. One of two most prominent challenges is to develop new
statistical techniques and methodologies to effectively handle mixed data by
making the data less heterogeneous with minimum loss of information. The other
challenge is that such methods must be able to apply in large-scale tasks when
dealing with huge amount of mixed data. To tackle these challenges, we
introduce parameter sharing and balancing extensions to our recent model, the
mixed-variate restricted Boltzmann machine (MV.RBM) which can transform
heterogeneous data into homogeneous representation. We also integrate
structured sparsity and distance metric learning into RBM-based models. Our
proposed methods are applied in various applications including latent patient
profile modelling in medical data analysis and representation learning for
image retrieval. The experimental results demonstrate the models perform better
than baseline methods in medical data and outperform state-of-the-art rivals in
image dataset.
",1,0,0,1,0,0
1002,Near-field coupling of gold plasmonic antennas for sub-100 nm magneto-thermal microscopy,"  The development of spintronic technology with increasingly dense, high-speed,
and complex devices will be accelerated by accessible microscopy techniques
capable of probing magnetic phenomena on picosecond time scales and at deeply
sub-micron length scales. A recently developed time-resolved magneto-thermal
microscope provides a path towards this goal if it is augmented with a
picosecond, nanoscale heat source. We theoretically study adiabatic
nanofocusing and near-field heat induction using conical gold plasmonic
antennas to generate sub-100 nm thermal gradients for time-resolved
magneto-thermal imaging. Finite element calculations of antenna-sample
interactions reveal focused electromagnetic loss profiles that are either
peaked directly under the antenna or are annular, depending on the sample's
conductivity, the antenna's apex radius, and the tip-sample separation. We find
that the thermal gradient is confined to 40 nm to 60 nm full width at half
maximum for realistic ranges of sample conductivity and apex radius. To
mitigate this variation, which is undesirable for microscopy, we investigate
the use of a platinum capping layer on top of the sample as a thermal
transduction layer to produce heat uniformly across different sample materials.
After determining the optimal capping layer thickness, we simulate the
evolution of the thermal gradient in the underlying sample layer, and find that
the temporal width is below 10 ps. These results lay a theoretical foundation
for nanoscale, time-resolved magneto-thermal imaging.
",0,1,0,0,0,0
1003,A reproducible effect size is more useful than an irreproducible hypothesis test to analyze high throughput sequencing datasets,"  Motivation: P values derived from the null hypothesis significance testing
framework are strongly affected by sample size, and are known to be
irreproducible in underpowered studies, yet no suitable replacement has been
proposed. Results: Here we present implementations of non-parametric
standardized median effect size estimates, dNEF, for high-throughput sequencing
datasets. Case studies are shown for transcriptome and tag-sequencing datasets.
The dNEF measure is shown to be more repro- ducible and robust than P values
and requires sample sizes as small as 3 to reproducibly identify differentially
abundant features. Availability: Source code and binaries freely available at:
this https URL, omicplotR, and
this https URL.
",0,0,0,0,1,0
1004,High temperature thermodynamics of the honeycomb-lattice Kitaev-Heisenberg model: A high temperature series expansion study,"  We develop high temperature series expansions for the thermodynamic
properties of the honeycomb-lattice Kitaev-Heisenberg model. Numerical results
for uniform susceptibility, heat capacity and entropy as a function of
temperature for different values of the Kitaev coupling $K$ and Heisenberg
exachange coupling $J$ (with $|J|\le |K|$) are presented. These expansions show
good convergence down to a temperature of a fraction of $K$ and in some cases
down to $T=K/10$. In the Kitaev exchange dominated regime, the inverse
susceptibility has a nearly linear temperature dependence over a wide
temperature range. However, we show that already at temperatures $10$-times the
Curie-Weiss temperature, the effective Curie-Weiss constant estimated from the
data can be off by a factor of 2. We find that the magnitude of the heat
capacity maximum at the short-range order peak, is substantially smaller for
small $J/K$ than for $J$ of order or larger than $K$. We suggest that this
itself represents a simple marker for the relative importance of the Kitaev
terms in these systems. Somewhat surprisingly, both heat capacity and
susceptibility data on Na$_2$IrO$_3$ are consistent with a dominant {\it
antiferromagnetic} Kitaev exchange constant of about $300-400$ $K$.
",0,1,0,0,0,0
1005,"Laplace Beltrami operator in the Baran metric and pluripotential equilibrium measure: the ball, the simplex and the sphere","  The Baran metric $\delta_E$ is a Finsler metric on the interior of $E\subset
\R^n$ arising from Pluripotential Theory. We consider the few instances, namely
$E$ being the ball, the simplex, or the sphere, where $\delta_E$ is known to be
Riemaniann and we prove that the eigenfunctions of the associated Laplace
Beltrami operator (with no boundary conditions) are the orthogonal polynomials
with respect to the pluripotential equilibrium measure $\mu_E$ of $E.$ We
conjecture that this may hold in a wider generality.
The considered differential operators have been already introduced in the
framework of orthogonal polynomials and studied in connection with certain
symmetry groups. In this work instead we highlight the relationships between
orthogonal polynomials with respect to $\mu_E$ and the Riemaniann structure
naturally arising from Pluripotential Theory
",0,0,1,0,0,0
1006,Magnetic polarons in a nonequilibrium polariton condensate,"  We consider a condensate of exciton-polaritons in a diluted magnetic
semiconductor microcavity. Such system may exhibit magnetic self-trapping in
the case of sufficiently strong coupling between polaritons and magnetic ions
embedded in the semiconductor. We investigate the effect of the nonequilibrium
nature of exciton-polaritons on the physics of the resulting self-trapped
magnetic polarons. We find that multiple polarons can exist at the same time,
and derive a critical condition for self-trapping which is different to the one
predicted previously in the equilibrium case. Using the Bogoliubov-de Gennes
approximation, we calculate the excitation spectrum and provide a physical
explanation in terms of the effective magnetic attraction between polaritons,
mediated by the ion subsystem.
",0,1,0,0,0,0
1007,Inference in Sparse Graphs with Pairwise Measurements and Side Information,"  We consider the statistical problem of recovering a hidden ""ground truth""
binary labeling for the vertices of a graph up to low Hamming error from noisy
edge and vertex measurements. We present new algorithms and a sharp
finite-sample analysis for this problem on trees and sparse graphs with poor
expansion properties such as hypergrids and ring lattices. Our method
generalizes and improves over that of Globerson et al. (2015), who introduced
the problem for two-dimensional grid lattices.
For trees we provide a simple, efficient, algorithm that infers the ground
truth with optimal Hamming error has optimal sample complexity and implies
recovery results for all connected graphs. Here, the presence of side
information is critical to obtain a non-trivial recovery rate. We then show how
to adapt this algorithm to tree decompositions of edge-subgraphs of certain
graph families such as lattices, resulting in optimal recovery error rates that
can be obtained efficiently
The thrust of our analysis is to 1) use the tree decomposition along with
edge measurements to produce a small class of viable vertex labelings and 2)
apply an analysis influenced by statistical learning theory to show that we can
infer the ground truth from this class using vertex measurements. We show the
power of our method in several examples including hypergrids, ring lattices,
and the Newman-Watts model for small world graphs. For two-dimensional grids,
our results improve over Globerson et al. (2015) by obtaining optimal recovery
in the constant-height regime.
",1,0,0,0,0,0
1008,Oracle Importance Sampling for Stochastic Simulation Models,"  We consider the problem of estimating an expected outcome from a stochastic
simulation model using importance sampling. We propose a two-stage procedure
that involves a regression stage and a sampling stage to construct our
estimator. We introduce a parametric and a nonparametric regression estimator
in the first stage and study how the allocation between the two stages affects
the performance of final estimator. We derive the oracle property for both
approaches. We analyze the empirical performances of our approaches using two
simulated data and a case study on wind turbine reliability evaluation.
",0,0,0,1,0,0
1009,The Generalized Cross Validation Filter,"  Generalized cross validation (GCV) is one of the most important approaches
used to estimate parameters in the context of inverse problems and
regularization techniques. A notable example is the determination of the
smoothness parameter in splines. When the data are generated by a state space
model, like in the spline case, efficient algorithms are available to evaluate
the GCV score with complexity that scales linearly in the data set size.
However, these methods are not amenable to on-line applications since they rely
on forward and backward recursions. Hence, if the objective has been evaluated
at time $t-1$ and new data arrive at time t, then O(t) operations are needed to
update the GCV score. In this paper we instead show that the update cost is
$O(1)$, thus paving the way to the on-line use of GCV. This result is obtained
by deriving the novel GCV filter which extends the classical Kalman filter
equations to efficiently propagate the GCV score over time. We also illustrate
applications of the new filter in the context of state estimation and on-line
regularized linear system identification.
",1,0,0,1,0,0
1010,Coherence of Biochemical Oscillations is Bounded by Driving Force and Network Topology,"  Biochemical oscillations are prevalent in living organisms. Systems with a
small number of constituents cannot sustain coherent oscillations for an
indefinite time because of fluctuations in the period of oscillation. We show
that the number of coherent oscillations that quantifies the precision of the
oscillator is universally bounded by the thermodynamic force that drives the
system out of equilibrium and by the topology of the underlying biochemical
network of states. Our results are valid for arbitrary Markov processes, which
are commonly used to model biochemical reactions. We apply our results to a
model for a single KaiC protein and to an activator-inhibitor model that
consists of several molecules. From a mathematical perspective, based on strong
numerical evidence, we conjecture a universal constraint relating the imaginary
and real parts of the first non-trivial eigenvalue of a stochastic matrix.
",0,1,0,0,0,0
1011,How Do Classifiers Induce Agents To Invest Effort Strategically?,"  Algorithms are often used to produce decision-making rules that classify or
evaluate individuals. When these individuals have incentives to be classified a
certain way, they may behave strategically to influence their outcomes. We
develop a model for how strategic agents can invest effort in order to change
the outcomes they receive, and we give a tight characterization of when such
agents can be incentivized to invest specified forms of effort into improving
their outcomes as opposed to ""gaming"" the classifier. We show that whenever any
""reasonable"" mechanism can do so, a simple linear mechanism suffices.
",0,0,0,1,0,0
1012,Guiding Reinforcement Learning Exploration Using Natural Language,"  In this work we present a technique to use natural language to help
reinforcement learning generalize to unseen environments. This technique uses
neural machine translation, specifically the use of encoder-decoder networks,
to learn associations between natural language behavior descriptions and
state-action information. We then use this learned model to guide agent
exploration using a modified version of policy shaping to make it more
effective at learning in unseen environments. We evaluate this technique using
the popular arcade game, Frogger, under ideal and non-ideal conditions. This
evaluation shows that our modified policy shaping algorithm improves over a
Q-learning agent as well as a baseline version of policy shaping.
",1,0,0,1,0,0
1013,Of the People: Voting Is More Effective with Representative Candidates,"  In light of the classic impossibility results of Arrow and Gibbard and
Satterthwaite regarding voting with ordinal rules, there has been recent
interest in characterizing how well common voting rules approximate the social
optimum. In order to quantify the quality of approximation, it is natural to
consider the candidates and voters as embedded within a common metric space,
and to ask how much further the chosen candidate is from the population as
compared to the socially optimal one. We use this metric preference model to
explore a fundamental and timely question: does the social welfare of a
population improve when candidates are representative of the population? If so,
then by how much, and how does the answer depend on the complexity of the
metric space?
We restrict attention to the most fundamental and common social choice
setting: a population of voters, two independently drawn candidates, and a
majority rule election. When candidates are not representative of the
population, it is known that the candidate selected by the majority rule can be
thrice as far from the population as the socially optimal one. We examine how
this ratio improves when candidates are drawn independently from the population
of voters. Our results are two-fold: When the metric is a line, the ratio
improves from $3$ to $4-2\sqrt{2}$, roughly $1.1716$; this bound is tight. When
the metric is arbitrary, we show a lower bound of $1.5$ and a constant upper
bound strictly better than $2$ on the approximation ratio of the majority rule.
The positive result depends in part on the assumption that candidates are
independent and identically distributed. However, we show that independence
alone is not enough to achieve the upper bound: even when candidates are drawn
independently, if the population of candidates can be different from the
voters, then an upper bound of $2$ on the approximation is tight.
",1,0,0,0,0,0
1014,Cell Coverage Extension with Orthogonal Random Precoding for Massive MIMO Systems,"  In this paper, we investigate a coverage extension scheme based on orthogonal
random precoding (ORP) for the downlink of massive multiple-input
multiple-output (MIMO) systems. In this scheme, a precoding matrix consisting
of orthogonal vectors is employed at the transmitter to enhance the maximum
signal-to-interference-plus-noise ratio (SINR) of the user. To analyze and
optimize the ORP scheme in terms of cell coverage, we derive the analytical
expressions of the downlink coverage probability for two receiver structures,
namely, the single-antenna (SA) receiver and multiple-antenna receiver with
antenna selection (AS). The simulation results show that the analytical
expressions accurately capture the coverage behaviors of the systems employing
the ORP scheme. It is also shown that the optimal coverage performance is
achieved when a single precoding vector is used under the condition that the
threshold of the signal-to-noise ratio of the coverage is greater than one. The
performance of the ORP scheme is further analyzed when different random
precoder groups are utilized over multiple time slots to exploit precoding
diversity. The numerical results show that the proposed ORP scheme over
multiple time slots provides a substantial coverage gain over the space-time
coding scheme despite its low feedback overhead.
",1,0,1,0,0,0
1015,Hidden Community Detection in Social Networks,"  We introduce a new paradigm that is important for community detection in the
realm of network analysis. Networks contain a set of strong, dominant
communities, which interfere with the detection of weak, natural community
structure. When most of the members of the weak communities also belong to
stronger communities, they are extremely hard to be uncovered. We call the weak
communities the hidden community structure.
We present a novel approach called HICODE (HIdden COmmunity DEtection) that
identifies the hidden community structure as well as the dominant community
structure. By weakening the strength of the dominant structure, one can uncover
the hidden structure beneath. Likewise, by reducing the strength of the hidden
structure, one can more accurately identify the dominant structure. In this
way, HICODE tackles both tasks simultaneously.
Extensive experiments on real-world networks demonstrate that HICODE
outperforms several state-of-the-art community detection methods in uncovering
both the dominant and the hidden structure. In the Facebook university social
networks, we find multiple non-redundant sets of communities that are strongly
associated with residential hall, year of registration or career position of
the faculties or students, while the state-of-the-art algorithms mainly locate
the dominant ground truth category. In the Due to the difficulty of labeling
all ground truth communities in real-world datasets, HICODE provides a
promising approach to pinpoint the existing latent communities and uncover
communities for which there is no ground truth. Finding this unknown structure
is an extremely important community detection problem.
",1,1,0,1,0,0
1016,Two-photon exchange correction to the hyperfine splitting in muonic hydrogen,"  We reevaluate the Zemach, recoil and polarizability corrections to the
hyperfine splitting in muonic hydrogen expressing them through the low-energy
proton structure constants and obtain the precise values of the Zemach radius
and two-photon exchange (TPE) contribution. The uncertainty of TPE correction
to S energy levels in muonic hydrogen of 105 ppm exceeds the ppm accuracy level
of the forthcoming 1S hyperfine splitting measurements at PSI, J-PARC and
RIKEN-RAL.
",0,1,0,0,0,0
1017,Ising Models with Latent Conditional Gaussian Variables,"  Ising models describe the joint probability distribution of a vector of
binary feature variables. Typically, not all the variables interact with each
other and one is interested in learning the presumably sparse network structure
of the interacting variables. However, in the presence of latent variables, the
conventional method of learning a sparse model might fail. This is because the
latent variables induce indirect interactions of the observed variables. In the
case of only a few latent conditional Gaussian variables these spurious
interactions contribute an additional low-rank component to the interaction
parameters of the observed Ising model. Therefore, we propose to learn a sparse
+ low-rank decomposition of the parameters of an Ising model using a convex
regularized likelihood problem. We show that the same problem can be obtained
as the dual of a maximum-entropy problem with a new type of relaxation, where
the sample means collectively need to match the expected values only up to a
given tolerance. The solution to the convex optimization problem has
consistency properties in the high-dimensional setting, where the number of
observed binary variables and the number of latent conditional Gaussian
variables are allowed to grow with the number of training samples.
",1,0,0,1,0,0
1018,Quasiparticles and charge transfer at the two surfaces of the honeycomb iridate Na$_2$IrO$_3$,"  Direct experimental investigations of the low-energy electronic structure of
the Na$_2$IrO$_3$ iridate insulator are sparse and draw two conflicting
pictures. One relies on flat bands and a clear gap, the other involves
dispersive states approaching the Fermi level, pointing to surface metallicity.
Here, by a combination of angle-resolved photoemission, photoemission electron
microscopy, and x-ray absorption, we show that the correct picture is more
complex and involves an anomalous band, arising from charge transfer from Na
atoms to Ir-derived states. Bulk quasiparticles do exist, but in one of the two
possible surface terminations the charge transfer is smaller and they remain
elusive.
",0,1,0,0,0,0
1019,Breaking Bivariate Records,"  We establish a fundamental property of bivariate Pareto records for
independent observations uniformly distributed in the unit square. We prove
that the asymptotic conditional distribution of the number of records broken by
an observation given that the observation sets a record is Geometric with
parameter 1/2.
",0,0,1,1,0,0
1020,A Bag-of-Paths Node Criticality Measure,"  This work compares several node (and network) criticality measures
quantifying to which extend each node is critical with respect to the
communication flow between nodes of the network, and introduces a new measure
based on the Bag-of-Paths (BoP) framework. Network disconnection simulation
experiments show that the new BoP measure outperforms all the other measures on
a sample of Erdos-Renyi and Albert-Barabasi graphs. Furthermore, a faster
(still O(n^3)), approximate, BoP criticality relying on the Sherman-Morrison
rank-one update of a matrix is introduced for tackling larger networks. This
approximate measure shows similar performances as the original, exact, one.
",1,1,0,0,0,0
1021,Generation and analysis of lamplighter programs,"  We consider a programming language based on the lamplighter group that uses
only composition and iteration as control structures. We derive generating
functions and counting formulas for this language and special subsets of it,
establishing lower and upper bounds on the growth rate of semantically distinct
programs. Finally, we show how to sample random programs and analyze the
distribution of runtimes induced by such sampling.
",1,0,1,0,0,0
1022,A Projection-Based Reformulation and Decomposition Algorithm for Global Optimization of a Class of Mixed Integer Bilevel Linear Programs,"  We propose an extended variant of the reformulation and decomposition
algorithm for solving a special class of mixed-integer bilevel linear programs
(MIBLPs) where continuous and integer variables are involved in both upper- and
lower-level problems. In particular, we consider MIBLPs with upper-level
constraints that involve lower-level variables. We assume that the inducible
region is nonempty and all variables are bounded. By using the reformulation
and decomposition scheme, an MIBLP is first converted into its equivalent
single-level formulation, then computed by a column-and-constraint generation
based decomposition algorithm. The solution procedure is enhanced by a
projection strategy that does not require the relatively complete response
property. To ensure its performance, we prove that our new method converges to
the global optimal solution in a finite number of iterations. A large-scale
computational study on random instances and instances of hierarchical supply
chain planning are presented to demonstrate the effectiveness of the algorithm.
",0,0,1,0,0,0
1023,Preduals for spaces of operators involving Hilbert spaces and trace-class operators,"  Continuing the study of preduals of spaces $\mathcal{L}(H,Y)$ of bounded,
linear maps, we consider the situation that $H$ is a Hilbert space. We
establish a natural correspondence between isometric preduals of
$\mathcal{L}(H,Y)$ and isometric preduals of $Y$.
The main ingredient is a Tomiyama-type result which shows that every
contractive projection that complements $\mathcal{L}(H,Y)$ in its bidual is
automatically a right $\mathcal{L}(H)$-module map.
As an application, we show that isometric preduals of
$\mathcal{L}(\mathcal{S}_1)$, the algebra of operators on the space of
trace-class operators, correspond to isometric preduals of $\mathcal{S}_1$
itself (and there is an abundance of them). On the other hand, the compact
operators are the unique predual of $\mathcal{S}_1$ making its multiplication
separately weak* continuous.
",0,0,1,0,0,0
1024,Computing maximum cliques in $B_2$-EPG graphs,"  EPG graphs, introduced by Golumbic et al. in 2009, are edge-intersection
graphs of paths on an orthogonal grid. The class $B_k$-EPG is the subclass of
EPG graphs where the path on the grid associated to each vertex has at most $k$
bends. Epstein et al. showed in 2013 that computing a maximum clique in
$B_1$-EPG graphs is polynomial. As remarked in [Heldt et al., 2014], when the
number of bends is at least $4$, the class contains $2$-interval graphs for
which computing a maximum clique is an NP-hard problem. The complexity status
of the Maximum Clique problem remains open for $B_2$ and $B_3$-EPG graphs. In
this paper, we show that we can compute a maximum clique in polynomial time in
$B_2$-EPG graphs given a representation of the graph.
Moreover, we show that a simple counting argument provides a
${2(k+1)}$-approximation for the coloring problem on $B_k$-EPG graphs without
knowing the representation of the graph. It generalizes a result of [Epstein et
al, 2013] on $B_1$-EPG graphs (where the representation was needed).
",1,0,0,0,0,0
1025,Interactions between Health Searchers and Search Engines,"  The Web is an important resource for understanding and diagnosing medical
conditions. Based on exposure to online content, people may develop undue
health concerns, believing that common and benign symptoms are explained by
serious illnesses. In this paper, we investigate potential strategies to mine
queries and searcher histories for clues that could help search engines choose
the most appropriate information to present in response to exploratory medical
queries. To do this, we performed a longitudinal study of health search
behavior using the logs of a popular search engine. We found that query
variations which might appear innocuous (e.g. ""bad headache"" vs ""severe
headache"") may hold valuable information about the searcher which could be used
by search engines to improve performance. Furthermore, we investigated how
medically concerned users respond differently to search engine result pages
(SERPs) and find that their disposition for clicking on concerning pages is
pronounced, potentially leading to a self-reinforcement of concern. Finally, we
studied to which degree variations in the SERP impact future search and
real-world health-seeking behavior and obtained some surprising results (e.g.,
viewing concerning pages may lead to a short-term reduction of real-world
health seeking).
",1,0,0,0,0,0
1026,Effect algebras as presheaves on finite Boolean algebras,"  For an effect algebra $A$, we examine the category of all morphisms from
finite Boolean algebras into $A$. This category can be described as a category
of elements of a presheaf $R(A)$ on the category of finite Boolean algebras. We
prove that some properties (being an orthoalgebra, the Riesz decomposition
property, being a Boolean algebra) of an effect algebra $A$ can be
characterized by properties of the category of elements of the presheaf $R(A)$.
We prove that the tensor product of of effect algebras arises as a left Kan
extension of the free product of finite Boolean algebras along the inclusion
functor. As a consequence, the tensor product of effect algebras can be
expressed by means of the Day convolution of presheaves on finite Boolean
algebras.
",0,0,1,0,0,0
1027,Training Deep Convolutional Neural Networks with Resistive Cross-Point Devices,"  In a previous work we have detailed the requirements to obtain a maximal
performance benefit by implementing fully connected deep neural networks (DNN)
in form of arrays of resistive devices for deep learning. This concept of
Resistive Processing Unit (RPU) devices we extend here towards convolutional
neural networks (CNNs). We show how to map the convolutional layers to RPU
arrays such that the parallelism of the hardware can be fully utilized in all
three cycles of the backpropagation algorithm. We find that the noise and bound
limitations imposed due to analog nature of the computations performed on the
arrays effect the training accuracy of the CNNs. Noise and bound management
techniques are presented that mitigate these problems without introducing any
additional complexity in the analog circuits and can be addressed by the
digital circuits. In addition, we discuss digitally programmable update
management and device variability reduction techniques that can be used
selectively for some of the layers in a CNN. We show that combination of all
those techniques enables a successful application of the RPU concept for
training CNNs. The techniques discussed here are more general and can be
applied beyond CNN architectures and therefore enables applicability of RPU
approach for large class of neural network architectures.
",1,0,0,1,0,0
1028,Absolute versus convective helical magnetorotational instabilities in Taylor-Couette flows,"  We study magnetic Taylor-Couette flow in a system having nondimensional radii
$r_i=1$ and $r_o=2$, and periodic in the axial direction with wavelengths
$h\ge100$. The rotation ratio of the inner and outer cylinders is adjusted to
be slightly in the Rayleigh-stable regime, where magnetic fields are required
to destabilize the flow, in this case triggering the axisymmetric helical
magnetorotational instability (HMRI). Two choices of imposed magnetic field are
considered, both having the same azimuthal component $B_\phi=r^{-1}$, but
differing axial components. The first choice has $B_z=0.1$, and yields the
familiar HMRI, consisting of unidirectionally traveling waves. The second
choice has $B_z\approx0.1\sin(2\pi z/h)$, and yields HMRI waves that travel in
opposite directions depending on the sign of $B_z$. The first configuration
corresponds to a convective instability, the second to an absolute instability.
The two variants behave very similarly regarding both linear onset as well as
nonlinear equilibration.
",0,1,0,0,0,0
1029,Symmetries and multipeakon solutions for the modified two-component Camassa-Holm system,"  Compared with the two-component Camassa-Holm system, the modified
two-component Camassa-Holm system introduces a regularized density which makes
possible the existence of solutions of lower regularity, and in particular of
multipeakon solutions. In this paper, we derive a new pointwise invariant for
the modified two-component Camassa-Holm system. The derivation of the invariant
uses directly the symmetry of the system, following the classical argument of
Noether's theorem. The existence of the multipeakon solutions can be directly
inferred from this pointwise invariant. This derivation shows the strong
connection between symmetries and the existence of special solutions. The
observation also holds for the scalar Camassa-Holm equation and, for
comparison, we have also included the corresponding derivation. Finally, we
compute explicitly the solutions obtained for the peakon-antipeakon case. We
observe the existence of a periodic solution which has not been reported in the
literature previously. This case shows the attractive effect that the
introduction of an elastic potential can have on the solutions.
",0,0,1,0,0,0
1030,Selection of quasi-stationary states in the Navier-Stokes equation on the torus,"  The two dimensional incompressible Navier-Stokes equation on $D_\delta := [0,
2\pi\delta] \times [0, 2\pi]$ with $\delta \approx 1$, periodic boundary
conditions, and viscosity $0 < \nu \ll 1$ is considered. Bars and dipoles, two
explicitly given quasi-stationary states of the system, evolve on the time
scale $\mathcal{O}(e^{-\nu t})$ and have been shown to play a key role in its
long-time evolution. Of particular interest is the role that $\delta$ plays in
selecting which of these two states is observed. Recent numerical studies
suggest that, after a transient period of rapid decay of the high Fourier
modes, the bar state will be selected if $\delta \neq 1$, while the dipole will
be selected if $\delta = 1$. Our results support this claim and seek to
mathematically formalize it. We consider the system in Fourier space, project
it onto a center manifold consisting of the lowest eight Fourier modes, and use
this as a model to study the selection of bars and dipoles. It is shown for
this ODE model that the value of $\delta$ controls the behavior of the
asymptotic ratio of the low modes, thus determining the likelihood of observing
a bar state or dipole after an initial transient period. Moreover, in our
model, for all $\delta \approx 1$, there is an initial time period in which the
high modes decay at the rapid rate $\mathcal{O}(e^{-t/\nu})$, while the low
modes evolve at the slower $\mathcal{O}(e^{-\nu t})$ rate. The results for the
ODE model are proven using energy estimates and invariant manifolds and further
supported by formal asymptotic expansions and numerics.
",0,0,1,0,0,0
1031,Geometric Enclosing Networks,"  Training model to generate data has increasingly attracted research attention
and become important in modern world applications. We propose in this paper a
new geometry-based optimization approach to address this problem. Orthogonal to
current state-of-the-art density-based approaches, most notably VAE and GAN, we
present a fresh new idea that borrows the principle of minimal enclosing ball
to train a generator G\left(\bz\right) in such a way that both training and
generated data, after being mapped to the feature space, are enclosed in the
same sphere. We develop theory to guarantee that the mapping is bijective so
that its inverse from feature space to data space results in expressive
nonlinear contours to describe the data manifold, hence ensuring data generated
are also lying on the data manifold learned from training data. Our model
enjoys a nice geometric interpretation, hence termed Geometric Enclosing
Networks (GEN), and possesses some key advantages over its rivals, namely
simple and easy-to-control optimization formulation, avoidance of mode
collapsing and efficiently learn data manifold representation in a completely
unsupervised manner. We conducted extensive experiments on synthesis and
real-world datasets to illustrate the behaviors, strength and weakness of our
proposed GEN, in particular its ability to handle multi-modal data and quality
of generated data.
",1,0,0,1,0,0
1032,A pliable lasso for the Cox model,"  We introduce a pliable lasso method for estimation of interaction effects in
the Cox proportional hazards model framework. The pliable lasso is a linear
model that includes interactions between covariates X and a set of modifying
variables Z and assumes sparsity of the main effects and interaction effects.
The hierarchical penalty excludes interaction effects when the corresponding
main effects are zero: this avoids overfitting and an explosion of model
complexity. We extend this method to the Cox model for survival data,
incorporating modifiers that are either fixed or varying in time into the
partial likelihood. For example, this allows modeling of survival times that
differ based on interactions of genes with age, gender, or other demographic
information. The optimization is done by blockwise coordinate descent on a
second order approximation of the objective.
",0,0,0,1,0,0
1033,Localized magnetic moments with tunable spin exchange in a gas of ultracold fermions,"  We report on the experimental realization of a state-dependent lattice for a
two-orbital fermionic quantum gas with strong interorbital spin exchange. In
our state-dependent lattice, the ground and metastable excited electronic
states of $^{173}$Yb take the roles of itinerant and localized magnetic
moments, respectively. Repulsive on-site interactions in conjunction with the
tunnel mobility lead to spin exchange between mobile and localized particles,
modeling the coupling term in the well-known Kondo Hamiltonian. In addition, we
find that this exchange process can be tuned resonantly by varying the on-site
confinement. We attribute this to a resonant coupling to center-of-mass excited
bound states of one interorbital scattering channel.
",0,1,0,0,0,0
1034,Khintchine's Theorem with random fractions,"  We prove versions of Khintchine's Theorem (1924) for approximations by
rational numbers whose numerators lie in randomly chosen sets of integers, and
we explore the extent to which the monotonicity assumption can be removed.
Roughly speaking, we show that if the number of available fractions for each
denominator grows too fast, then the monotonicity assumption cannot be removed.
There are questions in this random setting which may be seen as cognates of the
Duffin-Schaeffer Conjecture (1941), and are likely to be more accessible. We
point out that the direct random analogue of the Duffin-Schaeffer Conjecture,
like the Duffin-Schaeffer Conjecture itself, implies Catlin's Conjecture
(1976). It is not obvious whether the Duffin-Schaeffer Conjecture and its
random version imply one another, and it is not known whether Catlin's
Conjecture implies either of them. The question of whether Catlin implies
Duffin-Schaeffer has been unsettled for decades.
",0,0,1,0,0,0
1035,A Method of Generating Random Weights and Biases in Feedforward Neural Networks with Random Hidden Nodes,"  Neural networks with random hidden nodes have gained increasing interest from
researchers and practical applications. This is due to their unique features
such as very fast training and universal approximation property. In these
networks the weights and biases of hidden nodes determining the nonlinear
feature mapping are set randomly and are not learned. Appropriate selection of
the intervals from which weights and biases are selected is extremely
important. This topic has not yet been sufficiently explored in the literature.
In this work a method of generating random weights and biases is proposed. This
method generates the parameters of the hidden nodes in such a way that
nonlinear fragments of the activation functions are located in the input space
regions with data and can be used to construct the surface approximating a
nonlinear target function. The weights and biases are dependent on the input
data range and activation function type. The proposed methods allows us to
control the generalization degree of the model. These all lead to improvement
in approximation performance of the network. Several experiments show very
promising results.
",1,0,0,1,0,0
1036,The Relative Performance of Ensemble Methods with Deep Convolutional Neural Networks for Image Classification,"  Artificial neural networks have been successfully applied to a variety of
machine learning tasks, including image recognition, semantic segmentation, and
machine translation. However, few studies fully investigated ensembles of
artificial neural networks. In this work, we investigated multiple widely used
ensemble methods, including unweighted averaging, majority voting, the Bayes
Optimal Classifier, and the (discrete) Super Learner, for image recognition
tasks, with deep neural networks as candidate algorithms. We designed several
experiments, with the candidate algorithms being the same network structure
with different model checkpoints within a single training process, networks
with same structure but trained multiple times stochastically, and networks
with different structure. In addition, we further studied the over-confidence
phenomenon of the neural networks, as well as its impact on the ensemble
methods. Across all of our experiments, the Super Learner achieved best
performance among all the ensemble methods in this study.
",1,0,0,1,0,0
1037,Representation of big data by dimension reduction,"  Suppose the data consist of a set $S$ of points $x_j, 1 \leq j \leq J$,
distributed in a bounded domain $D \subset R^N$, where $N$ and $J$ are large
numbers. In this paper an algorithm is proposed for checking whether there
exists a manifold $\mathbb{M}$ of low dimension near which many of the points
of $S$ lie and finding such $\mathbb{M}$ if it exists. There are many dimension
reduction algorithms, both linear and non-linear. Our algorithm is simple to
implement and has some advantages compared with the known algorithms. If there
is a manifold of low dimension near which most of the data points lie, the
proposed algorithm will find it. Some numerical results are presented
illustrating the algorithm and analyzing its performance compared to the
classical PCA (principal component analysis) and Isomap.
",1,0,0,1,0,0
1038,Out-degree reducing partitions of digraphs,"  Let $k$ be a fixed integer. We determine the complexity of finding a
$p$-partition $(V_1, \dots, V_p)$ of the vertex set of a given digraph such
that the maximum out-degree of each of the digraphs induced by $V_i$, ($1\leq
i\leq p$) is at least $k$ smaller than the maximum out-degree of $D$. We show
that this problem is polynomial-time solvable when $p\geq 2k$ and ${\cal
NP}$-complete otherwise. The result for $k=1$ and $p=2$ answers a question
posed in \cite{bangTCS636}. We also determine, for all fixed non-negative
integers $k_1,k_2,p$, the complexity of deciding whether a given digraph of
maximum out-degree $p$ has a $2$-partition $(V_1,V_2)$ such that the digraph
induced by $V_i$ has maximum out-degree at most $k_i$ for $i\in [2]$. It
follows from this characterization that the problem of deciding whether a
digraph has a 2-partition $(V_1,V_2)$ such that each vertex $v\in V_i$ has at
least as many neighbours in the set $V_{3-i}$ as in $V_i$, for $i=1,2$ is
${\cal NP}$-complete. This solves a problem from \cite{kreutzerEJC24} on
majority colourings.
",1,0,0,0,0,0
1039,Introduction to Plasma Physics,"  These notes are intended to provide a brief primer in plasma physics,
introducing common definitions, basic properties, and typical processes found
in plasmas. These concepts are inherent in contemporary plasma-based
accelerator schemes, and thus provide a foundation for the more advanced
expositions that follow in this volume. No prior knowledge of plasma physics is
required, but the reader is assumed to be familiar with basic electrodynamics
and fluid mechanics.
",0,1,0,0,0,0
1040,Presymplectic convexity and (ir)rational polytopes,"  In this paper, we extend the Atiyah--Guillemin--Sternberg convexity theorem
and Delzant's classification of symplectic toric manifolds to presymplectic
manifolds. We also define and study the Morita equivalence of presymplectic
toric manifolds and of their corresponding framed momentum polytopes, which may
be rational or non-rational. Toric orbifolds, quasifolds and non-commutative
toric varieties may be viewed as the quotient of our presymplectic toric
manifolds by the kernel isotropy foliation of the presymplectic form.
",0,0,1,0,0,0
1041,Unsupervised Learning of Mixture Regression Models for Longitudinal Data,"  This paper is concerned with learning of mixture regression models for
individuals that are measured repeatedly. The adjective ""unsupervised"" implies
that the number of mixing components is unknown and has to be determined,
ideally by data driven tools. For this purpose, a novel penalized method is
proposed to simultaneously select the number of mixing components and to
estimate the mixing proportions and unknown parameters in the models. The
proposed method is capable of handling both continuous and discrete responses
by only requiring the first two moment conditions of the model distribution. It
is shown to be consistent in both selecting the number of components and
estimating the mixing proportions and unknown regression parameters. Further, a
modified EM algorithm is developed to seamlessly integrate model selection and
estimation. Simulation studies are conducted to evaluate the finite sample
performance of the proposed procedure. And it is further illustrated via an
analysis of a primary biliary cirrhosis data set.
",0,0,0,1,0,0
1042,Anomalous electron states,"  By the certain macroscopic perturbations in condensed matter anomalous
electron wells can be formed due to a local reduction of electromagnetic zero
point energy. These wells are narrow, of the width $\sim 10^{-11}cm$, and with
the depth $\sim 1MeV$. Such anomalous states, from the formal standpoint of
quantum mechanics, correspond to a singular solution of a wave equation
produced by the non-physical $\delta(\vec R)$ source. The resolution, on the
level of the Standard Model, of the tiny region around the formal singularity
shows that the state is physical. The creation of those states in an atomic
system is of the formal probability $\exp(-1000)$. The probability becomes not
small under a perturbation which rapidly varies in space, on the scale
$10^{-11}cm$. In condensed matter such perturbation may relate to acoustic
shock waves. In this process the short scale is the length of the standing de
Broglie wave of a reflected lattice atom. Under electron transitions in the
anomalous well (anomalous atom) $keV$ X-rays are expected to be emitted. A
macroscopic amount of anomalous atoms, of the size $10^{-11}cm$ each, can be
formed in a solid resulting in ${\it collapsed}$ ${\it matter}$ with $10^9$
times enhanced density.
",0,1,0,0,0,0
1043,Theoretical calculation of the fine-structure constant and the permittivity of the vacuum,"  Light traveling through the vacuum interacts with virtual particles similarly
to the way that light traveling through a dielectric interacts with ordinary
matter. And just as the permittivity of a dielectric can be calculated, the
permittivity $\epsilon_0$ of the vacuum can be calculated, yielding an equation
for the fine-structure constant $\alpha$. The most important contributions to
the value of $\alpha$ arise from interactions in the vacuum of photons with
virtual, bound states of charged lepton-antilepton pairs. Considering only
these contributions, the fully screened $\alpha \cong 1/(8^2\sqrt{3\pi/2})
\cong 1/139$.
",0,1,0,0,0,0
1044,LEADER: fast estimates of asteroid shape elongation and spin latitude distributions from scarce photometry,"  Many asteroid databases with lightcurve brightness measurements (e.g. WISE,
Pan-STARRS1) contain enormous amounts of data for asteroid shape and spin
modelling. While lightcurve inversion is not plausible for individual targets
with scarce data, it is possible for large populations with thousands of
asteroids, where the distributions of the shape and spin characteristics of the
populations are obtainable.
We aim to introduce a software implementation of a method that computes the
joint shape elongation p and spin latitude beta distributions for a population,
with the brightness observations given in an asteroid database. Other main
goals are to include a method for performing validity checks of the algorithm,
and a tool for a statistical comparison of populations.
The LEADER software package read the brightness measurement data for a
user-defined subpopulation from a given database. The observations were used to
compute estimates of the brightness variations of the population members. A
cumulative distribution function (CDF) was constructed of these estimates. A
superposition of known analytical basis functions yielded this CDF as a
function of the (shape, spin) distribution. The joint distribution can be
reconstructed by solving a linear constrained inverse problem. To test the
validity of the method, the algorithm can be run with synthetic asteroid
models, where the shape and spin characteristics are known, and by using the
geometries taken from the examined database.
LEADER is a fast and robust software package for solving shape and spin
distributions for large populations. There are major differences in the quality
and coverage of measurements depending on the database used, so synthetic
simulations are always necessary before a database can be reliably used. We
show examples of differences in the results when switching to another database.
",0,1,0,0,0,0
1045,Calibrated Projection in MATLAB: Users' Manual,"  We present the calibrated-projection MATLAB package implementing the method
to construct confidence intervals proposed by Kaido, Molinari and Stoye (2017).
This manual provides details on how to use the package for inference on
projections of partially identified parameters. It also explains how to use the
MATLAB functions we developed to compute confidence intervals on solutions of
nonlinear optimization problems with estimated constraints.
",0,0,0,1,0,0
1046,Atomic Clock Measurements of Quantum Scattering Phase Shifts Spanning Feshbach Resonances at Ultralow Fields,"  We use an atomic fountain clock to measure quantum scattering phase shifts
precisely through a series of narrow, low-field Feshbach resonances at average
collision energies below $1\,\mu$K. Our low spread in collision energy yields
phase variations of order $\pm \pi/2$ for target atoms in several $F,m_F$
states. We compare them to a theoretical model and establish the accuracy of
the measurements and the theoretical uncertainties from the fitted potential.
We find overall excellent agreement, with small statistically significant
differences that remain unexplained.
",0,1,0,0,0,0
1047,Temporal processing and context dependency in C. elegans mechanosensation,"  A quantitative understanding of how sensory signals are transformed into
motor outputs places useful constraints on brain function and helps reveal the
brain's underlying computations. We investigate how the nematode C. elegans
responds to time-varying mechanosensory signals using a high-throughput
optogenetic assay and automated behavior quantification. In the prevailing
picture of the touch circuit, the animal's behavior is determined by which
neurons are stimulated and by the stimulus amplitude. In contrast, we find that
the behavioral response is tuned to temporal properties of mechanosensory
signals, like its integral and derivative, that extend over many seconds.
Mechanosensory signals, even in the same neurons, can be tailored to elicit
different behavioral responses. Moreover, we find that the animal's response
also depends on its behavioral context. Most dramatically, the animal ignores
all tested mechanosensory stimuli during turns. Finally, we present a
linear-nonlinear model that predicts the animal's behavioral response to
stimulus.
",0,0,0,0,1,0
1048,On the putative essential discreteness of q-generalized entropies,"  It has been argued in [EPL {\bf 90} (2010) 50004], entitled {\it Essential
discreteness in generalized thermostatistics with non-logarithmic entropy},
that ""continuous Hamiltonian systems with long-range interactions and the
so-called q-Gaussian momentum distributions are seen to be outside the scope of
non-extensive statistical mechanics"". The arguments are clever and appealing.
We show here that, however, some mathematical subtleties render them
unconvincing
",0,1,0,0,0,0
1049,Spin Distribution of Primordial Black Holes,"  We estimate the spin distribution of primordial black holes based on the
recent study of the critical phenomena in the gravitational collapse of a
rotating radiation fluid. We find that primordial black holes are mostly slowly
rotating.
",0,1,0,0,0,0
1050,Automated flow for compressing convolution neural networks for efficient edge-computation with FPGA,"  Deep convolutional neural networks (CNN) based solutions are the current
state- of-the-art for computer vision tasks. Due to the large size of these
models, they are typically run on clusters of CPUs or GPUs. However, power
requirements and cost budgets can be a major hindrance in adoption of CNN for
IoT applications. Recent research highlights that CNN contain significant
redundancy in their structure and can be quantized to lower bit-width
parameters and activations, while maintaining acceptable accuracy. Low
bit-width and especially single bit-width (binary) CNN are particularly
suitable for mobile applications based on FPGA implementation, due to the
bitwise logic operations involved in binarized CNN. Moreover, the transition to
lower bit-widths opens new avenues for performance optimizations and model
improvement. In this paper, we present an automatic flow from trained
TensorFlow models to FPGA system on chip implementation of binarized CNN. This
flow involves quantization of model parameters and activations, generation of
network and model in embedded-C, followed by automatic generation of the FPGA
accelerator for binary convolutions. The automated flow is demonstrated through
implementation of binarized ""YOLOV2"" on the low cost, low power Cyclone- V FPGA
device. Experiments on object detection using binarized YOLOV2 demonstrate
significant performance benefit in terms of model size and inference speed on
FPGA as compared to CPU and mobile CPU platforms. Furthermore, the entire
automated flow from trained models to FPGA synthesis can be completed within
one hour.
",1,0,0,0,0,0
1051,Pulse rate estimation using imaging photoplethysmography: generic framework and comparison of methods on a publicly available dataset,"  Objective: to establish an algorithmic framework and a benchmark dataset for
comparing methods of pulse rate estimation using imaging photoplethysmography
(iPPG). Approach: first we reveal essential steps of pulse rate estimation from
facial video and review methods applied at each of the steps. Then we
investigate performance of these methods for DEAP dataset
www.eecs.qmul.ac.uk/mmv/datasets/deap/ containing facial videos and reference
contact photoplethysmograms. Main results: best assessment precision is
achieved when pulse rate is estimated using continuous wavelet transform from
iPPG extracted by the POS method (overall mean absolute error below 2 heart
beats per minute). Significance: we provide a generic framework for theoretical
comparison of methods for pulse rate estimation from iPPG and report results
for the most popular methods on a publicly available dataset that can be used
as a benchmark.
",0,1,0,0,0,0
1052,Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,"  Convolutional neural networks have recently demonstrated high-quality
reconstruction for single-image super-resolution. In this paper, we propose the
Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively
reconstruct the sub-band residuals of high-resolution images. At each pyramid
level, our model takes coarse-resolution feature maps as input, predicts the
high-frequency residuals, and uses transposed convolutions for upsampling to
the finer level. Our method does not require the bicubic interpolation as the
pre-processing step and thus dramatically reduces the computational complexity.
We train the proposed LapSRN with deep supervision using a robust Charbonnier
loss function and achieve high-quality reconstruction. Furthermore, our network
generates multi-scale predictions in one feed-forward pass through the
progressive reconstruction, thereby facilitates resource-aware applications.
Extensive quantitative and qualitative evaluations on benchmark datasets show
that the proposed algorithm performs favorably against the state-of-the-art
methods in terms of speed and accuracy.
",1,0,0,0,0,0
1053,Foundation for a series of efficient simulation algorithms,"  Compute the coarsest simulation preorder included in an initial preorder is
used to reduce the resources needed to analyze a given transition system. This
technique is applied on many models like Kripke structures, labeled graphs,
labeled transition systems or even word and tree automata. Let (Q,
$\rightarrow$) be a given transition system and Rinit be an initial preorder
over Q. Until now, algorithms to compute Rsim , the coarsest simulation
included in Rinit , are either memory efficient or time efficient but not both.
In this paper we propose the foundation for a series of efficient simulation
algorithms with the introduction of the notion of maximal transitions and the
notion of stability of a preorder with respect to a coarser one. As an
illustration we solve an open problem by providing the first algorithm with the
best published time complexity, O(|Psim |.|$\rightarrow$|), and a bit space
complexity in O(|Psim |^2. log(|Psim |) + |Q|. log(|Q|)), with Psim the
partition induced by Rsim.
",1,0,0,0,0,0
1054,A Review of Macroscopic Motion in Thermodynamic Equilibrium,"  A principle on the macroscopic motion of systems in thermodynamic
equilibrium, rarely discussed in texts, is reviewed: Very small but still
macroscopic parts of a fully isolated system in thermal equilibrium move as if
points of a rigid body, macroscopic energy being dissipated to increase
internal energy, and increase entropy along. It appears particularly important
in Space physics, when dissipation involves long-range fields at
Electromagnetism and Gravitation, rather than short-range contact forces. It is
shown how new physics, Special Relativity as regards Electromagnetism, first
Newtonian theory then General Relativity as regards Gravitation, determine
different dissipative processes involved in the approach to that equilibrium.
",0,1,0,0,0,0
1055,Emergent electronic structure of CaFe2As2,"  CaFe2As2 exhibits collapsed tetragonal (cT) structure and varied exotic
behavior under pressure at low temperatures that led to debate on linking the
structural changes to its exceptional electronic properties like
superconductivity, magnetism, etc. Here, we investigate the electronic
structure of CaFe2As2 forming in different structures employing density
functional theory. The results indicate better stability of the cT phase with
enhancement in hybridization induced effects and shift of the energy bands
towards lower energies. The Fermi surface centered around $\Gamma$ point
gradually vanishes with the increase in pressure. Consequently, the nesting
between the hole and electron Fermi surfaces associated to the spin density
wave state disappears indicating a pathway to achieve the proximity to quantum
fluctuations. The magnetic moment at the Fe sites diminishes in the cT phase
consistent with the magnetic susceptibility results. Notably, the hybridization
of Ca 4s states (Ca-layer may be treated as a charge reservoir layer akin to
those in cuprate superconductors) is significantly enhanced in the cT phase
revealing its relevance in its interesting electronic properties.
",0,1,0,0,0,0
1056,Lord Kelvin's method of images approach to the Rotenberg model and its asymptotics,"  We study a mathematical model of cell populations dynamics proposed by M.
Rotenberg and investigated by M. Boulanouar. Here, a cell is characterized by
her maturity and speed of maturation. The growth of cell populations is
described by a partial differential equation with a boundary condition. In the
first part of the paper we exploit semigroup theory approach and apply Lord
Kelvin's method of images in order to give a new proof that the model is well
posed. Next, we use a semi-explicit formula for the semigroup related to the
model obtained by the method of images in order to give growth estimates for
the semigroup. The main part of the paper is devoted to the asymptotic
behaviour of the semigroup. We formulate conditions for the asymptotic
stability of the semigroup in the case in which the average number of viable
daughters per mitosis equals one. To this end we use methods developed by K.
Pich??r and R. Rudnicki.
",0,0,1,0,0,0
1057,Study of the Magnetizing Relationship of the Kickers for CSNS,"  The extraction system of CSNS mainly consists of two kinds of magnets: eight
kickers and one lambertson magnet. In this paper, firstly, the magnetic test
results of the eight kickers were introduced and then the filed uniformity and
magnetizing relationship of the kickers were given. Secondly, during the beam
commissioning in the future, in order to obtain more accurate magnetizing
relationship, a new method to measure the magnetizing coefficients of the
kickers by the real extraction beam was given and the data analysis would also
be processed.
",0,1,0,0,0,0
1058,"Smart ""Predict, then Optimize""","  Many real-world analytics problems involve two significant challenges:
prediction and optimization. Due to the typically complex nature of each
challenge, the standard paradigm is to predict, then optimize. By and large,
machine learning tools are intended to minimize prediction error and do not
account for how the predictions will be used in a downstream optimization
problem. In contrast, we propose a new and very general framework, called Smart
""Predict, then Optimize"" (SPO), which directly leverages the optimization
problem structure, i.e., its objective and constraints, for designing
successful analytics tools. A key component of our framework is the SPO loss
function, which measures the quality of a prediction by comparing the objective
values of the solutions generated using the predicted and observed parameters,
respectively. Training a model with respect to the SPO loss is computationally
challenging, and therefore we also develop a surrogate loss function, called
the SPO+ loss, which upper bounds the SPO loss, has desirable convexity
properties, and is statistically consistent under mild conditions. We also
propose a stochastic gradient descent algorithm which allows for situations in
which the number of training samples is large, model regularization is desired,
and/or the optimization problem of interest is nonlinear or integer. Finally,
we perform computational experiments to empirically verify the success of our
SPO framework in comparison to the standard predict-then-optimize approach.
",1,0,0,1,0,0
1059,U-SLADS: Unsupervised Learning Approach for Dynamic Dendrite Sampling,"  Novel data acquisition schemes have been an emerging need for scanning
microscopy based imaging techniques to reduce the time in data acquisition and
to minimize probing radiation in sample exposure. Varies sparse sampling
schemes have been studied and are ideally suited for such applications where
the images can be reconstructed from a sparse set of measurements. Dynamic
sparse sampling methods, particularly supervised learning based iterative
sampling algorithms, have shown promising results for sampling pixel locations
on the edges or boundaries during imaging. However, dynamic sampling for
imaging skeleton-like objects such as metal dendrites remains difficult. Here,
we address a new unsupervised learning approach using Hierarchical Gaussian
Mixture Mod- els (HGMM) to dynamically sample metal dendrites. This technique
is very useful if the users are interested in fast imaging the primary and
secondary arms of metal dendrites in solidification process in materials
science.
",0,0,0,1,0,0
1060,On a registration-based approach to sensor network localization,"  We consider a registration-based approach for localizing sensor networks from
range measurements. This is based on the assumption that one can find
overlapping cliques spanning the network. That is, for each sensor, one can
identify geometric neighbors for which all inter-sensor ranges are known. Such
cliques can be efficiently localized using multidimensional scaling. However,
since each clique is localized in some local coordinate system, we are required
to register them in a global coordinate system. In other words, our approach is
based on transforming the localization problem into a problem of registration.
In this context, the main contributions are as follows. First, we describe an
efficient method for partitioning the network into overlapping cliques. Second,
we study the problem of registering the localized cliques, and formulate a
necessary rigidity condition for uniquely recovering the global sensor
coordinates. In particular, we present a method for efficiently testing
rigidity, and a proposal for augmenting the partitioned network to enforce
rigidity. A recently proposed semidefinite relaxation of global registration is
used for registering the cliques. We present simulation results on random and
structured sensor networks to demonstrate that the proposed method compares
favourably with state-of-the-art methods in terms of run-time, accuracy, and
scalability.
",1,0,1,0,0,0
1061,Density estimation on small datasets,"  How might a smooth probability distribution be estimated, with accurately
quantified uncertainty, from a limited amount of sampled data? Here we describe
a field-theoretic approach that addresses this problem remarkably well in one
dimension, providing an exact nonparametric Bayesian posterior without relying
on tunable parameters or large-data approximations. Strong non-Gaussian
constraints, which require a non-perturbative treatment, are found to play a
major role in reducing distribution uncertainty. A software implementation of
this method is provided.
",1,0,0,0,1,0
1062,"Generalized Euler classes, differential forms and commutative DGAs","  In the context of commutative differential graded algebras over $\mathbb Q$,
we show that an iteration of ""odd spherical fibration"" creates a ""total space""
commutative differential graded algebra with only odd degree cohomology. Then
we show for such a commutative differential graded algebra that, for any of its
""fibrations"" with ""fiber"" of finite cohomological dimension, the induced map on
cohomology is injective.
",0,0,1,0,0,0
1063,Episodic memory for continual model learning,"  Both the human brain and artificial learning agents operating in real-world
or comparably complex environments are faced with the challenge of online model
selection. In principle this challenge can be overcome: hierarchical Bayesian
inference provides a principled method for model selection and it converges on
the same posterior for both off-line (i.e. batch) and online learning. However,
maintaining a parameter posterior for each model in parallel has in general an
even higher memory cost than storing the entire data set and is consequently
clearly unfeasible. Alternatively, maintaining only a limited set of models in
memory could limit memory requirements. However, sufficient statistics for one
model will usually be insufficient for fitting a different kind of model,
meaning that the agent loses information with each model change. We propose
that episodic memory can circumvent the challenge of limited memory-capacity
online model selection by retaining a selected subset of data points. We design
a method to compute the quantities necessary for model selection even when the
data is discarded and only statistics of one (or few) learnt models are
available. We demonstrate on a simple model that a limited-sized episodic
memory buffer, when the content is optimised to retain data with statistics not
matching the current representation, can resolve the fundamental challenge of
online model selection.
",1,0,0,1,0,0
1064,Security Trust Zone in 5G Networks,"  Fifth Generation (5G) telecommunication system is going to deliver a flexible
radio access network (RAN). Security functions such as authorization,
authentication and accounting (AAA) are expected to be distributed from central
clouds to edge clouds. We propose a novel architectural security solution that
applies to 5G networks. It is called Trust Zone (TZ) that is designed as an
enhancement of the 5G AAA in the edge cloud. TZ also provides an autonomous and
decentralized security policy for different tenants under variable network
conditions. TZ also initiates an ability of disaster cognition and extends the
security functionalities to a set of flexible and highly available emergency
services in the edge cloud.
",1,0,0,0,0,0
1065,Upper-Bounding the Regularization Constant for Convex Sparse Signal Reconstruction,"  Consider reconstructing a signal $x$ by minimizing a weighted sum of a convex
differentiable negative log-likelihood (NLL) (data-fidelity) term and a convex
regularization term that imposes a convex-set constraint on $x$ and enforces
its sparsity using $\ell_1$-norm analysis regularization. We compute upper
bounds on the regularization tuning constant beyond which the regularization
term overwhelmingly dominates the NLL term so that the set of minimum points of
the objective function does not change. Necessary and sufficient conditions for
irrelevance of sparse signal regularization and a condition for the existence
of finite upper bounds are established. We formulate an optimization problem
for finding these bounds when the regularization term can be globally minimized
by a feasible $x$ and also develop an alternating direction method of
multipliers (ADMM) type method for their computation. Simulation examples show
that the derived and empirical bounds match.
",0,0,1,1,0,0
1066,On the Privacy of the Opal Data Release: A Response,"  This document is a response to a report from the University of Melbourne on
the privacy of the Opal dataset release. The Opal dataset was released by
Data61 (CSIRO) in conjunction with the Transport for New South Wales (TfNSW).
The data consists of two separate weeks of ""tap-on/tap-off"" data of individuals
who used any of the four different modes of public transport from TfNSW: buses,
light rail, train and ferries. These taps are recorded through the smart
ticketing system, known as Opal, available in the state of New South Wales,
Australia.
",1,0,0,0,0,0
1067,Long time behavior of Gross-Pitaevskii equation at positive temperature,"  The stochastic Gross-Pitaevskii equation is used as a model to describe
Bose-Einstein condensation at positive temperature. The equation is a complex
Ginzburg Landau equation with a trapping potential and an additive space-time
white noise. Two important questions for this system are the global existence
of solutions in the support of the Gibbs measure, and the convergence of those
solutions to the equilibrium for large time. In this paper, we give a proof of
these two results in one space dimension. In order to prove the convergence to
equilibrium, we use the associated purely dissipative equation as an auxiliary
equation, for which the convergence may be obtained using standard techniques.
Global existence is obtained for all initial data, and not almost surely with
respect to the invariant measure.
",0,0,1,0,0,0
1068,Isomorphism and Morita equivalence classes for crossed products of irrational rotation algebras by cyclic subgroups of $SL_2(\mathbb{Z})$,"  Let $\theta, \theta'$ be irrational numbers and $A, B$ be matrices in
$SL_2(\mathbb{Z})$ of infinite order. We compute the $K$-theory of the crossed
product $\mathcal{A}_{\theta}\rtimes_A \mathbb{Z}$ and show that
$\mathcal{A}_{\theta} \rtimes_A\mathbb{Z}$ and $\mathcal{A}_{\theta'} \rtimes_B
\mathbb{Z}$ are $*$-isomorphic if and only if $\theta = \pm\theta'
\pmod{\mathbb{Z}}$ and $I-A^{-1}$ is matrix equivalent to $I-B^{-1}$. Combining
this result and an explicit construction of equivariant bimodules, we show that
$\mathcal{A}_{\theta} \rtimes_A\mathbb{Z}$ and $\mathcal{A}_{\theta'} \rtimes_B
\mathbb{Z}$ are Morita equivalent if and only if $\theta$ and $\theta'$ are in
the same $GL_2(\mathbb{Z})$ orbit and $I-A^{-1}$ is matrix equivalent to
$I-B^{-1}$. Finally, we determine the Morita equivalence class of
$\mathcal{A}_{\theta} \rtimes F$ for any finite subgroup $F$ of
$SL_2(\mathbb{Z})$.
",0,0,1,0,0,0
1069,Model Predictive Control for Distributed Microgrid Battery Energy Storage Systems,"  This paper proposes a new convex model predictive control strategy for
dynamic optimal power flow between battery energy storage systems distributed
in an AC microgrid. The proposed control strategy uses a new problem
formulation, based on a linear d-q reference frame voltage-current model and
linearised power flow approximations. This allows the optimal power flows to be
solved as a convex optimisation problem, for which fast and robust solvers
exist. The proposed method does not assume real and reactive power flows are
decoupled, allowing line losses, voltage constraints and converter current
constraints to be addressed. In addition, non-linear variations in the charge
and discharge efficiencies of lithium ion batteries are analysed and included
in the control strategy. Real-time digital simulations were carried out for an
islanded microgrid based on the IEEE 13 bus prototypical feeder, with
distributed battery energy storage systems and intermittent photovoltaic
generation. It is shown that the proposed control strategy approaches the
performance of a strategy based on non-convex optimisation, while reducing the
required computation time by a factor of 1000, making it suitable for a
real-time model predictive control implementation.
",1,0,0,0,0,0
1070,On noncommutative geometry of the Standard Model: fermion multiplet as internal forms,"  We unveil the geometric nature of the multiplet of fundamental fermions in
the Standard Model of fundamental particles as a noncommutative analogue of de
Rham forms on the internal finite quantum space.
",0,0,1,0,0,0
1071,A Review of Dynamic Network Models with Latent Variables,"  We present a selective review of statistical modeling of dynamic networks. We
focus on models with latent variables, specifically, the latent space models
and the latent class models (or stochastic blockmodels), which investigate both
the observed features and the unobserved structure of networks. We begin with
an overview of the static models, and then we introduce the dynamic extensions.
For each dynamic model, we also discuss its applications that have been studied
in the literature, with the data source listed in Appendix. Based on the
review, we summarize a list of open problems and challenges in dynamic network
modeling with latent variables.
",0,0,0,1,0,0
1072,LevelHeaded: Making Worst-Case Optimal Joins Work in the Common Case,"  Pipelines combining SQL-style business intelligence (BI) queries and linear
algebra (LA) are becoming increasingly common in industry. As a result, there
is a growing need to unify these workloads in a single framework.
Unfortunately, existing solutions either sacrifice the inherent benefits of
exclusively using a relational database (e.g. logical and physical
independence) or incur orders of magnitude performance gaps compared to
specialized engines (or both). In this work we study applying a new type of
query processing architecture to standard BI and LA benchmarks. To do this we
present a new in-memory query processing engine called LevelHeaded. LevelHeaded
uses worst-case optimal joins as its core execution mechanism for both BI and
LA queries. With LevelHeaded, we show how crucial optimizations for BI and LA
queries can be captured in a worst-case optimal query architecture. Using these
optimizations, LevelHeaded outperforms other relational database engines
(LogicBlox, MonetDB, and HyPer) by orders of magnitude on standard LA
benchmarks, while performing on average within 31% of the best-of-breed BI
(HyPer) and LA (Intel MKL) solutions on their own benchmarks. Our results show
that such a single query processing architecture is capable of delivering
competitive performance on both BI and LA queries.
",1,0,0,0,0,0
1073,Few-shot learning of neural networks from scratch by pseudo example optimization,"  In this paper, we propose a simple but effective method for training neural
networks with a limited amount of training data. Our approach inherits the idea
of knowledge distillation that transfers knowledge from a deep or wide
reference model to a shallow or narrow target model. The proposed method
employs this idea to mimic predictions of reference estimators that are more
robust against overfitting than the network we want to train. Different from
almost all the previous work for knowledge distillation that requires a large
amount of labeled training data, the proposed method requires only a small
amount of training data. Instead, we introduce pseudo training examples that
are optimized as a part of model parameters. Experimental results for several
benchmark datasets demonstrate that the proposed method outperformed all the
other baselines, such as naive training of the target model and standard
knowledge distillation.
",0,0,0,1,0,0
1074,Identities and congruences involving the Fubini polynomials,"  In this paper, we investigate the umbral representation of the Fubini
polynomials $F_{x}^{n}:=F_{n}(x)$ to derive some properties involving these
polynomials. For any prime number $p$ and any polynomial $f$ with integer
coefficients, we show $(f(F_{x}))^{p}\equiv f(F_{x})$ and we give other curious
congruences.
",0,0,1,0,0,0
1075,Introduction to Delay Models and Their Wave Solutions,"  In this paper, a brief review of delay population models and their
applications in ecology is provided. The inclusion of diffusion and nonlocality
terms in delay models has given more capabilities to these models enabling them
to capture several ecological phenomena such as the Allee effect, waves of
invasive species and spatio-temporal competitions of interacting species.
Moreover, recent advances in the studies of traveling and stationary wave
solutions of delay models are outlined. In particular, the existence of
stationary and traveling wave solutions of delay models, stability of wave
solutions, formation of wavefronts in the special domain, and possible outcomes
of delay models are discussed.
",0,0,1,0,0,0
1076,On Dummett's Pragmatist Justification Procedure,"  I show that propositional intuitionistic logic is complete with respect to an
adaptation of Dummett's pragmatist justification procedure. In particular,
given a pragmatist justification of an argument, I show how to obtain a natural
deduction derivation of the conclusion of the argument from, at most, the same
assumptions.
",0,0,1,0,0,0
1077,Evidence for a radiatively driven disc-wind in PDS 456?,"  We present a newly discovered correlation between the wind outflow velocity
and the X-ray luminosity in the luminous ($L_{\rm bol}\sim10^{47}\,\rm
erg\,s^{-1}$) nearby ($z=0.184$) quasar PDS\,456. All the contemporary
XMM-Newton, NuSTAR and Suzaku observations from 2001--2014 were revisited and
we find that the centroid energy of the blueshifted Fe\,K absorption profile
increases with luminosity. This translates into a correlation between the wind
outflow velocity and the hard X-ray luminosity (between 7--30\,keV) where we
find that $v_{\rm w}/c \propto L_{7-30}^{\gamma}$ where $\gamma=0.22\pm0.04$.
We also show that this is consistent with a wind that is predominately
radiatively driven, possibly resulting from the high Eddington ratio of
PDS\,456.
",0,1,0,0,0,0
1078,From a normal insulator to a topological insulator in plumbene,"  Plumbene, similar to silicene, has a buckled honeycomb structure with a large
band gap ($\sim 400$ meV). All previous studies have shown that it is a normal
insulator. Here, we perform first-principles calculations and employ a
sixteen-band tight-binding model with nearest-neighbor and
next-nearest-neighbor hopping terms to investigate electronic structures and
topological properties of the plumbene monolayer. We find that it can become a
topological insulator with a large bulk gap ($\sim 200$ meV) through electron
doping, and the nontrivial state is very robust with respect to external
strain. Plumbene can be an ideal candidate for realizing the quantum spin Hall
effect at room temperature. By investigating effects of external electric and
magnetic fields on electronic structures and transport properties of plumbene,
we present two rich phase diagrams with and without electron doping, and
propose a theoretical design for a four-state spin-valley filter.
",0,1,0,0,0,0
1079,High-sensitivity Kinetic Inductance Detectors for CALDER,"  Providing a background discrimination tool is crucial for enhancing the
sensitivity of next-generation experiments searching for neutrinoless double-
beta decay. The development of high-sensitivity (< 20 eV RMS) cryogenic light
detectors allows simultaneous read-out of the light and heat signals and
enables background suppression through particle identification. The Cryogenic
wide- Area Light Detector with Excellent Resolution (CALDER) R&D already proved
the potential of this technique using the phonon-mediated Kinetic Inductance
Detectors (KIDs) approach. The first array prototype with 4 Aluminum KIDs on a
2 $\times$ 2 cm2 Silicon substrate showed a baseline resolution of 154 $\pm$ 7
eV RMS. Improving the design and the readout of the resonator, the next CALDER
prototype featured an energy resolution of 82 $\pm$ 4 eV, by sampling the same
substrate with a single Aluminum KID.
",0,1,0,0,0,0
1080,Bounding the composition length of primitive permutation groups and completely reducible linear groups,"  We obtain upper bounds on the composition length of a finite permutation
group in terms of the degree and the number of orbits, and analogous bounds for
primitive, quasiprimitive and semiprimitive groups. Similarly, we obtain upper
bounds on the composition length of a finite completely reducible linear group
in terms of some of its parameters. In almost all cases we show that the bounds
are sharp, and describe the extremal examples.
",0,0,1,0,0,0
1081,A Bernstein Inequality For Spatial Lattice Processes,"  In this article we present a Bernstein inequality for sums of random
variables which are defined on a spatial lattice structure. The inequality can
be used to derive concentration inequalities. It can be useful to obtain
consistency properties for nonparametric estimators of conditional expectation
functions.
",0,0,1,1,0,0
1082,An Exploration of Approaches to Integrating Neural Reranking Models in Multi-Stage Ranking Architectures,"  We explore different approaches to integrating a simple convolutional neural
network (CNN) with the Lucene search engine in a multi-stage ranking
architecture. Our models are trained using the PyTorch deep learning toolkit,
which is implemented in C/C++ with a Python frontend. One obvious integration
strategy is to expose the neural network directly as a service. For this, we
use Apache Thrift, a software framework for building scalable cross-language
services. In exploring alternative architectures, we observe that once trained,
the feedforward evaluation of neural networks is quite straightforward.
Therefore, we can extract the parameters of a trained CNN from PyTorch and
import the model into Java, taking advantage of the Java Deeplearning4J library
for feedforward evaluation. This has the advantage that the entire end-to-end
system can be implemented in Java. As a third approach, we can extract the
neural network from PyTorch and ""compile"" it into a C++ program that exposes a
Thrift service. We evaluate these alternatives in terms of performance (latency
and throughput) as well as ease of integration. Experiments show that
feedforward evaluation of the convolutional neural network is significantly
slower in Java, while the performance of the compiled C++ network does not
consistently beat the PyTorch implementation.
",1,0,0,0,0,0
1083,Dispersive Regimes of the Dicke Model,"  We study two dispersive regimes in the dynamics of $N$ two-level atoms
interacting with a bosonic mode for long interaction times. Firstly, we analyze
the dispersive multiqubit quantum Rabi model for the regime in which the qubit
frequencies are equal and smaller than the mode frequency, and for values of
the coupling strength similar or larger than the mode frequency, namely, the
deep strong coupling regime. Secondly, we address an interaction that is
dependent on the photon number, where the coupling strength is comparable to
the geometric mean of the qubit and mode frequencies. We show that the
associated dynamics is analytically tractable and provide useful frameworks
with which to analyze the system behavior. In the deep strong coupling regime,
we unveil the structure of unexpected resonances for specific values of the
coupling, present for $N\ge2$, and in the photon-number-dependent regime we
demonstrate that all the nontrivial dynamical behavior occurs in the atomic
degrees of freedom for a given Fock state. We verify these assertions with
numerical simulations of the qubit population and photon-statistic dynamics.
",0,1,0,0,0,0
1084,"ZebraLancer: Crowdsource Knowledge atop Open Blockchain, Privately and Anonymously","  We design and implement the first private and anonymous decentralized
crowdsourcing system ZebraLancer. It realizes the fair exchange (i.e. security
against malicious workers and dishonest requesters) without using any
third-party arbiter. More importantly, it overcomes two fundamental challenges
of decentralization, i.e. data leakage and identity breach.
First, our outsource-then-prove methodology resolves the critical tension
between blockchain transparency and data confidentiality without sacrificing
the fairness of exchange. ZebraLancer ensures: a requester will not pay more
than what data deserve, according to a policy announced when her task is
published through the blockchain; each worker indeed gets a payment based on
the policy, if submits data to the blockchain; the above properties are
realized not only without a central arbiter, but also without leaking the data
to blockchain network.
Furthermore, the blockchain transparency might allow one to infer private
information of workers/requesters through their participation history.
ZebraLancer solves the problem by allowing anonymous participations without
surrendering user accountability. Specifically, workers cannot misuse anonymity
to submit multiple times to reap rewards, and an anonymous requester cannot
maliciously submit colluded answers to herself to repudiate payments. The idea
behind is a subtle linkability: if one authenticates twice in a task, everybody
can tell, or else staying anonymous. To realize such delicate linkability, we
put forth a novel cryptographic notion, the common-prefix-linkable anonymous
authentication.
Finally, we implement our protocol for a common image annotation task and
deploy it in a test net of Ethereum. The experiment results show the
applicability of our protocol and highlight subtleties of tailoring the
protocol to be compatible with the existing real-world open blockchain.
",1,0,0,0,0,0
1085,"Fast, Better Training Trick -- Random Gradient","  In this paper, we will show an unprecedented method to accelerate training
and improve performance, which called random gradient (RG). This method can be
easier to the training of any model without extra calculation cost, we use
Image classification, Semantic segmentation, and GANs to confirm this method
can improve speed which is training model in computer vision. The central idea
is using the loss multiplied by a random number to random reduce the
back-propagation gradient. We can use this method to produce a better result in
Pascal VOC, Cifar, Cityscapes datasets.
",0,0,0,1,0,0
1086,Expressions of Sentiments During Code Reviews: Male vs. Female,"  Background: As most of the software development organizations are
male-dominated, female developers encountering various negative workplace
experiences reported feeling like they ""do not belong"". Exposures to
discriminatory expletives or negative critiques from their male colleagues may
further exacerbate those feelings. Aims: The primary goal of this study is to
identify the differences in expressions of sentiments between male and female
developers during various software engineering tasks. Method: On this goal, we
mined the code review repositories of six popular open source projects. We used
a semi-automated approach leveraging the name as well as multiple social
networks to identify the gender of a developer. Using SentiSE, a customized and
state-of-the-art sentiment analysis tool for the software engineering domain,
we classify each communication as negative, positive, or neutral. We also
compute the frequencies of sentiment words, emoticons, and expletives used by
each developer. Results: Our results suggest that the likelihood of using
sentiment words, emoticons, and expletives during code reviews varies based on
the gender of a developer, as females are significantly less likely to express
sentiments than males. Although female developers were more neutral to their
male colleagues than to another female, male developers from three out of the
six projects were not only writing more frequent negative comments but also
withholding positive encouragements from their female counterparts. Conclusion:
Our results provide empirical evidence of another factor behind the negative
work place experiences encountered by the female developers that may be
contributing to the diminishing number of females in the SE industry.
",1,0,0,0,0,0
1087,Monotonicity patterns and functional inequalities for classical and generalized Wright functions,"  In this paper our aim is to present the completely monotonicity and convexity
properties for the Wright function. As consequences of these results, we
present some functional inequalities. Moreover, we derive the monotonicity and
log-convexity results for the generalized Wright functions. As applications, we
present several new inequalities (like Tur?­n type inequalities) and we prove
some geometric properties for four--parametric Mittag--Leffler functions.
",0,0,1,0,0,0
1088,Multiple VLAD encoding of CNNs for image classification,"  Despite the effectiveness of convolutional neural networks (CNNs) especially
in image classification tasks, the effect of convolution features on learned
representations is still limited. It mostly focuses on the salient object of
the images, but ignores the variation information on clutter and local. In this
paper, we propose a special framework, which is the multiple VLAD encoding
method with the CNNs features for image classification. Furthermore, in order
to improve the performance of the VLAD coding method, we explore the
multiplicity of VLAD encoding with the extension of three kinds of encoding
algorithms, which are the VLAD-SA method, the VLAD-LSA and the VLAD-LLC method.
Finally, we equip the spatial pyramid patch (SPM) on VLAD encoding to add the
spatial information of CNNs feature. In particular, the power of SPM leads our
framework to yield better performance compared to the existing method.
",1,0,0,0,0,0
1089,Index of Dirac operators and classification of topological insulators,"  Real and complex Clifford bundles and Dirac operators defined on them are
considered. By using the index theorems of Dirac operators, table of
topological invariants is constructed from the Clifford chessboard. Through the
relations between K-theory groups, Grothendieck groups and symmetric spaces,
the periodic table of topological insulators and superconductors is obtained.
This gives the result that the periodic table of real and complex topological
phases is originated from the Clifford chessboard and index theorems.
",0,1,0,0,0,0
1090,Centroid vetting of transiting planet candidates from the Next Generation Transit Survey,"  The Next Generation Transit Survey (NGTS), operating in Paranal since 2016,
is a wide-field survey to detect Neptunes and super-Earths transiting bright
stars, which are suitable for precise radial velocity follow-up and
characterisation. Thereby, its sub-mmag photometric precision and ability to
identify false positives are crucial. Particularly, variable background objects
blended in the photometric aperture frequently mimic Neptune-sized transits and
are costly in follow-up time. These objects can best be identified with the
centroiding technique: if the photometric flux is lost off-centre during an
eclipse, the flux centroid shifts towards the centre of the target star.
Although this method has successfully been employed by the Kepler mission, it
has previously not been implemented from the ground. We present a
fully-automated centroid vetting algorithm developed for NGTS, enabled by our
high-precision auto-guiding. Our method allows detecting centroid shifts with
an average precision of 0.75 milli-pixel, and down to 0.25 milli-pixel for
specific targets, for a pixel size of 4.97 arcsec. The algorithm is now part of
the NGTS candidate vetting pipeline and automatically employed for all detected
signals. Further, we develop a joint Bayesian fitting model for all photometric
and centroid data, allowing to disentangle which object (target or background)
is causing the signal, and what its astrophysical parameters are. We
demonstrate our method on two NGTS objects of interest. These achievements make
NGTS the first ground-based wide-field transit survey ever to successfully
apply the centroiding technique for automated candidate vetting, enabling the
production of a robust candidate list before follow-up.
",0,1,0,0,0,0
1091,Galaxy And Mass Assembly: the evolution of the cosmic spectral energy distribution from z = 1 to z = 0,"  We present the evolution of the Cosmic Spectral Energy Distribution (CSED)
from $z = 1 - 0$. Our CSEDs originate from stacking individual spectral energy
distribution fits based on panchromatic photometry from the Galaxy and Mass
Assembly (GAMA) and COSMOS datasets in ten redshift intervals with completeness
corrections applied. Below $z = 0.45$, we have credible SED fits from 100 nm to
1 mm. Due to the relatively low sensitivity of the far-infrared data, our
far-infrared CSEDs contain a mix of predicted and measured fluxes above $z =
0.45$. Our results include appropriate errors to highlight the impact of these
corrections. We show that the bolometric energy output of the Universe has
declined by a factor of roughly four -- from $5.1 \pm 1.0$ at $z \sim 1$ to
$1.3 \pm 0.3 \times 10^{35}~h_{70}$~W~Mpc$^{-3}$ at the current epoch. We show
that this decrease is robust to cosmic variance, SED modelling and other
various types of error. Our CSEDs are also consistent with an increase in the
mean age of stellar populations. We also show that dust attenuation has
decreased over the same period, with the photon escape fraction at 150~nm
increasing from $16 \pm 3$ at $z \sim 1$ to $24 \pm 5$ per cent at the current
epoch, equivalent to a decrease in $A_\mathrm{FUV}$ of 0.4~mag. Our CSEDs
account for $68 \pm 12$ and $61 \pm 13$ per cent of the cosmic optical and
infrared backgrounds respectively as defined from integrated galaxy counts and
are consistent with previous estimates of the cosmic infrared background with
redshift.
",0,1,0,0,0,0
1092,Large sums of Hecke eigenvalues of holomorphic cusp forms,"  Let $f$ be a Hecke cusp form of weight $k$ for the full modular group, and
let $\{\lambda_f(n)\}_{n\geq 1}$ be the sequence of its normalized Fourier
coefficients. Motivated by the problem of the first sign change of
$\lambda_f(n)$, we investigate the range of $x$ (in terms of $k$) for which
there are cancellations in the sum $S_f(x)=\sum_{n\leq x} \lambda_f(n)$. We
first show that $S_f(x)=o(x\log x)$ implies that $\lambda_f(n)<0$ for some
$n\leq x$. We also prove that $S_f(x)=o(x\log x)$ in the range $\log x/\log\log
k\to \infty$ assuming the Riemann hypothesis for $L(s, f)$, and furthermore
that this range is best possible unconditionally. More precisely, we establish
the existence of many Hecke cusp forms $f$ of large weight $k$, for which
$S_f(x)\gg_A x\log x$, when $x=(\log k)^A.$ Our results are $GL_2$ analogues of
work of Granville and Soundararajan for character sums, and could also be
generalized to other families of automorphic forms.
",0,0,1,0,0,0
1093,EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples,"  Recent studies have highlighted the vulnerability of deep neural networks
(DNNs) to adversarial examples - a visually indistinguishable adversarial image
can easily be crafted to cause a well-trained model to misclassify. Existing
methods for crafting adversarial examples are based on $L_2$ and $L_\infty$
distortion metrics. However, despite the fact that $L_1$ distortion accounts
for the total variation and encourages sparsity in the perturbation, little has
been developed for crafting $L_1$-based adversarial examples. In this paper, we
formulate the process of attacking DNNs via adversarial examples as an
elastic-net regularized optimization problem. Our elastic-net attacks to DNNs
(EAD) feature $L_1$-oriented adversarial examples and include the
state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,
CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial
examples with small $L_1$ distortion and attains similar attack performance to
the state-of-the-art methods in different attack scenarios. More importantly,
EAD leads to improved attack transferability and complements adversarial
training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in
adversarial machine learning and security implications of DNNs.
",1,0,0,1,0,0
1094,Playtime Measurement with Survival Analysis,"  Maximizing product use is a central goal of many businesses, which makes
retention and monetization two central analytics metrics in games. Player
retention may refer to various duration variables quantifying product use:
total playtime or session playtime are popular research targets, and active
playtime is well-suited for subscription games. Such research often has the
goal of increasing player retention or conversely decreasing player churn.
Survival analysis is a framework of powerful tools well suited for retention
type data. This paper contributes new methods to game analytics on how playtime
can be analyzed using survival analysis without covariates. Survival and hazard
estimates provide both a visual and an analytic interpretation of the playtime
phenomena as a funnel type nonparametric estimate. Metrics based on the
survival curve can be used to aggregate this playtime information into a single
statistic. Comparison of survival curves between cohorts provides a scientific
AB-test. All these methods work on censored data and enable computation of
confidence intervals. This is especially important in time and sample limited
data which occurs during game development. Throughout this paper, we illustrate
the application of these methods to real world game development problems on the
Hipster Sheep mobile game.
",1,0,0,1,0,0
1095,Asymptotic formula of the number of Newton polygons,"  In this paper, we enumerate Newton polygons asymptotically. The number of
Newton polygons is computable by a simple recurrence equation, but unexpectedly
the asymptotic formula of its logarithm contains growing oscillatory terms. As
the terms come from non-trivial zeros of the Riemann zeta function, an
estimation of the amplitude of the oscillating part is equivalent to the
Riemann hypothesis.
",0,0,1,0,0,0
1096,Invariant-based inverse engineering of crane control parameters,"  By applying invariant-based inverse engineering in the small-oscillations
regime, we design the time dependence of the control parameters of an overhead
crane (trolley displacement and rope length), to transport a load between two
positions at different heights with minimal final energy excitation for a
microcanonical ensemble of initial conditions. The analogies between ion
transport in multisegmented traps or neutral atom transport in moving optical
lattices and load manipulation by cranes opens a route for a useful transfer of
techniques among very different fields.
",0,1,0,0,0,0
1097,Leaf Space Isometries of Singular Riemannian Foliations and Their Spectral Properties,"  In this paper, the authors consider leaf spaces of singular Riemannian
foliations $\mathcal{F}$ on compact manifolds $M$ and the associated
$\mathcal{F}$-basic spectrum on $M$, $spec_B(M, \mathcal{F}),$ counted with
multiplicities. Recently, a notion of smooth isometry $\varphi:
M_1/\mathcal{F}_1\rightarrow M_2/\mathcal{F}_2$ between the leaf spaces of such
singular Riemannian foliations $(M_1,\mathcal{F}_1)$ and $(M_2,\mathcal{F}_2)$
has appeared in the literature. In this paper, the authors provide an example
to show that the existence a smooth isometry of leaf spaces as above is not
sufficient to guarantee the equality of $spec_B(M_1,\mathcal{F}_1)$ and
$spec_B(M_2,\mathcal{F}_2).$ The authors then prove that if some additional
conditions involving the geometry of the leaves are satisfied, then the
equality of $spec_B(M_1,\mathcal{F}_1)$ and $spec_B(M_2,\mathcal{F}_2)$ is
guaranteed. Consequences and applications to orbifold spectral theory,
isometric group actions, and their reductions are also explored.
",0,0,1,0,0,0
1098,Backward Monte-Carlo applied to muon transport,"  We discuss a backward Monte-Carlo technique for muon transport problem, with
emphasis on its application in muography. Backward Monte-Carlo allows exclusive
sampling of a final state by reversing the simulation flow. In practice it can
be made analogous to an adjoint Monte-Carlo, though it is more versatile for
muon transport. A backward Monte-Carlo was implemented as a dedicated muon
transport library: PUMAS. It is shown for case studies relevant for muography
imaging that the implementations of forward and backward Monte-Carlo schemes
agree to better than 1%.
",0,1,0,0,0,0
1099,Functional importance of noise in neuronal information processing,"  Noise is an inherent part of neuronal dynamics, and thus of the brain. It can
be observed in neuronal activity at different spatiotemporal scales, including
in neuronal membrane potentials, local field potentials,
electroencephalography, and magnetoencephalography. A central research topic in
contemporary neuroscience is to elucidate the functional role of noise in
neuronal information processing. Experimental studies have shown that a
suitable level of noise may enhance the detection of weak neuronal signals by
means of stochastic resonance. In response, theoretical research, based on the
theory of stochastic processes, nonlinear dynamics, and statistical physics,
has made great strides in elucidating the mechanism and the many benefits of
stochastic resonance in neuronal systems. In this perspective, we review recent
research dedicated to neuronal stochastic resonance in biophysical mathematical
models. We also explore the regulation of neuronal stochastic resonance, and we
outline important open questions and directions for future research. A deeper
understanding of neuronal stochastic resonance may afford us new insights into
the highly impressive information processing in the brain.
",0,0,0,0,1,0
1100,Stochastic Variance Reduction Methods for Policy Evaluation,"  Policy evaluation is a crucial step in many reinforcement-learning
procedures, which estimates a value function that predicts states' long-term
value under a given policy. In this paper, we focus on policy evaluation with
linear function approximation over a fixed dataset. We first transform the
empirical policy evaluation problem into a (quadratic) convex-concave saddle
point problem, and then present a primal-dual batch gradient method, as well as
two stochastic variance reduction methods for solving the problem. These
algorithms scale linearly in both sample size and feature dimension. Moreover,
they achieve linear convergence even when the saddle-point problem has only
strong concavity in the dual variables but no strong convexity in the primal
variables. Numerical experiments on benchmark problems demonstrate the
effectiveness of our methods.
",1,0,1,1,0,0
1101,Self-consistent dynamical model of the Broad Line Region,"  We develope a self-consistent description of the Broad Line Region based on
the concept of the failed wind powered by the radiation pressure acting on
dusty accretion disk atmosphere in Keplerian motion. The material raised high
above the disk is illuminated, dust evaportes, and the matter falls back
towards the disk. This material is the source of emission lines. The model
predicts the inner and outer radius of the region, the cloud dynamics under the
dust radiation pressure and, subsequently, just the gravitational field of the
central black hole, which results in assymetry between the rise and fall.
Knowledge of the dynamics allows to predict the shapes of the emission lines as
functions of the basic parameters of an active nucleus: black hole mass,
accretion rate, black hole spin (or accretion efficiency) and the viewing angle
with respect to the symmetry axis. Here we show preliminary results based on
analytical approximations to the cloud motion.
",0,1,0,0,0,0
1102,Measuring the polarization of electromagnetic fields using Rabi-rate measurements with spatial resolution: experiment and theory,"  When internal states of atoms are manipulated using coherent optical or
radio-frequency (RF) radiation, it is essential to know the polarization of the
radiation with respect to the quantization axis of the atom. We first present a
measurement of the two-dimensional spatial distribution of the electric-field
amplitude of a linearly-polarized pulsed RF electric field at $\sim 25.6\,$GHz
and its angle with respect to a static electric field. The measurements exploit
coherent population transfer between the $35$s and $35$p Rydberg states of
helium atoms in a pulsed supersonic beam. Based on this experimental result, we
develop a general framework in the form of a set of equations relating the five
independent polarization parameters of a coherently oscillating field in a
fixed laboratory frame to Rabi rates of transitions between a ground and three
excited states of an atom with arbitrary quantization axis. We then explain how
these equations can be used to fully characterize the polarization in a minimum
of five Rabi rate measurements by rotation of an external bias-field, or,
knowing the polarization of the driving field, to determine the orientation of
the static field using two measurements. The presented technique is not limited
to Rydberg atoms and RF fields but can also be applied to characterize optical
fields. The technique has potential for sensing the spatiotemporal properties
of electromagnetic fields, e.g., in metrology devices or in hybrid experiments
involving atoms close to surfaces.
",0,1,0,0,0,0
1103,Software-based Microarchitectural Attacks,"  Modern processors are highly optimized systems where every single cycle of
computation time matters. Many optimizations depend on the data that is being
processed. Software-based microarchitectural attacks exploit effects of these
optimizations. Microarchitectural side-channel attacks leak secrets from
cryptographic computations, from general purpose computations, or from the
kernel. This leakage even persists across all common isolation boundaries, such
as processes, containers, and virtual machines. Microarchitectural fault
attacks exploit the physical imperfections of modern computer systems.
Shrinking process technology introduces effects between isolated hardware
elements that can be exploited by attackers to take control of the entire
system. These attacks are especially interesting in scenarios where the
attacker is unprivileged or even sandboxed.
In this thesis, we focus on microarchitectural attacks and defenses on
commodity systems. We investigate known and new side channels and show that
microarchitectural attacks can be fully automated. Furthermore, we show that
these attacks can be mounted in highly restricted environments such as
sandboxed JavaScript code in websites. We show that microarchitectural attacks
exist on any modern computer system, including mobile devices (e.g.,
smartphones), personal computers, and commercial cloud systems. This thesis
consists of two parts. In the first part, we provide background on modern
processor architectures and discuss state-of-the-art attacks and defenses in
the area of microarchitectural side-channel attacks and microarchitectural
fault attacks. In the second part, a selection of our papers are provided
without modification from their original publications. I have co-authored these
papers, which have subsequently been anonymously peer-reviewed, accepted, and
presented at renowned international conferences.
",1,0,0,0,0,0
1104,Pixelwise Instance Segmentation with a Dynamically Instantiated Network,"  Semantic segmentation and object detection research have recently achieved
rapid progress. However, the former task has no notion of different instances
of the same object, and the latter operates at a coarse, bounding-box level. We
propose an Instance Segmentation system that produces a segmentation map where
each pixel is assigned an object class and instance identity label. Most
approaches adapt object detectors to produce segments instead of boxes. In
contrast, our method is based on an initial semantic segmentation module, which
feeds into an instance subnetwork. This subnetwork uses the initial
category-level segmentation, along with cues from the output of an object
detector, within an end-to-end CRF to predict instances. This part of our model
is dynamically instantiated to produce a variable number of instances per
image. Our end-to-end approach requires no post-processing and considers the
image holistically, instead of processing independent proposals. Therefore,
unlike some related work, a pixel cannot belong to multiple instances.
Furthermore, far more precise segmentations are achieved, as shown by our
state-of-the-art results (particularly at high IoU thresholds) on the Pascal
VOC and Cityscapes datasets.
",1,0,0,0,0,0
1105,Binary Matrix Factorization via Dictionary Learning,"  Matrix factorization is a key tool in data analysis; its applications include
recommender systems, correlation analysis, signal processing, among others.
Binary matrices are a particular case which has received significant attention
for over thirty years, especially within the field of data mining. Dictionary
learning refers to a family of methods for learning overcomplete basis (also
called frames) in order to efficiently encode samples of a given type; this
area, now also about twenty years old, was mostly developed within the signal
processing field. In this work we propose two binary matrix factorization
methods based on a binary adaptation of the dictionary learning paradigm to
binary matrices. The proposed algorithms focus on speed and scalability; they
work with binary factors combined with bit-wise operations and a few auxiliary
integer ones. Furthermore, the methods are readily applicable to online binary
matrix factorization. Another important issue in matrix factorization is the
choice of rank for the factors; we address this model selection problem with an
efficient method based on the Minimum Description Length principle. Our
preliminary results show that the proposed methods are effective at producing
interpretable factorizations of various data types of different nature.
",0,0,0,1,0,0
1106,Enriching Complex Networks with Word Embeddings for Detecting Mild Cognitive Impairment from Speech Transcripts,"  Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose.
Linguistic features, mainly from parsers, have been used to detect MCI, but
this is not suitable for large-scale assessments. MCI disfluencies produce
non-grammatical speech that requires manual or high precision automatic
correction of transcripts. In this paper, we modeled transcripts into complex
networks and enriched them with word embedding (CNE) to better represent short
texts produced in neuropsychological assessments. The network measurements were
applied with well-known classifiers to automatically identify MCI in
transcripts, in a binary classification task. A comparison was made with the
performance of traditional approaches using Bag of Words (BoW) and linguistic
features for three datasets: DementiaBank in English, and Cinderella and
Arizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using
only complex networks, while Support Vector Machine was superior to other
classifiers. CNE provided the highest accuracies for DementiaBank and
Cinderella, but BoW was more efficient for the Arizona-Battery dataset probably
owing to its short narratives. The approach using linguistic features yielded
higher accuracy if the transcriptions of the Cinderella dataset were manually
revised. Taken together, the results indicate that complex networks enriched
with embedding is promising for detecting MCI in large-scale assessments
",1,0,0,0,0,0
1107,The many faces of degeneracy in conic optimization,"  Slater's condition -- existence of a ""strictly feasible solution"" -- is a
common assumption in conic optimization. Without strict feasibility,
first-order optimality conditions may be meaningless, the dual problem may
yield little information about the primal, and small changes in the data may
render the problem infeasible. Hence, failure of strict feasibility can
negatively impact off-the-shelf numerical methods, such as primal-dual interior
point methods, in particular. New optimization modelling techniques and convex
relaxations for hard nonconvex problems have shown that the loss of strict
feasibility is a more pronounced phenomenon than has previously been realized.
In this text, we describe various reasons for the loss of strict feasibility,
whether due to poor modelling choices or (more interestingly) rich underlying
structure, and discuss ways to cope with it and, in many pronounced cases, how
to use it as an advantage. In large part, we emphasize the facial reduction
preprocessing technique due to its mathematical elegance, geometric
transparency, and computational potential.
",0,0,1,0,0,0
1108,Strong homotopy types of acyclic categories and $??$-complexes,"  We extend the homotopy theories based on point reduction for finite spaces
and simplicial complexes to finite acyclic categories and $\Delta$-complexes,
respectively. The functors of classifying spaces and face posets are compatible
with these homotopy theories. In contrast with the classical settings of finite
spaces and simplicial complexes, the universality of morphisms and simplices
plays a central role in this paper.
",0,0,1,0,0,0
1109,Boundary problems for the fractional and tempered fractional operators,"  For characterizing the Brownian motion in a bounded domain: $\Omega$, it is
well-known that the boundary conditions of the classical diffusion equation
just rely on the given information of the solution along the boundary of a
domain; on the contrary, for the L??vy flights or tempered L??vy flights in a
bounded domain, it involves the information of a solution in the complementary
set of $\Omega$, i.e., $\mathbb{R}^n\backslash \Omega$, with the potential
reason that paths of the corresponding stochastic process are discontinuous.
Guided by probability intuitions and the stochastic perspectives of anomalous
diffusion, we show the reasonable ways, ensuring the clear physical meaning and
well-posedness of the partial differential equations (PDEs), of specifying
`boundary' conditions for space fractional PDEs modeling the anomalous
diffusion. Some properties of the operators are discussed, and the
well-posednesses of the PDEs with generalized boundary conditions are proved.
",0,0,1,0,0,0
1110,Bohm's approach to quantum mechanics: Alternative theory or practical picture?,"  Since its inception Bohmian mechanics has been generally regarded as a
hidden-variable theory aimed at providing an objective description of quantum
phenomena. To date, this rather narrow conception of Bohm's proposal has caused
it more rejection than acceptance. Now, after 65 years of Bohmian mechanics,
should still be such an interpretational aspect the prevailing appraisal? Why
not favoring a more pragmatic view, as a legitimate picture of quantum
mechanics, on equal footing in all respects with any other more conventional
quantum picture? These questions are used here to introduce a discussion on an
alternative way to deal with Bohmian mechanics at present, enhancing its aspect
as an efficient and useful picture or formulation to tackle, explore, describe
and explain quantum phenomena where phase and correlation (entanglement) are
key elements. This discussion is presented through two complementary blocks.
The first block is aimed at briefly revisiting the historical context that gave
rise to the appearance of Bohmian mechanics, and how this approach or analogous
ones have been used in different physical contexts. This discussion is used to
emphasize a more pragmatic view to the detriment of the more conventional
hidden-variable (ontological) approach that has been a leitmotif within the
quantum foundations. The second block focuses on some particular formal aspects
of Bohmian mechanics supporting the view presented here, with special emphasis
on the physical meaning of the local phase field and the associated velocity
field encoded within the wave function. As an illustration, a simple model of
Young's two-slit experiment is considered. The simplicity of this model allows
to understand in an easy manner how the information conveyed by the Bohmian
formulation relates to other more conventional concepts in quantum mechanics.
This sort of pedagogical application is also aimed at ...
",0,1,0,0,0,0
1111,Continuous-wave virtual-state lasing from cold ytterbium atoms,"  While conventional lasers are based on gain media with three or four real
levels, unconventional lasers including virtual levels and two-photon processes
offer new opportunities. We study lasing that involves a two-photon process
through a virtual lower level, which we realize in a cloud of cold ytterbium
atoms that are magneto-optically trapped inside a cavity. We pump the atoms on
the narrow $^1$S$_0$ $\to$ $^3$P$_1$ line and generate laser emission on the
same transition. Lasing is verified by a threshold behavior of output power
vs.\ pump power and atom number, a flat $g^{(2)}$ correlation function above
threshold, and the polarization properties of the output. In the proposed
lasing mechanism the MOT beams create the virtual lower level of the lasing
transition. The laser process runs continuously, needs no further repumping,
and might be adapted to other atoms or transitions such as the ultra narrow
$^1$S$_0$ $\to$ $^3$P$_0$ clock transition in ytterbium.
",0,1,0,0,0,0
1112,Assessment of Future Changes in Intensity-Duration-Frequency Curves for Southern Ontario using North American (NA)-CORDEX Models with Nonstationary Methods,"  The evaluation of possible climate change consequence on extreme rainfall has
significant implications for the design of engineering structure and
socioeconomic resources development. While many studies have assessed the
impact of climate change on design rainfall using global and regional climate
model (RCM) predictions, to date, there has been no comprehensive comparison or
evaluation of intensity-duration-frequency (IDF) statistics at regional scale,
considering both stationary versus nonstationary models for the future climate.
To understand how extreme precipitation may respond to future IDF curves, we
used an ensemble of three RCMs participating in the North-American (NA)-CORDEX
domain over eight rainfall stations across Southern Ontario, one of the most
densely populated and major economic region in Canada. The IDF relationships
are derived from multi-model RCM simulations and compared with the
station-based observations. We modeled precipitation extremes, at different
durations using extreme value distributions considering parameters that are
either stationary or nonstationary, as a linear function of time. Our results
showed that extreme precipitation intensity driven by future climate forcing
shows a significant increase in intensity for 10-year events in 2050s
(2030-2070) relative to 1970-2010 baseline period across most of the locations.
However, for longer return periods, an opposite trend is noted. Surprisingly,
in term of design storms, no significant differences were found when comparing
stationary and nonstationary IDF estimation methods for the future (2050s) for
the larger return periods. The findings, which are specific to regional
precipitation extremes, suggest no immediate reason for alarm, but the need for
progressive updating of the design standards in light of global warming.
",0,1,0,1,0,0
1113,Improved Algorithms for Computing the Cycle of Minimum Cost-to-Time Ratio in Directed Graphs,"  We study the problem of finding the cycle of minimum cost-to-time ratio in a
directed graph with $ n $ nodes and $ m $ edges. This problem has a long
history in combinatorial optimization and has recently seen interesting
applications in the context of quantitative verification. We focus on strongly
polynomial algorithms to cover the use-case where the weights are relatively
large compared to the size of the graph. Our main result is an algorithm with
running time $ \tilde O (m^{3/4} n^{3/2}) $, which gives the first improvement
over Megiddo's $ \tilde O (n^3) $ algorithm [JACM'83] for sparse graphs. We
further demonstrate how to obtain both an algorithm with running time $ n^3 /
2^{\Omega{(\sqrt{\log n})}} $ on general graphs and an algorithm with running
time $ \tilde O (n) $ on constant treewidth graphs. To obtain our main result,
we develop a parallel algorithm for negative cycle detection and single-source
shortest paths that might be of independent interest.
",1,0,0,0,0,0
1114,"Interplay of dust alignment, grain growth and magnetic fields in polarization: lessons from the emission-to-extinction ratio","  Polarized extinction and emission from dust in the interstellar medium (ISM)
are hard to interpret, as they have a complex dependence on dust optical
properties, grain alignment and magnetic field orientation. This is
particularly true in molecular clouds. The data available today are not yet
used to their full potential.
The combination of emission and extinction, in particular, provides
information not available from either of them alone. We combine data from the
scientific literature on polarized dust extinction with Planck data on
polarized emission and we use them to constrain the possible variations in dust
and environmental conditions inside molecular clouds, and especially
translucent lines of sight, taking into account magnetic field orientation.
We focus on the dependence between \lambda_max -- the wavelength of maximum
polarization in extinction -- and other observables such as the extinction
polarization, the emission polarization and the ratio of the two. We set out to
reproduce these correlations using Monte-Carlo simulations where the relevant
quantities in a dust model -- grain alignment, size distribution and magnetic
field orientation -- vary to mimic the diverse conditions expected inside
molecular clouds.
None of the quantities chosen can explain the observational data on its own:
the best results are obtained when all quantities vary significantly across and
within clouds. However, some of the data -- most notably the stars with low
emission-to-extinction polarization ratio -- are not reproduced by our
simulation. Our results suggest not only that dust evolution is necessary to
explain polarization in molecular clouds, but that a simple change in size
distribution is not sufficient to explain the data, and point the way for
future and more sophisticated models.
",0,1,0,0,0,0
1115,Asynchronous Distributed Variational Gaussian Processes for Regression,"  Gaussian processes (GPs) are powerful non-parametric function estimators.
However, their applications are largely limited by the expensive computational
cost of the inference procedures. Existing stochastic or distributed
synchronous variational inferences, although have alleviated this issue by
scaling up GPs to millions of samples, are still far from satisfactory for
real-world large applications, where the data sizes are often orders of
magnitudes larger, say, billions. To solve this problem, we propose ADVGP, the
first Asynchronous Distributed Variational Gaussian Process inference for
regression, on the recent large-scale machine learning platform,
PARAMETERSERVER. ADVGP uses a novel, flexible variational framework based on a
weight space augmentation, and implements the highly efficient, asynchronous
proximal gradient optimization. While maintaining comparable or better
predictive performance, ADVGP greatly improves upon the efficiency of the
existing variational methods. With ADVGP, we effortlessly scale up GP
regression to a real-world application with billions of samples and demonstrate
an excellent, superior prediction accuracy to the popular linear models.
",0,0,0,1,0,0
1116,Bayesian Unification of Gradient and Bandit-based Learning for Accelerated Global Optimisation,"  Bandit based optimisation has a remarkable advantage over gradient based
approaches due to their global perspective, which eliminates the danger of
getting stuck at local optima. However, for continuous optimisation problems or
problems with a large number of actions, bandit based approaches can be
hindered by slow learning. Gradient based approaches, on the other hand,
navigate quickly in high-dimensional continuous spaces through local
optimisation, following the gradient in fine grained steps. Yet, apart from
being susceptible to local optima, these schemes are less suited for online
learning due to their reliance on extensive trial-and-error before the optimum
can be identified. In this paper, we propose a Bayesian approach that unifies
the above two paradigms in one single framework, with the aim of combining
their advantages. At the heart of our approach we find a stochastic linear
approximation of the function to be optimised, where both the gradient and
values of the function are explicitly captured. This allows us to learn from
both noisy function and gradient observations, and predict these properties
across the action space to support optimisation. We further propose an
accompanying bandit driven exploration scheme that uses Bayesian credible
bounds to trade off exploration against exploitation. Our empirical results
demonstrate that by unifying bandit and gradient based learning, one obtains
consistently improved performance across a wide spectrum of problem
environments. Furthermore, even when gradient feedback is unavailable, the
flexibility of our model, including gradient prediction, still allows us
outperform competing approaches, although with a smaller margin. Due to the
pervasiveness of bandit based optimisation, our scheme opens up for improved
performance both in meta-optimisation and in applications where gradient
related information is readily available.
",1,0,0,0,0,0
1117,Stacco: Differentially Analyzing Side-Channel Traces for Detecting SSL/TLS Vulnerabilities in Secure Enclaves,"  Intel Software Guard Extension (SGX) offers software applications enclave to
protect their confidentiality and integrity from malicious operating systems.
The SSL/TLS protocol, which is the de facto standard for protecting
transport-layer network communications, has been broadly deployed for a secure
communication channel. However, in this paper, we show that the marriage
between SGX and SSL may not be smooth sailing.
Particularly, we consider a category of side-channel attacks against SSL/TLS
implementations in secure enclaves, which we call the control-flow inference
attacks. In these attacks, the malicious operating system kernel may perform a
powerful man-in-the-kernel attack to collect execution traces of the enclave
programs at page, cacheline, or branch level, while positioning itself in the
middle of the two communicating parties. At the center of our work is a
differential analysis framework, dubbed Stacco, to dynamically analyze the
SSL/TLS implementations and detect vulnerabilities that can be exploited as
decryption oracles. Surprisingly, we found exploitable vulnerabilities in the
latest versions of all the SSL/TLS libraries we have examined.
To validate the detected vulnerabilities, we developed a man-in-the-kernel
adversary to demonstrate Bleichenbacher attacks against the latest OpenSSL
library running in the SGX enclave (with the help of Graphene) and completely
broke the PreMasterSecret encrypted by a 4096-bit RSA public key with only
57286 queries. We also conducted CBC padding oracle attacks against the latest
GnuTLS running in Graphene-SGX and an open-source SGX-implementation of mbedTLS
(i.e., mbedTLS-SGX) that runs directly inside the enclave, and showed that it
only needs 48388 and 25717 queries, respectively, to break one block of AES
ciphertext. Empirical evaluation suggests these man-in-the-kernel attacks can
be completed within 1 or 2 hours.
",1,0,0,0,0,0
1118,Automatic Extrinsic Calibration for Lidar-Stereo Vehicle Sensor Setups,"  Sensor setups consisting of a combination of 3D range scanner lasers and
stereo vision systems are becoming a popular choice for on-board perception
systems in vehicles; however, the combined use of both sources of information
implies a tedious calibration process. We present a method for extrinsic
calibration of lidar-stereo camera pairs without user intervention. Our
calibration approach is aimed to cope with the constraints commonly found in
automotive setups, such as low-resolution and specific sensor poses. To
demonstrate the performance of our method, we also introduce a novel approach
for the quantitative assessment of the calibration results, based on a
simulation environment. Tests using real devices have been conducted as well,
proving the usability of the system and the improvement over the existing
approaches. Code is available at this http URL
",1,0,0,0,0,0
1119,"Representations of Super $W(2,2)$ algebra $\mathfrak{L}$","  In paper, we study the representation theory of super $W(2,2)$ algebra
${\mathfrak{L}}$. We prove that ${\mathfrak{L}}$ has no mixed irreducible
modules and give the classification of irreducible modules of intermediate
series. We determinate the conjugate-linear anti-involution of ${\mathfrak{L}}$
and give the unitary modules of intermediate series.
",0,0,1,0,0,0
1120,Effective Reformulation of Query for Code Search using Crowdsourced Knowledge and Extra-Large Data Analytics,"  Software developers frequently issue generic natural language queries for
code search while using code search engines (e.g., GitHub native search,
Krugle). Such queries often do not lead to any relevant results due to
vocabulary mismatch problems. In this paper, we propose a novel technique that
automatically identifies relevant and specific API classes from Stack Overflow
Q & A site for a programming task written as a natural language query, and then
reformulates the query for improved code search. We first collect candidate API
classes from Stack Overflow using pseudo-relevance feedback and two term
weighting algorithms, and then rank the candidates using Borda count and
semantic proximity between query keywords and the API classes. The semantic
proximity has been determined by an analysis of 1.3 million questions and
answers of Stack Overflow. Experiments using 310 code search queries report
that our technique suggests relevant API classes with 48% precision and 58%
recall which are 32% and 48% higher respectively than those of the
state-of-the-art. Comparisons with two state-of-the-art studies and three
popular search engines (e.g., Google, Stack Overflow, and GitHub native search)
report that our reformulated queries (1) outperform the queries of the
state-of-the-art, and (2) significantly improve the code search results
provided by these contemporary search engines.
",1,0,0,0,0,0
1121,Detecting Bot Activity in the Ethereum Blockchain Network,"  The Ethereum blockchain network is a decentralized platform enabling smart
contract execution and transactions of Ether (ETH) [1], its designated
cryptocurrency. Ethereum is the second most popular cryptocurrency with a
market cap of more than 100 billion USD, with hundreds of thousands of
transactions executed daily by hundreds of thousands of unique wallets. Tens of
thousands of those wallets are newly generated each day. The Ethereum platform
enables anyone to freely open multiple new wallets [2] free of charge
(resulting in a large number of wallets that are controlled by the same
entities). This attribute makes the Ethereum network a breeding space for
activity by software robots (bots). The existence of bots is widespread in
different digital technologies and there are various approaches to detect their
activity such as rule-base, clustering, machine learning and more [3,4]. In
this work we demonstrate how bot detection can be implemented using a network
theory approach.
",1,0,0,0,0,0
1122,Near-infrared laser thermal conjunctivoplasty,"  Conjunctivochalasis is a common cause of tear dysfunction due to the
conjunctiva becoming loose and wrinkly with age. The current solutions to this
disease include either surgical excision in the operating room, or
thermoreduction of the loose tissue with hot wire in the clinic. We developed a
near-infrared (NIR) laser thermal conjunctivoplasty (LTC) system, which gently
shrinks the redundant tissue. The NIR light is mainly absorbed by water, so the
heating is even and there is no bleeding. The system utilizes a 1460-nm
programmable laser diode system as a light source. A miniaturized handheld
probe delivers the laser light and focuses the laser into a 10x1 mm2 line. A
foot pedal is used to deliver a preset number of calibrated laser pulses. A
fold of loose conjunctiva is grasped by a pair of forceps. The infrared laser
light is delivered through an optical fiber and a laser line is focused exactly
on the conjunctival fold by a cylindrical lens. Ex vivo experiments using
porcine eye were performed with the optimal laser parameters. It was found that
up to 50% of conjunctiva shrinkage could be achieved.
",0,1,0,0,0,0
1123,Superconductivity at 7.3 K in the 133-type Cr-based RbCr3As3 single crystals,"  Here we report the preparation and superconductivity of the 133-type Cr-based
quasi-one-dimensional (Q1D) RbCr3As3 single crystals. The samples were prepared
by the deintercalation of Rb+ ions from the 233-type Rb2Cr3As3 crystals which
were grown from a high-temperature solution growth method. The RbCr3As3
compound crystallizes in a centrosymmetric structure with the space group of
P63/m (No. 176) different with its non-centrosymmetric Rb2Cr3As3
superconducting precursor, and the refined lattice parameters are a = 9.373(3)
{\AA} and c = 4.203(7) {\AA}. Electrical resistivity and magnetic
susceptibility characterizations reveal the occurrence of superconductivity
with an interestingly higher onset Tc of 7.3 K than other Cr-based
superconductors, and a high upper critical field Hc2(0) near 70 T in this
133-type RbCr3As3 crystals.
",0,1,0,0,0,0
1124,Solution of parabolic free boundary problems using transmuted heat polynomials,"  A numerical method for free boundary problems for the equation \[
u_{xx}-q(x)u=u_t \] is proposed. The method is based on recent results from
transmutation operators theory allowing one to construct efficiently a complete
system of solutions for this equation generalizing the system of heat
polynomials. The corresponding implementation algorithm is presented.
",0,0,1,0,0,0
1125,"Neural-Network Quantum States, String-Bond States, and Chiral Topological States","  Neural-Network Quantum States have been recently introduced as an Ansatz for
describing the wave function of quantum many-body systems. We show that there
are strong connections between Neural-Network Quantum States in the form of
Restricted Boltzmann Machines and some classes of Tensor-Network states in
arbitrary dimensions. In particular we demonstrate that short-range Restricted
Boltzmann Machines are Entangled Plaquette States, while fully connected
Restricted Boltzmann Machines are String-Bond States with a nonlocal geometry
and low bond dimension. These results shed light on the underlying architecture
of Restricted Boltzmann Machines and their efficiency at representing many-body
quantum states. String-Bond States also provide a generic way of enhancing the
power of Neural-Network Quantum States and a natural generalization to systems
with larger local Hilbert space. We compare the advantages and drawbacks of
these different classes of states and present a method to combine them
together. This allows us to benefit from both the entanglement structure of
Tensor Networks and the efficiency of Neural-Network Quantum States into a
single Ansatz capable of targeting the wave function of strongly correlated
systems. While it remains a challenge to describe states with chiral
topological order using traditional Tensor Networks, we show that
Neural-Network Quantum States and their String-Bond States extension can
describe a lattice Fractional Quantum Hall state exactly. In addition, we
provide numerical evidence that Neural-Network Quantum States can approximate a
chiral spin liquid with better accuracy than Entangled Plaquette States and
local String-Bond States. Our results demonstrate the efficiency of neural
networks to describe complex quantum wave functions and pave the way towards
the use of String-Bond States as a tool in more traditional machine-learning
applications.
",0,1,0,1,0,0
1126,Nondestructive testing of grating imperfections using grating-based X-ray phase-contrast imaging,"  We reported the usage of grating-based X-ray phase-contrast imaging in
nondestructive testing of grating imperfections. It was found that
electroplating flaws could be easily detected by conventional absorption
signal, and in particular, we observed that the grating defects resulting from
uneven ultraviolet exposure could be clearly discriminated with phase-contrast
signal. The experimental results demonstrate that grating-based X-ray
phase-contrast imaging, with a conventional low-brilliance X-ray source, a
large field of view and a reasonable compact setup, which simultaneously yields
phase- and attenuation-contrast signal of the sample, can be ready-to-use in
fast nondestructive testing of various imperfections in gratings and other
similar photoetching products.
",0,1,0,0,0,0
1127,End-to-End Information Extraction without Token-Level Supervision,"  Most state-of-the-art information extraction approaches rely on token-level
labels to find the areas of interest in text. Unfortunately, these labels are
time-consuming and costly to create, and consequently, not available for many
real-life IE tasks. To make matters worse, token-level labels are usually not
the desired output, but just an intermediary step. End-to-end (E2E) models,
which take raw text as input and produce the desired output directly, need not
depend on token-level labels. We propose an E2E model based on pointer
networks, which can be trained directly on pairs of raw input and output text.
We evaluate our model on the ATIS data set, MIT restaurant corpus and the MIT
movie corpus and compare to neural baselines that do use token-level labels. We
achieve competitive results, within a few percentage points of the baselines,
showing the feasibility of E2E information extraction without the need for
token-level labels. This opens up new possibilities, as for many tasks
currently addressed by human extractors, raw input and output data are
available, but not token-level labels.
",1,0,0,0,0,0
1128,Lipschitz regularity of solutions to two-phase free boundary problems,"  We prove Lipschitz continuity of viscosity solutions to a class of two-phase
free boundary problems governed by fully nonlinear operators.
",0,0,1,0,0,0
1129,VTA: An Open Hardware-Software Stack for Deep Learning,"  Hardware acceleration is an enabler for ubiquitous and efficient deep
learning. With hardware accelerators being introduced in datacenter and edge
devices, it is time to acknowledge that hardware specialization is central to
the deep learning system stack.
This technical report presents the Versatile Tensor Accelerator (VTA), an
open, generic, and customizable deep learning accelerator design. VTA is a
programmable accelerator that exposes a RISC-like programming abstraction to
describe operations at the tensor level. We designed VTA to expose the most
salient and common characteristics of mainstream deep learning accelerators,
such as tensor operations, DMA load/stores, and explicit compute/memory
arbitration.
VTA is more than a standalone accelerator design: it's an end-to-end solution
that includes drivers, a JIT runtime, and an optimizing compiler stack based on
TVM. The current release of VTA includes a behavioral hardware simulator, as
well as the infrastructure to deploy VTA on low-cost FPGA development boards
for fast prototyping.
By extending the TVM stack with a customizable, and open source deep learning
hardware accelerator design, we are exposing a transparent end-to-end deep
learning stack from the high-level deep learning framework, down to the actual
hardware design and implementation. This forms a truly end-to-end, from
software-to-hardware open source stack for deep learning systems.
",0,0,0,1,0,0
1130,Efficient Localized Inference for Large Graphical Models,"  We propose a new localized inference algorithm for answering marginalization
queries in large graphical models with the correlation decay property. Given a
query variable and a large graphical model, we define a much smaller model in a
local region around the query variable in the target model so that the marginal
distribution of the query variable can be accurately approximated. We introduce
two approximation error bounds based on the Dobrushin's comparison theorem and
apply our bounds to derive a greedy expansion algorithm that efficiently guides
the selection of neighbor nodes for localized inference. We verify our
theoretical bounds on various datasets and demonstrate that our localized
inference algorithm can provide fast and accurate approximation for large
graphical models.
",1,0,0,1,0,0
1131,Multi-kink collisions in the $??^6$ model,"  We study simultaneous collisions of two, three, and four kinks and antikinks
of the $\phi^6$ model at the same spatial point. Unlike the $\phi^4$ kinks, the
$\phi^6$ kinks are asymmetric and this enriches the variety of the collision
scenarios. In our numerical simulations we observe both reflection and bound
state formation depending on the number of kinks and on their spatial ordering
in the initial configuration. We also analyze the extreme values of the energy
densities and the field gradient observed during the collisions. Our results
suggest that very high energy densities can be produced in multi-kink
collisions in a controllable manner. Appearance of high energy density spots in
multi-kink collisions can be important in various physical applications of the
Klein-Gordon model.
",0,1,0,0,0,0
1132,InverseFaceNet: Deep Monocular Inverse Face Rendering,"  We introduce InverseFaceNet, a deep convolutional inverse rendering framework
for faces that jointly estimates facial pose, shape, expression, reflectance
and illumination from a single input image. By estimating all parameters from
just a single image, advanced editing possibilities on a single face image,
such as appearance editing and relighting, become feasible in real time. Most
previous learning-based face reconstruction approaches do not jointly recover
all dimensions, or are severely limited in terms of visual quality. In
contrast, we propose to recover high-quality facial pose, shape, expression,
reflectance and illumination using a deep neural network that is trained using
a large, synthetically created training corpus. Our approach builds on a novel
loss function that measures model-space similarity directly in parameter space
and significantly improves reconstruction accuracy. We further propose a
self-supervised bootstrapping process in the network training loop, which
iteratively updates the synthetic training corpus to better reflect the
distribution of real-world imagery. We demonstrate that this strategy
outperforms completely synthetically trained networks. Finally, we show
high-quality reconstructions and compare our approach to several
state-of-the-art approaches.
",1,0,0,0,0,0
1133,Exact partial information decompositions for Gaussian systems based on dependency constraints,"  The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a
theoretical framework to characterize and quantify the structure of
multivariate information sharing. A new method (Idep) has recently been
proposed for computing a two-predictor PID over discrete spaces.
[arXiv:1709.06653] A lattice of maximum entropy probability models is
constructed based on marginal dependency constraints, and the unique
information that a particular predictor has about the target is defined as the
minimum increase in joint predictor-target mutual information when that
particular predictor-target marginal dependency is constrained. Here, we apply
the Idep approach to Gaussian systems, for which the marginally constrained
maximum entropy models are Gaussian graphical models. Closed form solutions for
the Idep PID are derived for both univariate and multivariate Gaussian systems.
Numerical and graphical illustrations are provided, together with practical and
theoretical comparisons of the Idep PID with the minimum mutual information PID
(Immi). [arXiv:1411.2832] In particular, it is proved that the Immi method
generally produces larger estimates of redundancy and synergy than does the
Idep method. In discussion of the practical examples, the PIDs are complemented
by the use of deviance tests for the comparison of Gaussian graphical models.
",0,0,0,1,1,0
1134,Deep Multimodal Subspace Clustering Networks,"  We present convolutional neural network (CNN) based approaches for
unsupervised multimodal subspace clustering. The proposed framework consists of
three main stages - multimodal encoder, self-expressive layer, and multimodal
decoder. The encoder takes multimodal data as input and fuses them to a latent
space representation. The self-expressive layer is responsible for enforcing
the self-expressiveness property and acquiring an affinity matrix corresponding
to the data points. The decoder reconstructs the original input data. The
network uses the distance between the decoder's reconstruction and the original
input in its training. We investigate early, late and intermediate fusion
techniques and propose three different encoders corresponding to them for
spatial fusion. The self-expressive layers and multimodal decoders are
essentially the same for different spatial fusion-based approaches. In addition
to various spatial fusion-based methods, an affinity fusion-based network is
also proposed in which the self-expressive layer corresponding to different
modalities is enforced to be the same. Extensive experiments on three datasets
show that the proposed methods significantly outperform the state-of-the-art
multimodal subspace clustering methods.
",0,0,0,1,0,0
1135,Finite-time generalization of the thermodynamic uncertainty relation,"  For fluctuating currents in non-equilibrium steady states, the recently
discovered thermodynamic uncertainty relation expresses a fundamental relation
between their variance and the overall entropic cost associated with the
driving. We show that this relation holds not only for the long-time limit of
fluctuations, as described by large deviation theory, but also for fluctuations
on arbitrary finite time scales. This generalization facilitates applying the
thermodynamic uncertainty relation to single molecule experiments, for which
infinite timescales are not accessible. Importantly, often this finite-time
variant of the relation allows inferring a bound on the entropy production that
is even stronger than the one obtained from the long-time limit. We illustrate
the relation for the fluctuating work that is performed by a stochastically
switching laser tweezer on a trapped colloidal particle.
",0,1,0,0,0,0
1136,Composition Properties of Inferential Privacy for Time-Series Data,"  With the proliferation of mobile devices and the internet of things,
developing principled solutions for privacy in time series applications has
become increasingly important. While differential privacy is the gold standard
for database privacy, many time series applications require a different kind of
guarantee, and a number of recent works have used some form of inferential
privacy to address these situations.
However, a major barrier to using inferential privacy in practice is its lack
of graceful composition -- even if the same or related sensitive data is used
in multiple releases that are safe individually, the combined release may have
poor privacy properties. In this paper, we study composition properties of a
form of inferential privacy called Pufferfish when applied to time-series data.
We show that while general Pufferfish mechanisms may not compose gracefully, a
specific Pufferfish mechanism, called the Markov Quilt Mechanism, which was
recently introduced, has strong composition properties comparable to that of
pure differential privacy when applied to time series data.
",1,0,0,1,0,0
1137,Landau-Ginzburg theory of cortex dynamics: Scale-free avalanches emerge at the edge of synchronization,"  Understanding the origin, nature, and functional significance of complex
patterns of neural activity, as recorded by diverse electrophysiological and
neuroimaging techniques, is a central challenge in neuroscience. Such patterns
include collective oscillations emerging out of neural synchronization as well
as highly heterogeneous outbursts of activity interspersed by periods of
quiescence, called ""neuronal avalanches."" Much debate has been generated about
the possible scale invariance or criticality of such avalanches and its
relevance for brain function. Aimed at shedding light onto this, here we
analyze the large-scale collective properties of the cortex by using a
mesoscopic approach following the principle of parsimony of Landau-Ginzburg.
Our model is similar to that of Wilson-Cowan for neural dynamics but crucially,
includes stochasticity and space; synaptic plasticity and inhibition are
considered as possible regulatory mechanisms. Detailed analyses uncover a phase
diagram including down-state, synchronous, asynchronous, and up-state phases
and reveal that empirical findings for neuronal avalanches are consistently
reproduced by tuning our model to the edge of synchronization. This reveals
that the putative criticality of cortical dynamics does not correspond to a
quiescent-to-active phase transition as usually assumed in theoretical
approaches but to a synchronization phase transition, at which incipient
oscillations and scale-free avalanches coexist. Furthermore, our model also
accounts for up and down states as they occur (e.g., during deep sleep). This
approach constitutes a framework to rationalize the possible collective phases
and phase transitions of cortical networks in simple terms, thus helping to
shed light on basic aspects of brain functioning from a very broad perspective.
",0,0,0,0,1,0
1138,Recurrent Autoregressive Networks for Online Multi-Object Tracking,"  The main challenge of online multi-object tracking is to reliably associate
object trajectories with detections in each video frame based on their tracking
history. In this work, we propose the Recurrent Autoregressive Network (RAN), a
temporal generative modeling framework to characterize the appearance and
motion dynamics of multiple objects over time. The RAN couples an external
memory and an internal memory. The external memory explicitly stores previous
inputs of each trajectory in a time window, while the internal memory learns to
summarize long-term tracking history and associate detections by processing the
external memory. We conduct experiments on the MOT 2015 and 2016 datasets to
demonstrate the robustness of our tracking method in highly crowded and
occluded scenes. Our method achieves top-ranked results on the two benchmarks.
",1,0,0,0,0,0
1139,The occurrence of transverse and longitudinal electric currents in the classical plasma under the action of N transverse electromagnetic waves,"  Classical plasma with arbitrary degree of degeneration of electronic gas is
considered. In plasma N (N>2) collinear electromagnatic waves are propagated.
It is required to find the response of plasma to these waves. Distribution
function in square-law approximation on quantities of two small parameters from
Vlasov equation is received. The formula for electric current calculation is
deduced. It is demonstrated that the nonlinearity account leads to occurrence
of the longitudinal electric current directed along a wave vector. This
longitudinal current is orthogonal to the known transversal current received at
the linear analysis. The case of small values of wave number is considered.
",0,1,0,0,0,0
1140,Well-posedness of a Model for the Growth of Tree Stems and Vines,"  The paper studies a PDE model for the growth of a tree stem or a vine, having
the form of a differential inclusion with state constraints. The equations
describe the elongation due to cell growth, and the response to gravity and to
external obstacles.
The main theorem shows that the evolution problem is well posed, until a
specific ""breakdown configuration"" is reached. A formula is proved,
characterizing the reaction produced by unilateral constraints. At a.e. time t,
this is determined by the minimization of an elastic energy functional under
suitable constraints.
",0,0,1,0,0,0
1141,Yonsei evolutionary population synthesis (YEPS). II. Spectro-photometric evolution of helium-enhanced stellar populations,"  The discovery of multiple stellar populations in Milky Way globular clusters
(GCs) has stimulated various follow-up studies on helium-enhanced stellar
populations. Here we present the evolutionary population synthesis models for
the spectro-photometric evolution of simple stellar populations (SSPs) with
varying initial helium abundance ($Y_{\rm ini}$). We show that $Y_{\rm ini}$
brings about {dramatic} changes in spectro-photometric properties of SSPs. Like
the normal-helium SSPs, the integrated spectro-photometric evolution of
helium-enhanced SSPs is also dependent on metallicity and age for a given
$Y_{\rm ini}$. {We discuss the implications and prospects for the
helium-enhanced populations in relation to the second-generation populations
found in the Milky Way GCs.} All of the models are available at
\url{this http URL}.
",0,1,0,0,0,0
1142,Large deviation theorem for random covariance matrices,"  We establish a large deviation theorem for the empirical spectral
distribution of random covariance matrices whose entries are independent random
variables with mean 0, variance 1 and having controlled forth moments. Some new
properties of Laguerre polynomials are also given.
",0,0,1,0,0,0
1143,Hindsight policy gradients,"  A reinforcement learning agent that needs to pursue different goals across
episodes requires a goal-conditional policy. In addition to their potential to
generalize desirable behavior to unseen goals, such policies may also enable
higher-level planning based on subgoals. In sparse-reward environments, the
capacity to exploit information about the degree to which an arbitrary goal has
been achieved while another goal was intended appears crucial to enable sample
efficient learning. However, reinforcement learning agents have only recently
been endowed with such capacity for hindsight. In this paper, we demonstrate
how hindsight can be introduced to policy gradient methods, generalizing this
idea to a broad class of successful algorithms. Our experiments on a diverse
selection of sparse-reward environments show that hindsight leads to a
remarkable increase in sample efficiency.
",1,0,0,0,0,0
1144,Abundances in photoionized nebulae of the Local Group and nucleosynthesis of intermediate mass stars,"  Photoionized nebulae, comprising HII regions and planetary nebulae, are
excellent laboratories to investigate the nucleosynthesis and chemical
evolution of several elements in the Galaxy and other galaxies of the Local
Group. Our purpose in this investigation is threefold: (i) compare the
abundances of HII regions and planetary nebulae in each system in order to
investigate the differences derived from the age and origin of these objects,
(ii) compare the chemical evolution in different systems, such as the Milky
Way, the Magellanic Clouds, and other galaxies of the Local Group, and (iii)
investigate to what extent the nucleosynthesis contributions from the
progenitor stars affect the observed abundances in planetary nebulae, which
constrains the nucleosynthesis of intermediate mass stars. We show that all
objects in the samples present similar trends concerning distance-independent
correlations, and some constraints can be defined on the production of He and N
by the PN progenitor stars.
",0,1,0,0,0,0
1145,Are Thousands of Samples Really Needed to Generate Robust Gene-List for Prediction of Cancer Outcome?,"  The prediction of cancer prognosis and metastatic potential immediately after
the initial diagnoses is a major challenge in current clinical research. The
relevance of such a signature is clear, as it will free many patients from the
agony and toxic side-effects associated with the adjuvant chemotherapy
automatically and sometimes carelessly subscribed to them. Motivated by this
issue, Ein-Dor (2006) and Zuk (2007) presented a Bayesian model which leads to
the following conclusion: Thousands of samples are needed to generate a robust
gene list for predicting outcome. This conclusion is based on existence of some
statistical assumptions. The current work raises doubts over this determination
by showing that: (1) These assumptions are not consistent with additional
assumptions such as sparsity and Gaussianity. (2) The empirical Bayes
methodology which was suggested in order to test the relevant assumptions
doesn't detect severe violations of the model assumptions and consequently an
overestimation of the required sample size might be incurred.
",0,0,0,1,0,0
1146,Multi-scale analysis of lead-lag relationships in high-frequency financial markets,"  We propose a novel estimation procedure for scale-by-scale lead-lag
relationships of financial assets observed at a high-frequency in a
non-synchronous manner. The proposed estimation procedure does not require any
interpolation processing of the original data and is applicable to quite fine
resolution data. The validity of the proposed estimators is shown under the
continuous-time framework developed in our previous work Hayashi and Koike
(2016). An empirical application shows promising results of the proposed
approach.
",0,0,0,1,0,0
1147,Approximations of the allelic frequency spectrum in general supercritical branching populations,"  We consider a general branching population where the lifetimes of individuals
are i.i.d.\ with arbitrary distribution and where each individual gives birth
to new individuals at Poisson times independently from each other. In addition,
we suppose that individuals experience mutations at Poissonian rate $\theta$
under the infinitely many alleles assumption assuming that types are
transmitted from parents to offspring. This mechanism leads to a partition of
the population by type, called the allelic partition. The main object of this
work is the frequency spectrum $A(k,t)$ which counts the number of families of
size $k$ in the population at time $t$. The process $(A(k,t),\
t\in\mathbb{R}_+)$ is an example of non-Markovian branching process belonging
to the class of general branching processes counted by random characteristics.
In this work, we propose methods of approximation to replace the frequency
spectrum by simpler quantities. Our main goal is study the asymptotic error
made during these approximations through central limit theorems. In a last
section, we perform several numerical analysis using this model, in particular
to analyze the behavior of one of these approximations with respect to Sabeti's
Extended Haplotype Homozygosity [18].
",0,0,1,0,0,0
1148,Anomaly Detection Using Optimally-Placed Micro-PMU Sensors in Distribution Grids,"  As the distribution grid moves toward a tightly-monitored network, it is
important to automate the analysis of the enormous amount of data produced by
the sensors to increase the operators situational awareness about the system.
In this paper, focusing on Micro-Phasor Measurement Unit ($\mu$PMU) data, we
propose a hierarchical architecture for monitoring the grid and establish a set
of analytics and sensor fusion primitives for the detection of abnormal
behavior in the control perimeter. Due to the key role of the $\mu$PMU devices
in our architecture, a source-constrained optimal $\mu$PMU placement is also
described that finds the best location of the devices with respect to our
rules. The effectiveness of the proposed methods are tested through the
synthetic and real $\mu$PMU data.
",1,0,0,0,0,0
1149,Electron-Phonon Interaction in Ternary Rare-Earth Copper Antimonides LaCuSb2 and La(Cu0.8Ag0.2)Sb2 probed by Yanson Point-Contact Spectroscopy,"  Investigation of the electron-phonon interaction (EPI) in LaCuSb2 and
La(Cu0.8Ag0.2)Sb2 compounds by Yanson point-contact spectroscopy (PCS) has been
carried out. Point-contact spectra display a pronounced broad maximum in the
range of 10?ú20 mV caused by EPI. Variation of the position of this maximum
is likely connected with anisotropic phonon spectrum in these layered
compounds. The absence of phonon features after the main maximum allows the
assessment of the Debye energy of about 40 meV. The EPI constant for the
LaCuSb2 compound was estimated to be {\lambda}=0.2+/-0.03. A zero-bias minimum
in differential resistance for the latter compound is observed for some point
contacts, which vanishes at about 6 K, pointing to the formation of
superconducting phase under point contact, while superconducting critical
temperature of the bulk sample is only 1K.
",0,1,0,0,0,0
1150,Ultrahigh Magnetic Field Phases in Frustrated Triangular-lattice Magnet CuCrO$_2$,"  The magnetic phases of a triangular-lattice antiferromagnet, CuCrO$_2$, were
investigated in magnetic fields along to the $c$ axis, $H$ // [001], up to 120
T. Faraday rotation and magneto-absorption spectroscopy were used to unveil the
rich physics of magnetic phases. An up-up-down (UUD) magnetic structure phase
was observed around 90--105 T at temperatures around 10 K. Additional distinct
anomalies adjacent to the UUD phase were uncovered and the Y-shaped and the
V-shaped phases are proposed to be viable candidates. These ordered phases are
emerged as a result of the interplay of geometrical spin frustration, single
ion anisotropy and thermal fluctuations in an environment of extremely high
magnetic fields.
",0,1,0,0,0,0
1151,Preserving Differential Privacy in Convolutional Deep Belief Networks,"  The remarkable development of deep learning in medicine and healthcare domain
presents obvious privacy issues, when deep neural networks are built on users'
personal and highly sensitive data, e.g., clinical records, user profiles,
biomedical images, etc. However, only a few scientific studies on preserving
privacy in deep learning have been conducted. In this paper, we focus on
developing a private convolutional deep belief network (pCDBN), which
essentially is a convolutional deep belief network (CDBN) under differential
privacy. Our main idea of enforcing epsilon-differential privacy is to leverage
the functional mechanism to perturb the energy-based objective functions of
traditional CDBNs, rather than their results. One key contribution of this work
is that we propose the use of Chebyshev expansion to derive the approximate
polynomial representation of objective functions. Our theoretical analysis
shows that we can further derive the sensitivity and error bounds of the
approximate polynomial representation. As a result, preserving differential
privacy in CDBNs is feasible. We applied our model in a health social network,
i.e., YesiWell data, and in a handwriting digit dataset, i.e., MNIST data, for
human behavior prediction, human behavior classification, and handwriting digit
recognition tasks. Theoretical analysis and rigorous experimental evaluations
show that the pCDBN is highly effective. It significantly outperforms existing
solutions.
",1,0,0,1,0,0
1152,Injectivity almost everywhere and mappings with finite distortion in nonlinear elasticity,"  We show that a sufficient condition for the weak limit of a sequence of
$W^1_q$-homeomorphisms with finite distortion to be almost everywhere injective
for $q \geq n-1$, can be stated by means of composition operators. Applying
this result, we study nonlinear elasticity problems with respect to these new
classes of mappings. Furthermore, we impose loose growth conditions on the
stored-energy function for the class of $W^1_n$-homeomorphisms with finite
distortion and integrable inner as well as outer distortion coefficients.
",0,0,1,0,0,0
1153,A probabilistic approach to the leader problem in random graphs,"  Consider the classical Erdos-Renyi random graph process wherein one starts
with an empty graph on $n$ vertices at time $t=0$. At each stage, an edge is
chosen uniformly at random and placed in the graph. After the original
fundamental work in [19], Erd?s suggested that one should view the original
random graph process as a ""race of components"". This suggested understanding
functionals such as the time for fixation of the identity of the maximal
component, sometimes referred to as the ""leader problem"". Using refined
combinatorial techniques, {\L}uczak [25] provided a complete analysis of this
question including the close relationship to the critical scaling window of the
Erdos-Renyi process. In this paper, we abstract this problem to the context of
the multiplicative coalescent which by the work of Aldous in [3] describes the
evolution of the Erdos-Renyi random graph in the critical regime. Further,
different entrance boundaries of this process have arisen in the study of heavy
tailed network models in the critical regime with degree exponent $\tau \in
(3,4)$. The leader problem in the context of the Erdos-Renyi random graph also
played an important role in the study of the scaling limit of the minimal
spanning tree on the complete graph [2]. In this paper we provide a
probabilistic analysis of the leader problem for the multiplicative coalescent
in the context of entrance boundaries of relevance to critical random graphs.
As a special case we recover {\L}uczak's result in [25] for the Erdos-Renyi
random graph.
",0,0,1,0,0,0
1154,An improvement on LSB+ method,"  The Least Significant Bit (LSB) substitution is an old and simple data hiding
method that could almost effortlessly be implemented in spatial or transform
domain over any digital media. This method can be attacked by several
steganalysis methods, because it detectably changes statistical and perceptual
characteristics of the cover signal. A typical method for steganalysis of the
LSB substitution is the histogram attack that attempts to diagnose anomalies in
the cover image's histogram. A well-known method to stand the histogram attack
is the LSB+ steganography that intentionally embeds some extra bits to make the
histogram look natural. However, the LSB+ method still affects the perceptual
and statistical characteristics of the cover signal. In this paper, we propose
a new method for image steganography, called LSB++, which improves over the
LSB+ image steganography by decreasing the amount of changes made to the
perceptual and statistical attributes of the cover image. We identify some
sensitive pixels affecting the signal characteristics, and then lock and keep
them from the extra bit embedding process of the LSB+ method, by introducing a
new embedding key. Evaluation results show that, without reducing the embedding
capacity, our method can decrease potentially detectable changes caused by the
embedding process.
",1,0,0,0,0,0
1155,Oxygen reduction mechanisms in nanostructured La0.8Sr0.2MnO3 cathodes for Solid Oxide Fuel Cells,"  In this work we outline the mechanisms contributing to the oxygen reduction
reaction in nanostructured cathodes of La0.8Sr0.2MnO3 (LSM) for Solid Oxide
Fuel Cells (SOFC). These cathodes, developed from LSM nanostructured tubes, can
be used at lower temperatures compared to microstructured ones, and this is a
crucial fact to avoid the degradation of the fuel cell components. This
reduction of the operating temperatures stems mainly from two factors: i) the
appearance of significant oxide ion diffusion through the cathode material in
which the nanostructure plays a key role and ii) an optimized gas phase
diffusion of oxygen through the porous structure of the cathode, which becomes
negligible. A detailed analysis of our Electrochemical Impedance Spectroscopy
supported by first principles calculations point towards an improved overall
cathodic performance driven by a fast transport of oxide ions through the
cathode surface.
",0,1,0,0,0,0
1156,Learning Interpretable Models with Causal Guarantees,"  Machine learning has shown much promise in helping improve the quality of
medical, legal, and economic decision-making. In these applications, machine
learning models must satisfy two important criteria: (i) they must be causal,
since the goal is typically to predict individual treatment effects, and (ii)
they must be interpretable, so that human decision makers can validate and
trust the model predictions. There has recently been much progress along each
direction independently, yet the state-of-the-art approaches are fundamentally
incompatible. We propose a framework for learning causal interpretable
models---from observational data---that can be used to predict individual
treatment effects. Our framework can be used with any algorithm for learning
interpretable models. Furthermore, we prove an error bound on the treatment
effects predicted by our model. Finally, in an experiment on real-world data,
we show that the models trained using our framework significantly outperform a
number of baselines.
",1,0,0,1,0,0
1157,Achromatic super-oscillatory lenses with sub-wavelength focusing,"  Lenses are crucial to light-enabled technologies. Conventional lenses have
been perfected to achieve near-diffraction-limited resolution and minimal
chromatic aberrations. However, such lenses are bulky and cannot focus light
into a hotspot smaller than half wavelength of light. Pupil filters, initially
suggested by Toraldo di Francia, can overcome the resolution constraints of
conventional lenses, but are not intrinsically chromatically corrected. Here we
report single-element planar lenses that not only deliver sub-wavelength
focusing (beating the diffraction limit of conventional refractive lenses) but
also focus light of different colors into the same hotspot. Using the principle
of super-oscillations we designed and fabricated a range of binary dielectric
and metallic lenses for visible and infrared parts of the spectrum that are
manufactured on silicon wafers, silica substrates and optical fiber tips. Such
low cost, compact lenses could be useful in mobile devices, data storage,
surveillance, robotics, space applications, imaging, manufacturing with light,
and spatially resolved nonlinear microscopies.
",0,1,0,0,0,0
1158,Time-dependent linear-response variational Monte Carlo,"  We present the extension of variational Monte Carlo (VMC) to the calculation
of electronic excitation energies and oscillator strengths using time-dependent
linear-response theory. By exploiting the analogy existing between the linear
method for wave-function optimisation and the generalised eigenvalue equation
of linear-response theory, we formulate the equations of linear-response VMC
(LR-VMC). This LR-VMC approach involves the first-and second-order derivatives
of the wave function with respect to the parameters. We perform first tests of
the LR-VMC method within the Tamm-Dancoff approximation using
single-determinant Jastrow-Slater wave functions with different Slater basis
sets on some singlet and triplet excitations of the beryllium atom. Comparison
with reference experimental data and with configuration-interaction-singles
(CIS) results shows that LR-VMC generally outperforms CIS for excitation
energies and is thus a promising approach for calculating electronic
excited-state properties of atoms and molecules.
",0,1,0,0,0,0
1159,Wireless Power Transfer for Distributed Estimation in Sensor Networks,"  This paper studies power allocation for distributed estimation of an unknown
scalar random source in sensor networks with a multiple-antenna fusion center
(FC), where wireless sensors are equipped with radio-frequency based energy
harvesting technology. The sensors' observation is locally processed by using
an uncoded amplify-and-forward scheme. The processed signals are then sent to
the FC, and are coherently combined at the FC, at which the best linear
unbiased estimator (BLUE) is adopted for reliable estimation. We aim to solve
the following two power allocation problems: 1) minimizing distortion under
various power constraints; and 2) minimizing total transmit power under
distortion constraints, where the distortion is measured in terms of
mean-squared error of the BLUE. Two iterative algorithms are developed to solve
the non-convex problems, which converge at least to a local optimum. In
particular, the above algorithms are designed to jointly optimize the
amplification coefficients, energy beamforming, and receive filtering. For each
problem, a suboptimal design, a single-antenna FC scenario, and a common
harvester deployment for colocated sensors, are also studied. Using the
powerful semidefinite relaxation framework, our result is shown to be valid for
any number of sensors, each with different noise power, and for an arbitrarily
number of antennas at the FC.
",1,0,1,0,0,0
1160,About a non-standard interpolation problem,"  Using algebraic methods, and motivated by the one variable case, we study a
multipoint interpolation problem in the setting of several complex variables.
The duality realized by the residue generator associated with an underlying
Gorenstein algebra, using the Lagrange interpolation polynomial, plays a key
role in the arguments.
",0,0,1,0,0,0
1161,Quantum spin liquid signatures in Kitaev-like frustrated magnets,"  Motivated by recent experiments on $\alpha$-RuCl$_3$, we investigate a
possible quantum spin liquid ground state of the honeycomb-lattice spin model
with bond-dependent interactions. We consider the $K-\Gamma$ model, where $K$
and $\Gamma$ represent the Kitaev and symmetric-anisotropic interactions
between spin-1/2 moments on the honeycomb lattice. Using the infinite density
matrix renormalization group (iDMRG), we provide compelling evidence for the
existence of quantum spin liquid phases in an extended region of the phase
diagram. In particular, we use transfer matrix spectra to show the evolution of
two-particle excitations with well-defined two-dimensional dispersion, which is
a strong signature of quantum spin liquid. These results are compared with
predictions from Majorana mean-field theory and used to infer the quasiparticle
excitation spectra. Further, we compute the dynamical structure factor using
finite size cluster computations and show that the results resemble the
scattering continuum seen in neutron scattering experiments on
$\alpha$-RuCl$_3$. We discuss these results in light of recent and future
experiments.
",0,1,0,0,0,0
1162,Equivalent electric circuit of magnetosphere-ionosphere-atmosphere interaction,"  The aim of this study is to investigate the magnetospheric disturbances
effects on complicated nonlinear system of atmospheric processes. During
substorms and storms, the ionosphere was subjected to rather a significant
Joule heating, and the power of precipitating energetic particles was also
great. Nevertheless, there were no abnormal variations of meteoparameters in
the lower atmosphere. If there is a mechanism for the powerful magnetospheric
disturbance effect on meteorological processes in the atmosphere, it supposes a
more complicated series of many intermediates, and is not associated directly
with the energy that arrives into the ionosphere during storms. I discuss the
problem of the effect of the solar wind electric field sharp increase via the
global electric circuit during magnetospheric disturbances on the cloud layer
formation.
",0,1,0,0,0,0
1163,Charge polarization effects on the optical response of blue-emitting superlattices,"  In the new approach to study the optical response of periodic structures,
successfully applied to study the optical properties of blue-emitting InGaN/GaN
superlattices, the spontaneous charge polarization was neglected. To search the
effect of this quantum confined Stark phenomenon we study the optical response,
assuming parabolic band edge modulations in the conduction and valence bands.
We discuss the consequences on the eigenfunction symmetries and the ensuing
optical transition selection rules. Using the new approach in the WKB
approximation of the finite periodic systems theory, we determine the energy
eigenvalues, their corresponding eigenfunctions and the subband structures in
the conduction and valence bands. We calculate the photoluminescence as a
function of the charge localization strength, and compare with the experimental
result. We show that for subbands close to the barrier edge the optical
response and the surface states are sensitive to charge polarization strength.
",0,1,0,0,0,0
1164,Replica analysis of overfitting in regression models for time-to-event data,"  Overfitting, which happens when the number of parameters in a model is too
large compared to the number of data points available for determining these
parameters, is a serious and growing problem in survival analysis. While modern
medicine presents us with data of unprecedented dimensionality, these data
cannot yet be used effectively for clinical outcome prediction. Standard error
measures in maximum likelihood regression, such as p-values and z-scores, are
blind to overfitting, and even for Cox's proportional hazards model (the main
tool of medical statisticians), one finds in literature only rules of thumb on
the number of samples required to avoid overfitting. In this paper we present a
mathematical theory of overfitting in regression models for time-to-event data,
which aims to increase our quantitative understanding of the problem and
provide practical tools with which to correct regression outcomes for the
impact of overfitting. It is based on the replica method, a statistical
mechanical technique for the analysis of heterogeneous many-variable systems
that has been used successfully for several decades in physics, biology, and
computer science, but not yet in medical statistics. We develop the theory
initially for arbitrary regression models for time-to-event data, and verify
its predictions in detail for the popular Cox model.
",0,1,0,1,0,0
1165,System Description: Russell - A Logical Framework for Deductive Systems,"  Russell is a logical framework for the specification and implementation of
deductive systems. It is a high-level language with respect to Metamath
language, so inherently it uses a Metamath foundations, i.e. it doesn't rely on
any particular formal calculus, but rather is a pure logical framework. The
main difference with Metamath is in the proof language and approach to syntax:
the proofs have a declarative form, i.e. consist of actual expressions, which
are used in proofs, while syntactic grammar rules are separated from the
meaningful rules of inference.
Russell is implemented in c++14 and is distributed under GPL v3 license. The
repository contains translators from Metamath to Russell and back. Original
Metamath theorem base (almost 30 000 theorems) can be translated to Russell,
verified, translated back to Metamath and verified with the original Metamath
verifier. Russell can be downloaded from the repository
this https URL
",1,0,1,0,0,0
1166,Spinor analysis,"  ""Let us call the novel quantities which, in addition to the vectors and
tensors, have appeared in the quantum mechanics of the spinning electron, and
which in the case of the Lorentz group are quite differently transformed from
tensors, as spinors for short. Is there no spinor analysis that every physicist
can learn, such as tensor analysis, and with the aid of which all the possible
spinors can be formed, and secondly, all the invariant equations in which
spinors occur?"" So Mr Ehrenfest asked me and the answer will be given below.
",0,1,0,0,0,0
1167,Identifiability of phylogenetic parameters from k-mer data under the coalescent,"  Distances between sequences based on their $k$-mer frequency counts can be
used to reconstruct phylogenies without first computing a sequence alignment.
Past work has shown that effective use of k-mer methods depends on 1)
model-based corrections to distances based on $k$-mers and 2) breaking long
sequences into blocks to obtain repeated trials from the sequence-generating
process. Good performance of such methods is based on having many high-quality
blocks with many homologous sites, which can be problematic to guarantee a
priori.
Nature provides natural blocks of sequences into homologous regions---namely,
the genes. However, directly using past work in this setting is problematic
because of possible discordance between different gene trees and the underlying
species tree. Using the multispecies coalescent model as a basis, we derive
model-based moment formulas that involve the divergence times and the
coalescent parameters. From this setting, we prove identifiability results for
the tree and branch length parameters under the Jukes-Cantor model of sequence
mutations.
",0,0,1,0,0,0
1168,Short-Time Nonlinear Effects in the Exciton-Polariton System,"  In the exciton-polariton system, a linear dispersive photon field is coupled
to a nonlinear exciton field. Short-time analysis of the lossless system shows
that, when the photon field is excited, the time required for that field to
exhibit nonlinear effects is longer than the time required for the nonlinear
Schr??dinger equation, in which the photon field itself is nonlinear. When the
initial condition is scaled by $\epsilon^\alpha$, it is found that the relative
error committed by omitting the nonlinear term in the exciton-polariton system
remains within $\epsilon$ for all times up to $t=C\epsilon^\beta$, where
$\beta=(1-\alpha(p-1))/(p+2)$. This is in contrast to $\beta=1-\alpha(p-1)$ for
the nonlinear Schr??dinger equation.
",0,0,1,0,0,0
1169,GTC Observations of an Overdense Region of LAEs at z=6.5,"  We present the results of our search for the faint galaxies near the end of
the Reionisation Epoch. This has been done using very deep OSIRIS images
obtained at the Gran Telescopio Canarias (GTC). Our observations focus around
two close, massive Lyman Alpha Emitters (LAEs) at redshift 6.5, discovered in
the SXDS field within a large-scale overdense region (Ouchi et al. 2010). The
total GTC observing time in three medium band filters (F883w35, F913w25 and
F941w33) is over 34 hours covering $7.0\times8.5$ arcmin$^2$ (or $\sim30,000$
Mpc$^3$ at $z=6.5$). In addition to the two spectroscopically confirmed LAEs in
the field, we have identified 45 other LAE candidates. The preliminary
luminosity function derived from our observations, assuming a spectroscopic
confirmation success rate of $\frac{2}{3}$ as in previous surveys, suggests
this area is about 2 times denser than the general field galaxy population at
$z=6.5$. If confirmed spectroscopically, our results will imply the discovery
of one of the earliest protoclusters in the universe, which will evolve to
resemble the most massive galaxy clusters today.
",0,1,0,0,0,0
1170,"Comment on Photothermal radiometry parametric identifiability theory for reliable and unique nondestructive coating thickness and thermophysical measurements, J. Appl. Phys. 121(9), 095101 (2017)","  A recent paper [X. Guo, A. Mandelis, J. Tolev and K. Tang, J. Appl. Phys.,
121, 095101 (2017)] intends to demonstrate that from the photothermal
radiometry signal obtained on a coated opaque sample in 1D transfer, one should
be able to identify separately the following three parameters of the coating:
thermal diffusivity, thermal conductivity and thickness. In this comment, it is
shown that the three parameters are correlated in the considered experimental
arrangement, the identifiability criterion is in error and the thickness
inferred therefrom is not trustable.
",0,1,0,0,0,0
1171,Computing the projected reachable set of switched affine systems: an application to systems biology,"  A fundamental question in systems biology is what combinations of mean and
variance of the species present in a stochastic biochemical reaction network
are attainable by perturbing the system with an external signal. To address
this question, we show that the moments evolution in any generic network can be
either approximated or, under suitable assumptions, computed exactly as the
solution of a switched affine system. Motivated by this application, we propose
a new method to approximate the reachable set of switched affine systems. A
remarkable feature of our approach is that it allows one to easily compute
projections of the reachable set for pairs of moments of interest, without
requiring the computation of the full reachable set, which can be prohibitive
for large networks. As a second contribution, we also show how to select the
external signal in order to maximize the probability of reaching a target set.
To illustrate the method we study a renown model of controlled gene expression
and we derive estimates of the reachable set, for the protein mean and
variance, that are more accurate than those available in the literature and
consistent with experimental data.
",1,0,1,0,0,0
1172,Temporal Action Localization by Structured Maximal Sums,"  We address the problem of temporal action localization in videos. We pose
action localization as a structured prediction over arbitrary-length temporal
windows, where each window is scored as the sum of frame-wise classification
scores. Additionally, our model classifies the start, middle, and end of each
action as separate components, allowing our system to explicitly model each
action's temporal evolution and take advantage of informative temporal
dependencies present in this structure. In this framework, we localize actions
by searching for the structured maximal sum, a problem for which we develop a
novel, provably-efficient algorithmic solution. The frame-wise classification
scores are computed using features from a deep Convolutional Neural Network
(CNN), which are trained end-to-end to directly optimize for a novel structured
objective. We evaluate our system on the THUMOS 14 action detection benchmark
and achieve competitive performance.
",1,0,0,0,0,0
1173,Using Transfer Learning for Image-Based Cassava Disease Detection,"  Cassava is the third largest source of carbohydrates for human food in the
world but is vulnerable to virus diseases, which threaten to destabilize food
security in sub-Saharan Africa. Novel methods of cassava disease detection are
needed to support improved control which will prevent this crisis. Image
recognition offers both a cost effective and scalable technology for disease
detection. New transfer learning methods offer an avenue for this technology to
be easily deployed on mobile devices. Using a dataset of cassava disease images
taken in the field in Tanzania, we applied transfer learning to train a deep
convolutional neural network to identify three diseases and two types of pest
damage (or lack thereof). The best trained model accuracies were 98% for brown
leaf spot (BLS), 96% for red mite damage (RMD), 95% for green mite damage
(GMD), 98% for cassava brown streak disease (CBSD), and 96% for cassava mosaic
disease (CMD). The best model achieved an overall accuracy of 93% for data not
used in the training process. Our results show that the transfer learning
approach for image recognition of field images offers a fast, affordable, and
easily deployable strategy for digital plant disease detection.
",1,0,0,0,0,0
1174,Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments,"  Ability to continuously learn and adapt from limited experience in
nonstationary environments is an important milestone on the path towards
general intelligence. In this paper, we cast the problem of continuous
adaptation into the learning-to-learn framework. We develop a simple
gradient-based meta-learning algorithm suitable for adaptation in dynamically
changing and adversarial scenarios. Additionally, we design a new multi-agent
competitive environment, RoboSumo, and define iterated adaptation games for
testing various aspects of continuous adaptation strategies. We demonstrate
that meta-learning enables significantly more efficient adaptation than
reactive baselines in the few-shot regime. Our experiments with a population of
agents that learn and compete suggest that meta-learners are the fittest.
",1,0,0,0,0,0
1175,Alpha-Divergences in Variational Dropout,"  We investigate the use of alternative divergences to Kullback-Leibler (KL) in
variational inference(VI), based on the Variational Dropout \cite{kingma2015}.
Stochastic gradient variational Bayes (SGVB) \cite{aevb} is a general framework
for estimating the evidence lower bound (ELBO) in Variational Bayes. In this
work, we extend the SGVB estimator with using Alpha-Divergences, which are
alternative to divergences to VI' KL objective. The Gaussian dropout can be
seen as a local reparametrization trick of the SGVB objective. We extend the
Variational Dropout to use alpha divergences for variational inference. Our
results compare $\alpha$-divergence variational dropout with standard
variational dropout with correlated and uncorrelated weight noise. We show that
the $\alpha$-divergence with $\alpha \rightarrow 1$ (or KL divergence) is still
a good measure for use in variational inference, in spite of the efficient use
of Alpha-divergences for Dropout VI \cite{Li17}. $\alpha \rightarrow 1$ can
yield the lowest training error, and optimizes a good lower bound for the
evidence lower bound (ELBO) among all values of the parameter $\alpha \in
[0,\infty)$.
",1,0,0,1,0,0
1176,Curvature properties of Robinson-Trautman metric,"  The curvature properties of Robinson-Trautman metric have been investigated.
It is shown that Robinson-Trautman metric admits several kinds of
pseudosymmetric type structures such as Weyl pseudosymmetric, Ricci
pseudosymmetric, pseudosymmetric Weyl conformal curvature tensor etc. Also it
is shown that the difference $R\cdot R - Q(S,R)$ is linearly dependent with
$Q(g,C)$ but the metric is not Ricci generalized pseudosymmetric. Moreover, it
is proved that this metric is Roter type, 2-quasi-Einstein, Ricci tensor is
Riemann compatible and its Weyl conformal curvature 2-forms are recurrent. It
is also shown that the energy momentum tensor of the metric is pseudosymmetric
and the conditions under which such tensor is of Codazzi type and cyclic
parallel have been investigated. Finally, we have made a comparison between the
curvature properties of Robinson-Trautman metric and Som-Raychaudhuri metric.
",0,0,1,0,0,0
1177,Dehn invariant of flexible polyhedra,"  We prove that the Dehn invariant of any flexible polyhedron in Euclidean
space of dimension greater than or equal to 3 is constant during the flexion.
In dimensions 3 and 4 this implies that any flexible polyhedron remains
scissors congruent to itself during the flexion. This proves the Strong Bellows
Conjecture posed by Connelly in 1979. It was believed that this conjecture was
disproved by Alexandrov and Connelly in 2009. However, we find an error in
their counterexample. Further, we show that the Dehn invariant of a flexible
polyhedron in either sphere or Lobachevsky space of dimension greater than or
equal to 3 is constant during the flexion if and only if this polyhedron
satisfies the usual Bellows Conjecture, i.e., its volume is constant during
every flexion of it. Using previous results due to the first listed author, we
deduce that the Dehn invariant is constant during the flexion for every bounded
flexible polyhedron in odd-dimensional Lobachevsky space and for every flexible
polyhedron with sufficiently small edge lengths in any space of constant
curvature of dimension greater than or equal to 3.
",0,0,1,0,0,0
1178,On Evaluation of Embodied Navigation Agents,"  Skillful mobile operation in three-dimensional environments is a primary
topic of study in Artificial Intelligence. The past two years have seen a surge
of creative work on navigation. This creative output has produced a plethora of
sometimes incompatible task definitions and evaluation protocols. To coordinate
ongoing and future research in this area, we have convened a working group to
study empirical methodology in navigation research. The present document
summarizes the consensus recommendations of this working group. We discuss
different problem statements and the role of generalization, present evaluation
measures, and provide standard scenarios that can be used for benchmarking.
",1,0,0,0,0,0
1179,Single Magnetic Impurity in Tilted Dirac Surface States,"  We utilize variational method to investigate the Kondo screening of a
spin-1/2 magnetic impurity in tilted Dirac surface states with the Dirac cone
tilted along the $k_y$-axis. We mainly study about the effect of the tilting
term on the binding energy and the spin-spin correlation between magnetic
impurity and conduction electrons, and compare the results with the
counterparts in a two dimensional helical metal. The binding energy has a
critical value while the Dirac cone is slightly tilted. However, as the tilting
term increases, the density of states around the Fermi surface becomes
significant, such that the impurity and the host material always favor a bound
state. The diagonal and the off-diagonal terms of the spin-spin correlation
between the magnetic impurity and conduction electrons are also studied. Due to
the spin-orbit coupling and the tilting of the spectra, various components of
spin-spin correlation show very strong anisotropy in coordinate space, and are
of power-law decay with respect to the spatial displacements.
",0,1,0,0,0,0
1180,Leveraging the Path Signature for Skeleton-based Human Action Recognition,"  Human action recognition in videos is one of the most challenging tasks in
computer vision. One important issue is how to design discriminative features
for representing spatial context and temporal dynamics. Here, we introduce a
path signature feature to encode information from intra-frame and inter-frame
contexts. A key step towards leveraging this feature is to construct the proper
trajectories (paths) for the data steam. In each frame, the correlated
constraints of human joints are treated as small paths, then the spatial path
signature features are extracted from them. In video data, the evolution of
these spatial features over time can also be regarded as paths from which the
temporal path signature features are extracted. Eventually, all these features
are concatenated to constitute the input vector of a fully connected neural
network for action classification. Experimental results on four standard
benchmark action datasets, J-HMDB, SBU Dataset, Berkeley MHAD, and NTURGB+D
demonstrate that the proposed approach achieves state-of-the-art accuracy even
in comparison with recent deep learning based models.
",1,0,0,0,0,0
1181,How Many Subpopulations is Too Many? Exponential Lower Bounds for Inferring Population Histories,"  Reconstruction of population histories is a central problem in population
genetics. Existing coalescent-based methods, like the seminal work of Li and
Durbin (Nature, 2011), attempt to solve this problem using sequence data but
have no rigorous guarantees. Determining the amount of data needed to correctly
reconstruct population histories is a major challenge. Using a variety of tools
from information theory, the theory of extremal polynomials, and approximation
theory, we prove new sharp information-theoretic lower bounds on the problem of
reconstructing population structure -- the history of multiple subpopulations
that merge, split and change sizes over time. Our lower bounds are exponential
in the number of subpopulations, even when reconstructing recent histories. We
demonstrate the sharpness of our lower bounds by providing algorithms for
distinguishing and learning population histories with matching dependence on
the number of subpopulations.
",0,0,0,0,1,0
1182,Source localization in an ocean waveguide using supervised machine learning,"  Source localization in ocean acoustics is posed as a machine learning problem
in which data-driven methods learn source ranges directly from observed
acoustic data. The pressure received by a vertical linear array is preprocessed
by constructing a normalized sample covariance matrix (SCM) and used as the
input. Three machine learning methods (feed-forward neural networks (FNN),
support vector machines (SVM) and random forests (RF)) are investigated in this
paper, with focus on the FNN. The range estimation problem is solved both as a
classification problem and as a regression problem by these three machine
learning algorithms. The results of range estimation for the Noise09 experiment
are compared for FNN, SVM, RF and conventional matched-field processing and
demonstrate the potential of machine learning for underwater source
localization..
",1,1,0,0,0,0
1183,Mining Illegal Insider Trading of Stocks: A Proactive Approach,"  Illegal insider trading of stocks is based on releasing non-public
information (e.g., new product launch, quarterly financial report, acquisition
or merger plan) before the information is made public. Detecting illegal
insider trading is difficult due to the complex, nonlinear, and non-stationary
nature of the stock market. In this work, we present an approach that detects
and predicts illegal insider trading proactively from large heterogeneous
sources of structured and unstructured data using a deep-learning based
approach combined with discrete signal processing on the time series data. In
addition, we use a tree-based approach that visualizes events and actions to
aid analysts in their understanding of large amounts of unstructured data.
Using existing data, we have discovered that our approach has a good success
rate in detecting illegal insider trading patterns.
",0,0,0,1,0,1
1184,Discovery of Extreme [OIII]+H$?ý$ Emitting Galaxies Tracing an Overdensity at z~3.5 in CDF-South,"  Using deep multi-wavelength photometry of galaxies from ZFOURGE, we group
galaxies at $2.5<z<4.0$ by the shape of their spectral energy distributions
(SEDs). We identify a population of galaxies with excess emission in the
$K_s$-band, which corresponds to [OIII]+H$\beta$ emission at $2.95<z<3.65$.
This population includes 78% of the bluest galaxies with UV slopes steeper than
$\beta = -2$. We de-redshift and scale this photometry to build two composite
SEDs, enabling us to measure equivalent widths of these Extreme [OIII]+H$\beta$
Emission Line Galaxies (EELGs) at $z\sim3.5$. We identify 60 galaxies that
comprise a composite SED with [OIII]+H$\beta$ rest-frame equivalent width of
$803\pm228$\AA\ and another 218 galaxies in a composite SED with equivalent
width of $230\pm90$\AA. These EELGs are analogous to the `green peas' found in
the SDSS, and are thought to be undergoing their first burst of star formation
due to their blue colors ($\beta < -1.6$), young ages
($\log(\rm{age}/yr)\sim7.2$), and low dust attenuation values. Their strong
nebular emission lines and compact sizes (typically $\sim1.4$ kpc) are
consistent with the properties of the star-forming galaxies possibly
responsible for reionizing the universe at $z>6$. Many of the EELGs also
exhibit Lyman-$\alpha$ emission. Additionally, we find that many of these
sources are clustered in an overdensity in the Chandra Deep Field South, with
five spectroscopically confirmed members at $z=3.474 \pm 0.004$. The spatial
distribution and photometric redshifts of the ZFOURGE population further
confirm the overdensity highlighted by the EELGs.
",0,1,0,0,0,0
1185,Predictive Simulations for Tuning Electronic and Optical Properties of SubPc Derivatives,"  Boron subphthalocyanine chloride is an electron donor material used in small
molecule organic photovoltaics with an unusually large molecular dipole moment.
Using first-principles calculations, we investigate enhancing the electronic
and optical properties of boron subphthalocyanine chloride, by substituting the
boron and chlorine atoms with other trivalent and halogen atoms in order to
modify the molecular dipole moment. Gas phase molecular structures and
properties are predicted with hybrid functionals. Using positions and
orientations of the known compounds as the starting coordinates for these
molecules, stable crystalline structures are derived following a procedure that
involves perturbation and accurate total energy minimization. Electronic
structure and photonic properties of the predicted crystals are computed using
the GW method and the Bethe-Salpeter equation, respectively. Finally, a simple
transport model is use to demonstrate the importance of molecular dipole
moments on device performance.
",0,1,0,0,0,0
1186,Learning to attend in a brain-inspired deep neural network,"  Recent machine learning models have shown that including attention as a
component results in improved model accuracy and interpretability, despite the
concept of attention in these approaches only loosely approximating the brain's
attention mechanism. Here we extend this work by building a more brain-inspired
deep network model of the primate ATTention Network (ATTNet) that learns to
shift its attention so as to maximize the reward. Using deep reinforcement
learning, ATTNet learned to shift its attention to the visual features of a
target category in the context of a search task. ATTNet's dorsal layers also
learned to prioritize these shifts of attention so as to maximize success of
the ventral pathway classification and receive greater reward. Model behavior
was tested against the fixations made by subjects searching images for the same
cued category. Both subjects and ATTNet showed evidence for attention being
preferentially directed to target goals, behaviorally measured as oculomotor
guidance to targets. More fundamentally, ATTNet learned to shift its attention
to target like objects and spatially route its visual inputs to accomplish the
task. This work makes a step toward a better understanding of the role of
attention in the brain and other computational systems.
",0,0,0,0,1,0
1187,Anisotropic functional Laplace deconvolution,"  In the present paper we consider the problem of estimating a
three-dimensional function $f$ based on observations from its noisy Laplace
convolution. Our study is motivated by the analysis of Dynamic Contrast
Enhanced (DCE) imaging data. We construct an adaptive wavelet-Laguerre
estimator of $f$, derive minimax lower bounds for the $L^2$-risk when $f$
belongs to a three-dimensional Laguerre-Sobolev ball and demonstrate that the
wavelet-Laguerre estimator is adaptive and asymptotically near-optimal in a
wide range of Laguerre-Sobolev spaces. We carry out a limited simulations study
and show that the estimator performs well in a finite sample setting. Finally,
we use the technique for the solution of the Laplace deconvolution problem on
the basis of DCE Computerized Tomography data.
",0,0,0,1,0,0
1188,Prediction of many-electron wavefunctions using atomic potentials,"  For a given many-electron molecule, it is possible to define a corresponding
one-electron Schr??dinger equation, using potentials derived from simple
atomic densities, whose solution predicts fairly accurate molecular orbitals
for single- and multi-determinant wavefunctions for the molecule. The energy is
not predicted and must be evaluated by calculating Coulomb and exchange
interactions over the predicted orbitals. Potentials are found by minimizing
the energy of predicted wavefunctions. There exist slightly less accurate
average potentials for first-row atoms that can be used without modification in
different molecules. For a test set of molecules representing different bonding
environments, these average potentials give wavefunctions with energies that
deviate from exact self-consistent field or configuration interaction energies
by less than 0.08 eV and 0.03 eV per bond or valence electron pair,
respectively.
",0,1,0,0,0,0
1189,Free energy of formation of a crystal nucleus in incongruent solidification: Implication for modeling the crystallization of aqueous nitric acid droplets in type 1 polar stratospheric clouds,"  Using the formalism of the classical nucleation theory, we derive an
expression for the reversible work $W_*$ of formation of a binary crystal
nucleus in a liquid binary solution of non-stoichiometric composition
(incongruent crystallization). Applied to the crystallization of aqueous nitric
acid (NA) droplets, the new expression more adequately takes account of the
effect of nitric acid vapor compared to the conventional expression of
MacKenzie, Kulmala, Laaksonen, and Vesala (MKLV) [J.Geophys.Res. 102, 19729
(1997)]. The predictions of both MKLV and modified expressions for the average
liquid-solid interfacial tension $\sigma^{ls}$ of nitric acid dihydrate (NAD)
crystals are compared by using existing experimental data on the incongruent
crystallization of aqueous NA droplets of composition relevant to polar
stratospheric clouds (PSCs). The predictions based on the MKLV expression are
higher by about 5% compared to predictions based on our modified expression.
This results in similar differences between the predictions of both expressions
for the solid-vapor interfacial tension $\sigma^{sv}$ of NAD crystal nuclei.
The latter can be obtained by analyzing of experimental data on crystal
nucleation rates in aqueous NA droplets and exploiting the dominance of the
surface-stimulated mode of crystal nucleation in small droplets and its
negligibility in large ones. Applying that method, our expression for $W_*$
provides an estimate for $\sigma^{sv}$ of NAD in the range from 92 dyn/cm to
100 dyn/cm, while the MKLV expression predicts it in the range from 95 dyn/cm
to 105 dyn/cm. The predictions of both expressions for $W_*$ become identical
in the case of congruent crystallization; this was also demonstrated by
applying our method to the nucleation of nitric acid trihydrate (NAT) crystals
in PSC droplets of stoichiometric composition.
",0,1,0,0,0,0
1190,Ensemble learning with Conformal Predictors: Targeting credible predictions of conversion from Mild Cognitive Impairment to Alzheimer's Disease,"  Most machine learning classifiers give predictions for new examples
accurately, yet without indicating how trustworthy predictions are. In the
medical domain, this hampers their integration in decision support systems,
which could be useful in the clinical practice. We use a supervised learning
approach that combines Ensemble learning with Conformal Predictors to predict
conversion from Mild Cognitive Impairment to Alzheimer's Disease. Our goal is
to enhance the classification performance (Ensemble learning) and complement
each prediction with a measure of credibility (Conformal Predictors). Our
results showed the superiority of the proposed approach over a similar ensemble
framework with standard classifiers.
",0,0,0,1,0,0
1191,Parameter Sharing Deep Deterministic Policy Gradient for Cooperative Multi-agent Reinforcement Learning,"  Deep reinforcement learning for multi-agent cooperation and competition has
been a hot topic recently. This paper focuses on cooperative multi-agent
problem based on actor-critic methods under local observations settings. Multi
agent deep deterministic policy gradient obtained state of art results for some
multi-agent games, whereas, it cannot scale well with growing amount of agents.
In order to boost scalability, we propose a parameter sharing deterministic
policy gradient method with three variants based on neural networks, including
actor-critic sharing, actor sharing and actor sharing with partially shared
critic. Benchmarks from rllab show that the proposed method has advantages in
learning speed and memory efficiency, well scales with growing amount of
agents, and moreover, it can make full use of reward sharing and
exchangeability if possible.
",1,0,0,0,0,0
1192,Repair Strategies for Storage on Mobile Clouds,"  We study the data reliability problem for a community of devices forming a
mobile cloud storage system. We consider the application of regenerating codes
for file maintenance within a geographically-limited area. Such codes require
lower bandwidth to regenerate lost data fragments compared to file replication
or reconstruction. We investigate threshold-based repair strategies where data
repair is initiated after a threshold number of data fragments have been lost
due to node mobility. We show that at a low departure-to-repair rate regime, a
lazy repair strategy in which repairs are initiated after several nodes have
left the system outperforms eager repair in which repairs are initiated after a
single departure. This optimality is reversed when nodes are highly mobile. We
further compare distributed and centralized repair strategies and derive the
optimal repair threshold for minimizing the average repair cost per unit of
time, as a function of underlying code parameters. In addition, we examine
cooperative repair strategies and show performance improvements compared to
non-cooperative codes. We investigate several models for the time needed for
node repair including a simple fixed time model that allows for the computation
of closed-form expressions and a more realistic model that takes into account
the number of repaired nodes. We derive the conditions under which the former
model approximates the latter. Finally, an extended model where additional
failures are allowed during the repair process is investigated. Overall, our
results establish the joint effect of code design and repair algorithms on the
maintenance cost of distributed storage systems.
",1,0,0,0,0,0
1193,Mean-variance portfolio selection under partial information with drift uncertainty,"  This paper studies a mean-variance portfolio selection problem under partial
information with drift uncertainty. It is proved that all the contingent claims
in this model are attainable in the sense of Xiong and Zhou. Further, we
propose a numerical scheme to approximate the optimal portfolio. Malliavin
calculus and the strong law of large numbers play important roles in this
scheme.
",0,0,0,0,0,1
1194,Learning from MOM's principles: Le Cam's approach,"  We obtain estimation error rates for estimators obtained by aggregation of
regularized median-of-means tests, following a construction of Le Cam. The
results hold with exponentially large probability -- as in the gaussian
framework with independent noise- under only weak moments assumptions on data
and without assuming independence between noise and design. Any norm may be
used for regularization. When it has some sparsity inducing power we recover
sparse rates of convergence.
The procedure is robust since a large part of data may be corrupted, these
outliers have nothing to do with the oracle we want to reconstruct. Our general
risk bound is of order \begin{equation*} \max\left(\mbox{minimax rate in the
i.i.d. setup}, \frac{\text{number of outliers}}{\text{number of
observations}}\right) \enspace. \end{equation*}In particular, the number of
outliers may be as large as (number of data) $\times$(minimax rate) without
affecting this rate. The other data do not have to be identically distributed
but should only have equivalent $L^1$ and $L^2$ moments.
For example, the minimax rate $s \log(ed/s)/N$ of recovery of a $s$-sparse
vector in $\mathbb{R}^d$ is achieved with exponentially large probability by a
median-of-means version of the LASSO when the noise has $q_0$ moments for some
$q_0>2$, the entries of the design matrix should have $C_0\log(ed)$ moments and
the dataset can be corrupted up to $C_1 s \log(ed/s)$ outliers.
",0,0,1,1,0,0
1195,On a Neumann-type series for modified Bessel functions of the first kind,"  In this paper, we are interested in a Neumann-type series for modified Bessel
functions of the first kind which arises in the study of Dunkl operators
associated with dihedral groups and as an instance of the Laguerre semigroup
constructed by Ben Said-Kobayashi-Orsted. We first revisit the particular case
corresponding to the group of square-preserving symmetries for which we give
two new and different proofs other than the existing ones. The first proof uses
the expansion of powers in a Neumann series of Bessel functions while the
second one is based on a quadratic transformation for the Gauss hypergeometric
function and opens the way to derive further expressions when the orders of the
underlying dihedral groups are powers of two. More generally, we give another
proof of De Bie \& al formula expressing this series as a $\Phi_2$-Horn
confluent hypergeometric function. In the course of proving, we shed the light
on the occurrence of multiple angles in their formula through elementary
symmetric functions, and get a new representation of Gegenbauer polynomials.
",0,0,1,0,0,0
1196,Generalized Log-sine integrals and Bell polynomials,"  In this paper, we investigate the integral of $x^n\log^m(\sin(x))$ for
natural numbers $m$ and $n$. In doing so, we recover some well-known results
and remark on some relations to the log-sine integral
$\operatorname{Ls}_{n+m+1}^{(n)}(\theta)$. Later, we use properties of Bell
polynomials to find a closed expression for the derivative of the central
binomial and shifted central binomial coefficients in terms of polygamma
functions and harmonic numbers.
",0,0,1,0,0,0
1197,A Modern Search for Wolf-Rayet Stars in the Magellanic Clouds. III. A Third Year of Discoveries,"  For the past three years we have been conducting a survey for WR stars in the
Large and Small Magellanic Clouds (LMC, SMC). Our previous work has resulted in
the discovery of a new type of WR star in the LMC, which we are calling WN3/O3.
These stars have the emission-line properties of a WN3 star (strong N V but no
N IV), plus the absorption-line properties of an O3 star (Balmer hydrogen plus
Pickering He II but no He I). Yet these stars are 15x fainter than an O3 V star
would be by itself, ruling out these being WN3+O3 binaries. Here we report the
discovery of two more members of this class, bringing the total number of these
objects to 10, 6.5% of the LMC's total WR population. The optical spectra of
nine of these WN3/O3s are virtually indistinguishable from each other, but one
of the newly found stars is significantly different, showing a lower excitation
emission and absorption spectrum (WN4/O4-ish). In addition, we have newly
classified three unusual Of-type stars, including one with a strong C III 4650
line, and two rapidly rotating ""Oef"" stars. We also ""rediscovered"" a low mass
x-ray binary, RX J0513.9-6951, and demonstrate its spectral variability.
Finally, we discuss the spectra of ten low priority WR candidates that turned
out not to have He II emission. These include both a Be star and a B[e] star.
",0,1,0,0,0,0
1198,Mathematical modeling of Zika disease in pregnant women and newborns with microcephaly in Brazil,"  We propose a new mathematical model for the spread of Zika virus. Special
attention is paid to the transmission of microcephaly. Numerical simulations
show the accuracy of the model with respect to the Zika outbreak occurred in
Brazil.
",0,0,1,0,0,0
1199,A Noninformative Prior on a Space of Distribution Functions,"  In a given problem, the Bayesian statistical paradigm requires the
specification of a prior distribution that quantifies relevant information
about the unknowns of main interest external to the data. In cases where little
such information is available, the problem under study may possess an
invariance under a transformation group that encodes a lack of information,
leading to a unique prior---this idea was explored at length by E.T. Jaynes.
Previous successful examples have included location-scale invariance under
linear transformation, multiplicative invariance of the rate at which events in
a counting process are observed, and the derivation of the Haldane prior for a
Bernoulli success probability. In this paper we show that this method can be
extended, by generalizing Jaynes, in two ways: (1) to yield families of
approximately invariant priors, and (2) to the infinite-dimensional setting,
yielding families of priors on spaces of distribution functions. Our results
can be used to describe conditions under which a particular Dirichlet Process
posterior arises from an optimal Bayesian analysis, in the sense that
invariances in the prior and likelihood lead to one and only one posterior
distribution.
",0,0,1,1,0,0
1200,Towards a realistic NNLIF model: Analysis and numerical solver for excitatory-inhibitory networks with delay and refractory periods,"  The Network of Noisy Leaky Integrate and Fire (NNLIF) model describes the
behavior of a neural network at mesoscopic level. It is one of the simplest
self-contained mean-field models considered for that purpose. Even so, to study
the mathematical properties of the model some simplifications were necessary
C?­ceres-Carrillo-Perthame(2011), C?­ceres-Perthame(2014),
C?­ceres-Schneider(2017), which disregard crucial phenomena. In this work we
deal with the general NNLIF model without simplifications. It involves a
network with two populations (excitatory and inhibitory), with transmission
delays between the neurons and where the neurons remain in a refractory state
for a certain time. We have studied the number of steady states in terms of the
model parameters, the long time behaviour via the entropy method and
Poincar??'s inequality, blow-up phenomena, and the importance of transmission
delays between excitatory neurons to prevent blow-up and to give rise to
synchronous solutions. Besides analytical results, we have presented a
numerical resolutor for this model, based on high order flux-splitting WENO
schemes and an explicit third order TVD Runge-Kutta method, in order to
describe the wide range of phenomena exhibited by the network: blow-up,
asynchronous/synchronous solutions and instability/stability of the steady
states; the solver also allows us to observe the time evolution of the firing
rates, refractory states and the probability distributions of the excitatory
and inhibitory populations.
",0,0,1,0,0,0
1201,An upper bound on the distinguishing index of graphs with minimum degree at least two,"  The distinguishing index of a simple graph $G$, denoted by $D'(G)$, is the
least number of labels in an edge labeling of $G$ not preserved by any
non-trivial automorphism. It was conjectured by Pil?niak (2015) that for any
2-connected graph $D'(G) \leq \lceil \sqrt{\Delta (G)}\rceil +1$. We prove a
more general result for the distinguishing index of graphs with minimum degree
at least two from which the conjecture follows. Also we present graphs $G$ for
which $D'(G)\leq \lceil \sqrt{\Delta }\rceil$.
",0,0,1,0,0,0
1202,Adversarial Pseudo Healthy Synthesis Needs Pathology Factorization,"  Pseudo healthy synthesis, i.e. the creation of a subject-specific `healthy'
image from a pathological one, could be helpful in tasks such as anomaly
detection, understanding changes induced by pathology and disease or even as
data augmentation. We treat this task as a factor decomposition problem: we aim
to separate what appears to be healthy and where disease is (as a map). The two
factors are then recombined (by a network) to reconstruct the input disease
image. We train our models in an adversarial way using either paired or
unpaired settings, where we pair disease images and maps (as segmentation
masks) when available. We quantitatively evaluate the quality of pseudo healthy
images. We show in a series of experiments, performed in ISLES and BraTS
datasets, that our method is better than conditional GAN and CycleGAN,
highlighting challenges in using adversarial methods in the image translation
task of pseudo healthy image generation.
",1,0,0,1,0,0
1203,"Closure operators, frames, and neatest representations","  Given a poset $P$ and a standard closure operator $\Gamma:\wp(P)\to\wp(P)$ we
give a necessary and sufficient condition for the lattice of $\Gamma$-closed
sets of $\wp(P)$ to be a frame in terms of the recursive construction of the
$\Gamma$-closure of sets. We use this condition to show that given a set
$\mathcal{U}$ of distinguished joins from $P$, the lattice of
$\mathcal{U}$-ideals of $P$ fails to be a frame if and only if it fails to be
$\sigma$-distributive, with $\sigma$ depending on the cardinalities of sets in
$\mathcal{U}$. From this we deduce that if a poset has the property that
whenever $a\wedge(b\vee c)$ is defined for $a,b,c\in P$ it is necessarily equal
to $(a\wedge b)\vee (a\wedge c)$, then it has an $(\omega,3)$-representation.
This answers a question from the literature.
",0,0,1,0,0,0
1204,The structure of rationally factorized Lax type flows and their analytical integrability,"  The work is devoted to constructing a wide class of differential-functional
dynamical systems, whose rich algebraic structure makes their integrability
analytically effective. In particular, there is analyzed in detail the operator
Lax type equations for factorized seed elements, there is proved an important
theorem about their operator factorization and the related analytical solution
scheme to the corresponding nonlinear differential-functional dynamical
systems.
",0,1,0,0,0,0
1205,Learning Policy Representations in Multiagent Systems,"  Modeling agent behavior is central to understanding the emergence of complex
phenomena in multiagent systems. Prior work in agent modeling has largely been
task-specific and driven by hand-engineering domain-specific prior knowledge.
We propose a general learning framework for modeling agent behavior in any
multiagent system using only a handful of interaction data. Our framework casts
agent modeling as a representation learning problem. Consequently, we construct
a novel objective inspired by imitation learning and agent identification and
design an algorithm for unsupervised learning of representations of agent
policies. We demonstrate empirically the utility of the proposed framework in
(i) a challenging high-dimensional competitive environment for continuous
control and (ii) a cooperative environment for communication, on supervised
predictive tasks, unsupervised clustering, and policy optimization using deep
reinforcement learning.
",0,0,0,1,0,0
1206,Jamming-Resistant Receivers for the Massive MIMO Uplink,"  We design a jamming-resistant receiver scheme to enhance the robustness of a
massive MIMO uplink system against jamming. We assume that a jammer attacks the
system both in the pilot and data transmission phases. The key feature of the
proposed scheme is that, in the pilot phase, we estimate not only the
legitimate channel, but also the jamming channel by exploiting a purposely
unused pilot sequence. The jamming channel estimate is used to constructed
linear receive filters that reject the impact of the jamming signal. The
performance of the proposed scheme is analytically evaluated using asymptotic
properties of massive MIMO. The optimal regularized zero-forcing receiver and
the optimal power allocation are also studied. Numerical results are provided
to verify our analysis and show that the proposed scheme greatly improves the
achievable rates, as compared to conventional receivers. Interestingly, the
proposed scheme works particularly well under strong jamming attacks, since the
improved estimate of the jamming channel outweighs the extra jamming power.
",1,0,0,0,0,0
1207,Multiplex core-periphery organization of the human connectome,"  The behavior of many complex systems is determined by a core of densely
interconnected units. While many methods are available to identify the core of
a network when connections between nodes are all of the same type, a principled
approach to define the core when multiple types of connectivity are allowed is
still lacking. Here we introduce a general framework to define and extract the
core-periphery structure of multi-layer networks by explicitly taking into
account the connectivity of the nodes at each layer. We show how our method
works on synthetic networks with different size, density, and overlap between
the cores at the different layers. We then apply the method to multiplex brain
networks whose layers encode information both on the anatomical and the
functional connectivity among regions of the human cortex. Results confirm the
presence of the main known hubs, but also suggest the existence of novel brain
core regions that have been discarded by previous analysis which focused
exclusively on the structural layer. Our work is a step forward in the
identification of the core of the human connectome, and contributes to shed
light to a fundamental question in modern neuroscience.
",1,0,0,0,1,0
1208,Towards Learned Clauses Database Reduction Strategies Based on Dominance Relationship,"  Clause Learning is one of the most important components of a conflict driven
clause learning (CDCL) SAT solver that is effective on industrial instances.
Since the number of learned clauses is proved to be exponential in the worse
case, it is necessary to identify the most relevant clauses to maintain and
delete the irrelevant ones. As reported in the literature, several learned
clauses deletion strategies have been proposed. However the diversity in both
the number of clauses to be removed at each step of reduction and the results
obtained with each strategy creates confusion to determine which criterion is
better. Thus, the problem to select which learned clauses are to be removed
during the search step remains very challenging. In this paper, we propose a
novel approach to identify the most relevant learned clauses without favoring
or excluding any of the proposed measures, but by adopting the notion of
dominance relationship among those measures. Our approach bypasses the problem
of the diversity of results and reaches a compromise between the assessments of
these measures. Furthermore, the proposed approach also avoids another
non-trivial problem which is the amount of clauses to be deleted at each
reduction of the learned clause database.
",1,0,0,0,0,0
1209,Integrable structure of products of finite complex Ginibre random matrices,"  We consider the squared singular values of the product of $M$ standard
complex Gaussian matrices. Since the squared singular values form a
determinantal point process with a particular Meijer G-function kernel, the gap
probabilities are given by a Fredholm determinant based on this kernel. It was
shown by Strahov \cite{St14} that a hard edge scaling limit of the gap
probabilities is described by Hamiltonian differential equations which can be
formulated as an isomonodromic deformation system similar to the theory of the
Kyoto school. We generalize this result to the case of finite matrices by first
finding a representation of the finite kernel in integrable form. As a result
we obtain the Hamiltonian structure for a finite size matrices and formulate it
in terms of a $(M+1) \times (M+1)$ matrix Schlesinger system. The case $M=1$
reproduces the Tracy and Widom theory which results in the Painlev?? V
equation for the $(0,s)$ gap probability. Some integrals of motion for $M = 2$
are identified, and a coupled system of differential equations in two unknowns
is presented which uniquely determines the corresponding $(0,s)$ gap
probability.
",0,1,1,0,0,0
1210,Global well-posedness of the 3D primitive equations with horizontal viscosity and vertical diffusivity,"  In this paper, we consider the 3D primitive equations of oceanic and
atmospheric dynamics with only horizontal eddy viscosities in the horizontal
momentum equations and only vertical diffusivity in the temperature equation.
Global well-posedness of strong solutions is established for any initial data
such that the initial horizontal velocity $v_0\in H^2(\Omega)$ and the initial
temperature $T_0\in H^1(\Omega)\cap L^\infty(\Omega)$ with $\nabla_HT_0\in
L^q(\Omega)$, for some $q\in(2,\infty)$. Moreover, the strong solutions enjoy
correspondingly more regularities if the initial temperature belongs to
$H^2(\Omega)$. The main difficulties are the absence of the vertical viscosity
and the lack of the horizontal diffusivity, which, interact with each other,
thus causing the ""\,mismatching\,"" of regularities between the horizontal
momentum and temperature equations. To handle this ""mismatching"" of
regularities, we introduce several auxiliary functions, i.e., $\eta, \theta,
\varphi,$ and $\psi$ in the paper, which are the horizontal curls or some
appropriate combinations of the temperature with the horizontal divergences of
the horizontal velocity $v$ or its vertical derivative $\partial_zv$. To
overcome the difficulties caused by the absence of the horizontal diffusivity,
which leads to the requirement of some $L^1_t(W^{1,\infty}_\textbf{x})$-type a
priori estimates on $v$, we decompose the velocity into the
""temperature-independent"" and temperature-dependent parts and deal with them in
different ways, by using the logarithmic Sobolev inequalities of the
Br??zis-Gallouet-Wainger and Beale-Kato-Majda types, respectively.
Specifically, a logarithmic Sobolev inequality of the limiting type, introduced
in our previous work [12], is used, and a new logarithmic type Gronwall
inequality is exploited.
",0,1,1,0,0,0
1211,Superintegrable systems on 3-dimensional curved spaces: Eisenhart formalism and separability,"  The Eisenhart geometric formalism, which transforms an Euclidean natural
Hamiltonian $H=T+V$ into a geodesic Hamiltonian ${\cal T}$ with one additional
degree of freedom, is applied to the four families of quadratically
superintegrable systems with multiple separability in the Euclidean plane.
Firstly, the separability and superintegrability of such four geodesic
Hamiltonians ${\cal T}_r$ ($r=a,b,c,d$) in a three-dimensional curved space are
studied and then these four systems are modified with the addition of a
potential ${\cal U}_r$ leading to ${\cal H}_r={\cal T}_r +{\cal U}_r$.
Secondly, we study the superintegrability of the four Hamiltonians
$\widetilde{\cal H}_r= {\cal H}_r/ \mu_r$, where $\mu_r$ is a certain
position-dependent mass, that enjoys the same separability as the original
system ${\cal H}_r$. All the Hamiltonians here studied describe superintegrable
systems on non-Euclidean three-dimensional manifolds with a broken spherically
symmetry.
",0,1,1,0,0,0
1212,An Incentive-Based Online Optimization Framework for Distribution Grids,"  This paper formulates a time-varying social-welfare maximization problem for
distribution grids with distributed energy resources (DERs) and develops online
distributed algorithms to identify (and track) its solutions. In the considered
setting, network operator and DER-owners pursue given operational and economic
objectives, while concurrently ensuring that voltages are within prescribed
limits. The proposed algorithm affords an online implementation to enable
tracking of the solutions in the presence of time-varying operational
conditions and changing optimization objectives. It involves a strategy where
the network operator collects voltage measurements throughout the feeder to
build incentive signals for the DER-owners in real time; DERs then adjust the
generated/consumed powers in order to avoid the violation of the voltage
constraints while maximizing given objectives. The stability of the proposed
schemes is analytically established and numerically corroborated.
",1,0,1,0,0,0
1213,Ricci solitons on Ricci pseudosymmetric $(LCS)_n$-manifolds,"  The object of the present paper is to study some types of Ricci
pseudosymmetric $(LCS)_n$-manifolds whose metric is Ricci soliton. We found the
conditions when Ricci soliton on concircular Ricci pseudosymmetric, projective
Ricci pseudosymmetric, $W_{3}$-Ricci pseudosymmetric, conharmonic Ricci
pseudosymmetric, conformal Ricci pseudosymmetric $(LCS)_n$-manifolds to be
shrinking, steady and expanding. We also construct an example of concircular
Ricci pseudosymmetric $(LCS)_3$-manifold whose metric is Ricci soliton.
",0,0,1,0,0,0
1214,Optimal Gossip Algorithms for Exact and Approximate Quantile Computations,"  This paper gives drastically faster gossip algorithms to compute exact and
approximate quantiles.
Gossip algorithms, which allow each node to contact a uniformly random other
node in each round, have been intensely studied and been adopted in many
applications due to their fast convergence and their robustness to failures.
Kempe et al. [FOCS'03] gave gossip algorithms to compute important aggregate
statistics if every node is given a value. In particular, they gave a beautiful
$O(\log n + \log \frac{1}{\epsilon})$ round algorithm to $\epsilon$-approximate
the sum of all values and an $O(\log^2 n)$ round algorithm to compute the exact
$\phi$-quantile, i.e., the the $\lceil \phi n \rceil$ smallest value.
We give an quadratically faster and in fact optimal gossip algorithm for the
exact $\phi$-quantile problem which runs in $O(\log n)$ rounds. We furthermore
show that one can achieve an exponential speedup if one allows for an
$\epsilon$-approximation. We give an $O(\log \log n + \log \frac{1}{\epsilon})$
round gossip algorithm which computes a value of rank between $\phi n$ and
$(\phi+\epsilon)n$ at every node.% for any $0 \leq \phi \leq 1$ and $0 <
\epsilon < 1$. Our algorithms are extremely simple and very robust - they can
be operated with the same running times even if every transmission fails with
a, potentially different, constant probability. We also give a matching
$\Omega(\log \log n + \log \frac{1}{\epsilon})$ lower bound which shows that
our algorithm is optimal for all values of $\epsilon$.
",1,0,0,0,0,0
1215,Influence of Heat Treatment on the Corrosion Behavior of Purified Magnesium and AZ31 Alloy,"  Magnesium and its alloys are ideal for biodegradable implants due to their
biocompatibility and their low-stress shielding. However, they can corrode too
rapidly in the biological environment. The objective of this research was to
develop heat treatments to slow the corrosion of high purified magnesium and
AZ31 alloy in simulated body fluid at 37?øC. Heat treatments were performed
at different temperatures and times. Hydrogen evolution, weight loss, PDP, and
EIS methods were used to measure the corrosion rates. Results show that heat
treating can increase the corrosion resistance of HP-Mg by 2x and AZ31 by 10x.
",0,1,0,0,0,0
1216,Towards a scientific blockchain framework for reproducible data analysis,"  Publishing reproducible analyses is a long-standing and widespread challenge
for the scientific community, funding bodies and publishers. Although a
definitive solution is still elusive, the problem is recognized to affect all
disciplines and lead to a critical system inefficiency. Here, we propose a
blockchain-based approach to enhance scientific reproducibility, with a focus
on life science studies and precision medicine. While the interest of encoding
permanently into an immutable ledger all the study key information-including
endpoints, data and metadata, protocols, analytical methods and all
findings-has been already highlighted, here we apply the blockchain approach to
solve the issue of rewarding time and expertise of scientists that commit to
verify reproducibility. Our mechanism builds a trustless ecosystem of
researchers, funding bodies and publishers cooperating to guarantee digital and
permanent access to information and reproducible results. As a natural
byproduct, a procedure to quantify scientists' and institutions' reputation for
ranking purposes is obtained.
",1,0,0,0,0,0
1217,Time Series Anomaly Detection; Detection of anomalous drops with limited features and sparse examples in noisy highly periodic data,"  Google uses continuous streams of data from industry partners in order to
deliver accurate results to users. Unexpected drops in traffic can be an
indication of an underlying issue and may be an early warning that remedial
action may be necessary. Detecting such drops is non-trivial because streams
are variable and noisy, with roughly regular spikes (in many different shapes)
in traffic data. We investigated the question of whether or not we can predict
anomalies in these data streams. Our goal is to utilize Machine Learning and
statistical approaches to classify anomalous drops in periodic, but noisy,
traffic patterns. Since we do not have a large body of labeled examples to
directly apply supervised learning for anomaly classification, we approached
the problem in two parts. First we used TensorFlow to train our various models
including DNNs, RNNs, and LSTMs to perform regression and predict the expected
value in the time series. Secondly we created anomaly detection rules that
compared the actual values to predicted values. Since the problem requires
finding sustained anomalies, rather than just short delays or momentary
inactivity in the data, our two detection methods focused on continuous
sections of activity rather than just single points. We tried multiple
combinations of our models and rules and found that using the intersection of
our two anomaly detection methods proved to be an effective method of detecting
anomalies on almost all of our models. In the process we also found that not
all data fell within our experimental assumptions, as one data stream had no
periodicity, and therefore no time based model could predict it.
",1,0,0,1,0,0
1218,A parallel approach to bi-objective integer programming,"  To obtain a better understanding of the trade-offs between various
objectives, Bi-Objective Integer Programming (BOIP) algorithms calculate the
set of all non-dominated vectors and present these as the solution to a BOIP
problem. Historically, these algorithms have been compared in terms of the
number of single-objective IPs solved and total CPU time taken to produce the
solution to a problem. This is equitable, as researchers can often have access
to widely differing amounts of computing power. However, the real world has
recently seen a large uptake of multi-core processors in computers, laptops,
tablets and even mobile phones. With this in mind, we look at how to best
utilise parallel processing to improve the elapsed time of optimisation
algorithms. We present two methods of parallelising the recursive algorithm
presented by Ozlen, Burton and MacRae. Both new methods utilise two threads and
improve running times. One of the new methods, the Meeting algorithm, halves
running time to achieve near-perfect parallelisation. The results are compared
with the efficiency of parallelisation within the commercial IP solver IBM ILOG
CPLEX, and the new methods are both shown to perform better.
",1,0,1,0,0,0
1219,The adaptive zero-error capacity for a class of channels with noisy feedback,"  The adaptive zero-error capacity of discrete memoryless channels (DMC) with
noiseless feedback has been shown to be positive whenever there exists at least
one channel output ""disprover"", i.e. a channel output that cannot be reached
from at least one of the inputs. Furthermore, whenever there exists a
disprover, the adaptive zero-error capacity attains the Shannon (small-error)
capacity. Here, we study the zero-error capacity of a DMC when the channel
feedback is noisy rather than perfect. We show that the adaptive zero-error
capacity with noisy feedback is lower bounded by the forward channel's
zero-undetected error capacity, and show that under certain conditions this is
tight.
",1,0,0,0,0,0
1220,Comparison of Self-Aware and Organic Computing Systems,"  With increasing complexity and heterogeneity of computing devices, it has
become crucial for system to be autonomous, adaptive to dynamic environment,
robust, flexible, and having so called self-*properties. These autonomous
systems are called organic computing(OC) systems. OC system was proposed as a
solution to tackle complex systems. Design time decisions have been shifted to
run time in highly complex and interconnected systems as it is very hard to
consider all scenarios and their appropriate actions in advance. Consequently,
Self-awareness becomes crucial for these adaptive autonomous systems. To cope
with evolving environment and changing user needs, system need to have
knowledge about itself and its surroundings. Literature review shows that for
autonomous and intelligent systems, researchers are concerned about knowledge
acquisition, representation and learning which is necessary for a system to
adapt. This paper is written to compare self-awareness and organic computing by
discussing their definitions, properties and architecture.
",1,0,0,0,0,0
1221,First Order Methods beyond Convexity and Lipschitz Gradient Continuity with Applications to Quadratic Inverse Problems,"  We focus on nonconvex and nonsmooth minimization problems with a composite
objective, where the differentiable part of the objective is freed from the
usual and restrictive global Lipschitz gradient continuity assumption. This
longstanding smoothness restriction is pervasive in first order methods (FOM),
and was recently circumvent for convex composite optimization by Bauschke,
Bolte and Teboulle, through a simple and elegant framework which captures, all
at once, the geometry of the function and of the feasible set. Building on this
work, we tackle genuine nonconvex problems. We first complement and extend
their approach to derive a full extended descent lemma by introducing the
notion of smooth adaptable functions. We then consider a Bregman-based proximal
gradient methods for the nonconvex composite model with smooth adaptable
functions, which is proven to globally converge to a critical point under
natural assumptions on the problem's data. To illustrate the power and
potential of our general framework and results, we consider a broad class of
quadratic inverse problems with sparsity constraints which arises in many
fundamental applications, and we apply our approach to derive new globally
convergent schemes for this class.
",1,0,1,0,0,0
1222,An Army of Me: Sockpuppets in Online Discussion Communities,"  In online discussion communities, users can interact and share information
and opinions on a wide variety of topics. However, some users may create
multiple identities, or sockpuppets, and engage in undesired behavior by
deceiving others or manipulating discussions. In this work, we study
sockpuppetry across nine discussion communities, and show that sockpuppets
differ from ordinary users in terms of their posting behavior, linguistic
traits, as well as social network structure. Sockpuppets tend to start fewer
discussions, write shorter posts, use more personal pronouns such as ""I"", and
have more clustered ego-networks. Further, pairs of sockpuppets controlled by
the same individual are more likely to interact on the same discussion at the
same time than pairs of ordinary users. Our analysis suggests a taxonomy of
deceptive behavior in discussion communities. Pairs of sockpuppets can vary in
their deceptiveness, i.e., whether they pretend to be different users, or their
supportiveness, i.e., if they support arguments of other sockpuppets controlled
by the same user. We apply these findings to a series of prediction tasks,
notably, to identify whether a pair of accounts belongs to the same underlying
user or not. Altogether, this work presents a data-driven view of deception in
online discussion communities and paves the way towards the automatic detection
of sockpuppets.
",1,1,0,1,0,0
1223,Robust Bayesian Optimization with Student-t Likelihood,"  Bayesian optimization has recently attracted the attention of the automatic
machine learning community for its excellent results in hyperparameter tuning.
BO is characterized by the sample efficiency with which it can optimize
expensive black-box functions. The efficiency is achieved in a similar fashion
to the learning to learn methods: surrogate models (typically in the form of
Gaussian processes) learn the target function and perform intelligent sampling.
This surrogate model can be applied even in the presence of noise; however, as
with most regression methods, it is very sensitive to outlier data. This can
result in erroneous predictions and, in the case of BO, biased and inefficient
exploration. In this work, we present a GP model that is robust to outliers
which uses a Student-t likelihood to segregate outliers and robustly conduct
Bayesian optimization. We present numerical results evaluating the proposed
method in both artificial functions and real problems.
",1,0,0,1,0,0
1224,Vehicle Localization and Control on Roads with Prior Grade Map,"  We propose a map-aided vehicle localization method for GPS-denied
environments. This approach exploits prior knowledge of the road grade map and
vehicle on-board sensor measurements to accurately estimate the longitudinal
position of the vehicle. Real-time localization is crucial to systems that
utilize position-dependent information for planning and control. We validate
the effectiveness of the localization method on a hierarchical control system.
The higher level planner optimizes the vehicle velocity to minimize the energy
consumption for a given route by employing traffic condition and road grade
data. The lower level is a cruise control system that tracks the
position-dependent optimal reference velocity. Performance of the proposed
localization algorithm is evaluated using both simulations and experiments.
",1,0,0,0,0,0
1225,Estimating Tactile Data for Adaptive Grasping of Novel Objects,"  We present an adaptive grasping method that finds stable grasps on novel
objects. The main contributions of this paper is in the computation of the
probability of success of grasps in the vicinity of an already applied grasp.
Our method performs grasp adaptions by simulating tactile data for grasps in
the vicinity of the current grasp. The simulated data is used to evaluate
hypothetical grasps and thereby guide us toward better grasps. We demonstrate
the applicability of our method by constructing a system that can plan, apply
and adapt grasps on novel objects. Experiments are conducted on objects from
the YCB object set and the success rate of our method is 88%. Our experiments
show that the application of our grasp adaption method improves grasp stability
significantly.
",1,0,0,0,0,0
1226,Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints,"  Algorithm-dependent generalization error bounds are central to statistical
learning theory. A learning algorithm may use a large hypothesis space, but the
limited number of iterations controls its model capacity and generalization
error. The impacts of stochastic gradient methods on generalization error for
non-convex learning problems not only have important theoretical consequences,
but are also critical to generalization errors of deep learning.
In this paper, we study the generalization errors of Stochastic Gradient
Langevin Dynamics (SGLD) with non-convex objectives. Two theories are proposed
with non-asymptotic discrete-time analysis, using Stability and PAC-Bayesian
results respectively. The stability-based theory obtains a bound of
$O\left(\frac{1}{n}L\sqrt{\beta T_k}\right)$, where $L$ is uniform Lipschitz
parameter, $\beta$ is inverse temperature, and $T_k$ is aggregated step sizes.
For PAC-Bayesian theory, though the bound has a slower $O(1/\sqrt{n})$ rate,
the contribution of each step is shown with an exponentially decaying factor by
imposing $\ell^2$ regularization, and the uniform Lipschitz constant is also
replaced by actual norms of gradients along trajectory. Our bounds have no
implicit dependence on dimensions, norms or other capacity measures of
parameter, which elegantly characterizes the phenomenon of ""Fast Training
Guarantees Generalization"" in non-convex settings. This is the first
algorithm-dependent result with reasonable dependence on aggregated step sizes
for non-convex learning, and has important implications to statistical learning
aspects of stochastic gradient methods in complicated models such as deep
learning.
",1,0,1,1,0,0
1227,Near-UV OH Prompt Emission in the Innermost Coma of 103P/Hartley 2,"  The Deep Impact spacecraft fly-by of comet 103P/Hartley 2 occurred on 2010
November 4, one week after perihelion with a closest approach (CA) distance of
about 700 km. We used narrowband images obtained by the Medium Resolution
Imager (MRI) onboard the spacecraft to study the gas and dust in the innermost
coma. We derived an overall dust reddening of 15\%/100 nm between 345 and 749
nm and identified a blue enhancement in the dust coma in the sunward direction
within 5 km from the nucleus, which we interpret as a localized enrichment in
water ice. OH column density maps show an anti-sunward enhancement throughout
the encounter except for the highest resolution images, acquired at CA, where a
radial jet becomes visible in the innermost coma, extending up to 12 km from
the nucleus. The OH distribution in the inner coma is very different from that
expected for a fragment species. Instead, it correlates well with the water
vapor map derived by the HRI-IR instrument onboard Deep Impact
\citep{AHearn2011}. Radial profiles of the OH column density and derived water
production rates show an excess of OH emission during CA that cannot be
explained with pure fluorescence. We attribute this excess to a prompt emission
process where photodissociation of H$_2$O directly produces excited
OH*($A^2\it{\Sigma}^+$) radicals. Our observations provide the first direct
imaging of Near-UV prompt emission of OH. We therefore suggest the use of a
dedicated filter centered at 318.8 nm to directly trace the water in the coma
of comets.
",0,1,0,0,0,0
1228,Effective gravity and effective quantum equations in a system inspired by walking droplets experiments,"  In this paper we suggest a macroscopic toy system in which a potential-like
energy is generated by a non-uniform pulsation of the medium (i.e. pulsation of
transverse standing oscillations that the elastic medium of the system tends to
support at each point). This system is inspired by walking droplets experiments
with submerged barriers. We first show that a Poincar??-Lorentz covariant
formalization of the system causes inconsistency and contradiction. The
contradiction is solved by using a general covariant formulation and by
assuming a relation between the metric associated with the elastic medium and
the pulsation of the medium. (Calculations are performed in a Newtonian-like
metric, constant in time). We find ($i$) an effective Schr??dinger equation
with external potential, ($ii$) an effective de Broglie-Bohm guidance formula
and ($iii$) an energy of the `particle' which has a direct counterpart in
general relativity as well as in quantum mechanics. We analyze the wave and the
`particle' in an effective free fall and with a harmonic potential. This
potential-like energy is an effective gravitational potential, rooted in the
pulsation of the medium at each point. The latter, also conceivable as a
natural clock, makes easy to understand why proper time varies from place to
place.
",0,1,0,0,0,0
1229,A gradient estimate for nonlocal minimal graphs,"  We consider the class of measurable functions defined in all of
$\mathbb{R}^n$ that give rise to a nonlocal minimal graph over a ball of
$\mathbb{R}^n$. We establish that the gradient of any such function is bounded
in the interior of the ball by a power of its oscillation. This estimate,
together with previously known results, leads to the $C^\infty$ regularity of
the function in the ball. While the smoothness of nonlocal minimal graphs was
known for $n = 1, 2$ (but without a quantitative bound), in higher dimensions
only their continuity had been established.
To prove the gradient bound, we show that the normal to a nonlocal minimal
graph is a supersolution of a truncated fractional Jacobi operator, for which
we prove a weak Harnack inequality. To this end, we establish a new universal
fractional Sobolev inequality on nonlocal minimal surfaces.
Our estimate provides an extension to the fractional setting of the
celebrated gradient bounds of Finn and of Bombieri, De Giorgi & Miranda for
solutions of the classical mean curvature equation.
",0,0,1,0,0,0
1230,The GAPS Programme with HARPS-N@TNG XIV. Investigating giant planet migration history via improved eccentricity and mass determination for 231 transiting planets,"  We carried out a Bayesian homogeneous determination of the orbital parameters
of 231 transiting giant planets (TGPs) that are alone or have distant
companions; we employed DE-MCMC methods to analyse radial-velocity (RV) data
from the literature and 782 new high-accuracy RVs obtained with the HARPS-N
spectrograph for 45 systems over 3 years. Our work yields the largest sample of
systems with a transiting giant exoplanet and coherently determined orbital,
planetary, and stellar parameters. We found that the orbital parameters of TGPs
in non-compact planetary systems are clearly shaped by tides raised by their
host stars. Indeed, the most eccentric planets have relatively large orbital
separations and/or high mass ratios, as expected from the equilibrium tide
theory. This feature would be the outcome of high-eccentricity migration (HEM).
The distribution of $\alpha=a/a_R$, where $a$ and $a_R$ are the semi-major axis
and the Roche limit, for well-determined circular orbits peaks at 2.5; this
also agrees with expectations from the HEM. The few planets of our sample with
circular orbits and $\alpha >5$ values may have migrated through disc-planet
interactions instead of HEM. By comparing circularisation times with stellar
ages, we found that hot Jupiters with $a < 0.05$ au have modified tidal quality
factors $10^{5} < Q'_p < 10^{9}$, and that stellar $Q'_s > 10^{6}-10^{7}$ are
required to explain the presence of eccentric planets at the same orbital
distance. As a by-product of our analysis, we detected a non-zero eccentricity
for HAT-P-29; we determined that five planets that were previously regarded to
have hints of non-zero eccentricity have circular orbits or undetermined
eccentricities; we unveiled curvatures caused by distant companions in the RV
time series of HAT-P-2, HAT-P-22, and HAT-P-29; and we revised the planetary
parameters of CoRoT-1b.
",0,1,0,0,0,0
1231,Injective stabilization of additive functors. I. Preliminaries,"  This paper is the first one in a series of three dealing with the concept of
injective stabilization of the tensor product and its applications. Its primary
goal is to collect known facts and establish a basic operational calculus that
will be used in the subsequent parts. This is done in greater generality than
is necessary for the stated goal. Several results of independent interest are
also established. They include, among other things, connections with
satellites, an explicit construction of the stabilization of a finitely
presented functor, various exactness properties of the injectively stable
functors, a construction, from a functor and a short exact sequence, of a
doubly-infinite exact sequence by splicing the injective stabilization of the
functor and its derived functors. When specialized to the tensor product with a
finitely presented module, the injective stabilization with coefficients in the
ring is isomorphic to the 1-torsion functor. The Auslander-Reiten formula is
extended to a more general formula, which holds for arbitrary (i.e., not
necessarily finite) modules over arbitrary associative rings with identity.
Weakening of the assumptions in the theorems of Eilenberg and Watts leads to
characterizations of the requisite zeroth derived functors.
The subsequent papers, provide applications of the developed techniques.
Part~II deals with new notions of torsion module and cotorsion module of a
module. This is done for arbitrary modules over arbitrary rings. Part~III
introduces a new concept, called the asymptotic stabilization of the tensor
product. The result is closely related to different variants of stable homology
(these are generalizations of Tate homology to arbitrary rings). A comparison
transformation from Vogel homology to the asymptotic stabilization of the
tensor product is constructed and shown to be epic.
",0,0,1,0,0,0
1232,Three hypergraph eigenvector centralities,"  Eigenvector centrality is a standard network analysis tool for determining
the importance of (or ranking of) entities in a connected system that is
represented by a graph. However, many complex systems and datasets have natural
multi-way interactions that are more faithfully modeled by a hypergraph. Here
we extend the notion of graph eigenvector centrality to uniform hypergraphs.
Traditional graph eigenvector centralities are given by a positive eigenvector
of the adjacency matrix, which is guaranteed to exist by the Perron-Frobenius
theorem under some mild conditions. The natural representation of a hypergraph
is a hypermatrix (colloquially, a tensor). Using recently established
Perron-Frobenius theory for tensors, we develop three tensor eigenvectors
centralities for hypergraphs, each with different interpretations. We show that
these centralities can reveal different information on real-world data by
analyzing hypergraphs constructed from n-gram frequencies, co-tagging on stack
exchange, and drug combinations observed in patient emergency room visits.
",1,0,0,0,0,0
1233,Reinforcement Learning using Augmented Neural Networks,"  Neural networks allow Q-learning reinforcement learning agents such as deep
Q-networks (DQN) to approximate complex mappings from state spaces to value
functions. However, this also brings drawbacks when compared to other function
approximators such as tile coding or their generalisations, radial basis
functions (RBF) because they introduce instability due to the side effect of
globalised updates present in neural networks. This instability does not even
vanish in neural networks that do not have any hidden layers. In this paper, we
show that simple modifications to the structure of the neural network can
improve stability of DQN learning when a multi-layer perceptron is used for
function approximation.
",0,0,0,1,0,0
1234,Instantons and Fluctuations in a Lagrangian Model of Turbulence,"  We perform a detailed analytical study of the Recent Fluid Deformation (RFD)
model for the onset of Lagrangian intermittency, within the context of the
Martin-Siggia-Rose-Janssen-de Dominicis (MSRJD) path integral formalism. The
model is based, as a key point, upon local closures for the pressure Hessian
and the viscous dissipation terms in the stochastic dynamical equations for the
velocity gradient tensor. We carry out a power counting hierarchical
classification of the several perturbative contributions associated to
fluctuations around the instanton-evaluated MSRJD action, along the lines of
the cumulant expansion. The most relevant Feynman diagrams are then integrated
out into the renormalized effective action, for the computation of velocity
gradient probability distribution functions (vgPDFs). While the subleading
perturbative corrections do not affect the global shape of the vgPDFs in an
appreciable qualitative way, it turns out that they have a significant role in
the accurate description of their non-Gaussian cores.
",0,1,0,0,0,0
1235,The heavy path approach to Galton-Watson trees with an application to Apollonian networks,"  We study the heavy path decomposition of conditional Galton-Watson trees. In
a standard Galton-Watson tree conditional on its size $n$, we order all
children by their subtree sizes, from large (heavy) to small. A node is marked
if it is among the $k$ heaviest nodes among its siblings. Unmarked nodes and
their subtrees are removed, leaving only a tree of marked nodes, which we call
the $k$-heavy tree. We study various properties of these trees, including their
size and the maximal distance from any original node to the $k$-heavy tree. In
particular, under some moment condition, the $2$-heavy tree is with high
probability larger than $cn$ for some constant $c > 0$, and the maximal
distance from the $k$-heavy tree is $O(n^{1/(k+1)})$ in probability. As a
consequence, for uniformly random Apollonian networks of size $n$, the expected
size of the longest simple path is $\Omega(n)$.
",1,0,1,0,0,0
1236,Perishability of Data: Dynamic Pricing under Varying-Coefficient Models,"  We consider a firm that sells a large number of products to its customers in
an online fashion. Each product is described by a high dimensional feature
vector, and the market value of a product is assumed to be linear in the values
of its features. Parameters of the valuation model are unknown and can change
over time. The firm sequentially observes a product's features and can use the
historical sales data (binary sale/no sale feedbacks) to set the price of
current product, with the objective of maximizing the collected revenue. We
measure the performance of a dynamic pricing policy via regret, which is the
expected revenue loss compared to a clairvoyant that knows the sequence of
model parameters in advance.
We propose a pricing policy based on projected stochastic gradient descent
(PSGD) and characterize its regret in terms of time $T$, features dimension
$d$, and the temporal variability in the model parameters, $\delta_t$. We
consider two settings. In the first one, feature vectors are chosen
antagonistically by nature and we prove that the regret of PSGD pricing policy
is of order $O(\sqrt{T} + \sum_{t=1}^T \sqrt{t}\delta_t)$. In the second
setting (referred to as stochastic features model), the feature vectors are
drawn independently from an unknown distribution. We show that in this case,
the regret of PSGD pricing policy is of order $O(d^2 \log T + \sum_{t=1}^T
t\delta_t/d)$.
",1,0,0,1,0,0
1237,A fast algorithm for maximal propensity score matching,"  We present a new algorithm which detects the maximal possible number of
matched disjoint pairs satisfying a given caliper when a bipartite matching is
done with respect to a scalar index (e.g., propensity score), and constructs a
corresponding matching. Variable width calipers are compatible with the
technique, provided that the width of the caliper is a Lipschitz function of
the index. If the observations are ordered with respect to the index then the
matching needs $O(N)$ operations, where $N$ is the total number of subjects to
be matched. The case of 1-to-$n$ matching is also considered.
We offer also a new fast algorithm for optimal complete one-to-one matching
on a scalar index when the treatment and control groups are of the same size.
This allows us to improve greedy nearest neighbor matching on a scalar index.
Keywords: propensity score matching, nearest neighbor matching, matching with
caliper, variable width caliper.
",1,0,0,1,0,0
1238,"Fractional quantum Hall systems near nematicity: bimetric theory, composite fermions, and Dirac brackets","  We perform a detailed comparison of the Dirac composite fermion and the
recently proposed bimetric theory for a quantum Hall Jain states near half
filling. By tuning the composite Fermi liquid to the vicinity of a nematic
phase transition, we find that the two theories are equivalent to each other.
We verify that the single mode approximation for the response functions and the
static structure factor becomes reliable near the phase transition. We show
that the dispersion relation of the nematic mode near the phase transition can
be obtained from the Dirac brackets between the components of the nematic order
parameter. The dispersion is quadratic at low momenta and has a magnetoroton
minimum at a finite momentum, which is not related to any nearby inhomogeneous
phase.
",0,1,0,0,0,0
1239,Recognizing Objects In-the-wild: Where Do We Stand?,"  The ability to recognize objects is an essential skill for a robotic system
acting in human-populated environments. Despite decades of effort from the
robotic and vision research communities, robots are still missing good visual
perceptual systems, preventing the use of autonomous agents for real-world
applications. The progress is slowed down by the lack of a testbed able to
accurately represent the world perceived by the robot in-the-wild. In order to
fill this gap, we introduce a large-scale, multi-view object dataset collected
with an RGB-D camera mounted on a mobile robot. The dataset embeds the
challenges faced by a robot in a real-life application and provides a useful
tool for validating object recognition algorithms. Besides describing the
characteristics of the dataset, the paper evaluates the performance of a
collection of well-established deep convolutional networks on the new dataset
and analyzes the transferability of deep representations from Web images to
robotic data. Despite the promising results obtained with such representations,
the experiments demonstrate that object classification with real-life robotic
data is far from being solved. Finally, we provide a comparative study to
analyze and highlight the open challenges in robot vision, explaining the
discrepancies in the performance.
",1,0,0,0,0,0
1240,Generalizing Geometric Brownian Motion,"  To convert standard Brownian motion $Z$ into a positive process, Geometric
Brownian motion (GBM) $e^{\beta Z_t}, \beta >0$ is widely used. We generalize
this positive process by introducing an asymmetry parameter $ \alpha \geq 0$
which describes the instantaneous volatility whenever the process reaches a new
low. For our new process, $\beta$ is the instantaneous volatility as prices
become arbitrarily high. Our generalization preserves the positivity, constant
proportional drift, and tractability of GBM, while expressing the instantaneous
volatility as a randomly weighted $L^2$ mean of $\alpha$ and $\beta$. The
running minimum and relative drawup of this process are also analytically
tractable. Letting $\alpha = \beta$, our positive process reduces to Geometric
Brownian motion. By adding a jump to default to the new process, we introduce a
non-negative martingale with the same tractabilities. Assuming a security's
dynamics are driven by these processes in risk neutral measure, we price
several derivatives including vanilla, barrier and lookback options.
",0,0,0,0,0,1
1241,Challenges testing the no-hair theorem with gravitational waves,"  General relativity's no-hair theorem states that isolated astrophysical black
holes are described by only two numbers: mass and spin. As a consequence, there
are strict relationships between the frequency and damping time of the
different modes of a perturbed Kerr black hole. Testing the no-hair theorem has
been a longstanding goal of gravitational-wave astronomy. The recent detection
of gravitational waves from black hole mergers would seem to make such tests
imminent. We investigate how constraints on black hole ringdown parameters
scale with the loudness of the ringdown signal---subject to the constraint that
the post-merger remnant must be allowed to settle into a perturbative,
Kerr-like state. In particular, we require that---for a given detector---the
gravitational waveform predicted by numerical relativity is indistinguishable
from an exponentially damped sine after time $t^\text{cut}$. By requiring the
post-merger remnant to settle into such a perturbative state, we find that
confidence intervals for ringdown parameters do not necessarily shrink with
louder signals. In at least some cases, more sensitive measurements probe later
times without necessarily providing tighter constraints on ringdown frequencies
and damping times. Preliminary investigations are unable to explain this result
in terms of a numerical relativity artifact.
",0,1,0,0,0,0
1242,Speculation On a Source of Dark Matter,"  By drawing an analogy with superfluid 4He vortices we suggest that dark
matter may consist of irreducibly small remnants of cosmic strings.
",0,1,0,0,0,0
1243,Analyzing Cloud Optical Properties Using Sky Cameras,"  Clouds play a significant role in the fluctuation of solar radiation received
by the earth's surface. It is important to study the various cloud properties,
as it impacts the total solar irradiance falling on the earth's surface. One of
such important optical properties of the cloud is the Cloud Optical Thickness
(COT). It is defined with the amount of light that can pass through the clouds.
The COT values are generally obtained from satellite images. However, satellite
images have a low temporal- and spatial- resolutions; and are not suitable for
study in applications as solar energy generation and forecasting. Therefore,
ground-based sky cameras are now getting popular in such fields. In this paper,
we analyze the cloud optical thickness value, from the ground-based sky
cameras, and provide future research directions.
",0,1,0,0,0,0
1244,Response Formulae for $n$-point Correlations in Statistical Mechanical Systems and Application to a Problem of Coarse Graining,"  Predicting the response of a system to perturbations is a key challenge in
mathematical and natural sciences. Under suitable conditions on the nature of
the system, of the perturbation, and of the observables of interest, response
theories allow to construct operators describing the smooth change of the
invariant measure of the system of interest as a function of the small
parameter controlling the intensity of the perturbation. In particular,
response theories can be developed both for stochastic and chaotic
deterministic dynamical systems, where in the latter case stricter conditions
imposing some degree of structural stability are required. In this paper we
extend previous findings and derive general response formulae describing how
n-point correlations are affected by perturbations to the vector flow. We also
show how to compute the response of the spectral properties of the system to
perturbations. We then apply our results to the seemingly unrelated problem of
coarse graining in multiscale systems: we find explicit formulae describing the
change in the terms describing parameterisation of the neglected degrees of
freedom resulting from applying perturbations to the full system. All the terms
envisioned by the Mori-Zwanzig theory - the deterministic, stochastic, and
non-Markovian terms - are affected at 1st order in the perturbation. The
obtained results provide a more comprehesive understanding of the response of
statistical mechanical systems to perturbations and contribute to the goal of
constructing accurate and robust parameterisations and are of potential
relevance for fields like molecular dynamics, condensed matter, and geophysical
fluid dynamics. We envision possible applications of our general results to the
study of the response of climate variability to anthropogenic and natural
forcing and to the study of the equivalence of thermostatted statistical
mechanical systems.
",0,1,1,0,0,0
1245,The Australian PCEHR system: Ensuring Privacy and Security through an Improved Access Control Mechanism,"  An Electronic Health Record (EHR) is designed to store diverse data
accurately from a range of health care providers and to capture the status of a
patient by a range of health care providers across time. Realising the numerous
benefits of the system, EHR adoption is growing globally and many countries
invest heavily in electronic health systems. In Australia, the Government
invested $467 million to build key components of the Personally Controlled
Electronic Health Record (PCEHR) system in July 2012. However, in the last
three years, the uptake from individuals and health care providers has not been
satisfactory. Unauthorised access of the PCEHR was one of the major barriers.
We propose an improved access control model for the PCEHR system to resolve the
unauthorised access issue. We discuss the unauthorised access issue with real
examples and present a potential solution to overcome the issue to make the
PCEHR system a success in Australia.
",1,0,0,0,0,0
1246,Randomized Kernel Methods for Least-Squares Support Vector Machines,"  The least-squares support vector machine is a frequently used kernel method
for non-linear regression and classification tasks. Here we discuss several
approximation algorithms for the least-squares support vector machine
classifier. The proposed methods are based on randomized block kernel matrices,
and we show that they provide good accuracy and reliable scaling for
multi-class classification problems with relatively large data sets. Also, we
present several numerical experiments that illustrate the practical
applicability of the proposed methods.
",1,0,0,1,0,0
1247,Optimal Transport: Fast Probabilistic Approximation with Exact Solvers,"  We propose a simple subsampling scheme for fast randomized approximate
computation of optimal transport distances. This scheme operates on a random
subset of the full data and can use any exact algorithm as a black-box
back-end, including state-of-the-art solvers and entropically penalized
versions. It is based on averaging the exact distances between empirical
measures generated from independent samples from the original measures and can
easily be tuned towards higher accuracy or shorter computation times. To this
end, we give non-asymptotic deviation bounds for its accuracy in the case of
discrete optimal transport problems. In particular, we show that in many
important instances, including images (2D-histograms), the approximation error
is independent of the size of the full problem. We present numerical
experiments that demonstrate that a very good approximation in typical
applications can be obtained in a computation time that is several orders of
magnitude smaller than what is required for exact computation of the full
problem.
",0,0,0,1,0,0
1248,Reliable Clustering of Bernoulli Mixture Models,"  A Bernoulli Mixture Model (BMM) is a finite mixture of random binary vectors
with independent Bernoulli dimensions. The problem of clustering BMM data
arises in a variety of real-world applications, ranging from population
genetics to activity analysis in social networks. In this paper, we have
analyzed the information-theoretic PAC-learnability of BMMs, when the number of
clusters is unknown. In particular, we stipulate certain conditions on both
sample complexity and the dimension of the model in order to guarantee the
Probably Approximately Correct (PAC)-clusterability of a given dataset. To the
best of our knowledge, these findings are the first non-asymptotic (PAC) bounds
on the sample complexity of learning BMMs.
",1,0,0,1,0,0
1249,"Short-time behavior of the heat kernel and Weyl's law on $RCD^*(K, N)$-spaces","  In this paper, we prove pointwise convergence of heat kernels for
mGH-convergent sequences of $RCD^*(K,N)$-spaces. We obtain as a corollary
results on the short-time behavior of the heat kernel in $RCD^*(K,N)$-spaces.
We use then these results to initiate the study of Weyl's law in the $RCD$
setting
",0,0,1,0,0,0
1250,Football and Beer - a Social Media Analysis on Twitter in Context of the FIFA Football World Cup 2018,"  In many societies alcohol is a legal and common recreational substance and
socially accepted. Alcohol consumption often comes along with social events as
it helps people to increase their sociability and to overcome their
inhibitions. On the other hand we know that increased alcohol consumption can
lead to serious health issues, such as cancer, cardiovascular diseases and
diseases of the digestive system, to mention a few. This work examines alcohol
consumption during the FIFA Football World Cup 2018, particularly the usage of
alcohol related information on Twitter. For this we analyse the tweeting
behaviour and show that the tournament strongly increases the interest in beer.
Furthermore we show that countries who had to leave the tournament at early
stage might have done something good to their fans as the interest in beer
decreased again.
",1,0,0,0,0,0
1251,Cross-stream migration of a surfactant-laden deformable droplet in a Poiseuille flow,"  The motion of a viscous deformable droplet suspended in an unbounded
Poiseuille flow in the presence of bulk-insoluble surfactants is studied
analytically. Assuming the convective transport of fluid and heat to be
negligible, we perform a small-deformation perturbation analysis to obtain the
droplet migration velocity. The droplet dynamics strongly depends on the
distribution of surfactants along the droplet interface, which is governed by
the relative strength of convective transport of surfactants as compared with
the diffusive transport of surfactants. The present study is focused on the
following two limits: (i) when the surfactant transport is dominated by surface
diffusion, and (ii) when the surfactant transport is dominated by surface
convection. In the first limiting case, it is seen that the axial velocity of
the droplet decreases with increase in the advection of the surfactants along
the surface. The variation of cross-stream migration velocity, on the other
hand, is analyzed over three different regimes based on the ratio of the
viscosity of the droplet phase to that of the carrier phase. In the first
regime the migration velocity decreases with increase in surface advection of
the surfactants although there is no change in direction of droplet migration.
For the second regime, the direction of the cross-stream migration of the
droplet changes depending on different parameters. In the third regime, the
migration velocity is merely affected by any change in the surfactant
distribution. For the other limit of higher surface advection in comparison to
surface diffusion of the surfactants, the axial velocity of the droplet is
found to be independent of the surfactant distribution. However, the
cross-stream velocity is found to decrease with increase in non-uniformity in
surfactant distribution.
",0,1,0,0,0,0
1252,PCA in Data-Dependent Noise (Correlated-PCA): Nearly Optimal Finite Sample Guarantees,"  We study Principal Component Analysis (PCA) in a setting where a part of the
corrupting noise is data-dependent and, as a result, the noise and the true
data are correlated. Under a bounded-ness assumption on the true data and the
noise, and a simple assumption on data-noise correlation, we obtain a nearly
optimal sample complexity bound for the most commonly used PCA solution,
singular value decomposition (SVD). This bound is a significant improvement
over the bound obtained by Vaswani and Guo in recent work (NIPS 2016) where
this ""correlated-PCA"" problem was first studied; and it holds under a
significantly weaker data-noise correlation assumption than the one used for
this earlier result.
",1,0,0,1,0,0
1253,Using a Predator-Prey Model to Explain Variations of Cloud Spot Price,"  The spot pricing scheme has been considered to be resource-efficient for
providers and cost-effective for consumers in the Cloud market. Nevertheless,
unlike the static and straightforward strategies of trading on-demand and
reserved Cloud services, the market-driven mechanism for trading spot service
would be complicated for both implementation and understanding. The largely
invisible market activities and their complex interactions could especially
make Cloud consumers hesitate to enter the spot market. To reduce the
complexity in understanding the Cloud spot market, we decided to reveal the
backend information behind spot price variations. Inspired by the methodology
of reverse engineering, we developed a Predator-Prey model that can simulate
the interactions between demand and resource based on the visible spot price
traces. The simulation results have shown some basic regular patterns of market
activities with respect to Amazon's spot instance type m3.large. Although the
findings of this study need further validation by using practical data, our
work essentially suggests a promising approach (i.e.~using a Predator-Prey
model) to investigate spot market activities.
",1,0,0,0,0,0
1254,Separatrix crossing in rotation of a body with changing geometry of masses,"  We consider free rotation of a body whose parts move slowly with respect to
each other under the action of internal forces. This problem can be considered
as a perturbation of the Euler-Poinsot problem. The dynamics has an approximate
conservation law - an adiabatic invariant. This allows to describe the
evolution of rotation in the adiabatic approximation. The evolution leads to an
overturn in the rotation of the body: the vector of angular velocity crosses
the separatrix of the Euler-Poinsot problem. This crossing leads to a
quasi-random scattering in body's dynamics. We obtain formulas for
probabilities of capture into different domains in the phase space at
separatrix crossings.
",0,1,0,0,0,0
1255,SING: Symbol-to-Instrument Neural Generator,"  Recent progress in deep learning for audio synthesis opens the way to models
that directly produce the waveform, shifting away from the traditional paradigm
of relying on vocoders or MIDI synthesizers for speech or music generation.
Despite their successes, current state-of-the-art neural audio synthesizers
such as WaveNet and SampleRNN suffer from prohibitive training and inference
times because they are based on autoregressive models that generate audio
samples one at a time at a rate of 16kHz. In this work, we study the more
computationally efficient alternative of generating the waveform frame-by-frame
with large strides. We present SING, a lightweight neural audio synthesizer for
the original task of generating musical notes given desired instrument, pitch
and velocity. Our model is trained end-to-end to generate notes from nearly
1000 instruments with a single decoder, thanks to a new loss function that
minimizes the distances between the log spectrograms of the generated and
target waveforms. On the generalization task of synthesizing notes for pairs of
pitch and instrument not seen during training, SING produces audio with
significantly improved perceptual quality compared to a state-of-the-art
autoencoder based on WaveNet as measured by a Mean Opinion Score (MOS), and is
about 32 times faster for training and 2, 500 times faster for inference.
",1,0,0,0,0,0
1256,Path-by-path regularization by noise for scalar conservation laws,"  We prove a path-by-path regularization by noise result for scalar
conservation laws. In particular, this proves regularizing properties for
scalar conservation laws driven by fractional Brownian motion and generalizes
the respective results obtained in [Gess, Souganidis; Comm. Pure Appl. Math.
(2017)]. In addition, we introduce a new path-by-path scaling property which is
shown to be sufficient to imply regularizing effects.
",0,0,1,0,0,0
1257,An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog,"  We present a novel end-to-end trainable neural network model for
task-oriented dialog systems. The model is able to track dialog state, issue
API calls to knowledge base (KB), and incorporate structured KB query results
into system responses to successfully complete task-oriented dialogs. The
proposed model produces well-structured system responses by jointly learning
belief tracking and KB result processing conditioning on the dialog history. We
evaluate the model in a restaurant search domain using a dataset that is
converted from the second Dialog State Tracking Challenge (DSTC2) corpus.
Experiment results show that the proposed model can robustly track dialog state
given the dialog history. Moreover, our model demonstrates promising results in
producing appropriate system responses, outperforming prior end-to-end
trainable neural network models using per-response accuracy evaluation metrics.
",1,0,0,0,0,0
1258,Neural IR Meets Graph Embedding: A Ranking Model for Product Search,"  Recently, neural models for information retrieval are becoming increasingly
popular. They provide effective approaches for product search due to their
competitive advantages in semantic matching. However, it is challenging to use
graph-based features, though proved very useful in IR literature, in these
neural approaches. In this paper, we leverage the recent advances in graph
embedding techniques to enable neural retrieval models to exploit
graph-structured data for automatic feature extraction. The proposed approach
can not only help to overcome the long-tail problem of click-through data, but
also incorporate external heterogeneous information to improve search results.
Extensive experiments on a real-world e-commerce dataset demonstrate
significant improvement achieved by our proposed approach over multiple strong
baselines both as an individual retrieval model and as a feature used in
learning-to-rank frameworks.
",1,0,0,0,0,0
1259,"Scaling up the software development process, a case study highlighting the complexities of large team software development","  Diamond Light Source is the UK's National Synchrotron Facility and as such
provides access to world class experimental services for UK and international
researchers. As a user facility, that is one that focuses on providing a good
user experience to our varied visitors, Diamond invests heavily in software
infrastructure and staff. Over 100 members of the 600 strong workforce consider
software development as a significant tool to help them achieve their primary
role. These staff work on a diverse number of different software packages,
providing support for installation and configuration, maintenance and bug
fixing, as well as additional research and development of software when
required.
This talk focuses on one of the software projects undertaken to unify and
improve the user experience of several experiments. The ""mapping project"" is a
large 2 year, multi group project targeting the collection and processing
experiments which involve scanning an X-ray beam over a sample and building up
an image of that sample, similar to the way that google maps bring together
small pieces of information to produce a full map of the world. The project
itself is divided into several work packages, ranging from teams of one to 5 or
6 in size, with varying levels of time commitment to the project. This paper
aims to explore one of these work packages as a case study, highlighting the
experiences of the project team, the methodologies employed, their outcomes,
and the lessons learnt from the experience.
",1,0,0,0,0,0
1260,Lyapunov exponents for products of matrices,"  Let ${\bf M}=(M_1,\ldots, M_k)$ be a tuple of real $d\times d$ matrices.
Under certain irreducibility assumptions, we give checkable criteria for
deciding whether ${\bf M}$ possesses the following property: there exist two
constants $\lambda\in {\Bbb R}$ and $C>0$ such that for any $n\in {\Bbb N}$ and
any $i_1, \ldots, i_n \in \{1,\ldots, k\}$, either $M_{i_1} \cdots M_{i_n}={\bf
0}$ or $C^{-1} e^{\lambda n} \leq \| M_{i_1} \cdots M_{i_n} \| \leq C
e^{\lambda n}$, where $\|\cdot\|$ is a matrix norm. The proof is based on
symbolic dynamics and the thermodynamic formalism for matrix products. As
applications, we are able to check the absolute continuity of a class of
overlapping self-similar measures on ${\Bbb R}$, the absolute continuity of
certain self-affine measures in ${\Bbb R}^d$ and the dimensional regularity of
a class of sofic affine-invariant sets in the plane.
",0,0,1,0,0,0
1261,A definitive improvement of a game-theoretic bound and the long tightness game,"  The main goal of the paper is the full proof of a cardinal inequality for a
space with points $G_\delta $, obtained with the help of a long version of the
Menger game. This result, which improves a similar one of Scheepers and Tall,
was already established by the authors under the Continuum Hypothesis. The
paper is completed by few remarks on a long version of the tightness game.
",0,0,1,0,0,0
1262,Group-Server Queues,"  By analyzing energy-efficient management of data centers, this paper proposes
and develops a class of interesting {\it Group-Server Queues}, and establishes
two representative group-server queues through loss networks and impatient
customers, respectively. Furthermore, such two group-server queues are given
model descriptions and necessary interpretation. Also, simple mathematical
discussion is provided, and simulations are made to study the expected queue
lengths, the expected sojourn times and the expected virtual service times. In
addition, this paper also shows that this class of group-server queues are
often encountered in many other practical areas including communication
networks, manufacturing systems, transportation networks, financial networks
and healthcare systems. Note that the group-server queues are always used to
design effectively dynamic control mechanisms through regrouping and
recombining such many servers in a large-scale service system by means of, for
example, bilateral threshold control, and customers transfer to the buffer or
server groups. This leads to the large-scale service system that is divided
into several adaptive and self-organizing subsystems through scheduling of
batch customers and regrouping of service resources, which make the middle
layer of this service system more effectively managed and strengthened under a
dynamic, real-time and even reward optimal framework. Based on this,
performance of such a large-scale service system may be improved greatly in
terms of introducing and analyzing such group-server queues. Therefore, not
only analysis of group-server queues is regarded as a new interesting research
direction, but there also exists many theoretical challenges, basic
difficulties and open problems in the area of queueing networks.
",1,0,0,0,0,0
1263,MC$^2$: Multi-wavelength and dynamical analysis of the merging galaxy cluster ZwCl 0008.8+5215: An older and less massive Bullet Cluster,"  We analyze a rich dataset including Subaru/SuprimeCam, HST/ACS and WFC3,
Keck/DEIMOS, Chandra/ACIS-I, and JVLA/C and D array for the merging galaxy
cluster ZwCl 0008.8+5215. With a joint Subaru/HST weak gravitational lensing
analysis, we identify two dominant subclusters and estimate the masses to be
M$_{200}=\text{5.7}^{+\text{2.8}}_{-\text{1.8}}\times\text{10}^{\text{14}}\,\text{M}_{\odot}$
and 1.2$^{+\text{1.4}}_{-\text{0.6}}\times10^{14}$ M$_{\odot}$. We estimate the
projected separation between the two subclusters to be
924$^{+\text{243}}_{-\text{206}}$ kpc. We perform a clustering analysis on
confirmed cluster member galaxies and estimate the line of sight velocity
difference between the two subclusters to be 92$\pm$164 km s$^{-\text{1}}$. We
further motivate, discuss, and analyze the merger scenario through an analysis
of the 42 ks of Chandra/ACIS-I and JVLA/C and D polarization data. The X-ray
surface brightness profile reveals a remnant core reminiscent of the Bullet
Cluster. The X-ray luminosity in the 0.5-7.0 keV band is
1.7$\pm$0.1$\times$10$^{\text{44}}$ erg s$^{-\text{1}}$ and the X-ray
temperature is 4.90$\pm$0.13 keV. The radio relics are polarized up to 40$\%$.
We implement a Monte Carlo dynamical analysis and estimate the merger velocity
at pericenter to be 1800$^{+\text{400}}_{-\text{300}}$ km s$^{-\text{1}}$. ZwCl
0008.8+5215 is a low-mass version of the Bullet Cluster and therefore may prove
useful in testing alternative models of dark matter. We do not find significant
offsets between dark matter and galaxies, as the uncertainties are large with
the current lensing data. Furthermore, in the east, the BCG is offset from
other luminous cluster galaxies, which poses a puzzle for defining dark matter
-- galaxy offsets.
",0,1,0,0,0,0
1264,Bayesian adaptive bandit-based designs using the Gittins index for multi-armed trials with normally distributed endpoints,"  Adaptive designs for multi-armed clinical trials have become increasingly
popular recently in many areas of medical research because of their potential
to shorten development times and to increase patient response. However,
developing response-adaptive trial designs that offer patient benefit while
ensuring the resulting trial avoids bias and provides a statistically rigorous
comparison of the different treatments included is highly challenging. In this
paper, the theory of Multi-Armed Bandit Problems is used to define a family of
near optimal adaptive designs in the context of a clinical trial with a
normally distributed endpoint with known variance. Through simulation studies
based on an ongoing trial as a motivation we report the operating
characteristics (type I error, power, bias) and patient benefit of these
approaches and compare them to traditional and existing alternative designs.
These results are then compared to those recently published in the context of
Bernoulli endpoints. Many limitations and advantages are similar in both cases
but there are also important differences, specially with respect to type I
error control. This paper proposes a simulation-based testing procedure to
correct for the observed type I error inflation that bandit-based and adaptive
rules can induce. Results presented extend recent work by considering a
normally distributed endpoint, a very common case in clinical practice yet
mostly ignored in the response-adaptive theoretical literature, and illustrate
the potential advantages of using these methods in a rare disease context. We
also recommend a suitable modified implementation of the bandit-based adaptive
designs for the case of common diseases.
",0,0,0,1,0,0
1265,Convexification of Queueing Formulas by Mixed-Integer Second-Order Cone Programming: An Application to a Discrete Location Problem with Congestion,"  Mixed-Integer Second-Order Cone Programs (MISOCPs) form a nice class of
mixed-inter convex programs, which can be solved very efficiently due to the
recent advances in optimization solvers. Our paper bridges the gap between
modeling a class of optimization problems and using MISOCP solvers. It is shown
how various performance metrics of M/G/1 queues can be molded by different
MISOCPs. To motivate our method practically, it is first applied to a
challenging stochastic location problem with congestion, which is broadly used
to design socially optimal service networks. Four different MISOCPs are
developed and compared on sets of benchmark test problems. The new formulations
efficiently solve large-size test problems, which cannot be solved by the best
existing method. Then, the general applicability of our method is shown for
similar optimization problems that use queue-theoretic performance measures to
address customer satisfaction and service quality.
",1,0,0,0,0,0
1266,Discriminative Metric Learning with Deep Forest,"  A Discriminative Deep Forest (DisDF) as a metric learning algorithm is
proposed in the paper. It is based on the Deep Forest or gcForest proposed by
Zhou and Feng and can be viewed as a gcForest modification. The case of the
fully supervised learning is studied when the class labels of individual
training examples are known. The main idea underlying the algorithm is to
assign weights to decision trees in random forest in order to reduce distances
between objects from the same class and to increase them between objects from
different classes. The weights are training parameters. A specific objective
function which combines Euclidean and Manhattan distances and simplifies the
optimization problem for training the DisDF is proposed. The numerical
experiments illustrate the proposed distance metric algorithm.
",1,0,0,1,0,0
1267,An accurate and robust genuinely multidimensional Riemann solver for Euler equations based on TV flux splitting,"  A simple robust genuinely multidimensional convective pressure split (CPS) ,
contact preserving, shock stable Riemann solver (GM-K-CUSP-X) for Euler
equations of gas dynamics is developed. The convective and pressure components
of the Euler system are separated following the Toro-Vazquez type PDE flux
splitting [Toro et al, 2012]. Upwind discretization of these components are
achieved using the framework of Mandal et al [Mandal et al, 2015]. The
robustness of the scheme is studied on a few two dimensional test problems. The
results demonstrate the efficacy of the scheme over the corresponding
conventional two state version of the solver. Results from two classic strong
shock test cases associated with the infamous Carbuncle phenomenon, indicate
that the present solver is completely free of any such numerical instabilities
albeit possessing contact resolution abilities.Such a finding emphasizes the
pre-existing notion about the positive effects that multidimensional flow
modelling may have towards curing of shock instabilities.
",0,1,1,0,0,0
1268,Strong Convergence Rate of Splitting Schemes for Stochastic Nonlinear Schr??dinger Equations,"  We prove the optimal strong convergence rate of a fully discrete scheme,
based on a splitting approach, for a stochastic nonlinear Schr??dinger (NLS)
equation. The main novelty of our method lies on the uniform a priori estimate
and exponential integrability of a sequence of splitting processes which are
used to approximate the solution of the stochastic NLS equation. We show that
the splitting processes converge to the solution with strong order $1/2$. Then
we use the Crank--Nicolson scheme to temporally discretize the splitting
process and get the temporal splitting scheme which also possesses strong order
$1/2$. To obtain a full discretization, we apply this splitting Crank--Nicolson
scheme to the spatially discrete equation which is achieved through the
spectral Galerkin approximation. Furthermore, we establish the convergence of
this fully discrete scheme with optimal strong convergence rate
$\mathcal{O}(N^{-2}+\tau^\frac12)$, where $N$ denotes the dimension of the
approximate space and $\tau$ denotes the time step size. To the best of our
knowledge, this is the first result about strong convergence rates of
temporally numerical approximations and fully discrete schemes for stochastic
NLS equations, or even for stochastic partial differential equations (SPDEs)
with non-monotone coefficients. Numerical experiments verify our theoretical
result.
",0,0,1,0,0,0
1269,Controllability of Conjunctive Boolean Networks with Application to Gene Regulation,"  A Boolean network is a finite state discrete time dynamical system. At each
step, each variable takes a value from a binary set. The value update rule for
each variable is a local function which depends only on a selected subset of
variables. Boolean networks have been used in modeling gene regulatory
networks. We focus in this paper on a special class of Boolean networks, namely
the conjunctive Boolean networks (CBNs), whose value update rule is comprised
of only logic AND operations. It is known that any trajectory of a Boolean
network will enter a periodic orbit. Periodic orbits of a CBN have been
completely understood. In this paper, we investigate the orbit-controllability
and state-controllability of a CBN: We ask the question of how one can steer a
CBN to enter any periodic orbit or to reach any final state, from any initial
state. We establish necessary and sufficient conditions for a CBN to be
orbit-controllable and state-controllable. Furthermore, explicit control laws
are presented along the analysis.
",0,0,1,0,0,0
1270,Fast-neutron and gamma-ray imaging with a capillary liquid xenon converter coupled to a gaseous photomultiplier,"  Gamma-ray and fast-neutron imaging was performed with a novel liquid xenon
(LXe) scintillation detector read out by a Gaseous Photomultiplier (GPM). The
100 mm diameter detector prototype comprised a capillary-filled LXe
converter/scintillator, coupled to a triple-THGEM imaging-GPM, with its first
electrode coated by a CsI UV-photocathode, operated in Ne/5%CH4 cryogenic
temperatures. Radiation localization in 2D was derived from
scintillation-induced photoelectron avalanches, measured on the GPM's segmented
anode. The localization properties of Co-60 gamma-rays and a mixed
fast-neutron/gamma-ray field from an AmBe neutron source were derived from
irradiation of a Pb edge absorber. Spatial resolutions of 12+/-2 mm and 10+/-2
mm (FWHM) were reached with Co-60 and AmBe sources, respectively. The
experimental results are in good agreement with GEANT4 simulations. The
calculated ultimate expected resolutions for our application-relevant 4.4 and
15.1 MeV gamma-rays and 1-15 MeV neutrons are 2-4 mm and ~2 mm (FWHM),
respectively. These results indicate the potential applicability of the new
detector concept to Fast-Neutron Resonance Radiography (FNRR) and
Dual-Discrete-Energy Gamma Radiography (DDEGR) of large objects.
",0,1,0,0,0,0
1271,Is Task Board Customization Beneficial? - An Eye Tracking Study,"  The task board is an essential artifact in many agile development approaches.
It provides a good overview of the project status. Teams often customize their
task boards according to the team members' needs. They modify the structure of
boards, define colored codings for different purposes, and introduce different
card sizes. Although the customizations are intended to improve the task
board's usability and effectiveness, they may also complicate its comprehension
and use. The increased effort impedes the work of both the team and team
externals. Hence, task board customization is in conflict with the agile
practice of fast and easy overview for everyone. In an eye tracking study with
30 participants, we compared an original task board design with three
customized ones to investigate which design shortened the required time to
identify a particular story card. Our findings yield that only the customized
task board design with modified structures reduces the required time. The
original task board design is more beneficial than individual colored codings
and changed card sizes. According to our findings, agile teams should rethink
their current task board design. They may be better served by focusing on the
original task board design and by applying only carefully selected adjustments.
In case of customization, a task board's structure should be adjusted since
this is the only beneficial kind of customization, that additionally complies
more precisely with the concept of fast and easy project overview.
",1,0,0,0,0,0
1272,Characterizing the impact of model error in hydrogeologic time series recovery inverse problems,"  Hydrogeologic models are commonly over-smoothed relative to reality, owing to
the difficulty of obtaining accurate high-resolution information about the
subsurface. When used in an inversion context, such models may introduce
systematic biases which cannot be encapsulated by an unbiased ""observation
noise"" term of the type assumed by standard regularization theory and typical
Bayesian formulations. Despite its importance, model error is difficult to
encapsulate systematically and is often neglected. Here, model error is
considered for a hydrogeologically important class of inverse problems that
includes interpretation of hydraulic transients and contaminant source history
inference: reconstruction of a time series that has been convolved against a
transfer function (i.e., impulse response) that is only approximately known.
Using established harmonic theory along with two results established here
regarding triangular Toeplitz matrices, upper and lower error bounds are
derived for the effect of systematic model error on time series recovery for
both well-determined and over-determined inverse problems. A Monte Carlo study
of a realistic hydraulic reconstruction problem is presented, and the lower
error bound is seen informative about expected behavior. A possible diagnostic
criterion for blind transfer function characterization is also uncovered.
",0,0,1,0,0,0
1273,Suszko's Problem: Mixed Consequence and Compositionality,"  Suszko's problem is the problem of finding the minimal number of truth values
needed to semantically characterize a syntactic consequence relation. Suszko
proved that every Tarskian consequence relation can be characterized using only
two truth values. Malinowski showed that this number can equal three if some of
Tarski's structural constraints are relaxed. By so doing, Malinowski introduced
a case of so-called mixed consequence, allowing the notion of a designated
value to vary between the premises and the conclusions of an argument. In this
paper we give a more systematic perspective on Suszko's problem and on mixed
consequence. First, we prove general representation theorems relating
structural properties of a consequence relation to their semantic
interpretation, uncovering the semantic counterpart of substitution-invariance,
and establishing that (intersective) mixed consequence is fundamentally the
semantic counterpart of the structural property of monotonicity. We use those
to derive maximum-rank results proved recently in a different setting by French
and Ripley, as well as by Blasio, Marcos and Wansing, for logics with various
structural properties (reflexivity, transitivity, none, or both). We strengthen
these results into exact rank results for non-permeable logics (roughly, those
which distinguish the role of premises and conclusions). We discuss the
underlying notion of rank, and the associated reduction proposed independently
by Scott and Suszko. As emphasized by Suszko, that reduction fails to preserve
compositionality in general, meaning that the resulting semantics is no longer
truth-functional. We propose a modification of that notion of reduction,
allowing us to prove that over compact logics with what we call regular
connectives, rank results are maintained even if we request the preservation of
truth-functionality and additional semantic properties.
",1,0,1,0,0,0
1274,Optimization of distributions differences for classification,"  In this paper we introduce a new classification algorithm called Optimization
of Distributions Differences (ODD). The algorithm aims to find a transformation
from the feature space to a new space where the instances in the same class are
as close as possible to one another while the gravity centers of these classes
are as far as possible from one another. This aim is formulated as a
multiobjective optimization problem that is solved by a hybrid of an
evolutionary strategy and the Quasi-Newton method. The choice of the
transformation function is flexible and could be any continuous space function.
We experiment with a linear and a non-linear transformation in this paper. We
show that the algorithm can outperform 6 other state-of-the-art classification
methods, namely naive Bayes, support vector machines, linear discriminant
analysis, multi-layer perceptrons, decision trees, and k-nearest neighbors, in
12 standard classification datasets. Our results show that the method is less
sensitive to the imbalanced number of instances comparing to these methods. We
also show that ODD maintains its performance better than other classification
methods in these datasets, hence, offers a better generalization ability.
",1,0,0,1,0,0
1275,Galois descent of semi-affinoid spaces,"  We study the Galois descent of semi-affinoid non-archimedean analytic spaces.
These are the non-archimedean analytic spaces which admit an affine special
formal scheme as model over a complete discrete valuation ring, such as for
example open or closed polydiscs or polyannuli. Using Weil restrictions and
Galois fixed loci for semi-affinoid spaces and their formal models, we describe
a formal model of a $K$-analytic space $X$, provided that $X\otimes_KL$ is
semi-affinoid for some finite tamely ramified extension $L$ of $K$. As an
application, we study the forms of analytic annuli that are trivialized by a
wide class of Galois extensions that includes totally tamely ramified
extensions. In order to do so, we first establish a Weierstrass preparation
result for analytic functions on annuli, and use it to linearize finite order
automorphisms of annuli. Finally, we explain how from these results one can
deduce a non-archimedean analytic proof of the existence of resolutions of
singularities of surfaces in characteristic zero.
",0,0,1,0,0,0
1276,"Synthesis, Crystal Structure, and Physical Properties of New Layered Oxychalcogenide La2O2Bi3AgS6","  We have synthesized a new layered oxychalcogenide La2O2Bi3AgS6. From
synchrotron X-ray diffraction and Rietveld refinement, the crystal structure of
La2O2Bi3AgS6 was refined using a model of the P4/nmm space group with a =
4.0644(1) {\AA} and c = 19.412(1) {\AA}, which is similar to the related
compound LaOBiPbS3, while the interlayer bonds (M2-S1 bonds) are apparently
shorter in La2O2Bi3AgS6. The tunneling electron microscopy (TEM) image
confirmed the lattice constant derived from Rietveld refinement (c ~ 20 {\AA}).
The electrical resistivity and Seebeck coefficient suggested that the
electronic states of La2O2Bi3AgS6 are more metallic than those of LaOBiS2 and
LaOBiPbS3. The insertion of a rock-salt-type chalcogenide into the van der
Waals gap of BiS2-based layered compounds, such as LaOBiS2, will be a useful
strategy for designing new layered functional materials in the layered
chalcogenide family.
",0,1,0,0,0,0
1277,Planar Graph Perfect Matching is in NC,"  Is perfect matching in NC? That is, is there a deterministic fast parallel
algorithm for it? This has been an outstanding open question in theoretical
computer science for over three decades, ever since the discovery of RNC
matching algorithms. Within this question, the case of planar graphs has
remained an enigma: On the one hand, counting the number of perfect matchings
is far harder than finding one (the former is #P-complete and the latter is in
P), and on the other, for planar graphs, counting has long been known to be in
NC whereas finding one has resisted a solution.
In this paper, we give an NC algorithm for finding a perfect matching in a
planar graph. Our algorithm uses the above-stated fact about counting matchings
in a crucial way. Our main new idea is an NC algorithm for finding a face of
the perfect matching polytope at which $\Omega(n)$ new conditions, involving
constraints of the polytope, are simultaneously satisfied. Several other ideas
are also needed, such as finding a point in the interior of the minimum weight
face of this polytope and finding a balanced tight odd set in NC.
",1,0,0,0,0,0
1278,Recommendation under Capacity Constraints,"  In this paper, we investigate the common scenario where every candidate item
for recommendation is characterized by a maximum capacity, i.e., number of
seats in a Point-of-Interest (POI) or size of an item's inventory. Despite the
prevalence of the task of recommending items under capacity constraints in a
variety of settings, to the best of our knowledge, none of the known
recommender methods is designed to respect capacity constraints. To close this
gap, we extend three state-of-the art latent factor recommendation approaches:
probabilistic matrix factorization (PMF), geographical matrix factorization
(GeoMF), and bayesian personalized ranking (BPR), to optimize for both
recommendation accuracy and expected item usage that respects the capacity
constraints. We introduce the useful concepts of user propensity to listen and
item capacity. Our experimental results in real-world datasets, both for the
domain of item recommendation and POI recommendation, highlight the benefit of
our method for the setting of recommendation under capacity constraints.
",1,0,0,1,0,0
1279,Simulation to scaled city: zero-shot policy transfer for traffic control via autonomous vehicles,"  Using deep reinforcement learning, we train control policies for autonomous
vehicles leading a platoon of vehicles onto a roundabout. Using Flow, a library
for deep reinforcement learning in micro-simulators, we train two policies, one
policy with noise injected into the state and action space and one without any
injected noise. In simulation, the autonomous vehicle learns an emergent
metering behavior for both policies in which it slows to allow for smoother
merging. We then directly transfer this policy without any tuning to the
University of Delaware Scaled Smart City (UDSSC), a 1:25 scale testbed for
connected and automated vehicles. We characterize the performance of both
policies on the scaled city. We show that the noise-free policy winds up
crashing and only occasionally metering. However, the noise-injected policy
consistently performs the metering behavior and remains collision-free,
suggesting that the noise helps with the zero-shot policy transfer.
Additionally, the transferred, noise-injected policy leads to a 5% reduction of
average travel time and a reduction of 22% in maximum travel time in the UDSSC.
Videos of the controllers can be found at
this https URL.
",1,0,0,0,0,0
1280,Relative Singularity Categories,"  We study the following generalization of singularity categories. Let X be a
quasi-projective Gorenstein scheme with isolated singularities and A a
non-commutative resolution of singularities of X in the sense of Van den Bergh.
We introduce the relative singularity category as the Verdier quotient of the
bounded derived category of coherent sheaves on A modulo the category of
perfect complexes on X. We view it as a measure for the difference between X
and A. The main results of this thesis are the following.
(i) We prove an analogue of Orlov's localization result in our setup. If X
has isolated singularities, then this reduces the study of the relative
singularity categories to the affine case.
(ii) We prove Hom-finiteness and idempotent completeness of the relative
singularity categories in the complete local situation and determine its
Grothendieck group.
(iii) We give a complete and explicit description of the relative singularity
categories when X has only nodal singularities and the resolution is given by a
sheaf of Auslander algebras.
(iv) We study relations between relative singularity categories and classical
singularity categories. For a simple hypersurface singularity and its Auslander
resolution, we show that these categories determine each other.
(v) The developed technique leads to the following `purely commutative'
application: a description of Iyama & Wemyss triangulated category for rational
surface singularities in terms of the singularity category of the rational
double point resolution.
(vi) We give a description of singularity categories of gentle algebras.
",0,0,1,0,0,0
1281,Challenges to Keeping the Computer Industry Centered in the US,"  It is undeniable that the worldwide computer industry's center is the US,
specifically in Silicon Valley. Much of the reason for the success of Silicon
Valley had to do with Moore's Law: the observation by Intel co-founder Gordon
Moore that the number of transistors on a microchip doubled at a rate of
approximately every two years. According to the International Technology
Roadmap for Semiconductors, Moore's Law will end in 2021. How can we rethink
computing technology to restart the historic explosive performance growth?
Since 2012, the IEEE Rebooting Computing Initiative (IEEE RCI) has been working
with industry and the US government to find new computing approaches to answer
this question. In parallel, the CCC has held a number of workshops addressing
similar questions. This whitepaper summarizes some of the IEEE RCI and CCC
findings. The challenge for the US is to lead this new era of computing. Our
international competitors are not sitting still: China has invested
significantly in a variety of approaches such as neuromorphic computing, chip
fabrication facilities, computer architecture, and high-performance simulation
and data analytics computing, for example. We must act now, otherwise, the
center of the computer industry will move from Silicon Valley and likely move
off shore entirely.
",1,0,0,0,0,0
1282,"Contiguous Relations, Laplace's Methods and Continued Fractions for 3F2(1)","  Using contiguous relations we construct an infinite number of continued
fraction expansions for ratios of generalized hypergeometric series 3F2(1). We
establish exact error term estimates for their approximants and prove their
rapid convergences. To do so we develop a discrete version of Laplace's method
for hypergeometric series in addition to the use of ordinary (continuous)
Laplace's method for Euler's hypergeometric integrals.
",0,0,1,0,0,0
1283,Arimoto-R??nyi Conditional Entropy and Bayesian $M$-ary Hypothesis Testing,"  This paper gives upper and lower bounds on the minimum error probability of
Bayesian $M$-ary hypothesis testing in terms of the Arimoto-R??nyi conditional
entropy of an arbitrary order $\alpha$. The improved tightness of these bounds
over their specialized versions with the Shannon conditional entropy
($\alpha=1$) is demonstrated. In particular, in the case where $M$ is finite,
we show how to generalize Fano's inequality under both the conventional and
list-decision settings. As a counterpart to the generalized Fano's inequality,
allowing $M$ to be infinite, a lower bound on the Arimoto-R??nyi conditional
entropy is derived as a function of the minimum error probability. Explicit
upper and lower bounds on the minimum error probability are obtained as a
function of the Arimoto-R??nyi conditional entropy for both positive and
negative $\alpha$. Furthermore, we give upper bounds on the minimum error
probability as functions of the R??nyi divergence. In the setup of discrete
memoryless channels, we analyze the exponentially vanishing decay of the
Arimoto-R??nyi conditional entropy of the transmitted codeword given the
channel output when averaged over a random coding ensemble.
",1,0,1,1,0,0
1284,A note on the violation of Bell's inequality,"  With Bell's inequalities one has a formal expression to show how essentially
all local theories of natural phenomena that are formulated within the
framework of realism may be tested using a simple experimental arrangement. For
the case of entangled pairs of spin-1/2 particles we propose an alternative
measurement setup which is consistent to the necessary assumptions
corresponding to the derivation of the Bell inequalities. We find that the Bell
inequalities are never violated with respect to our suggested measurement
process.
",0,1,0,0,0,0
1285,Real-Time Model Predictive Control for Energy Management in Autonomous Underwater Vehicle,"  Improving endurance is crucial for extending the spatial and temporal
operation range of autonomous underwater vehicles (AUVs). Considering the
hardware constraints and the performance requirements, an intelligent energy
management system is required to extend the operation range of AUVs. This paper
presents a novel model predictive control (MPC) framework for energy-optimal
point-to-point motion control of an AUV. In this scheme, the energy management
problem of an AUV is reformulated as a surge motion optimization problem in two
stages. First, a system-level energy minimization problem is solved by managing
the trade-off between the energies required for overcoming the positive
buoyancy and surge drag force in static optimization. Next, an MPC with a
special cost function formulation is proposed to deal with transients and
system dynamics. A switching logic for handling the transition between the
static and dynamic stages is incorporated to reduce the computational efforts.
Simulation results show that the proposed method is able to achieve
near-optimal energy consumption with considerable lower computational
complexity.
",1,0,0,0,0,0
1286,Surjective H-Colouring over Reflexive Digraphs,"  The Surjective H-Colouring problem is to test if a given graph allows a
vertex-surjective homomorphism to a fixed graph H. The complexity of this
problem has been well studied for undirected (partially) reflexive graphs. We
introduce endo-triviality, the property of a structure that all of its
endomorphisms that do not have range of size 1 are automorphisms, as a means to
obtain complexity-theoretic classifications of Surjective H-Colouring in the
case of reflexive digraphs.
Chen [2014] proved, in the setting of constraint satisfaction problems, that
Surjective H-Colouring is NP-complete if H has the property that all of its
polymorphisms are essentially unary. We give the first concrete application of
his result by showing that every endo-trivial reflexive digraph H has this
property. We then use the concept of endo-triviality to prove, as our main
result, a dichotomy for Surjective H-Colouring when H is a reflexive
tournament: if H is transitive, then Surjective H-Colouring is in NL, otherwise
it is NP-complete.
By combining this result with some known and new results we obtain a
complexity classification for Surjective H-Colouring when H is a partially
reflexive digraph of size at most 3.
",1,0,0,0,0,0
1287,A modal typing system for self-referential programs and specifications,"  This paper proposes a modal typing system that enables us to handle
self-referential formulae, including ones with negative self-references, which
on one hand, would introduce a logical contradiction, namely Russell's paradox,
in the conventional setting, while on the other hand, are necessary to capture
a certain class of programs such as fixed-point combinators and objects with
so-called binary methods in object-oriented programming. The proposed system
provides a basis for axiomatic semantics of such a wider range of programs and
a new framework for natural construction of recursive programs in the
proofs-as-programs paradigm.
",1,0,0,0,0,0
1288,Scalable Realistic Recommendation Datasets through Fractal Expansions,"  Recommender System research suffers currently from a disconnect between the
size of academic data sets and the scale of industrial production systems. In
order to bridge that gap we propose to generate more massive user/item
interaction data sets by expanding pre-existing public data sets. User/item
incidence matrices record interactions between users and items on a given
platform as a large sparse matrix whose rows correspond to users and whose
columns correspond to items. Our technique expands such matrices to larger
numbers of rows (users), columns (items) and non zero values (interactions)
while preserving key higher order statistical properties. We adapt the
Kronecker Graph Theory to user/item incidence matrices and show that the
corresponding fractal expansions preserve the fat-tailed distributions of user
engagements, item popularity and singular value spectra of user/item
interaction matrices. Preserving such properties is key to building large
realistic synthetic data sets which in turn can be employed reliably to
benchmark Recommender Systems and the systems employed to train them. We
provide algorithms to produce such expansions and apply them to the MovieLens
20 million data set comprising 20 million ratings of 27K movies by 138K users.
The resulting expanded data set has 10 billion ratings, 2 million items and
864K users in its smaller version and can be scaled up or down. A larger
version features 655 billion ratings, 7 million items and 17 million users.
",1,0,0,1,0,0
1289,Implementation of a Distributed Coherent Quantum Observer,"  This paper considers the problem of implementing a previously proposed
distributed direct coupling quantum observer for a closed linear quantum
system. By modifying the form of the previously proposed observer, the paper
proposes a possible experimental implementation of the observer plant system
using a non-degenerate parametric amplifier and a chain of optical cavities
which are coupled together via optical interconnections. It is shown that the
distributed observer converges to a consensus in a time averaged sense in which
an output of each element of the observer estimates the specified output of the
quantum plant.
",1,0,1,0,0,0
1290,Stacking and stability,"  Stacking is a general approach for combining multiple models toward greater
predictive accuracy. It has found various application across different domains,
ensuing from its meta-learning nature. Our understanding, nevertheless, on how
and why stacking works remains intuitive and lacking in theoretical insight. In
this paper, we use the stability of learning algorithms as an elemental
analysis framework suitable for addressing the issue. To this end, we analyze
the hypothesis stability of stacking, bag-stacking, and dag-stacking and
establish a connection between bag-stacking and weighted bagging. We show that
the hypothesis stability of stacking is a product of the hypothesis stability
of each of the base models and the combiner. Moreover, in bag-stacking and
dag-stacking, the hypothesis stability depends on the sampling strategy used to
generate the training set replicates. Our findings suggest that 1) subsampling
and bootstrap sampling improve the stability of stacking, and 2) stacking
improves the stability of both subbagging and bagging.
",1,0,0,1,0,0
1291,Accelerating Discrete Wavelet Transforms on GPUs,"  The two-dimensional discrete wavelet transform has a huge number of
applications in image-processing techniques. Until now, several papers compared
the performance of such transform on graphics processing units (GPUs). However,
all of them only dealt with lifting and convolution computation schemes. In
this paper, we show that corresponding horizontal and vertical lifting parts of
the lifting scheme can be merged into non-separable lifting units, which halves
the number of steps. We also discuss an optimization strategy leading to a
reduction in the number of arithmetic operations. The schemes were assessed
using the OpenCL and pixel shaders. The proposed non-separable lifting scheme
outperforms the existing schemes in many cases, irrespective of its higher
complexity.
",1,0,0,0,0,0
1292,Accelerated Consensus via Min-Sum Splitting,"  We apply the Min-Sum message-passing protocol to solve the consensus problem
in distributed optimization. We show that while the ordinary Min-Sum algorithm
does not converge, a modified version of it known as Splitting yields
convergence to the problem solution. We prove that a proper choice of the
tuning parameters allows Min-Sum Splitting to yield subdiffusive accelerated
convergence rates, matching the rates obtained by shift-register methods. The
acceleration scheme embodied by Min-Sum Splitting for the consensus problem
bears similarities with lifted Markov chains techniques and with multi-step
first order methods in convex optimization.
",0,0,1,0,0,0
1293,Chemception: A Deep Neural Network with Minimal Chemistry Knowledge Matches the Performance of Expert-developed QSAR/QSPR Models,"  In the last few years, we have seen the transformative impact of deep
learning in many applications, particularly in speech recognition and computer
vision. Inspired by Google's Inception-ResNet deep convolutional neural network
(CNN) for image classification, we have developed ""Chemception"", a deep CNN for
the prediction of chemical properties, using just the images of 2D drawings of
molecules. We develop Chemception without providing any additional explicit
chemistry knowledge, such as basic concepts like periodicity, or advanced
features like molecular descriptors and fingerprints. We then show how
Chemception can serve as a general-purpose neural network architecture for
predicting toxicity, activity, and solvation properties when trained on a
modest database of 600 to 40,000 compounds. When compared to multi-layer
perceptron (MLP) deep neural networks trained with ECFP fingerprints,
Chemception slightly outperforms in activity and solvation prediction and
slightly underperforms in toxicity prediction. Having matched the performance
of expert-developed QSAR/QSPR deep learning models, our work demonstrates the
plausibility of using deep neural networks to assist in computational chemistry
research, where the feature engineering process is performed primarily by a
deep learning algorithm.
",1,0,0,1,0,0
1294,Constraining the Milky Way assembly history with Galactic Archaeology. Ludwig Biermann Award Lecture 2015,"  The aim of Galactic Archaeology is to recover the evolutionary history of the
Milky Way from its present day kinematical and chemical state. Because stars
move away from their birth sites, the current dynamical information alone is
not sufficient for this task. The chemical composition of stellar atmospheres,
on the other hand, is largely preserved over the stellar lifetime and, together
with accurate ages, can be used to recover the birthplaces of stars currently
found at the same Galactic radius. In addition to the availability of large
stellar samples with accurate 6D kinematics and chemical abundance
measurements, this requires detailed modeling with both dynamical and chemical
evolution taken into account. An important first step is to understand the
variety of dynamical processes that can take place in the Milky Way, including
the perturbative effects of both internal (bar and spiral structure) and
external (infalling satellites) agents. We discuss here (1) how to constrain
the Galactic bar, spiral structure, and merging satellites by their effect on
the local and global disc phase-space, (2) the effect of multiple patterns on
the disc dynamics, and (3) the importance of radial migration and merger
perturbations for the formation of the Galactic thick disc. Finally, we discuss
the construction of Milky Way chemo-dynamical models and relate to
observations.
",0,1,0,0,0,0
1295,A unified view of entropy-regularized Markov decision processes,"  We propose a general framework for entropy-regularized average-reward
reinforcement learning in Markov decision processes (MDPs). Our approach is
based on extending the linear-programming formulation of policy optimization in
MDPs to accommodate convex regularization functions. Our key result is showing
that using the conditional entropy of the joint state-action distributions as
regularization yields a dual optimization problem closely resembling the
Bellman optimality equations. This result enables us to formalize a number of
state-of-the-art entropy-regularized reinforcement learning algorithms as
approximate variants of Mirror Descent or Dual Averaging, and thus to argue
about the convergence properties of these methods. In particular, we show that
the exact version of the TRPO algorithm of Schulman et al. (2015) actually
converges to the optimal policy, while the entropy-regularized policy gradient
methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally,
we illustrate empirically the effects of using various regularization
techniques on learning performance in a simple reinforcement learning setup.
",1,0,0,1,0,0
1296,Arithmetic Circuits for Multilevel Qudits Based on Quantum Fourier Transform,"  We present some basic integer arithmetic quantum circuits, such as adders and
multipliers-accumulators of various forms, as well as diagonal operators, which
operate on multilevel qudits. The integers to be processed are represented in
an alternative basis after they have been Fourier transformed. Several
arithmetic circuits operating on Fourier transformed integers have appeared in
the literature for two level qubits. Here we extend these techniques on
multilevel qudits, as they may offer some advantages relative to qubits
implementations. The arithmetic circuits presented can be used as basic
building blocks for higher level algorithms such as quantum phase estimation,
quantum simulation, quantum optimization etc., but they can also be used in the
implementation of a quantum fractional Fourier transform as it is shown in a
companion work presented separately.
",1,0,0,0,0,0
1297,Quantifying the distribution of editorial power and manuscript decision bias at the mega-journal PLOS ONE,"  We analyzed the longitudinal activity of nearly 7,000 editors at the
mega-journal PLOS ONE over the 10-year period 2006-2015. Using the
article-editor associations, we develop editor-specific measures of power,
activity, article acceptance time, citation impact, and editorial renumeration
(an analogue to self-citation). We observe remarkably high levels of power
inequality among the PLOS ONE editors, with the top-10 editors responsible for
3,366 articles -- corresponding to 2.4% of the 141,986 articles we analyzed.
Such high inequality levels suggest the presence of unintended incentives,
which may reinforce unethical behavior in the form of decision-level biases at
the editorial level. Our results indicate that editors may become apathetic in
judging the quality of articles and susceptible to modes of power-driven
misconduct. We used the longitudinal dimension of editor activity to develop
two panel regression models which test and verify the presence of editor-level
bias. In the first model we analyzed the citation impact of articles, and in
the second model we modeled the decision time between an article being
submitted and ultimately accepted by the editor. We focused on two variables
that represent social factors that capture potential conflicts-of-interest: (i)
we accounted for the social ties between editors and authors by developing a
measure of repeat authorship among an editor's article set, and (ii) we
accounted for the rate of citations directed towards the editor's own
publications in the reference list of each article he/she oversaw. Our results
indicate that these two factors play a significant role in the editorial
decision process. Moreover, these two effects appear to increase with editor
age, which is consistent with behavioral studies concerning the evolution of
misbehavior and response to temptation in power-driven environments.
",1,1,0,0,0,0
1298,Threshold Constraints with Guarantees for Parity Objectives in Markov Decision Processes,"  The beyond worst-case synthesis problem was introduced recently by Bruy??re
et al. [BFRR14]: it aims at building system controllers that provide strict
worst-case performance guarantees against an antagonistic environment while
ensuring higher expected performance against a stochastic model of the
environment. Our work extends the framework of [BFRR14] and follow-up papers,
which focused on quantitative objectives, by addressing the case of
$\omega$-regular conditions encoded as parity objectives, a natural way to
represent functional requirements of systems.
We build strategies that satisfy a main parity objective on all plays, while
ensuring a secondary one with sufficient probability. This setting raises new
challenges in comparison to quantitative objectives, as one cannot easily mix
different strategies without endangering the functional properties of the
system. We establish that, for all variants of this problem, deciding the
existence of a strategy lies in ${\sf NP} \cap {\sf coNP}$, the same complexity
class as classical parity games. Hence, our framework provides additional
modeling power while staying in the same complexity class.
[BFRR14] V??ronique Bruy??re, Emmanuel Filiot, Mickael Randour, and
Jean-Fran??ois Raskin. Meet your expectations with guarantees: Beyond
worst-case synthesis in quantitative games. In Ernst W. Mayr and Natacha
Portier, editors, 31st International Symposium on Theoretical Aspects of
Computer Science, STACS 2014, March 5-8, 2014, Lyon, France, volume 25 of
LIPIcs, pages 199-213. Schloss Dagstuhl - Leibniz - Zentrum fuer Informatik,
2014.
",1,0,1,0,0,0
1299,GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures,"  VAEs (Variational AutoEncoders) have proved to be powerful in the context of
density modeling and have been used in a variety of contexts for creative
purposes. In many settings, the data we model possesses continuous attributes
that we would like to take into account at generation time. We propose in this
paper GLSR-VAE, a Geodesic Latent Space Regularization for the Variational
AutoEncoder architecture and its generalizations which allows a fine control on
the embedding of the data into the latent space. When augmenting the VAE loss
with this regularization, changes in the learned latent space reflects changes
of the attributes of the data. This deeper understanding of the VAE latent
space structure offers the possibility to modulate the attributes of the
generated data in a continuous way. We demonstrate its efficiency on a
monophonic music generation task where we manage to generate variations of
discrete sequences in an intended and playful way.
",1,0,0,1,0,0
1300,Timing Solution and Single-pulse Properties for Eight Rotating Radio Transients,"  Rotating radio transients (RRATs), loosely defined as objects that are
discovered through only their single pulses, are sporadic pulsars that have a
wide range of emission properties. For many of them, we must measure their
periods and determine timing solutions relying on the timing of their
individual pulses, while some of the less sporadic RRATs can be timed by using
folding techniques as we do for other pulsars. Here, based on Parkes and Green
Bank Telescope (GBT) observations, we introduce our results on eight RRATs
including their timing-derived rotation parameters, positions, and dispersion
measures (DMs), along with a comparison of the spin-down properties of RRATs
and normal pulsars. Using data for 24 RRATs, we find that their period
derivatives are generally larger than those of normal pulsars, independent of
any intrinsic correlation with period, indicating that RRATs' highly sporadic
emission may be associated with intrinsically larger magnetic fields. We carry
out Lomb$-$Scargle tests to search for periodicities in RRATs' pulse detection
times with long timescales. Periodicities are detected for all targets, with
significant candidates of roughly 3.4 hr for PSR J1623$-$0841 and 0.7 hr for
PSR J1839$-$0141. We also analyze their single-pulse amplitude distributions,
finding that log-normal distributions provide the best fits, as is the case for
most pulsars. However, several RRATs exhibit power-law tails, as seen for
pulsars emitting giant pulses. This, along with consideration of the selection
effects against the detection of weak pulses, imply that RRAT pulses generally
represent the tail of a normal intensity distribution.
",0,1,0,0,0,0
1301,Strong and broadly tunable plasmon resonances in thick films of aligned carbon nanotubes,"  Low-dimensional plasmonic materials can function as high quality terahertz
and infrared antennas at deep subwavelength scales. Despite these antennas'
strong coupling to electromagnetic fields, there is a pressing need to further
strengthen their absorption. We address this problem by fabricating thick films
of aligned, uniformly sized carbon nanotubes and showing that their plasmon
resonances are strong, narrow, and broadly tunable. With thicknesses ranging
from 25 to 250 nm, our films exhibit peak attenuation reaching 70%, quality
factors reaching 9, and electrostatically tunable peak frequencies by a factor
of 2.3x. Excellent nanotube alignment leads to the attenuation being 99%
linearly polarized along the nanotube axis. Increasing the film thickness
blueshifts the plasmon resonators down to peak wavelengths as low as 1.4
micrometers, promoting them to a new near-infrared regime in which they can
both overlap the S11 nanotube exciton energy and access the technologically
important infrared telecom band.
",0,1,0,0,0,0
1302,The coordination of centralised and distributed generation,"  In this paper, we analyse the interaction between centralised carbon emissive
technologies and distributed intermittent non-emissive technologies. A
representative consumer can satisfy his electricity demand by investing in
distributed generation (solar panels) and by buying power from a centralised
firm at a price the firm sets. Distributed generation is intermittent and
induces an externality cost to the consumer. The firm provides non-random
electricity generation subject to a carbon tax and to transmission costs. The
objective of the consumer is to satisfy her demand while minimising investment
costs, payments to the firm, and intermittency costs. The objective of the firm
is to satisfy the consumer's residual demand while minimising investment costs,
demand deviation costs, and maximising the payments from the consumer. We
formulate the investment decisions as McKean-Vlasov control problems with
stochastic coefficients. We provide explicit, price model-free solutions to the
optimal decision problems faced by each player, the solution of the Pareto
optimum, and the laissez-faire market situation represented by a Stackelberg
equilibrium where the firm is the leader. We find that, from the social
planner's point of view, the high adjustment cost of centralised technology
damages the development of distributed generation. The Stackelberg equilibrium
leads to significant deviation from the socially desirable ratio of centralised
versus distributed generation. In a situation where a power system is to be
built from zero, the optimal strategy of the firm is high price/low
market-share, but is low price/large market share for existing power systems.
Further, from a regulation policy, we find that a carbon tax or a subsidy to
distributed technology has the same efficiency in achieving a given level of
distributed generation.
",0,0,1,0,0,0
1303,Computation of annular capacity by Hamiltonian Floer theory of non-contractible periodic trajectories,"  The first author introduced a relative symplectic capacity $C$ for a
symplectic manifold $(N,\omega_N)$ and its subset $X$ which measures the
existence of non-contractible periodic trajectories of Hamiltonian isotopies on
the product of $N$ with the annulus $A_R=(R,R)\times\mathbb{R}/\mathbb{Z}$. In
the present paper, we give an exact computation of the capacity $C$ of the
$2n$-torus $\mathbb{T}^{2n}$ relative to a Lagrangian submanifold
$\mathbb{T}^n$ which implies the existence of non-contractible Hamiltonian
periodic trajectories on $A_R\times\mathbb{T}^{2n}$. Moreover, we give a lower
bound on the number of such trajectories.
",0,0,1,0,0,0
1304,A New Torsion Pendulum for Gravitational Reference Sensor Technology Development,"  We report on the design and sensitivity of a new torsion pendulum for
measuring the performance of ultra-precise inertial sensors and for the
development of associated technologies for space-based gravitational wave
observatories and geodesy missions. The apparatus comprises a 1 m-long, 50
um-diameter, tungsten fiber that supports an inertial member inside a vacuum
system. The inertial member is an aluminum crossbar with four hollow cubic test
masses at each end. This structure converts the rotation of the torsion
pendulum into translation of the test masses. Two test masses are enclosed in
capacitive sensors which provide readout and actuation. These test masses are
electrically insulated from the rest of the cross-bar and their electrical
charge is controlled by photoemission using fiber-coupled ultraviolet light
emitting diodes. The capacitive readout measures the test mass displacement
with a broadband sensitivity of 30 nm / sqrt(Hz), and is complemented by a
laser interferometer with a sensitivity of about 0.5 nm / sqrt(Hz). The
performance of the pendulum, as determined by the measured residual torque
noise and expressed in terms of equivalent force acting on a single test mass,
is roughly 200 fN / sqrt(Hz) around 2 mHz, which is about a factor of 20 above
the thermal noise limit of the fiber.
",0,1,0,0,0,0
1305,Optical Angular Momentum in Classical Electrodynamics,"  Invoking Maxwell's classical equations in conjunction with expressions for
the electromagnetic (EM) energy, momentum, force, and torque, we use a few
simple examples to demonstrate the nature of the EM angular momentum. The
energy and the angular momentum of an EM field will be shown to have an
intimate relationship; a source radiating EM angular momentum will, of
necessity, pick up an equal but opposite amount of mechanical angular momentum;
and the spin and orbital angular momenta of the EM field, when absorbed by a
small particle, will be seen to elicit different responses from the particle.
",0,1,0,0,0,0
1306,Efficient variational Bayesian neural network ensembles for outlier detection,"  In this work we perform outlier detection using ensembles of neural networks
obtained by variational approximation of the posterior in a Bayesian neural
network setting. The variational parameters are obtained by sampling from the
true posterior by gradient descent. We show our outlier detection results are
comparable to those obtained using other efficient ensembling methods.
",1,0,0,1,0,0
1307,Emergent high-spin state above 7 GPa in superconducting FeSe,"  The local electronic and magnetic properties of superconducting FeSe have
been investigated by K$\beta$ x-ray emission (XES) and simultaneous x-ray
absorption spectroscopy (XAS) at the Fe K-edge at high pressure and low
temperature. Our results indicate a sluggish decrease of the local Fe spin
moment under pressure up to 7~GPa, in line with previous reports, followed by a
sudden increase at higher pressure which has been hitherto unobserved. The
magnetic surge is preceded by an abrupt change of the Fe local structure as
observed by the decrease of the XAS pre-edge region intensity and corroborated
by ab-initio simulations. This pressure corresponds to a structural transition,
previously detected by x-ray diffraction, from the $Cmma$ form to the denser
$Pbnm$ form with octahedral coordination of iron. Finally, the near-edge region
of the XAS spectra shows a change before this transition at 5~GPa,
corresponding well with the onset pressure of the previously observed
enhancement of $T_c$. Our results emphasize the delicate interplay between
structural, magnetic, and superconducting properties in FeSe under pressure.
",0,1,0,0,0,0
1308,Verification in Staged Tile Self-Assembly,"  We prove the unique assembly and unique shape verification problems,
benchmark measures of self-assembly model power, are
$\mathrm{coNP}^{\mathrm{NP}}$-hard and contained in $\mathrm{PSPACE}$ (and in
$\mathrm{\Pi}^\mathrm{P}_{2s}$ for staged systems with $s$ stages). En route,
we prove that unique shape verification problem in the 2HAM is
$\mathrm{coNP}^{\mathrm{NP}}$-complete.
",1,0,0,0,0,0
1309,Combinets: Creativity via Recombination of Neural Networks,"  One of the defining characteristics of human creativity is the ability to
make conceptual leaps, creating something surprising from typical knowledge. In
comparison, deep neural networks often struggle to handle cases outside of
their training data, which is especially problematic for problems with limited
training data. Approaches exist to transfer knowledge from problems with
sufficient data to those with insufficient data, but they tend to require
additional training or a domain-specific method of transfer. We present a new
approach, conceptual expansion, that serves as a general representation for
reusing existing trained models to derive new models without backpropagation.
We evaluate our approach on few-shot variations of two tasks: image
classification and image generation, and outperform standard transfer learning
approaches.
",0,0,0,1,0,0
1310,Siamese Network of Deep Fisher-Vector Descriptors for Image Retrieval,"  This paper addresses the problem of large scale image retrieval, with the aim
of accurately ranking the similarity of a large number of images to a given
query image. To achieve this, we propose a novel Siamese network. This network
consists of two computational strands, each comprising of a CNN component
followed by a Fisher vector component. The CNN component produces dense, deep
convolutional descriptors that are then aggregated by the Fisher Vector method.
Crucially, we propose to simultaneously learn both the CNN filter weights and
Fisher Vector model parameters. This allows us to account for the evolving
distribution of deep descriptors over the course of the learning process. We
show that the proposed approach gives significant improvements over the
state-of-the-art methods on the Oxford and Paris image retrieval datasets.
Additionally, we provide a baseline performance measure for both these datasets
with the inclusion of 1 million distractors.
",1,0,0,0,0,0
1311,"Gender Disparities in Science? Dropout, Productivity, Collaborations and Success of Male and Female Computer Scientists","  Scientific collaborations shape ideas as well as innovations and are both the
substrate for, and the outcome of, academic careers. Recent studies show that
gender inequality is still present in many scientific practices ranging from
hiring to peer-review processes and grant applications. In this work, we
investigate gender-specific differences in collaboration patterns of more than
one million computer scientists over the course of 47 years. We explore how
these patterns change over years and career ages and how they impact scientific
success. Our results highlight that successful male and female scientists
reveal the same collaboration patterns: compared to scientists in the same
career age, they tend to collaborate with more colleagues than other
scientists, seek innovations as brokers and establish longer-lasting and more
repetitive collaborations. However, women are on average less likely to adapt
the collaboration patterns that are related with success, more likely to embed
into ego networks devoid of structural holes, and they exhibit stronger gender
homophily as well as a consistently higher dropout rate than men in all career
ages.
",1,1,0,0,0,0
1312,Estimating a network from multiple noisy realizations,"  Complex interactions between entities are often represented as edges in a
network. In practice, the network is often constructed from noisy measurements
and inevitably contains some errors. In this paper we consider the problem of
estimating a network from multiple noisy observations where edges of the
original network are recorded with both false positives and false negatives.
This problem is motivated by neuroimaging applications where brain networks of
a group of patients with a particular brain condition could be viewed as noisy
versions of an unobserved true network corresponding to the disease. The key to
optimally leveraging these multiple observations is to take advantage of
network structure, and here we focus on the case where the true network
contains communities. Communities are common in real networks in general and in
particular are believed to be presented in brain networks. Under a community
structure assumption on the truth, we derive an efficient method to estimate
the noise levels and the original network, with theoretical guarantees on the
convergence of our estimates. We show on synthetic networks that the
performance of our method is close to an oracle method using the true parameter
values, and apply our method to fMRI brain data, demonstrating that it
constructs stable and plausible estimates of the population network.
",0,0,1,1,0,0
1313,Autonomous drone race: A computationally efficient vision-based navigation and control strategy,"  Drone racing is becoming a popular sport where human pilots have to control
their drones to fly at high speed through complex environments and pass a
number of gates in a pre-defined sequence. In this paper, we develop an
autonomous system for drones to race fully autonomously using only onboard
resources. Instead of commonly used visual navigation methods, such as
simultaneous localization and mapping and visual inertial odometry, which are
computationally expensive for micro aerial vehicles (MAVs), we developed the
highly efficient snake gate detection algorithm for visual navigation, which
can detect the gate at 20HZ on a Parrot Bebop drone. Then, with the gate
detection result, we developed a robust pose estimation algorithm which has
better tolerance to detection noise than a state-of-the-art perspective-n-point
method. During the race, sometimes the gates are not in the drone's field of
view. For this case, a state prediction-based feed-forward control strategy is
developed to steer the drone to fly to the next gate. Experiments show that the
drone can fly a half-circle with 1.5m radius within 2 seconds with only 30cm
error at the end of the circle without any position feedback. Finally, the
whole system is tested in a complex environment (a showroom in the faculty of
Aerospace Engineering, TU Delft). The result shows that the drone can complete
the track of 15 gates with a speed of 1.5m/s which is faster than the speeds
exhibited at the 2016 and 2017 IROS autonomous drone races.
",1,0,0,0,0,0
1314,"Experimental investigations on nucleation, bubble growth, and micro-explosion characteristics during the combustion of ethanol/Jet A-1 fuel droplets","  The combustion characteristics of ethanol/Jet A-1 fuel droplets having three
different proportions of ethanol (10%, 30%, and 50% by vol.) are investigated
in the present study. The large volatility differential between ethanol and Jet
A-1 and the nominal immiscibility of the fuels seem to result in combustion
characteristics that are rather different from our previous work on butanol/Jet
A-1 droplets (miscible blends). Abrupt explosion was facilitated in fuel
droplets comprising lower proportions of ethanol (10%), possibly due to
insufficient nucleation sites inside the droplet and the partially unmixed fuel
mixture. For the fuel droplets containing higher proportions of ethanol (30%
and 50%), micro-explosion occurred through homogeneous nucleation, leading to
the ejection of secondary droplets and subsequent significant reduction in the
overall droplet lifetime. The rate of bubble growth is nearly similar in all
the blends of ethanol; however, the evolution of ethanol vapor bubble is
significantly faster than that of a vapor bubble in the blends of butanol. The
probability of disruptive behavior is considerably higher in ethanol/Jet A-1
blends than that of butanol/Jet A-1 blends. The Sauter mean diameter of the
secondary droplets produced from micro-explosion is larger for blends with a
higher proportion of ethanol. Both abrupt explosion and micro-explosion create
a large-scale distortion of the flame, which surrounds the parent droplet. The
secondary droplets generated from abrupt explosion undergo rapid evaporation
whereas the secondary droplets from micro-explosion carry their individual
flame and evaporate slowly. The growth of vapor bubble was also witnessed in
the secondary droplets, which leads to the further breakup of the droplet
(puffing/micro-explosion).
",0,1,0,0,0,0
1315,Hypergraph $p$-Laplacian: A Differential Geometry View,"  The graph Laplacian plays key roles in information processing of relational
data, and has analogies with the Laplacian in differential geometry. In this
paper, we generalize the analogy between graph Laplacian and differential
geometry to the hypergraph setting, and propose a novel hypergraph
$p$-Laplacian. Unlike the existing two-node graph Laplacians, this
generalization makes it possible to analyze hypergraphs, where the edges are
allowed to connect any number of nodes. Moreover, we propose a semi-supervised
learning method based on the proposed hypergraph $p$-Laplacian, and formalize
them as the analogue to the Dirichlet problem, which often appears in physics.
We further explore theoretical connections to normalized hypergraph cut on a
hypergraph, and propose normalized cut corresponding to hypergraph
$p$-Laplacian. The proposed $p$-Laplacian is shown to outperform standard
hypergraph Laplacians in the experiment on a hypergraph semi-supervised
learning and normalized cut setting.
",1,0,0,1,0,0
1316,Controlling motile disclinations in a thick nematogenic material with an electric field,"  Manipulating topological disclination networks that arise in a
symmetry-breaking phase transfor- mation in widely varied systems including
anisotropic materials can potentially lead to the design of novel materials
like conductive microwires, self-assembled resonators, and active anisotropic
matter. However, progress in this direction is hindered by a lack of control of
the kinetics and microstructure due to inherent complexity arising from
competing energy and topology. We have studied thermal and electrokinetic
effects on disclinations in a three-dimensional nonabsorbing nematic material
with a positive and negative sign of the dielectric anisotropy. The electric
flux lines are highly non-uniform in uniaxial media after an electric field
below the Fr??edericksz threshold is switched on, and the kinetics of the
disclination lines is slowed down. In biaxial media, depending on the sign of
the dielectric anisotropy, apart from the slowing down of the disclination
kinetics, a non-uniform electric field filters out disclinations of different
topology by inducing a kinetic asymmetry. These results enhance the current
understanding of forced disclination networks and establish the pre- sented
method, which we call fluctuating electronematics, as a potentially useful tool
for designing materials with novel properties in silico.
",0,1,0,0,0,0
1317,How Generative Adversarial Networks and Their Variants Work: An Overview,"  Generative Adversarial Networks (GAN) have received wide attention in the
machine learning field for their potential to learn high-dimensional, complex
real data distribution. Specifically, they do not rely on any assumptions about
the distribution and can generate real-like samples from latent space in a
simple manner. This powerful property leads GAN to be applied to various
applications such as image synthesis, image attribute editing, image
translation, domain adaptation and other academic fields. In this paper, we aim
to discuss the details of GAN for those readers who are familiar with, but do
not comprehend GAN deeply or who wish to view GAN from various perspectives. In
addition, we explain how GAN operates and the fundamental meaning of various
objective functions that have been suggested recently. We then focus on how the
GAN can be combined with an autoencoder framework. Finally, we enumerate the
GAN variants that are applied to various tasks and other fields for those who
are interested in exploiting GAN for their research.
",1,0,0,0,0,0
1318,Principal Boundary on Riemannian Manifolds,"  We revisit the classification problem and focus on nonlinear methods for
classification on manifolds. For multivariate datasets lying on an embedded
nonlinear Riemannian manifold within the higher-dimensional space, our aim is
to acquire a classification boundary between the classes with labels. Motivated
by the principal flow [Panaretos, Pham and Yao, 2014], a curve that moves along
a path of the maximum variation of the data, we introduce the principal
boundary. From the classification perspective, the principal boundary is
defined as an optimal curve that moves in between the principal flows traced
out from two classes of the data, and at any point on the boundary, it
maximizes the margin between the two classes. We estimate the boundary in
quality with its direction supervised by the two principal flows. We show that
the principal boundary yields the usual decision boundary found by the support
vector machine, in the sense that locally, the two boundaries coincide. By
means of examples, we illustrate how to find, use and interpret the principal
boundary.
",1,0,0,1,0,0
1319,Exploiting network topology for large-scale inference of nonlinear reaction models,"  The development of chemical reaction models aids understanding and prediction
in areas ranging from biology to electrochemistry and combustion. A systematic
approach to building reaction network models uses observational data not only
to estimate unknown parameters, but also to learn model structure. Bayesian
inference provides a natural approach to this data-driven construction of
models. Yet traditional Bayesian model inference methodologies that numerically
evaluate the evidence for each model are often infeasible for nonlinear
reaction network inference, as the number of plausible models can be
combinatorially large. Alternative approaches based on model-space sampling can
enable large-scale network inference, but their realization presents many
challenges. In this paper, we present new computational methods that make
large-scale nonlinear network inference tractable. First, we exploit the
topology of networks describing potential interactions among chemical species
to design improved ""between-model"" proposals for reversible-jump Markov chain
Monte Carlo. Second, we introduce a sensitivity-based determination of move
types which, when combined with network-aware proposals, yields significant
additional gains in sampling performance. These algorithms are demonstrated on
inference problems drawn from systems biology, with nonlinear differential
equation models of species interactions.
",1,0,0,1,0,0
1320,Numerical investigations of non-uniqueness for the Navier-Stokes initial value problem in borderline spaces,"  We consider the Cauchy problem for the incompressible Navier-Stokes equations
in $\mathbb{R}^3$ for a one-parameter family of explicit scale-invariant
axi-symmetric initial data, which is smooth away from the origin and invariant
under the reflection with respect to the $xy$-plane. Working in the class of
axi-symmetric fields, we calculate numerically scale-invariant solutions of the
Cauchy problem in terms of their profile functions, which are smooth. The
solutions are necessarily unique for small data, but for large data we observe
a breaking of the reflection symmetry of the initial data through a
pitchfork-type bifurcation. By a variation of previous results by Jia &
ÿver?­k (2013) it is known rigorously that if the behavior seen here
numerically can be proved, optimal non-uniqueness examples for the Cauchy
problem can be established, and two different solutions can exists for the same
initial datum which is divergence-free, smooth away from the origin, compactly
supported, and locally $(-1)$-homogeneous near the origin. In particular,
assuming our (finite-dimensional) numerics represents faithfully the behavior
of the full (infinite-dimensional) system, the problem of uniqueness of the
Leray-Hopf solutions (with non-smooth initial data) has a negative answer and,
in addition, the perturbative arguments such those by Kato (1984) and Koch &
Tataru (2001), or the weak-strong uniqueness results by Leray, Prodi, Serrin,
Ladyzhenskaya and others, already give essentially optimal results. There are
no singularities involved in the numerics, as we work only with smooth profile
functions. It is conceivable that our calculations could be upgraded to a
computer-assisted proof, although this would involve a substantial amount of
additional work and calculations, including a much more detailed analysis of
the asymptotic expansions of the solutions at large distances.
",0,1,1,0,0,0
1321,Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics,"  Inspired by the success of deep learning techniques in the physical and
chemical sciences, we apply a modification of an autoencoder type deep neural
network to the task of dimension reduction of molecular dynamics data. We can
show that our time-lagged autoencoder reliably finds low-dimensional embeddings
for high-dimensional feature spaces which capture the slow dynamics of the
underlying stochastic processes - beyond the capabilities of linear dimension
reduction techniques.
",1,1,0,1,0,0
1322,Some integrable maps and their Hirota bilinear forms,"  We introduce a two-parameter family of birational maps, which reduces to a
family previously found by Demskoi, Tran, van der Kamp and Quispel (DTKQ) when
one of the parameters is set to zero. The study of the singularity confinement
pattern for these maps leads to the introduction of a tau function satisfying a
homogeneous recurrence which has the Laurent property, and the tropical (or
ultradiscrete) analogue of this homogeneous recurrence confirms the quadratic
degree growth found empirically by Demskoi et al. We prove that the tau
function also satisfies two different bilinear equations, each of which is a
reduction of the Hirota-Miwa equation (also known as the discrete KP equation,
or the octahedron recurrence). Furthermore, these bilinear equations are
related to reductions of particular two-dimensional integrable lattice
equations, of discrete KdV or discrete Toda type. These connections, as well as
the cluster algebra structure of the bilinear equations, allow a direct
construction of Poisson brackets, Lax pairs and first integrals for the
birational maps. As a consequence of the latter results, we show how each
member of the family can be lifted to a system that is integrable in the
Liouville sense, clarifying observations made previously in the original DTKQ
case.
",0,1,0,0,0,0
1323,Dynamics of higher-order rational solitons for the nonlocal nonlinear Schrodinger equation with the self-induced parity-time-symmetric potential,"  The integrable nonlocal nonlinear Schrodinger (NNLS) equation with the
self-induced parity-time-symmetric potential [Phys. Rev. Lett. 110 (2013)
064105] is investigated, which is an integrable extension of the standard NLS
equation. Its novel higher-order rational solitons are found using the nonlocal
version of the generalized perturbation (1, N-1)-fold Darboux transformation.
These rational solitons illustrate abundant wave structures for the distinct
choices of parameters (e.g., the strong and weak interactions of bright and
dark rational solitons). Moreover, we also explore the dynamical behaviors of
these higher-order rational solitons with some small noises on the basis of
numerical simulations.
",0,1,1,0,0,0
1324,N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning,"  While bigger and deeper neural network architectures continue to advance the
state-of-the-art for many computer vision tasks, real-world adoption of these
networks is impeded by hardware and speed constraints. Conventional model
compression methods attempt to address this problem by modifying the
architecture manually or using pre-defined heuristics. Since the space of all
reduced architectures is very large, modifying the architecture of a deep
neural network in this way is a difficult task. In this paper, we tackle this
issue by introducing a principled method for learning reduced network
architectures in a data-driven way using reinforcement learning. Our approach
takes a larger `teacher' network as input and outputs a compressed `student'
network derived from the `teacher' network. In the first stage of our method, a
recurrent policy network aggressively removes layers from the large `teacher'
model. In the second stage, another recurrent policy network carefully reduces
the size of each remaining layer. The resulting network is then evaluated to
obtain a reward -- a score based on the accuracy and compression of the
network. Our approach uses this reward signal with policy gradients to train
the policies to find a locally optimal student network. Our experiments show
that we can achieve compression rates of more than 10x for models such as
ResNet-34 while maintaining similar performance to the input `teacher' network.
We also present a valuable transfer learning result which shows that policies
which are pre-trained on smaller `teacher' networks can be used to rapidly
speed up training on larger `teacher' networks.
",1,0,0,1,0,0
1325,Spectral analysis of stationary random bivariate signals,"  A novel approach towards the spectral analysis of stationary random bivariate
signals is proposed. Using the Quaternion Fourier Transform, we introduce a
quaternion-valued spectral representation of random bivariate signals seen as
complex-valued sequences. This makes possible the definition of a scalar
quaternion-valued spectral density for bivariate signals. This spectral density
can be meaningfully interpreted in terms of frequency-dependent polarization
attributes. A natural decomposition of any random bivariate signal in terms of
unpolarized and polarized components is introduced. Nonparametric spectral
density estimation is investigated, and we introduce the polarization
periodogram of a random bivariate signal. Numerical experiments support our
theoretical analysis, illustrating the relevance of the approach on synthetic
data.
",0,0,0,1,0,0
1326,Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition,"  We propose a method (TT-GP) for approximate inference in Gaussian Process
(GP) models. We build on previous scalable GP research including stochastic
variational inference based on inducing inputs, kernel interpolation, and
structure exploiting algebra. The key idea of our method is to use Tensor Train
decomposition for variational parameters, which allows us to train GPs with
billions of inducing inputs and achieve state-of-the-art results on several
benchmarks. Further, our approach allows for training kernels based on deep
neural networks without any modifications to the underlying GP model. A neural
network learns a multidimensional embedding for the data, which is used by the
GP to make the final prediction. We train GP and neural network parameters
end-to-end without pretraining, through maximization of GP marginal likelihood.
We show the efficiency of the proposed approach on several regression and
classification benchmark datasets including MNIST, CIFAR-10, and Airline.
",1,0,0,1,0,0
1327,Some preliminary results on the set of principal congruences of a finite lattice,"  In the second edition of the congruence lattice book, Problem 22.1 asks for a
characterization of subsets $Q$ of a finite distributive lattice $D$ such that
there is a finite lattice $L$ whose congruence lattice is isomorphic to $D$ and
under this isomorphism $Q$ corresponds the the principal congruences of $L$. In
this note, we prove some preliminary results.
",0,0,1,0,0,0
1328,Extracting 3D Vascular Structures from Microscopy Images using Convolutional Recurrent Networks,"  Vasculature is known to be of key biological significance, especially in the
study of cancer. As such, considerable effort has been focused on the automated
measurement and analysis of vasculature in medical and pre-clinical images. In
tumors in particular, the vascular networks may be extremely irregular and the
appearance of the individual vessels may not conform to classical descriptions
of vascular appearance. Typically, vessels are extracted by either a
segmentation and thinning pipeline, or by direct tracking. Neither of these
methods are well suited to microscopy images of tumor vasculature. In order to
address this we propose a method to directly extract a medial representation of
the vessels using Convolutional Neural Networks. We then show that these
two-dimensional centerlines can be meaningfully extended into 3D in anisotropic
and complex microscopy images using the recently popularized Convolutional Long
Short-Term Memory units (ConvLSTM). We demonstrate the effectiveness of this
hybrid convolutional-recurrent architecture over both 2D and 3D convolutional
comparators.
",1,0,0,0,0,0
1329,Search for Interstellar LiH in the Milky Way,"  We report the results of a sensitive search for the 443.952902 GHz $J=1-0$
transition of the LiH molecule toward two interstellar clouds in the Milky Way,
W49N and Sgr B2 (Main), that has been carried out using the Atacama Pathfinder
Experiment (APEX) telescope. The results obtained toward W49N place an upper
limit of $1.9 \times 10^{-11}\, (3\sigma)$ on the LiH abundance, $N({\rm
LiH})/N({\rm H}_2)$, in a foreground, diffuse molecular cloud along the
sight-line to W49N, corresponding to 0.5% of the solar system lithium
abundance. Those obtained toward Sgr B2 (Main) place an abundance limit $N({\rm
LiH})/N({\rm H}_2) < 3.6 \times 10^{-13} \,(3\sigma)$ in the dense gas within
the Sgr B2 cloud itself. These limits are considerably smaller that those
implied by the tentative detection of LiH reported previously for the $z=0.685$
absorber toward B0218+357.
",0,1,0,0,0,0
1330,Neural Probabilistic Model for Non-projective MST Parsing,"  In this paper, we propose a probabilistic parsing model, which defines a
proper conditional probability distribution over non-projective dependency
trees for a given sentence, using neural representations as inputs. The neural
network architecture is based on bi-directional LSTM-CNNs which benefits from
both word- and character-level representations automatically, by using
combination of bidirectional LSTM and CNN. On top of the neural network, we
introduce a probabilistic structured layer, defining a conditional log-linear
model over non-projective trees. We evaluate our model on 17 different
datasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree
Theorem (Tutte, 1984), the partition functions and marginals can be computed
efficiently, leading to a straight-forward end-to-end model training procedure
via back-propagation. Our parser achieves state-of-the-art parsing performance
on nine datasets.
",1,0,0,1,0,0
1331,Modularity of complex networks models,"  Modularity is designed to measure the strength of division of a network into
clusters (known also as communities). Networks with high modularity have dense
connections between the vertices within clusters but sparse connections between
vertices of different clusters. As a result, modularity is often used in
optimization methods for detecting community structure in networks, and so it
is an important graph parameter from a practical point of view. Unfortunately,
many existing non-spatial models of complex networks do not generate graphs
with high modularity; on the other hand, spatial models naturally create
clusters. We investigate this phenomenon by considering a few examples from
both sub-classes. We prove precise theoretical results for the classical model
of random d-regular graphs as well as the preferential attachment model, and
contrast these results with the ones for the spatial preferential attachment
(SPA) model that is a model for complex networks in which vertices are embedded
in a metric space, and each vertex has a sphere of influence whose size
increases if the vertex gains an in-link, and otherwise decreases with time.
The results obtained in this paper can be used for developing statistical tests
for models selection and to measure statistical significance of clusters
observed in complex networks.
",0,0,1,0,0,0
1332,BOLD5000: A public fMRI dataset of 5000 images,"  Vision science, particularly machine vision, has been revolutionized by
introducing large-scale image datasets and statistical learning approaches.
Yet, human neuroimaging studies of visual perception still rely on small
numbers of images (around 100) due to time-constrained experimental procedures.
To apply statistical learning approaches that integrate neuroscience, the
number of images used in neuroimaging must be significantly increased. We
present BOLD5000, a human functional MRI (fMRI) study that includes almost
5,000 distinct images depicting real-world scenes. Beyond dramatically
increasing image dataset size relative to prior fMRI studies, BOLD5000 also
accounts for image diversity, overlapping with standard computer vision
datasets by incorporating images from the Scene UNderstanding (SUN), Common
Objects in Context (COCO), and ImageNet datasets. The scale and diversity of
these image datasets, combined with a slow event-related fMRI design, enable
fine-grained exploration into the neural representation of a wide range of
visual features, categories, and semantics. Concurrently, BOLD5000 brings us
closer to realizing Marr's dream of a singular vision science - the intertwined
study of biological and computer vision.
",0,0,0,0,1,0
1333,Prospects for gravitational wave astronomy with next generation large-scale pulsar timing arrays,"  Next generation radio telescopes, namely the Five-hundred-meter Aperture
Spherical Telescope (FAST) and the Square Kilometer Array (SKA), will
revolutionize the pulsar timing arrays (PTAs) based gravitational wave (GW)
searches. We review some of the characteristics of FAST and SKA, and the
resulting PTAs, that are pertinent to the detection of gravitational wave
signals from individual supermassive black hole binaries.
",0,1,0,0,0,0
1334,On Identifiability of Nonnegative Matrix Factorization,"  In this letter, we propose a new identification criterion that guarantees the
recovery of the low-rank latent factors in the nonnegative matrix factorization
(NMF) model, under mild conditions. Specifically, using the proposed criterion,
it suffices to identify the latent factors if the rows of one factor are
\emph{sufficiently scattered} over the nonnegative orthant, while no structural
assumption is imposed on the other factor except being full-rank. This is by
far the mildest condition under which the latent factors are provably
identifiable from the NMF model.
",1,0,0,1,0,0
1335,Optimal Non-blocking Decentralized Supervisory Control Using G-Control Consistency,"  Supervisory control synthesis encounters with computational complexity. This
can be reduced by decentralized supervisory control approach. In this paper, we
define intrinsic control consistency for a pair of states of the plant.
G-control consistency (GCC) is another concept which is defined for a natural
projection w.r.t. the plant. We prove that, if a natural projection is output
control consistent for the closed language of the plant, and is a natural
observer for the marked language of the plant, then it is G-control consistent.
Namely, we relax the conditions for synthesis the optimal non-blocking
decentralized supervisory control by substituting GCC property for L-OCC and
Lm-observer properties of a natural projection. We propose a method to
synthesize the optimal non-blocking decentralized supervisory control based on
GCC property for a natural projection. In fact, we change the approach from
language-based properties of a natural projection to DES-based property by
defining GCC property.
",1,0,0,0,0,0
1336,Fair mixing: the case of dichotomous preferences,"  Agents vote to choose a fair mixture of public outcomes; each agent likes or
dislikes each outcome. We discuss three outstanding voting rules. The
Conditional Utilitarian rule, a variant of the random dictator, is
Strategyproof and guarantees to any group of like-minded agents an influence
proportional to its size. It is easier to compute and more efficient than the
familiar Random Priority rule. Its worst case (resp. average) inefficiency is
provably (resp. in numerical experiments) low if the number of agents is low.
The efficient Egalitarian rule protects similarly individual agents but not
coalitions. It is Excludable Strategyproof: I do not want to lie if I cannot
consume outcomes I claim to dislike. The efficient Nash Max Product rule offers
the strongest welfare guarantees to coalitions, who can force any outcome with
a probability proportional to their size. But it fails even the excludable form
of Strategyproofness.
",1,0,0,0,0,0
1337,Inverse system characterizations of the (hereditarily) just infinite property in profinite groups,"  We give criteria on an inverse system of finite groups that ensure the limit
is just infinite or hereditarily just infinite. More significantly, these
criteria are 'universal' in that all (hereditarily) just infinite profinite
groups arise as limits of the specified form.
This is a corrected and revised version of the article: 'Inverse system
characterizations of the (hereditarily) just infinite property in profinite
groups', Bull. LMS vol 44, 3 (2012) 413--425.
",0,0,1,0,0,0
1338,"p-FP: Extraction, Classification, and Prediction of Website Fingerprints with Deep Learning","  Recent advances in learning Deep Neural Network (DNN) architectures have
received a great deal of attention due to their ability to outperform
state-of-the-art classifiers across a wide range of applications, with little
or no feature engineering. In this paper, we broadly study the applicability of
deep learning to website fingerprinting. We show that unsupervised DNNs can be
used to extract low-dimensional feature vectors that improve the performance of
state-of-the-art website fingerprinting attacks. When used as classifiers, we
show that they can match or exceed performance of existing attacks across a
range of application scenarios, including fingerprinting Tor website traces,
fingerprinting search engine queries over Tor, defeating fingerprinting
defenses, and fingerprinting TLS-encrypted websites. Finally, we show that DNNs
can be used to predict the fingerprintability of a website based on its
contents, achieving 99% accuracy on a data set of 4500 website downloads.
",1,0,0,1,0,0
1339,Equal confidence weighted expectation value estimates,"  In this article the issues are discussed with the Bayesian approach,
least-square fits, and most-likely fits. Trying to counter these issues, a
method, based on weighted confidence, is proposed for estimating probabilities
and other observables. This method sums over different model parameter
combinations but does not require the need for making assumptions on priors or
underlying probability functions. Moreover, by construction the results are
invariant under reparametrization of the model parameters. In one case the
result appears similar as in Bayesian statistics but in general there is no
agreement. The binomial distribution is also studied which turns out to be
useful for making predictions on production processes without the need to make
further assumptions. In the last part, the case of a simple linear fit (a
multi-variate example) is studied using the standard approaches and the
confidence weighted approach.
",0,0,1,1,0,0
1340,Protein Folding and Machine Learning: Fundamentals,"  In spite of decades of research, much remains to be discovered about folding:
the detailed structure of the initial (unfolded) state, vestigial folding
instructions remaining only in the unfolded state, the interaction of the
molecule with the solvent, instantaneous power at each point within the
molecule during folding, the fact that the process is stable in spite of myriad
possible disturbances, potential stabilization of trajectory by chaos, and, of
course, the exact physical mechanism (code or instructions) by which the
folding process is specified in the amino acid sequence. Simulations based upon
microscopic physics have had some spectacular successes and continue to
improve, particularly as super-computer capabilities increase. The simulations,
exciting as they are, are still too slow and expensive to deal with the
enormous number of molecules of interest. In this paper, we introduce an
approximate model based upon physics, empirics, and information science which
is proposed for use in machine learning applications in which very large
numbers of sub-simulations must be made. In particular, we focus upon machine
learning applications in the learning phase and argue that our model is
sufficiently close to the physics that, in spite of its approximate nature, can
facilitate stepping through machine learning solutions to explore the mechanics
of folding mentioned above. We particularly emphasize the exploration of energy
flow (power) within the molecule during folding, the possibility of energy
scale invariance (above a threshold), vestigial information in the unfolded
state as attractive targets for such machine language analysis, and statistical
analysis of an ensemble of folding micro-steps.
",0,0,0,0,1,0
1341,Discrete configuration spaces of squares and hexagons,"  We consider generalizations of the familiar fifteen-piece sliding puzzle on
the 4 by 4 square grid. On larger grids with more pieces and more holes,
asymptotically how fast can we move the puzzle into the solved state? We also
give a variation with sliding hexagons. The square puzzles and the hexagon
puzzles are both discrete versions of configuration spaces of disks, which are
of interest in statistical mechanics and topological robotics. The
combinatorial theorems and proofs in this paper suggest followup questions in
both combinatorics and topology, and may turn out to be useful for proving
topological statements about configuration spaces.
",1,0,1,0,0,0
1342,On the non commutative Iwasawa main conjecture for abelian varieties over function fields,"  We establish the Iwasawa main conjecture for semi-stable abelian varieties
over a function field of characteristic $p$ under certain restrictive
assumptions. Namely we consider $p$-torsion free $p$-adic Lie extensions of the
base field which contain the constant $\mathbb Z_p$-extension and are
everywhere unramified. Under the classical $\mu=0$ hypothesis we give a proof
which mainly relies on the interpretation of the Selmer complex in terms of
$p$-adic cohomology [TV] together with the trace formulas of [EL1].
",0,0,1,0,0,0
1343,Absorption and Emission Probabilities of Electrons in Electric and Magnetic Fields for FEL,"  We consider induced emission of ultrarelativistic electrons in strong
electric (magnetic) fields that are uniform along the direction of the electron
motion and are not uniform in the transverse direction. The stimulated
absorption and emission probabilities are found in such system.
",0,1,0,0,0,0
1344,"Design, Development and Evaluation of a UAV to Study Air Quality in Qatar","  Measuring gases for air quality monitoring is a challenging task that claims
a lot of time of observation and large numbers of sensors. The aim of this
project is to develop a partially autonomous unmanned aerial vehicle (UAV)
equipped with sensors, in order to monitor and collect air quality real time
data in designated areas and send it to the ground base. This project is
designed and implemented by a multidisciplinary team from electrical and
computer engineering departments. The electrical engineering team responsible
for implementing air quality sensors for detecting real time data and transmit
it from the plane to the ground. On the other hand, the computer engineering
team is in charge of Interface sensors and provide platform to view and
visualize air quality data and live video streaming. The proposed project
contains several sensors to measure Temperature, Humidity, Dust, CO, CO2 and
O3. The collected data is transmitted to a server over a wireless internet
connection and the server will store, and supply these data to any party who
has permission to access it through android phone or website in semi-real time.
The developed UAV has carried several field tests in Al Shamal airport in
Qatar, with interesting results and proof of concept outcomes.
",1,0,0,0,0,0
1345,Gaussian Process bandits with adaptive discretization,"  In this paper, the problem of maximizing a black-box function $f:\mathcal{X}
\to \mathbb{R}$ is studied in the Bayesian framework with a Gaussian Process
(GP) prior. In particular, a new algorithm for this problem is proposed, and
high probability bounds on its simple and cumulative regret are established.
The query point selection rule in most existing methods involves an exhaustive
search over an increasingly fine sequence of uniform discretizations of
$\mathcal{X}$. The proposed algorithm, in contrast, adaptively refines
$\mathcal{X}$ which leads to a lower computational complexity, particularly
when $\mathcal{X}$ is a subset of a high dimensional Euclidean space. In
addition to the computational gains, sufficient conditions are identified under
which the regret bounds of the new algorithm improve upon the known results.
Finally an extension of the algorithm to the case of contextual bandits is
proposed, and high probability bounds on the contextual regret are presented.
",1,0,0,1,0,0
1346,Conditional Time Series Forecasting with Convolutional Neural Networks,"  We present a method for conditional time series forecasting based on an
adaptation of the recent deep convolutional WaveNet architecture. The proposed
network contains stacks of dilated convolutions that allow it to access a broad
range of history when forecasting, a ReLU activation function and conditioning
is performed by applying multiple convolutional filters in parallel to separate
time series which allows for the fast processing of data and the exploitation
of the correlation structure between the multivariate time series. We test and
analyze the performance of the convolutional network both unconditionally as
well as conditionally for financial time series forecasting using the S&P500,
the volatility index, the CBOE interest rate and several exchange rates and
extensively compare it to the performance of the well-known autoregressive
model and a long-short term memory network. We show that a convolutional
network is well-suited for regression-type problems and is able to effectively
learn dependencies in and between the series without the need for long
historical time series, is a time-efficient and easy to implement alternative
to recurrent-type networks and tends to outperform linear and recurrent models.
",0,0,0,1,0,0
1347,Automated Assistants to Identify and Prompt Action on Visual News Bias,"  Bias is a common problem in today's media, appearing frequently in text and
in visual imagery. Users on social media websites such as Twitter need better
methods for identifying bias. Additionally, activists --those who are motivated
to effect change related to some topic, need better methods to identify and
counteract bias that is contrary to their mission. With both of these use cases
in mind, in this paper we propose a novel tool called UnbiasedCrowd that
supports identification of, and action on bias in visual news media. In
particular, it addresses the following key challenges (1) identification of
bias; (2) aggregation and presentation of evidence to users; (3) enabling
activists to inform the public of bias and take action by engaging people in
conversation with bots. We describe a preliminary study on the Twitter platform
that explores the impressions that activists had of our tool, and how people
reacted and engaged with online bots that exposed visual bias. We conclude by
discussing design and implication of our findings for creating future systems
to identify and counteract the effects of news bias.
",1,0,0,0,0,0
1348,A Matched Filter Technique For Slow Radio Transient Detection And First Demonstration With The Murchison Widefield Array,"  Many astronomical sources produce transient phenomena at radio frequencies,
but the transient sky at low frequencies (<300 MHz) remains relatively
unexplored. Blind surveys with new widefield radio instruments are setting
increasingly stringent limits on the transient surface density on various
timescales. Although many of these instruments are limited by classical
confusion noise from an ensemble of faint, unresolved sources, one can in
principle detect transients below the classical confusion limit to the extent
that the classical confusion noise is independent of time. We develop a
technique for detecting radio transients that is based on temporal matched
filters applied directly to time series of images rather than relying on
source-finding algorithms applied to individual images. This technique has
well-defined statistical properties and is applicable to variable and transient
searches for both confusion-limited and non-confusion-limited instruments.
Using the Murchison Widefield Array as an example, we demonstrate that the
technique works well on real data despite the presence of classical confusion
noise, sidelobe confusion noise, and other systematic errors. We searched for
transients lasting between 2 minutes and 3 months. We found no transients and
set improved upper limits on the transient surface density at 182 MHz for flux
densities between ~20--200 mJy, providing the best limits to date for hour- and
month-long transients.
",0,1,0,0,0,0
1349,Shape and Energy Consistent Pseudopotentials for Correlated Electron systems,"  A method is developed for generating pseudopotentials for use in
correlated-electron calculations. The paradigms of shape and energy consistency
are combined and defined in terms of correlated-electron wave-functions. The
resulting energy consistent correlated electron pseudopotentials (eCEPPs) are
constructed for H, Li--F, Sc--Fe, and Cu. Their accuracy is quantified by
comparing the relaxed molecular geometries and dissociation energies they
provide with all electron results, with all quantities evaluated using coupled
cluster singles doubles and triples calculations. Errors inherent in the
pseudopotentials are also compared with those arising from a number of
approximations commonly used with pseudopotentials. The eCEPPs provide a
significant improvement in optimised geometries and dissociation energies for
small molecules, with errors for the latter being an order-of-magnitude smaller
than for Hartree-Fock-based pseudopotentials available in the literature.
Gaussian basis sets are optimised for use with these pseudopotentials.
",0,1,0,0,0,0
1350,Bayes model selection,"  We offer a general Bayes theoretic framework to tackle the model selection
problem under a two-step prior design: the first-step prior serves to assess
the model selection uncertainty, and the second-step prior quantifies the prior
belief on the strength of the signals within the model chosen from the first
step.
We establish non-asymptotic oracle posterior contraction rates under (i) a
new Bernstein-inequality condition on the log likelihood ratio of the
statistical experiment, (ii) a local entropy condition on the dimensionality of
the models, and (iii) a sufficient mass condition on the second-step prior near
the best approximating signal for each model. The first-step prior can be
designed generically. The resulting posterior mean also satisfies an oracle
inequality, thus automatically serving as an adaptive point estimator in a
frequentist sense. Model mis-specification is allowed in these oracle rates.
The new Bernstein-inequality condition not only eliminates the convention of
constructing explicit tests with exponentially small type I and II errors, but
also suggests the intrinsic metric to use in a given statistical experiment,
both as a loss function and as an entropy measurement. This gives a unified
reduction scheme for many experiments considered in Ghoshal & van der
Vaart(2007) and beyond. As an illustration for the scope of our general results
in concrete applications, we consider (i) trace regression, (ii)
shape-restricted isotonic/convex regression, (iii) high-dimensional partially
linear regression and (iv) covariance matrix estimation in the sparse factor
model. These new results serve either as theoretical justification of practical
prior proposals in the literature, or as an illustration of the generic
construction scheme of a (nearly) minimax adaptive estimator for a
multi-structured experiment.
",0,0,1,1,0,0
1351,Phase Congruency Parameter Optimization for Enhanced Detection of Image Features for both Natural and Medical Applications,"  Following the presentation and proof of the hypothesis that image features
are particularly perceived at points where the Fourier components are maximally
in phase, the concept of phase congruency (PC) is introduced. Subsequently, a
two-dimensional multi-scale phase congruency (2D-MSPC) is developed, which has
been an important tool for detecting and evaluation of image features. However,
the 2D-MSPC requires many parameters to be appropriately tuned for optimal
image features detection. In this paper, we defined a criterion for parameter
optimization of the 2D-MSPC, which is a function of its maximum and minimum
moments. We formulated the problem in various optimal and suboptimal
frameworks, and discussed the conditions and features of the suboptimal
solutions. The effectiveness of the proposed method was verified through
several examples, ranging from natural objects to medical images from patients
with a neurological disease, multiple sclerosis.
",1,0,1,0,0,0
1352,Community structure detection and evaluation during the pre- and post-ictal hippocampal depth recordings,"  Detecting and evaluating regions of brain under various circumstances is one
of the most interesting topics in computational neuroscience. However, the
majority of the studies on detecting communities of a functional connectivity
network of the brain is done on networks obtained from coherency attributes,
and not from correlation. This lack of studies, in part, is due to the fact
that many common methods for clustering graphs require the nodes of the network
to be `positively' linked together, a property that is guaranteed by a
coherency matrix, by definition. However, correlation matrices reveal more
information regarding how each pair of nodes are linked together. In this
study, for the first time we simultaneously examine four inherently different
network clustering methods (spectral, heuristic, and optimization methods)
applied to the functional connectivity networks of the CA1 region of the
hippocampus of an anaesthetized rat during pre-ictal and post-ictal states. The
networks are obtained from correlation matrices, and its results are compared
with the ones obtained by applying the same methods to coherency matrices. The
correlation matrices show a much finer community structure compared to the
coherency matrices. Furthermore, we examine the potential smoothing effect of
choosing various window sizes for computing the correlation/coherency matrices.
",1,0,0,0,1,0
1353,"Shapley effects for sensitivity analysis with correlated inputs: comparisons with Sobol' indices, numerical estimation and applications","  The global sensitivity analysis of a numerical model aims to quantify, by
means of sensitivity indices estimate, the contributions of each uncertain
input variable to the model output uncertainty. The so-called Sobol' indices,
which are based on the functional variance analysis, present a difficult
interpretation in the presence of statistical dependence between inputs. The
Shapley effect was recently introduced to overcome this problem as they
allocate the mutual contribution (due to correlation and interaction) of a
group of inputs to each individual input within the group.In this paper, using
several new analytical results, we study the effects of linear correlation
between some Gaussian input variables on Shapley effects, and compare these
effects to classical first-order and total Sobol' indices.This illustrates the
interest, in terms of sensitivity analysis setting and interpretation, of the
Shapley effects in the case of dependent inputs. We also investigate the
numerical convergence of the estimated Shapley effects. For the practical issue
of computationally demanding computer models, we show that the substitution of
the original model by a metamodel (here, kriging) makes it possible to estimate
these indices with precision at a reasonable computational cost.
",0,0,1,1,0,0
1354,Ridesourcing Car Detection by Transfer Learning,"  Ridesourcing platforms like Uber and Didi are getting more and more popular
around the world. However, unauthorized ridesourcing activities taking
advantages of the sharing economy can greatly impair the healthy development of
this emerging industry. As the first step to regulate on-demand ride services
and eliminate black market, we design a method to detect ridesourcing cars from
a pool of cars based on their trajectories. Since licensed ridesourcing car
traces are not openly available and may be completely missing in some cities
due to legal issues, we turn to transferring knowledge from public transport
open data, i.e, taxis and buses, to ridesourcing detection among ordinary
vehicles. We propose a two-stage transfer learning framework. In Stage 1, we
take taxi and bus data as input to learn a random forest (RF) classifier using
trajectory features shared by taxis/buses and ridesourcing/other cars. Then, we
use the RF to label all the candidate cars. In Stage 2, leveraging the subset
of high confident labels from the previous stage as input, we further learn a
convolutional neural network (CNN) classifier for ridesourcing detection, and
iteratively refine RF and CNN, as well as the feature set, via a co-training
process. Finally, we use the resulting ensemble of RF and CNN to identify the
ridesourcing cars in the candidate pool. Experiments on real car, taxi and bus
traces show that our transfer learning framework, with no need of a pre-labeled
ridesourcing dataset, can achieve similar accuracy as the supervised learning
methods.
",1,0,0,1,0,0
1355,A Semantic Cross-Species Derived Data Management Application,"  Managing dynamic information in large multi-site, multi-species, and
multi-discipline consortia is a challenging task for data management
applications. Often in academic research studies the goals for informatics
teams are to build applications that provide extract-transform-load (ETL)
functionality to archive and catalog source data that has been collected by the
research teams. In consortia that cross species and methodological or
scientific domains, building interfaces that supply data in a usable fashion
and make intuitive sense to scientists from dramatically different backgrounds
increases the complexity for developers. Further, reusing source data from
outside one's scientific domain is fraught with ambiguities in understanding
the data types, analysis methodologies, and how to combine the data with those
from other research teams. We report on the design, implementation, and
performance of a semantic data management application to support the NIMH
funded Conte Center at the University of California, Irvine. The Center is
testing a theory of the consequences of ""fragmented"" (unpredictable, high
entropy) early-life experiences on adolescent cognitive and emotional outcomes
in both humans and rodents. It employs cross-species neuroimaging, epigenomic,
molecular, and neuroanatomical approaches in humans and rodents to assess the
potential consequences of fragmented unpredictable experience on brain
structure and circuitry. To address this multi-technology, multi-species
approach, the system uses semantic web techniques based on the Neuroimaging
Data Model (NIDM) to facilitate data ETL functionality. We find this approach
enables a low-cost, easy to maintain, and semantically meaningful information
management system, enabling the diverse research teams to access and use the
data.
",1,0,0,0,0,0
1356,Retrosynthetic reaction prediction using neural sequence-to-sequence models,"  We describe a fully data driven model that learns to perform a retrosynthetic
reaction prediction task, which is treated as a sequence-to-sequence mapping
problem. The end-to-end trained model has an encoder-decoder architecture that
consists of two recurrent neural networks, which has previously shown great
success in solving other sequence-to-sequence prediction tasks such as machine
translation. The model is trained on 50,000 experimental reaction examples from
the United States patent literature, which span 10 broad reaction types that
are commonly used by medicinal chemists. We find that our model performs
comparably with a rule-based expert system baseline model, and also overcomes
certain limitations associated with rule-based expert systems and with any
machine learning approach that contains a rule-based expert system component.
Our model provides an important first step towards solving the challenging
problem of computational retrosynthetic analysis.
",1,0,0,1,0,0
1357,"Redshift, metallicity and size of two extended dwarf Irregular galaxies. A link between dwarf Irregulars and Ultra Diffuse Galaxies?","  We present the results of the spectroscopic and photometric follow-up of two
field galaxies that were selected as possible stellar counterparts of local
high velocity clouds. Our analysis shows that the two systems are distant (D>20
Mpc) dwarf irregular galaxies unrelated to the local HI clouds. However, the
newly derived distance and structural parameters reveal that the two galaxies
have luminosities and effective radii very similar to the recently identified
Ultra Diffuse Galaxies (UDGs). At odds with classical UDGs, they are remarkably
isolated, having no known giant galaxy within ~2.0 Mpc. Moreover, one of them
has a very high gas content compared to galaxies of similar stellar mass, with
a HI to stellar mass ratio M_HI/M_* ~90, typical of almost-dark dwarfs.
Expanding on this finding, we show that extended dwarf irregulars overlap the
distribution of UDGs in the M_V vs. log(r_e) plane and that the sequence
including dwarf spheroidals, dwarf irregulars and UDGs appears as continuously
populated in this plane.
",0,1,0,0,0,0
1358,Collapsing hyperk??hler manifolds,"  Given a projective hyperkahler manifold with a holomorphic Lagrangian
fibration, we prove that hyperkahler metrics with volume of the torus fibers
shrinking to zero collapse in the Gromov-Hausdorff sense (and smoothly away
from the singular fibers) to a compact metric space which is a half-dimensional
special Kahler manifold outside a singular set of real Hausdorff codimension 2
and is homeomorphic to the base projective space.
",0,0,1,0,0,0
1359,Fast amortized inference of neural activity from calcium imaging data with variational autoencoders,"  Calcium imaging permits optical measurement of neural activity. Since
intracellular calcium concentration is an indirect measurement of neural
activity, computational tools are necessary to infer the true underlying
spiking activity from fluorescence measurements. Bayesian model inversion can
be used to solve this problem, but typically requires either computationally
expensive MCMC sampling, or faster but approximate maximum-a-posteriori
optimization. Here, we introduce a flexible algorithmic framework for fast,
efficient and accurate extraction of neural spikes from imaging data. Using the
framework of variational autoencoders, we propose to amortize inference by
training a deep neural network to perform model inversion efficiently. The
recognition network is trained to produce samples from the posterior
distribution over spike trains. Once trained, performing inference amounts to a
fast single forward pass through the network, without the need for iterative
optimization or sampling. We show that amortization can be applied flexibly to
a wide range of nonlinear generative models and significantly improves upon the
state of the art in computation time, while achieving competitive accuracy. Our
framework is also able to represent posterior distributions over spike-trains.
We demonstrate the generality of our method by proposing the first
probabilistic approach for separating backpropagating action potentials from
putative synaptic inputs in calcium imaging of dendritic spines.
",1,0,0,1,0,0
1360,Hidden multiparticle excitation in weakly interacting Bose-Einstein Condensate,"  We investigate multiparticle excitation effect on a collective density
excitation as well as a single-particle excitation in a weakly interacting
Bose--Einstein condensate (BEC). We find that although the weakly interacting
BEC offers weak multiparticle excitation spectrum at low temperatures, this
multiparticle excitation effect may not remain hidden, but emerges as
bimodality in the density response function through the single-particle
excitation. Identification of spectra in the BEC between the single-particle
excitation and the density excitation is also assessed at nonzero temperatures,
which has been known to be unique nature in the BEC at absolute zero
temperature.
",0,1,0,0,0,0
1361,Hausdorff Measure: Lost in Translation,"  In the present article we describe how one can define Hausdorff measure
allowing empty elements in coverings, and using infinite countable coverings
only. In addition, we discuss how the use of different nonequivalent
interpretations of the notion ""countable set"", that is typical for classical
and modern mathematics, may lead to contradictions.
",0,0,1,0,0,0
1362,Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use,"  The popular Alternating Least Squares (ALS) algorithm for tensor
decomposition is efficient and easy to implement, but often converges to poor
local optima---particularly when the weights of the factors are non-uniform. We
propose a modification of the ALS approach that is as efficient as standard
ALS, but provably recovers the true factors with random initialization under
standard incoherence assumptions on the factors of the tensor. We demonstrate
the significant practical superiority of our approach over traditional ALS for
a variety of tasks on synthetic data---including tensor factorization on exact,
noisy and over-complete tensors, as well as tensor completion---and for
computing word embeddings from a third-order word tri-occurrence tensor.
",1,0,0,1,0,0
1363,Domain Generalization by Marginal Transfer Learning,"  Domain generalization is the problem of assigning class labels to an
unlabeled test data set, given several labeled training data sets drawn from
similar distributions. This problem arises in several applications where data
distributions fluctuate because of biological, technical, or other sources of
variation. We develop a distribution-free, kernel-based approach that predicts
a classifier from the marginal distribution of features, by leveraging the
trends present in related classification tasks. This approach involves
identifying an appropriate reproducing kernel Hilbert space and optimizing a
regularized empirical risk over the space. We present generalization error
analysis, describe universal kernels, and establish universal consistency of
the proposed methodology. Experimental results on synthetic data and three real
data applications demonstrate the superiority of the method with respect to a
pooling strategy.
",0,0,0,1,0,0
1364,(Non-)formality of the extended Swiss Cheese operads,"  We study two colored operads of configurations of little $n$-disks in a unit
$n$-disk, with the centers of the small disks of one color restricted to an
$m$-plane, $m<n$. We compute the rational homotopy type of these \emph{extended
Swiss Cheese operads} and show how they are connected to the rational homotopy
types of the inclusion maps from the little $m$-disks to the little $n$-disks
operad.
",0,0,1,0,0,0
1365,Pricing options and computing implied volatilities using neural networks,"  This paper proposes a data-driven approach, by means of an Artificial Neural
Network (ANN), to value financial options and to calculate implied volatilities
with the aim of accelerating the corresponding numerical methods. With ANNs
being universal function approximators, this method trains an optimized ANN on
a data set generated by a sophisticated financial model, and runs the trained
ANN as an agent of the original solver in a fast and efficient way. We test
this approach on three different types of solvers, including the analytic
solution for the Black-Scholes equation, the COS method for the Heston
stochastic volatility model and Brent's iterative root-finding method for the
calculation of implied volatilities. The numerical results show that the ANN
solver can reduce the computing time significantly.
",1,0,0,0,0,1
1366,Effect of magnetization on the tunneling anomaly in compressible quantum Hall states,"  Tunneling of electrons into a two-dimensional electron system is known to
exhibit an anomaly at low bias, in which the tunneling conductance vanishes due
to a many-body interaction effect. Recent experiments have measured this
anomaly between two copies of the half-filled Landau level as a function of
in-plane magnetic field, and they suggest that increasing spin polarization
drives a deeper suppression of tunneling. Here we present a theory of the
tunneling anomaly between two copies of the partially spin-polarized
Halperin-Lee-Read state, and we show that the conventional description of the
tunneling anomaly, based on the Coulomb self-energy of the injected charge
packet, is inconsistent with the experimental observation. We propose that the
experiment is operating in a different regime, not previously considered, in
which the charge-spreading action is determined by the compressibility of the
composite fermions.
",0,1,0,0,0,0
1367,Learning to Acquire Information,"  We consider the problem of diagnosis where a set of simple observations are
used to infer a potentially complex hidden hypothesis. Finding the optimal
subset of observations is intractable in general, thus we focus on the problem
of active diagnosis, where the agent selects the next most-informative
observation based on the results of previous observations. We show that under
the assumption of uniform observation entropy, one can build an implication
model which directly predicts the outcome of the potential next observation
conditioned on the results of past observations, and selects the observation
with the maximum entropy. This approach enjoys reduced computation complexity
by bypassing the complicated hypothesis space, and can be trained on
observation data alone, learning how to query without knowledge of the hidden
hypothesis.
",1,0,0,1,0,0
1368,How hard is it to cross the room? -- Training (Recurrent) Neural Networks to steer a UAV,"  This work explores the feasibility of steering a drone with a (recurrent)
neural network, based on input from a forward looking camera, in the context of
a high-level navigation task. We set up a generic framework for training a
network to perform navigation tasks based on imitation learning. It can be
applied to both aerial and land vehicles. As a proof of concept we apply it to
a UAV (Unmanned Aerial Vehicle) in a simulated environment, learning to cross a
room containing a number of obstacles. So far only feedforward neural networks
(FNNs) have been used to train UAV control. To cope with more complex tasks, we
propose the use of recurrent neural networks (RNN) instead and successfully
train an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision
based control is a sequential prediction problem, known for its highly
correlated input data. The correlation makes training a network hard,
especially an RNN. To overcome this issue, we investigate an alternative
sampling method during training, namely window-wise truncated backpropagation
through time (WW-TBPTT). Further, end-to-end training requires a lot of data
which often is not available. Therefore, we compare the performance of
retraining only the Fully Connected (FC) and LSTM control layers with networks
which are trained end-to-end. Performing the relatively simple task of crossing
a room already reveals important guidelines and good practices for training
neural control networks. Different visualizations help to explain the behavior
learned.
",1,0,0,0,0,0
1369,Range-efficient consistent sampling and locality-sensitive hashing for polygons,"  Locality-sensitive hashing (LSH) is a fundamental technique for similarity
search and similarity estimation in high-dimensional spaces. The basic idea is
that similar objects should produce hash collisions with probability
significantly larger than objects with low similarity. We consider LSH for
objects that can be represented as point sets in either one or two dimensions.
To make the point sets finite size we consider the subset of points on a grid.
Directly applying LSH (e.g. min-wise hashing) to these point sets would require
time proportional to the number of points. We seek to achieve time that is much
lower than direct approaches.
Technically, we introduce new primitives for range-efficient consistent
sampling (of independent interest), and show how to turn such samples into LSH
values. Another application of our technique is a data structure for quickly
estimating the size of the intersection or union of a set of preprocessed
polygons. Curiously, our consistent sampling method uses transformation to a
geometric problem.
",1,0,0,0,0,0
1370,Decoupled Greedy Learning of CNNs,"  A commonly cited inefficiency of neural network training by back-propagation
is the update locking problem: each layer must wait for the signal to propagate
through the network before updating. We consider and analyze a training
procedure, Decoupled Greedy Learning (DGL), that addresses this problem more
effectively and at scales beyond those of previous solutions. It is based on a
greedy relaxation of the joint training objective, recently shown to be
effective in the context of Convolutional Neural Networks (CNNs) on large-scale
image classification. We consider an optimization of this objective that
permits us to decouple the layer training, allowing for layers or modules in
networks to be trained with a potentially linear parallelization in layers. We
show theoretically and empirically that this approach converges. In addition,
we empirically find that it can lead to better generalization than sequential
greedy optimization and even standard end-to-end back-propagation. We show that
an extension of this approach to asynchronous settings, where modules can
operate with large communication delays, is possible with the use of a replay
buffer. We demonstrate the effectiveness of DGL on the CIFAR-10 datasets
against alternatives and on the large-scale ImageNet dataset, where we are able
to effectively train VGG and ResNet-152 models.
",1,0,0,1,0,0
1371,Discrete time Pontryagin maximum principle for optimal control problems under state-action-frequency constraints,"  We establish a Pontryagin maximum principle for discrete time optimal control
problems under the following three types of constraints: a) constraints on the
states pointwise in time, b) constraints on the control actions pointwise in
time, and c) constraints on the frequency spectrum of the optimal control
trajectories. While the first two types of constraints are already included in
the existing versions of the Pontryagin maximum principle, it turns out that
the third type of constraints cannot be recast in any of the standard forms of
the existing results for the original control system. We provide two different
proofs of our Pontryagin maximum principle in this article, and include several
special cases fine-tuned to control-affine nonlinear and linear system models.
In particular, for minimization of quadratic cost functions and linear time
invariant control systems, we provide tight conditions under which the optimal
controls under frequency constraints are either normal or abnormal.
",1,0,1,0,0,0
1372,Quantitative evaluation of an active Chemotaxis model in Discrete time,"  A system of $N$ particles in a chemical medium in $\mathbb{R}^{d}$ is studied
in a discrete time setting. Underlying interacting particle system in
continuous time can be expressed as \begin{eqnarray} dX_{i}(t)
&=&[-(I-A)X_{i}(t) + \bigtriangledown h(t,X_{i}(t))]dt + dW_{i}(t), \,\,
X_{i}(0)=x_{i}\in \mathbb{R}^{d}\,\,\forall i=1,\ldots,N\nonumber\\
\frac{\partial}{\partial t} h(t,x)&=&-\alpha h(t,x) + D\bigtriangleup h(t,x)
+\frac{\beta}{n} \sum_{i=1}^{N} g(X_{i}(t),x),\quad h(0,\cdot) =
h(\cdot).\label{main} \end{eqnarray} where $X_{i}(t)$ is the location of the
$i$th particle at time $t$ and $h(t,x)$ is the function measuring the
concentration of the medium at location $x$ with $h(0,x) = h(x)$. In this
article we describe a general discrete time non-linear formulation of the
aforementioned model and a strongly coupled particle system approximating it.
Similar models have been studied before (Budhiraja et al.(2011)) under a
restrictive compactness assumption on the domain of particles. In current work
the particles take values in $\R^{d}$ and consequently the stability analysis
is particularly challenging. We provide sufficient conditions for the existence
of a unique fixed point for the dynamical system governing the large $N$
asymptotics of the particle empirical measure. We also provide uniform in time
convergence rates for the particle empirical measure to the corresponding limit
measure under suitable conditions on the model.
",0,0,1,0,0,0
1373,Deep Learning the Ising Model Near Criticality,"  It is well established that neural networks with deep architectures perform
better than shallow networks for many tasks in machine learning. In statistical
physics, while there has been recent interest in representing physical data
with generative modelling, the focus has been on shallow neural networks. A
natural question to ask is whether deep neural networks hold any advantage over
shallow networks in representing such data. We investigate this question by
using unsupervised, generative graphical models to learn the probability
distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep
belief networks, and deep restricted Boltzmann networks are trained on thermal
spin configurations from this system, and compared to the shallow architecture
of the restricted Boltzmann machine. We benchmark the models, focussing on the
accuracy of generating energetic observables near the phase transition, where
these quantities are most difficult to approximate. Interestingly, after
training the generative networks, we observe that the accuracy essentially
depends only on the number of neurons in the first hidden layer of the network,
and not on other model details such as network depth or model type. This is
evidence that shallow networks are more efficient than deep networks at
representing physical probability distributions associated with Ising systems
near criticality.
",1,1,0,1,0,0
1374,A supervised approach to time scale detection in dynamic networks,"  For any stream of time-stamped edges that form a dynamic network, an
important choice is the aggregation granularity that an analyst uses to bin the
data. Picking such a windowing of the data is often done by hand, or left up to
the technology that is collecting the data. However, the choice can make a big
difference in the properties of the dynamic network. This is the time scale
detection problem. In previous work, this problem is often solved with a
heuristic as an unsupervised task. As an unsupervised problem, it is difficult
to measure how well a given algorithm performs. In addition, we show that the
quality of the windowing is dependent on which task an analyst wants to perform
on the network after windowing. Therefore the time scale detection problem
should not be handled independently from the rest of the analysis of the
network.
We introduce a framework that tackles both of these issues: By measuring the
performance of the time scale detection algorithm based on how well a given
task is accomplished on the resulting network, we are for the first time able
to directly compare different time scale detection algorithms to each other.
Using this framework, we introduce time scale detection algorithms that take a
supervised approach: they leverage ground truth on training data to find a good
windowing of the test data. We compare the supervised approach to previous
approaches and several baselines on real data.
",1,0,0,0,0,0
1375,Binarized octree generation for Cartesian adaptive mesh refinement around immersed geometries,"  We revisit the generation of balanced octrees for adaptive mesh refinement
(AMR) of Cartesian domains with immersed complex geometries. In a recent short
note [Hasbestan and Senocak, J. Comput. Phys. vol. 351:473-477 (2017)], we
showed that the data-locality of the Z-order curve in hashed linear octree
generation methods may not be perfect because of potential collisions in the
hash table. Building on that observation, we propose a binarized octree
generation method that complies with the Z-order curve exactly. Similar to a
hashed linear octree generation method, we use Morton encoding to index the
nodes of an octree, but use a red-black tree in place of the hash table.
Red-black tree is a special kind of a binary tree, which we use for insertion
and deletion of elements during mesh adaptation. By strictly working with the
bitwise representation of the octree, we remove computer hardware limitations
on the depth of adaptation on a single processor. Additionally, we introduce a
geometry encoding technique for rapidly tagging the solid geometry for
refinement. Our results for several geometries with different levels of
adaptations show that the binarized octree generation outperforms the linear
octree generation in terms of runtime performance at the expense of only a
slight increase in memory usage. We provide the current AMR capability as
open-source software.
",1,1,0,0,0,0
1376,Adversarial Examples: Opportunities and Challenges,"  With the advent of the era of artificial intelligence(AI), deep neural
networks (DNNs) have shown huge superiority over human in image recognition,
speech processing, autonomous vehicles and medical diagnosis. However, recent
studies indicate that DNNs are vulnerable to adversarial examples (AEs) which
are designed by attackers to fool deep learning models. Different from real
examples, AEs can hardly be distinguished from human eyes, but mislead the
model to predict incorrect outputs and therefore threaten security critical
deep-learning applications. In recent years, the generation and defense of AEs
have become a research hotspot in the field of AI security. This article
reviews the latest research progress of AEs. First, we introduce the concept,
cause, characteristic and evaluation metrics of AEs, then give a survey on the
state-of-the-art AE generation methods with the discussion of advantages and
disadvantages. After that we review the existing defenses and discuss their
limitations. Finally, the future research opportunities and challenges of AEs
are prospected.
",0,0,0,1,0,0
1377,Decoupling Learning Rules from Representations,"  In the artificial intelligence field, learning often corresponds to changing
the parameters of a parameterized function. A learning rule is an algorithm or
mathematical expression that specifies precisely how the parameters should be
changed. When creating an artificial intelligence system, we must make two
decisions: what representation should be used (i.e., what parameterized
function should be used) and what learning rule should be used to search
through the resulting set of representable functions. Using most learning
rules, these two decisions are coupled in a subtle (and often unintentional)
way. That is, using the same learning rule with two different representations
that can represent the same sets of functions can result in two different
outcomes. After arguing that this coupling is undesirable, particularly when
using artificial neural networks, we present a method for partially decoupling
these two decisions for a broad class of learning rules that span unsupervised
learning, reinforcement learning, and supervised learning.
",1,0,0,1,0,0
1378,Schur P-positivity and involution Stanley symmetric functions,"  The involution Stanley symmetric functions $\hat{F}_y$ are the stable limits
of the analogues of Schubert polynomials for the orbits of the orthogonal group
in the flag variety. These symmetric functions are also generating functions
for involution words, and are indexed by the involutions in the symmetric
group. By construction each $\hat{F}_y$ is a sum of Stanley symmetric functions
and therefore Schur positive. We prove the stronger fact that these power
series are Schur $P$-positive. We give an algorithm to efficiently compute the
decomposition of $\hat{F}_y$ into Schur $P$-summands, and prove that this
decomposition is triangular with respect to the dominance order on partitions.
As an application, we derive pattern avoidance conditions which characterize
the involution Stanley symmetric functions which are equal to Schur
$P$-functions. We deduce as a corollary that the involution Stanley symmetric
function of the reverse permutation is a Schur $P$-function indexed by a
shifted staircase shape. These results lead to alternate proofs of theorems of
Ardila-Serrano and DeWitt on skew Schur functions which are Schur
$P$-functions. We also prove new Pfaffian formulas for certain related
involution Schubert polynomials.
",0,0,1,0,0,0
1379,A Riemannian gossip approach to subspace learning on Grassmann manifold,"  In this paper, we focus on subspace learning problems on the Grassmann
manifold. Interesting applications in this setting include low-rank matrix
completion and low-dimensional multivariate regression, among others. Motivated
by privacy concerns, we aim to solve such problems in a decentralized setting
where multiple agents have access to (and solve) only a part of the whole
optimization problem. The agents communicate with each other to arrive at a
consensus, i.e., agree on a common quantity, via the gossip protocol.
We propose a novel cost function for subspace learning on the Grassmann
manifold, which is a weighted sum of several sub-problems (each solved by an
agent) and the communication cost among the agents. The cost function has a
finite sum structure. In the proposed modeling approach, different agents learn
individual local subspace but they achieve asymptotic consensus on the global
learned subspace. The approach is scalable and parallelizable. Numerical
experiments show the efficacy of the proposed decentralized algorithms on
various matrix completion and multivariate regression benchmarks.
",1,0,1,0,0,0
1380,"Space-Valued Diagrams, Type-Theoretically (Extended Abstract)","  Topologists are sometimes interested in space-valued diagrams over a given
index category, but it is tricky to say what such a diagram even is if we look
for a notion that is stable under equivalence. The same happens in (homotopy)
type theory, where it is known only for special cases how one can define a type
of type-valued diagrams over a given index category. We offer several
constructions. We first show how to define homotopy coherent diagrams which
come with all higher coherence laws explicitly, with two variants that come
with assumption on the index category or on the type theory. Further, we
present a construction of diagrams over certain Reedy categories. As an
application, we add the degeneracies to the well-known construction of
semisimplicial types, yielding a construction of simplicial types up to any
given finite level. The current paper is only an extended abstract, and a full
version is to follow. In the full paper, we will show that the different
notions of diagrams are equivalent to each other and to the known notion of
Reedy fibrant diagrams whenever the statement makes sense. In the current
paper, we only sketch some core ideas of the proofs.
",1,0,1,0,0,0
1381,Properties of cyanobacterial UV-absorbing pigments suggest their evolution was driven by optimizing photon dissipation rather than photoprotection,"  An ancient repertoire of UV absorbing pigments which survive today in the
phylogenetically oldest extant photosynthetic organisms the cyanobacteria point
to a direction in evolutionary adaptation of the pigments and their associated
biota from largely UVC absorbing pigments in the Archean to pigments covering
ever more of the longer wavelength UV and visible in the Phanerozoic.Such a
scenario implies selection of photon dissipation rather than photoprotection
over the evolutionary history of life.This is consistent with the thermodynamic
dissipation theory of the origin and evolution of life which suggests that the
most important hallmark of biological evolution has been the covering of Earths
surface with organic pigment molecules and water to absorb and dissipate ever
more completely the prevailing surface solar spectrum.In this article we
compare a set of photophysical photochemical biosynthetic and other germane
properties of the two dominant classes of cyanobacterial UV absorbing pigments
the mycosporine like amino acids MAAs and scytonemins.Pigment wavelengths of
maximum absorption correspond with the time dependence of the prevailing Earth
surface solar spectrum and we proffer this as evidence for the selection of
photon dissipation rather than photoprotection over the history of life on
Earth.
",0,1,0,0,0,0
1382,Output Impedance Diffusion into Lossy Power Lines,"  Output impedances are inherent elements of power sources in the electrical
grids. In this paper, we give an answer to the following question: What is the
effect of output impedances on the inductivity of the power network? To address
this question, we propose a measure to evaluate the inductivity of a power
grid, and we compute this measure for various types of output impedances.
Following this computation, it turns out that network inductivity highly
depends on the algebraic connectivity of the network. By exploiting the derived
expressions of the proposed measure, one can tune the output impedances in
order to enforce a desired level of inductivity on the power system.
Furthermore, the results show that the more ""connected"" the network is, the
more the output impedances diffuse into the network. Finally, using Kron
reduction, we provide examples that demonstrate the utility and validity of the
method.
",1,0,0,0,0,0
1383,Enhancing the significance of gravitational wave bursts through signal classification,"  The quest to observe gravitational waves challenges our ability to
discriminate signals from detector noise. This issue is especially relevant for
transient gravitational waves searches with a robust eyes wide open approach,
the so called all- sky burst searches. Here we show how signal classification
methods inspired by broad astrophysical characteristics can be implemented in
all-sky burst searches preserving their generality. In our case study, we apply
a multivariate analyses based on artificial neural networks to classify waves
emitted in compact binary coalescences. We enhance by orders of magnitude the
significance of signals belonging to this broad astrophysical class against the
noise background. Alternatively, at a given level of mis-classification of
noise events, we can detect about 1/4 more of the total signal population. We
also show that a more general strategy of signal classification can actually be
performed, by testing the ability of artificial neural networks in
discriminating different signal classes. The possible impact on future
observations by the LIGO-Virgo network of detectors is discussed by analysing
recoloured noise from previous LIGO-Virgo data with coherent WaveBurst, one of
the flagship pipelines dedicated to all-sky searches for transient
gravitational waves.
",0,1,0,0,0,0
1384,Model-based Clustering with Sparse Covariance Matrices,"  Finite Gaussian mixture models are widely used for model-based clustering of
continuous data. Nevertheless, since the number of model parameters scales
quadratically with the number of variables, these models can be easily
over-parameterized. For this reason, parsimonious models have been developed
via covariance matrix decompositions or assuming local independence. However,
these remedies do not allow for direct estimation of sparse covariance matrices
nor do they take into account that the structure of association among the
variables can vary from one cluster to the other. To this end, we introduce
mixtures of Gaussian covariance graph models for model-based clustering with
sparse covariance matrices. A penalized likelihood approach is employed for
estimation and a general penalty term on the graph configurations can be used
to induce different levels of sparsity and incorporate prior knowledge. Model
estimation is carried out using a structural-EM algorithm for parameters and
graph structure estimation, where two alternative strategies based on a genetic
algorithm and an efficient stepwise search are proposed for inference. With
this approach, sparse component covariance matrices are directly obtained. The
framework results in a parsimonious model-based clustering of the data via a
flexible model for the within-group joint distribution of the variables.
Extensive simulated data experiments and application to illustrative datasets
show that the method attains good classification performance and model quality.
",0,0,0,1,0,0
1385,An Assessment of Data Transfer Performance for Large-Scale Climate Data Analysis and Recommendations for the Data Infrastructure for CMIP6,"  We document the data transfer workflow, data transfer performance, and other
aspects of staging approximately 56 terabytes of climate model output data from
the distributed Coupled Model Intercomparison Project (CMIP5) archive to the
National Energy Research Supercomputing Center (NERSC) at the Lawrence Berkeley
National Laboratory required for tracking and characterizing extratropical
storms, a phenomena of importance in the mid-latitudes. We present this
analysis to illustrate the current challenges in assembling multi-model data
sets at major computing facilities for large-scale studies of CMIP5 data.
Because of the larger archive size of the upcoming CMIP6 phase of model
intercomparison, we expect such data transfers to become of increasing
importance, and perhaps of routine necessity. We find that data transfer rates
using the ESGF are often slower than what is typically available to US
residences and that there is significant room for improvement in the data
transfer capabilities of the ESGF portal and data centers both in terms of
workflow mechanics and in data transfer performance. We believe performance
improvements of at least an order of magnitude are within technical reach using
current best practices, as illustrated by the performance we achieved in
transferring the complete raw data set between two high performance computing
facilities. To achieve these performance improvements, we recommend: that
current best practices (such as the Science DMZ model) be applied to the data
servers and networks at ESGF data centers; that sufficient financial and human
resources be devoted at the ESGF data centers for systems and network
engineering tasks to support high performance data movement; and that
performance metrics for data transfer between ESGF data centers and major
computing facilities used for climate data analysis be established, regularly
tested, and published.
",1,1,0,0,0,0
1386,Generalization for Adaptively-chosen Estimators via Stable Median,"  Datasets are often reused to perform multiple statistical analyses in an
adaptive way, in which each analysis may depend on the outcomes of previous
analyses on the same dataset. Standard statistical guarantees do not account
for these dependencies and little is known about how to provably avoid
overfitting and false discovery in the adaptive setting. We consider a natural
formalization of this problem in which the goal is to design an algorithm that,
given a limited number of i.i.d.~samples from an unknown distribution, can
answer adaptively-chosen queries about that distribution.
We present an algorithm that estimates the expectations of $k$ arbitrary
adaptively-chosen real-valued estimators using a number of samples that scales
as $\sqrt{k}$. The answers given by our algorithm are essentially as accurate
as if fresh samples were used to evaluate each estimator. In contrast, prior
work yields error guarantees that scale with the worst-case sensitivity of each
estimator. We also give a version of our algorithm that can be used to verify
answers to such queries where the sample complexity depends logarithmically on
the number of queries $k$ (as in the reusable holdout technique).
Our algorithm is based on a simple approximate median algorithm that
satisfies the strong stability guarantees of differential privacy. Our
techniques provide a new approach for analyzing the generalization guarantees
of differentially private algorithms.
",1,0,0,1,0,0
1387,Automated Problem Identification: Regression vs Classification via Evolutionary Deep Networks,"  Regression or classification? This is perhaps the most basic question faced
when tackling a new supervised learning problem. We present an Evolutionary
Deep Learning (EDL) algorithm that automatically solves this by identifying the
question type with high accuracy, along with a proposed deep architecture.
Typically, a significant amount of human insight and preparation is required
prior to executing machine learning algorithms. For example, when creating deep
neural networks, the number of parameters must be selected in advance and
furthermore, a lot of these choices are made based upon pre-existing knowledge
of the data such as the use of a categorical cross entropy loss function.
Humans are able to study a dataset and decide whether it represents a
classification or a regression problem, and consequently make decisions which
will be applied to the execution of the neural network. We propose the
Automated Problem Identification (API) algorithm, which uses an evolutionary
algorithm interface to TensorFlow to manipulate a deep neural network to decide
if a dataset represents a classification or a regression problem. We test API
on 16 different classification, regression and sentiment analysis datasets with
up to 10,000 features and up to 17,000 unique target values. API achieves an
average accuracy of $96.3\%$ in identifying the problem type without hardcoding
any insights about the general characteristics of regression or classification
problems. For example, API successfully identifies classification problems even
with 1000 target values. Furthermore, the algorithm recommends which loss
function to use and also recommends a neural network architecture. Our work is
therefore a step towards fully automated machine learning.
",1,0,0,1,0,0
1388,Attribution of extreme rainfall in Southeast China during May 2015,"  Anthropogenic climate change increased the probability that a short-duration,
intense rainfall event would occur in parts of southeast China. This type of
event occurred in May 2015, causing serious flooding.
",0,1,0,0,0,0
1389,The Kellogg property and boundary regularity for p-harmonic functions with respect to the Mazurkiewicz boundary and other compactifications,"  In this paper boundary regularity for p-harmonic functions is studied with
respect to the Mazurkiewicz boundary and other compactifications. In
particular, the Kellogg property (which says that the set of irregular boundary
points has capacity zero) is obtained for a large class of compactifications,
but also two examples when it fails are given. This study is done for complete
metric spaces equipped with doubling measures supporting a p-Poincar??
inequality, but the results are new also in unweighted Euclidean spaces.
",0,0,1,0,0,0
1390,Nonparametric Inference via Bootstrapping the Debiased Estimator,"  In this paper, we propose to construct confidence bands by bootstrapping the
debiased kernel density estimator (for density estimation) and the debiased
local polynomial regression estimator (for regression analysis). The idea of
using a debiased estimator was first introduced in Calonico et al. (2015),
where they construct a confidence interval of the density function (and
regression function) at a given point by explicitly estimating stochastic
variations. We extend their ideas and propose a bootstrap approach for
constructing confidence bands that is uniform for every point in the support.
We prove that the resulting bootstrap confidence band is asymptotically valid
and is compatible with most tuning parameter selection approaches, such as the
rule of thumb and cross-validation. We further generalize our method to
confidence sets of density level sets and inverse regression problems.
Simulation studies confirm the validity of the proposed confidence bands/sets.
",0,0,1,1,0,0
1391,Solving constraint-satisfaction problems with distributed neocortical-like neuronal networks,"  Finding actions that satisfy the constraints imposed by both external inputs
and internal representations is central to decision making. We demonstrate that
some important classes of constraint satisfaction problems (CSPs) can be solved
by networks composed of homogeneous cooperative-competitive modules that have
connectivity similar to motifs observed in the superficial layers of neocortex.
The winner-take-all modules are sparsely coupled by programming neurons that
embed the constraints onto the otherwise homogeneous modular computational
substrate. We show rules that embed any instance of the CSPs planar four-color
graph coloring, maximum independent set, and Sudoku on this substrate, and
provide mathematical proofs that guarantee these graph coloring problems will
convergence to a solution. The network is composed of non-saturating linear
threshold neurons. Their lack of right saturation allows the overall network to
explore the problem space driven through the unstable dynamics generated by
recurrent excitation. The direction of exploration is steered by the constraint
neurons. While many problems can be solved using only linear inhibitory
constraints, network performance on hard problems benefits significantly when
these negative constraints are implemented by non-linear multiplicative
inhibition. Overall, our results demonstrate the importance of instability
rather than stability in network computation, and also offer insight into the
computational role of dual inhibitory mechanisms in neural circuits.
",0,0,0,0,1,0
1392,Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks,"  A major challenge in brain tumor treatment planning and quantitative
evaluation is determination of the tumor extent. The noninvasive magnetic
resonance imaging (MRI) technique has emerged as a front-line diagnostic tool
for brain tumors without ionizing radiation. Manual segmentation of brain tumor
extent from 3D MRI volumes is a very time-consuming task and the performance is
highly relied on operator's experience. In this context, a reliable fully
automatic segmentation method for the brain tumor segmentation is necessary for
an efficient measurement of the tumor extent. In this study, we propose a fully
automatic method for brain tumor segmentation, which is developed using U-Net
based deep convolutional networks. Our method was evaluated on Multimodal Brain
Tumor Image Segmentation (BRATS 2015) datasets, which contain 220 high-grade
brain tumor and 54 low-grade tumor cases. Cross-validation has shown that our
method can obtain promising segmentation efficiently.
",1,0,0,0,0,0
1393,Asymptotic Blind-spot Analysis of Localization Networks under Correlated Blocking using a Poisson Line Process,"  In a localization network, the line-of-sight between anchors (transceivers)
and targets may be blocked due to the presence of obstacles in the environment.
Due to the non-zero size of the obstacles, the blocking is typically correlated
across both anchor and target locations, with the extent of correlation
increasing with obstacle size. If a target does not have line-of-sight to a
minimum number of anchors, then its position cannot be estimated unambiguously
and is, therefore, said to be in a blind-spot. However, the analysis of the
blind-spot probability of a given target is challenging due to the inherent
randomness in the obstacle locations and sizes. In this letter, we develop a
new framework to analyze the worst-case impact of correlated blocking on the
blind-spot probability of a typical target; in particular, we model the
obstacles by a Poisson line process and the anchor locations by a Poisson point
process. For this setup, we define the notion of the asymptotic blind-spot
probability of the typical target and derive a closed-form expression for it as
a function of the area distribution of a typical Poisson-Voronoi cell. As an
upper bound for the more realistic case when obstacles have finite dimensions,
the asymptotic blind-spot probability is useful as a design tool to ensure that
the blind-spot probability of a typical target does not exceed a desired
threshold, $\epsilon$.
",1,0,0,0,0,0
1394,The relation between galaxy morphology and colour in the EAGLE simulation,"  We investigate the relation between kinematic morphology, intrinsic colour
and stellar mass of galaxies in the EAGLE cosmological hydrodynamical
simulation. We calculate the intrinsic u-r colours and measure the fraction of
kinetic energy invested in ordered corotation of 3562 galaxies at z=0 with
stellar masses larger than $10^{10}M_{\odot}$. We perform a visual inspection
of gri-composite images and find that our kinematic morphology correlates
strongly with visual morphology. EAGLE produces a galaxy population for which
morphology is tightly correlated with the location in the colour- mass diagram,
with the red sequence mostly populated by elliptical galaxies and the blue
cloud by disc galaxies. Satellite galaxies are more likely to be on the red
sequence than centrals, and for satellites the red sequence is morphologically
more diverse. These results show that the connection between mass, intrinsic
colour and morphology arises from galaxy formation models that reproduce the
observed galaxy mass function and sizes.
",0,1,0,0,0,0
1395,An alternative to continuous univariate distributions supported on a bounded interval: The BMT distribution,"  In this paper, we introduce the BMT distribution as an unimodal alternative
to continuous univariate distributions supported on a bounded interval. The
ideas behind the mathematical formulation of this new distribution come from
computer aid geometric design, specifically from Bezier curves. First, we
review general properties of a distribution given by parametric equations and
extend the definition of a Bezier distribution. Then, after proposing the BMT
cumulative distribution function, we derive its probability density function
and a closed-form expression for quantile function, median, interquartile
range, mode, and moments. The domain change from [0,1] to [c,d] is mentioned.
Estimation of parameters is approached by the methods of maximum likelihood and
maximum product of spacing. We test the numerical estimation procedures using
some simulated data. Usefulness and flexibility of the new distribution are
illustrated in three real data sets. The BMT distribution has a significant
potential to estimate domain parameters and to model data outside the scope of
the beta or similar distributions.
",0,0,1,1,0,0
1396,Deep Object Centric Policies for Autonomous Driving,"  While learning visuomotor skills in an end-to-end manner is appealing, deep
neural networks are often uninterpretable and fail in surprising ways. For
robotics tasks, such as autonomous driving, models that explicitly represent
objects may be more robust to new scenes and provide intuitive visualizations.
We describe a taxonomy of object-centric models which leverage both object
instances and end-to-end learning. In the Grand Theft Auto V simulator, we show
that object centric models outperform object-agnostic methods in scenes with
other vehicles and pedestrians, even with an imperfect detector. We also
demonstrate that our architectures perform well on real world environments by
evaluating on the Berkeley DeepDrive Video dataset.
",1,0,0,0,0,0
1397,A Search for Laser Emission with Megawatt Thresholds from 5600 FGKM Stars,"  We searched high resolution spectra of 5600 nearby stars for emission lines
that are both inconsistent with a natural origin and unresolved spatially, as
would be expected from extraterrestrial optical lasers. The spectra were
obtained with the Keck 10-meter telescope, including light coming from within
0.5 arcsec of the star, corresponding typically to within a few to tens of au
of the star, and covering nearly the entire visible wavelength range from 3640
to 7890 angstroms. We establish detection thresholds by injecting synthetic
laser emission lines into our spectra and blindly analyzing them for
detections. We compute flux density detection thresholds for all wavelengths
and spectral types sampled. Our detection thresholds for the power of the
lasers themselves range from 3 kW to 13 MW, independent of distance to the star
but dependent on the competing ""glare"" of the spectral energy distribution of
the star and on the wavelength of the laser light, launched from a benchmark,
diffraction-limited 10-meter class telescope. We found no such laser emission
coming from the planetary region around any of the 5600 stars. As they contain
roughly 2000 lukewarm, Earth-size planets, we rule out models of the Milky Way
in which over 0.1 percent of warm, Earth-size planets harbor technological
civilizations that, intentionally or not, are beaming optical lasers toward us.
A next generation spectroscopic laser search will be done by the Breakthrough
Listen initiative, targeting more stars, especially stellar types overlooked
here including spectral types O, B, A, early F, late M, and brown dwarfs, and
astrophysical exotica.
",0,1,0,0,0,0
1398,Learning Large Scale Ordinary Differential Equation Systems,"  Learning large scale nonlinear ordinary differential equation (ODE) systems
from data is known to be computationally and statistically challenging. We
present a framework together with the adaptive integral matching (AIM)
algorithm for learning polynomial or rational ODE systems with a sparse network
structure. The framework allows for time course data sampled from multiple
environments representing e.g. different interventions or perturbations of the
system. The algorithm AIM combines an initial penalised integral matching step
with an adapted least squares step based on solving the ODE numerically. The R
package episode implements AIM together with several other algorithms and is
available from CRAN. It is shown that AIM achieves state-of-the-art network
recovery for the in silico phosphoprotein abundance data from the eighth DREAM
challenge with an AUROC of 0.74, and it is demonstrated via a range of
numerical examples that AIM has good statistical properties while being
computationally feasible even for large systems.
",0,0,1,1,0,0
1399,Linear Time Clustering for High Dimensional Mixtures of Gaussian Clouds,"  Clustering mixtures of Gaussian distributions is a fundamental and
challenging problem that is ubiquitous in various high-dimensional data
processing tasks. While state-of-the-art work on learning Gaussian mixture
models has focused primarily on improving separation bounds and their
generalization to arbitrary classes of mixture models, less emphasis has been
paid to practical computational efficiency of the proposed solutions. In this
paper, we propose a novel and highly efficient clustering algorithm for $n$
points drawn from a mixture of two arbitrary Gaussian distributions in
$\mathbb{R}^p$. The algorithm involves performing random 1-dimensional
projections until a direction is found that yields a user-specified clustering
error $e$. For a 1-dimensional separation parameter $\gamma$ satisfying
$\gamma=Q^{-1}(e)$, the expected number of such projections is shown to be
bounded by $o(\ln p)$, when $\gamma$ satisfies $\gamma\leq
c\sqrt{\ln{\ln{p}}}$, with $c$ as the separability parameter of the two
Gaussians in $\mathbb{R}^p$. Consequently, the expected overall running time of
the algorithm is linear in $n$ and quasi-linear in $p$ at $o(\ln{p})O(np)$, and
the sample complexity is independent of $p$. This result stands in contrast to
prior works which provide polynomial, with at-best quadratic, running time in
$p$ and $n$. We show that our bound on the expected number of 1-dimensional
projections extends to the case of three or more Gaussian components, and we
present a generalization of our results to mixture distributions beyond the
Gaussian model.
",1,0,0,0,0,0
1400,Estimation of Relationship between Stimulation Current and Force Exerted during Isometric Contraction,"  In this study, we developed a method to estimate the relationship between
stimulation current and volatility during isometric contraction. In functional
electrical stimulation (FES), joints are driven by applying voltage to muscles.
This technology has been used for a long time in the field of rehabilitation,
and recently application oriented research has been reported. However,
estimation of the relationship between stimulus value and exercise capacity has
not been discussed to a great extent. Therefore, in this study, a human muscle
model was estimated using the transfer function estimation method with fast
Fourier transform. It was found that the relationship between stimulation
current and force exerted could be expressed by a first-order lag system. In
verification of the force estimate, the ability of the proposed model to
estimate the exerted force under steady state response was found to be good.
",0,0,0,0,1,0
1401,Facial Keypoints Detection,"  Detect facial keypoints is a critical element in face recognition. However,
there is difficulty to catch keypoints on the face due to complex influences
from original images, and there is no guidance to suitable algorithms. In this
paper, we study different algorithms that can be applied to locate keyponits.
Specifically: our framework (1)prepare the data for further investigation
(2)Using PCA and LBP to process the data (3) Apply different algorithms to
analysis data, including linear regression models, tree based model, neural
network and convolutional neural network, etc. Finally we will give our
conclusion and further research topic. A comprehensive set of experiments on
dataset demonstrates the effectiveness of our framework.
",1,0,0,1,0,0
1402,Transitions from a Kondo-like diamagnetic insulator into a modulated ferromagnetic metal in $\bm{\mathrm{FeGa}_{3-y}\mathrm{Ge}_y}$,"  One initial and essential question of magnetism is whether the magnetic
properties of a material are governed by localized moments or itinerant
electrons. Here we expose the case for the weakly ferromagnetic system
FeGa$_{3-y}$Ge$_y$ wherein these two opposite models are reconciled, such that
the magnetic susceptibility is quantitatively explained by taking into account
the effects of spin-spin correlation. With the electron doping introduced by Ge
substitution, the diamagnetic insulating parent compound FeGa$_3$ becomes a
paramagnetic metal as early as at $ y=0.01 $, and turns into a weakly
ferromagnetic metal around the quantum critical point $ y=0.15 $. Within the
ferromagnetic regime of FeGa$_{3-y}$Ge$_y$, the magnetic properties are of a
weakly itinerant ferromagnetic nature, located in the intermediate regime
between the localized and the itinerant dominance. Our analysis implies a
potential universality for all itinerant-electron ferromagnets.
",0,1,0,0,0,0
1403,"Sample, computation vs storage tradeoffs for classification using tensor subspace models","  In this paper, we exhibit the tradeoffs between the (training) sample,
computation and storage complexity for the problem of supervised classification
using signal subspace estimation. Our main tool is the use of tensor subspaces,
i.e. subspaces with a Kronecker structure, for embedding the data into lower
dimensions. Among the subspaces with a Kronecker structure, we show that using
subspaces with a hierarchical structure for representing data leads to improved
tradeoffs. One of the main reasons for the improvement is that embedding data
into these hierarchical Kronecker structured subspaces prevents overfitting at
higher latent dimensions.
",1,0,0,1,0,0
1404,One-step Estimation of Networked Population Size with Anonymity Using Respondent-Driven Capture-Recapture and Hashing,"  Estimates of population size for hidden and hard-to-reach individuals are of
particular interest to health officials when health problems are concentrated
in such populations. Efforts to derive these estimates are often frustrated by
a range of factors including social stigma or an association with illegal
activities that ordinarily preclude conventional survey strategies. This paper
builds on and extends prior work that proposed a method to meet these
challenges. Here we describe a rigorous formalization of a one-step,
network-based population estimation procedure that can be employed under
conditions of anonymity. The estimation procedure is designed to be implemented
alongside currently accepted strategies for research with hidden populations.
Simulation experiments are described that test the efficacy of the method
across a range of implementation conditions and hidden population sizes. The
results of these experiments show that reliable population estimates can be
derived for hidden, networked population as large as 12,500 and perhaps larger
for one family of random graphs. As such, the method shows potential for
cost-effective implementation health and disease surveillance officials
concerned with hidden populations. Limitations and future work are discussed in
the concluding section.
",1,0,0,1,0,0
1405,Real single ion solvation free energies with quantum mechanical simulation,"  Single ion solvation free energies are one of the most important properties
of electrolyte solutions and yet there is ongoing debate about what these
values are. Only the values for neutral ion pairs are known. Here, we use DFT
interaction potentials with molecular dynamics simulation (DFT-MD) combined
with a modified version of the quasi-chemical theory (QCT) to calculate these
energies for the lithium and fluoride ions. A method to correct for the error
in the DFT functional is developed and very good agreement with the
experimental value for the lithium fluoride pair is obtained. Moreover, this
method partitions the energies into physically intuitive terms such as surface
potential, cavity and charging energies which are amenable to descriptions with
reduced models. Our research suggests that lithium's solvation free energy is
dominated by the free energetics of a charged hard sphere, whereas fluoride
exhibits significant quantum mechanical behavior that cannot be simply
described with a reduced model.
",0,1,0,0,0,0
1406,Crowdsourcing with Sparsely Interacting Workers,"  We consider estimation of worker skills from worker-task interaction data
(with unknown labels) for the single-coin crowd-sourcing binary classification
model in symmetric noise. We define the (worker) interaction graph whose nodes
are workers and an edge between two nodes indicates whether or not the two
workers participated in a common task. We show that skills are asymptotically
identifiable if and only if an appropriate limiting version of the interaction
graph is irreducible and has odd-cycles. We then formulate a weighted rank-one
optimization problem to estimate skills based on observations on an
irreducible, aperiodic interaction graph. We propose a gradient descent scheme
and show that for such interaction graphs estimates converge asymptotically to
the global minimum. We characterize noise robustness of the gradient scheme in
terms of spectral properties of signless Laplacians of the interaction graph.
We then demonstrate that a plug-in estimator based on the estimated skills
achieves state-of-art performance on a number of real-world datasets. Our
results have implications for rank-one matrix completion problem in that
gradient descent can provably recover $W \times W$ rank-one matrices based on
$W+1$ off-diagonal observations of a connected graph with a single odd-cycle.
",1,0,0,0,0,0
1407,Training deep learning based denoisers without ground truth data,"  Recent deep learning based denoisers often outperform state-of-the-art
conventional denoisers such as BM3D. They are typically trained to minimize the
mean squared error (MSE) between the output of a deep neural network and the
ground truth image. In deep learning based denoisers, it is important to use
high quality noiseless ground truth for high performance, but it is often
challenging or even infeasible to obtain such a clean image in application
areas such as hyperspectral remote sensing and medical imaging. We propose a
Stein's Unbiased Risk Estimator (SURE) based method for training deep neural
network denoisers only with noisy images. We demonstrated that our SURE based
method without ground truth was able to train deep neural network denoisers to
yield performance close to deep learning denoisers trained with ground truth
and to outperform state-of-the-art BM3D. Further improvements were achieved by
including noisy test images for training denoiser networks using our proposed
SURE based method.
",0,0,0,1,0,0
1408,Language Design and Renormalization,"  Here we consider some well-known facts in syntax from a physics perspective,
which allows us to establish some remarkable equivalences. Specifically, we
observe that the operation MERGE put forward by N. Chomsky in 1995 can be
interpreted as a physical information coarse-graining. Thus, MERGE in
linguistics entails information renormalization in physics, according to
different time scales. We make this point mathematically formal in terms of
language models, i.e., probability distributions over word sequences, widely
used in natural language processing as well as other ambits. In this setting,
MERGE corresponds to a 3-index probability tensor implementing a
coarse-graining, akin to a probabilistic context-free grammar. The probability
vectors of meaningful sentences are naturally given by stochastic tensor
networks (TN) that are mostly loop-free, such as Tree Tensor Networks and
Matrix Product States. These structures have short-ranged correlations in the
syntactic distance by construction and, because of the peculiarities of human
language, they are extremely efficient to manipulate computationally. We also
propose how to obtain such language models from probability distributions of
certain TN quantum states, which we show to be efficiently preparable by a
quantum computer. Moreover, using tools from entanglement theory, we use these
quantum states to prove classical lower bounds on the perplexity of the
probability distribution for a set of words in a sentence. Implications of
these results are discussed in the ambits of theoretical and computational
linguistics, artificial intelligence, programming languages, RNA and protein
sequencing, quantum many-body systems, and beyond. Our work shows how many of
the key linguistic ideas from the last century, including developments in
computational linguistics, fit perfectly with known physical concepts linked to
renormalization.
",1,1,0,0,0,0
1409,On the geometry of the moduli space of sheaves supported on curves of genus two in a quadric surface,"  We study the moduli space of stable sheaves of Euler characteristic 2,
supported on curves of arithmetic genus 2 contained in a smooth quadric
surface. We show that this moduli space is rational. We compute its Betti
numbers and we give a classification of the stable sheaves involving locally
free resolutions.
",0,0,1,0,0,0
1410,"Attention Solves Your TSP, Approximately","  The development of efficient (heuristic) algorithms for practical
combinatorial optimization problems is costly, so we want to automatically
learn them instead. We show the feasibility of this approach on the important
Travelling Salesman Problem (TSP). We learn a heuristic algorithm that uses a
Neural Network policy to construct a tour. As an alternative to the Pointer
Network, our model is based entirely on (graph) attention layers and is
invariant to the input order of the nodes. We train the model efficiently using
REINFORCE with a simple and robust baseline based on a deterministic (greedy)
rollout of the best policy so far. We significantly improve over results from
previous works that consider learned heuristics for the TSP, reducing the
optimality gap for a single tour construction from 1.51% to 0.32% for instances
with 20 nodes, from 4.59% to 1.71% for 50 nodes and from 6.89% to 4.43% for 100
nodes. Additionally, we improve over a recent Reinforcement Learning framework
for two variants of the Vehicle Routing Problem (VRP).
",0,0,0,1,0,0
1411,A Distributed Online Pricing Strategy for Demand Response Programs,"  We study a demand response problem from utility (also referred to as
operator)'s perspective with realistic settings, in which the utility faces
uncertainty and limited communication. Specifically, the utility does not know
the cost function of consumers and cannot have multiple rounds of information
exchange with consumers. We formulate an optimization problem for the utility
to minimize its operational cost considering time-varying demand response
targets and responses of consumers. We develop a joint online learning and
pricing algorithm. In each time slot, the utility sends out a price signal to
all consumers and estimates the cost functions of consumers based on their
noisy responses. We measure the performance of our algorithm using regret
analysis and show that our online algorithm achieves logarithmic regret with
respect to the operating horizon. In addition, our algorithm employs linear
regression to estimate the aggregate response of consumers, making it easy to
implement in practice. Simulation experiments validate the theoretic results
and show that the performance gap between our algorithm and the offline
optimality decays quickly.
",1,0,1,0,0,0
1412,Highly Nonlinear and Low Confinement Loss Photonic Crystal Fiber Using GaP Slot Core,"  This paper presents a triangular lattice photonic crystal fiber with very
high nonlinear coefficient. Finite element method (FEM) is used to scrutinize
different optical properties of proposed highly nonlinear photonic crystal
fiber (HNL-PCF). The HNL-PCF exhibits a high nonlinearity up to $10\times10^{4}
W^{-1}km^{-1}$ over the wavelength of 1500 nm to 1700 nm. Moreover, proposed
HNL-PCF shows a very low confinement loss of $10^{-3} dB/km$ at 1550 nm
wavelength. Furthermore, chromatic dispersion, dispersion slope, effective area
etc. are also analyzed thoroughly. The proposed fiber will be a suitable
candidate for broadband dispersion compensation, sensor devices and
supercontinuum generation.
",0,1,0,0,0,0
1413,Is Epicurus the father of Reinforcement Learning?,"  The Epicurean Philosophy is commonly thought as simplistic and hedonistic.
Here I discuss how this is a misconception and explore its link to
Reinforcement Learning. Based on the letters of Epicurus, I construct an
objective function for hedonism which turns out to be equivalent of the
Reinforcement Learning objective function when omitting the discount factor. I
then discuss how Plato and Aristotle 's views that can be also loosely linked
to Reinforcement Learning, as well as their weaknesses in relationship to it.
Finally, I emphasise the close affinity of the Epicurean views and the Bellman
equation.
",1,0,0,1,0,0
1414,Low-Precision Floating-Point Schemes for Neural Network Training,"  The use of low-precision fixed-point arithmetic along with stochastic
rounding has been proposed as a promising alternative to the commonly used
32-bit floating point arithmetic to enhance training neural networks training
in terms of performance and energy efficiency. In the first part of this paper,
the behaviour of the 12-bit fixed-point arithmetic when training a
convolutional neural network with the CIFAR-10 dataset is analysed, showing
that such arithmetic is not the most appropriate for the training phase. After
that, the paper presents and evaluates, under the same conditions, alternative
low-precision arithmetics, starting with the 12-bit floating-point arithmetic.
These two representations are then leveraged using local scaling in order to
increase accuracy and get closer to the baseline 32-bit floating-point
arithmetic. Finally, the paper introduces a simplified model in which both the
outputs and the gradients of the neural networks are constrained to
power-of-two values, just using 7 bits for their representation. The evaluation
demonstrates a minimal loss in accuracy for the proposed Power-of-Two neural
network, avoiding the use of multiplications and divisions and thereby,
significantly reducing the training time as well as the energy consumption and
memory requirements during the training and inference phases.
",0,0,0,1,0,0
1415,Deep Person Re-Identification with Improved Embedding and Efficient Training,"  Person re-identification task has been greatly boosted by deep convolutional
neural networks (CNNs) in recent years. The core of which is to enlarge the
inter-class distinction as well as reduce the intra-class variance. However, to
achieve this, existing deep models prefer to adopt image pairs or triplets to
form verification loss, which is inefficient and unstable since the number of
training pairs or triplets grows rapidly as the number of training data grows.
Moreover, their performance is limited since they ignore the fact that
different dimension of embedding may play different importance. In this paper,
we propose to employ identification loss with center loss to train a deep model
for person re-identification. The training process is efficient since it does
not require image pairs or triplets for training while the inter-class
distinction and intra-class variance are well handled. To boost the
performance, a new feature reweighting (FRW) layer is designed to explicitly
emphasize the importance of each embedding dimension, thus leading to an
improved embedding. Experiments on several benchmark datasets have shown the
superiority of our method over the state-of-the-art alternatives on both
accuracy and speed.
",1,0,0,0,0,0
1416,Unsupervised speech representation learning using WaveNet autoencoders,"  We consider the task of unsupervised extraction of meaningful latent
representations of speech by applying autoencoding neural networks to speech
waveforms. The goal is to learn a representation able to capture high level
semantic content from the signal, e.g. phoneme identities, while being
invariant to confounding low level details in the signal such as the underlying
pitch contour or background noise. The behavior of autoencoder models depends
on the kind of constraint that is applied to the latent representation. We
compare three variants: a simple dimensionality reduction bottleneck, a
Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE
(VQ-VAE). We analyze the quality of learned representations in terms of speaker
independence, the ability to predict phonetic content, and the ability to
accurately reconstruct individual spectrogram frames. Moreover, for discrete
encodings extracted using the VQ-VAE, we measure the ease of mapping them to
phonemes. We introduce a regularization scheme that forces the representations
to focus on the phonetic content of the utterance and report performance
comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic
unit discovery task.
",1,0,0,1,0,0
1417,Many-body localization caused by temporal disorder,"  The many-body localization (MBL) is commonly related to a strong spatial
disorder. We show that MBL may alternatively be generated by adding a temporal
disorder to periodically driven many-body systems. We reach this conclusion by
mapping the evolution of such systems on the dynamics of the time-independent,
disordered, Hubbard-like models. Our result opens the way to experimental
studies of MBL in systems that reveal crystalline structures in the time
domain. In particular, we discuss two relevant setups which can be implemented
in experiments on ultra-cold atomic gases.
",0,1,0,0,0,0
1418,"Second-generation p-values: improved rigor, reproducibility, & transparency in statistical analyses","  Verifying that a statistically significant result is scientifically
meaningful is not only good scientific practice, it is a natural way to control
the Type I error rate. Here we introduce a novel extension of the p-value - a
second-generation p-value - that formally accounts for scientific relevance and
leverages this natural Type I Error control. The approach relies on a
pre-specified interval null hypothesis that represents the collection of effect
sizes that are scientifically uninteresting or are practically null. The
second-generation p-value is the proportion of data-supported hypotheses that
are also null hypotheses. As such, second-generation p-values indicate when the
data are compatible with null hypotheses, or with alternative hypotheses, or
when the data are inconclusive. Moreover, second-generation p-values provide a
proper scientific adjustment for multiple comparisons and reduce false
discovery rates. This is an advance for environments rich in data, where
traditional p-value adjustments are needlessly punitive. Second-generation
p-values promote transparency, rigor and reproducibility of scientific results
by a priori specifying which candidate hypotheses are practically meaningful
and by providing a more reliable statistical summary of when the data are
compatible with alternative or null hypotheses.
",0,0,0,1,0,0
1419,Latency Optimal Broadcasting in Noisy Wireless Mesh Networks,"  In this paper, we adopt a new noisy wireless network model introduced very
recently by Censor-Hillel et al. in [ACM PODC 2017, CHHZ17]. More specifically,
for a given noise parameter $p\in [0,1],$ any sender has a probability of $p$
of transmitting noise or any receiver of a single transmission in its
neighborhood has a probability $p$ of receiving noise.
In this paper, we first propose a new asymptotically latency-optimal
approximation algorithm (under faultless model) that can complete
single-message broadcasting task in $D+O(\log^2 n)$ time units/rounds in any
WMN of size $n,$ and diameter $D$. We then show this diameter-linear
broadcasting algorithm remains robust under the noisy wireless network model
and also improves the currently best known result in CHHZ17 by a
$\Theta(\log\log n)$ factor.
In this paper, we also further extend our robust single-message broadcasting
algorithm to $k$ multi-message broadcasting scenario and show it can broadcast
$k$ messages in $O(D+k\log n+\log^2 n)$ time rounds. This new robust
multi-message broadcasting scheme is not only asymptotically optimal but also
answers affirmatively the problem left open in CHHZ17 on the existence of an
algorithm that is robust to sender and receiver faults and can broadcast $k$
messages in $O(D+k\log n + polylog(n))$ time rounds.
",1,0,0,0,0,0
1420,Construction of Directed 2K Graphs,"  We study the problem of constructing synthetic graphs that resemble
real-world directed graphs in terms of their degree correlations. We define the
problem of directed 2K construction (D2K) that takes as input the directed
degree sequence (DDS) and a joint degree and attribute matrix (JDAM) so as to
capture degree correlation specifically in directed graphs. We provide
necessary and sufficient conditions to decide whether a target D2K is
realizable, and we design an efficient algorithm that creates realizations with
that target D2K. We evaluate our algorithm in creating synthetic graphs that
target real-world directed graphs (such as Twitter) and we show that it brings
significant benefits compared to state-of-the-art approaches.
",1,0,0,0,0,0
1421,Pattern Generation Strategies for Improving Recognition of Handwritten Mathematical Expressions,"  Recognition of Handwritten Mathematical Expressions (HMEs) is a challenging
problem because of the ambiguity and complexity of two-dimensional handwriting.
Moreover, the lack of large training data is a serious issue, especially for
academic recognition systems. In this paper, we propose pattern generation
strategies that generate shape and structural variations to improve the
performance of recognition systems based on a small training set. For data
generation, we employ the public databases: CROHME 2014 and 2016 of online
HMEs. The first strategy employs local and global distortions to generate shape
variations. The second strategy decomposes an online HME into sub-online HMEs
to get more structural variations. The hybrid strategy combines both these
strategies to maximize shape and structural variations. The generated online
HMEs are converted to images for offline HME recognition. We tested our
strategies in an end-to-end recognition system constructed from a recent deep
learning model: Convolutional Neural Network and attention-based
encoder-decoder. The results of experiments on the CROHME 2014 and 2016
databases demonstrate the superiority and effectiveness of our strategies: our
hybrid strategy achieved classification rates of 48.78% and 45.60%,
respectively, on these databases. These results are competitive compared to
others reported in recent literature. Our generated datasets are openly
available for research community and constitute a useful resource for the HME
recognition research in future.
",1,0,0,0,0,0
1422,Actions of automorphism groups of Lie groups,"  This is an expository article on properties of actions on Lie groups by
subgroups of their automorphism groups. After recalling various results on the
structure of the automorphism groups, we discuss actions with dense orbits,
invariant and quasi-invariant measures, the induced actions on the spaces of
probability measures on the groups, and results concerning various issues in
ergodic theory, topological dynamics, smooth dynamical systems, and probability
theory on Lie groups.
",0,0,1,0,0,0
1423,Interplay between relativistic energy corrections and resonant excitations in x-ray multiphoton ionization dynamics of Xe atoms,"  In this paper, we theoretically study x-ray multiphoton ionization dynamics
of heavy atoms taking into account relativistic and resonance effects. When an
atom is exposed to an intense x-ray pulse generated by an x-ray free-electron
laser (XFEL), it is ionized to a highly charged ion via a sequence of
single-photon ionization and accompanying relaxation processes, and its final
charge state is limited by the last ionic state that can be ionized by a
single-photon ionization. If x-ray multiphoton ionization involves deep
inner-shell electrons in heavy atoms, energy shifts by relativistic effects
play an important role in ionization dynamics, as pointed out in [Phys.\ Rev.\
Lett.\ \textbf{110}, 173005 (2013)]. On the other hand, if the x-ray beam has a
broad energy bandwidth, the high-intensity x-ray pulse can drive resonant
photo-excitations for a broad range of ionic states and ionize even beyond the
direct one-photon ionization limit, as first proposed in [Nature\ Photon.\
\textbf{6}, 858 (2012)]. To investigate both relativistic and resonance
effects, we extend the \textsc{xatom} toolkit to incorporate relativistic
energy corrections and resonant excitations in x-ray multiphoton ionization
dynamics calculations. Charge-state distributions are calculated for Xe atoms
interacting with intense XFEL pulses at a photon energy of 1.5~keV and 5.5~keV,
respectively. For both photon energies, we demonstrate that the role of
resonant excitations in ionization dynamics is altered due to significant
shifts of orbital energy levels by relativistic effects. Therefore it is
necessary to take into account both effects to accurately simulate multiphoton
multiple ionization dynamics at high x-ray intensity.
",0,1,0,0,0,0
1424,Collective spin excitations of helices and magnetic skyrmions: review and perspectives of magnonics in non-centrosymmetric magnets,"  Magnetic materials hosting correlated electrons play an important role for
information technology and signal processing. The currently used ferro-, ferri-
and antiferromagnetic materials provide microscopic moments (spins) that are
mainly collinear. Recently more complex spin structures such as spin helices
and cycloids have regained a lot of interest. The interest has been initiated
by the discovery of the skyrmion lattice phase in non-centrosymmetric helical
magnets. In this review we address how spin helices and skyrmion lattices
enrich the microwave characteristics of magnetic materials. When discussing
perspectives for microwave electronics and magnonics we focus particularly on
insulating materials as they avoid eddy current losses, offer low spin-wave
damping, and might allow for electric field control of collective spin
excitations. Thereby, they further fuel the vision of magnonics operated at low
energy consumption.
",0,1,0,0,0,0
1425,On the Relation between Color Image Denoising and Classification,"  Large amount of image denoising literature focuses on single channel images
and often experimentally validates the proposed methods on tens of images at
most. In this paper, we investigate the interaction between denoising and
classification on large scale dataset. Inspired by classification models, we
propose a novel deep learning architecture for color (multichannel) image
denoising and report on thousands of images from ImageNet dataset as well as
commonly used imagery. We study the importance of (sufficient) training data,
how semantic class information can be traded for improved denoising results. As
a result, our method greatly improves PSNR performance by 0.34 - 0.51 dB on
average over state-of-the art methods on large scale dataset. We conclude that
it is beneficial to incorporate in classification models. On the other hand, we
also study how noise affect classification performance. In the end, we come to
a number of interesting conclusions, some being counter-intuitive.
",1,0,0,0,0,0
1426,A simplicial decomposition framework for large scale convex quadratic programming,"  In this paper, we analyze in depth a simplicial decomposition like
algorithmic framework for large scale convex quadratic programming. In
particular, we first propose two tailored strategies for handling the master
problem. Then, we describe a few techniques for speeding up the solution of the
pricing problem. We report extensive numerical experiments on both real
portfolio optimization and general quadratic programming problems, showing the
efficiency and robustness of the method when compared to Cplex.
",0,0,1,0,0,0
1427,A Logic of Blockchain Updates,"  Blockchains are distributed data structures that are used to achieve
consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts
(like Ethereum). Although blockchains gained a lot of popularity recently,
there is no logic-based model for blockchains available. We introduce BCL, a
dynamic logic to reason about blockchain updates, and show that BCL is sound
and complete with respect to a simple blockchain model.
",1,0,0,0,0,0
1428,A proof of the Flaherty-Keller formula on the effective property of densely packed elastic composites,"  We prove in a mathematically rigorous way the asymptotic formula of Flaherty
and Keller on the effective property of densely packed periodic elastic
composites with hard inclusions. The proof is based on the primal-dual
variational principle, where the upper bound is derived by using the
Keller-type test functions and the lower bound by singular functions made of
nuclei of strain. Singular functions are solutions of the Lam?? system and
capture precisely singular behavior of the stress in the narrow region between
two adjacent hard inclusions.
",0,0,1,0,0,0
1429,Regret Bounds for Reinforcement Learning via Markov Chain Concentration,"  We give a simple optimistic algorithm for which it is easy to derive regret
bounds of $\tilde{O}(\sqrt{t_{\rm mix} SAT})$ after $T$ steps in uniformly
ergodic Markov decision processes with $S$ states, $A$ actions, and mixing time
parameter $t_{\rm mix}$. These bounds are the first regret bounds in the
general, non-episodic setting with an optimal dependence on all given
parameters. They could only be improved by using an alternative mixing time
parameter.
",0,0,0,1,0,0
1430,Superradiance with local phase-breaking effects,"  We study the superradiant evolution of a set of $N$ two-level systems
spontaneously radiating under the effect of phase-breaking mechanisms. We
investigate the dynamics generated by non-radiative losses and pure dephasing,
and their interplay with spontaneous emission. Our results show that in the
parameter region relevant to many solid-state cavity quantum electrodynamics
experiments, even with a dephasing rate much faster than the radiative lifetime
of a single two-level system, a sub-optimal collective superfluorescent burst
is still observable. We also apply our theory to the dilute excitation regime,
often used to describe optical excitations in solid-state systems. In this
regime, excitations can be described in terms of bright and dark bosonic
quasiparticles. We show how the effect of dephasing and losses in this regime
translates into inter-mode scattering rates and quasiparticle lifetimes.
",0,1,0,0,0,0
1431,Kinetic model of selectivity and conductivity of the KcsA filter,"  We introduce a self-consistent multi-species kinetic theory based on the
structure of the narrow voltage-gated potassium channel. Transition rates
depend on a complete energy spectrum with contributions including the
dehydration amongst species, interaction with the dipolar charge of the filter
and, bulk solution properties. It displays high selectivity between species
coexisting with fast conductivity, and Coulomb blockade phenomena, and it fits
well to data.
",0,1,0,0,0,0
1432,The affine approach to homogeneous geodesics in homogeneous Finsler spaces,"  In a recent paper, it was claimed that any homogeneous Finsler space of odd
dimension admits a homogeneous geodesic through any point. For the proof, the
algebraic method dealing with the reductive decomposition of the Lie algebra of
the isometry group was used. However, the proof contains a serious gap. In the
present paper, homogeneous geodesics in Finsler homogeneous spaces are studied
using the affine method, which was developed in earlier papers by the author.
The mentioned statement is proved correctly and it is further proved that any
homogeneous Berwald space or homogeneous reversible Finsler space admits a
homogeneous geodesic through any point.
",0,0,1,0,0,0
1433,About Synchronized Globular Cluster Formation over Supra-galactic Scales,"  Observational and theoretical arguments support the idea that violent events
connected with $AGN$ activity and/or intense star forming episodes have played
a significant role in the early phases of galaxy formation at high red shifts.
Being old stellar systems, globular clusters seem adequate candidates to search
for the eventual signatures that might have been left by those energetic
phenomena. The analysis of the colour distributions of several thousands of
globular clusters in the Virgo and Fornax galaxy clusters reveals the existence
of some interesting and previously undetected features. A simple pattern
recognition technique, indicates the presence of ""colour modulations"",
distinctive for each galaxy cluster. The results suggest that the globular
cluster formation process has not been completely stochastic but, rather,
included a significant fraction of globulars that formed in a synchronized way
and over supra-galactic spatial scales.
",0,1,0,0,0,0
1434,Geometric counting on wavefront real spherical spaces,"  We provide $L^p$-versus $L^\infty$-bounds for eigenfunctions on a real
spherical space $Z$ of wavefront type. It is shown that these bounds imply a
non-trivial error term estimate for lattice counting on $Z$. The paper also
serves as an introduction to geometric counting on spaces of the mentioned
type. Section 7 on higher rank is new and extends the result from v1 to higher
rank. Final version. To appear in Acta Math. Sinica.
",0,0,1,0,0,0
1435,Fate of the spin-\frac{1}{2} Kondo effect in the presence of temperature gradients,"  We consider a strongly interacting quantum dot connected to two leads held at
quite different temperatures. Our aim is to study the behavior of the Kondo
effect in the presence of large thermal biases. We use three different
approaches, namely, a perturbation formalism based on the Kondo Hamiltonian, a
slave-boson mean-field theory for the Anderson model at large charging energies
and a truncated equation-of-motion approach beyond the Hartree-Fock
approximation. The two former formalisms yield a suppression of the Kondo peak
for thermal gradients above the Kondo temperature, showing a remarkably good
agreement despite their different ranges of validity. The third technique
allows us to analyze the full density of states within a wide range of
energies. Additionally, we have investigated the quantum transport properties
(electric current and thermocurrent) beyond linear response. In the
voltage-driven case, we reproduce the split differential conductance due to the
presence of different electrochemical potentials. In the temperature-driven
case, we observe a strongly nonlinear thermocurrent as a function of the
applied thermal gradient. Depending on the parameters, we can find nontrivial
zeros in the electric current for finite values of the temperature bias.
Importantly, these thermocurrent zeros yield direct access to the system's
characteristic energy scales (Kondo temperature and charging energy).
",0,1,0,0,0,0
1436,Extragalactic source population studies at very high energies in the Cherenkov Telescope Array era,"  The Cherenkov Telescope Array (CTA) is the next generation ground-based
$\gamma$-ray observatory. It will provide an order of magnitude better
sensitivity and an extended energy coverage, 20 GeV - 300 TeV, relative to
current Imaging Atmospheric Cherenkov Telescopes (IACTs). IACTs, despite
featuring an excellent sensitivity, are characterized by a limited field of
view that makes the blind search of new sources very time inefficient.
Fortunately, the $\textit{Fermi}$-LAT collaboration recently released a new
catalog of 1,556 sources detected in the 10 GeV - 2 TeV range by the Large Area
Telescope (LAT) in the first 7 years of its operation (the 3FHL catalog). This
catalog is currently the most appropriate description of the sky that will be
energetically accessible to CTA. Here, we discuss a detailed analysis of the
extragalactic source population (mostly blazars) that will be studied in the
near future by CTA. This analysis is based on simulations built from the
expected array configurations and information reported in the 3FHL catalog.
These results show the improvements that CTA will provide on the extragalactic
TeV source population studies, which will be carried out by Key Science
Projects as well as dedicated proposals.
",0,1,0,0,0,0
1437,Modeling the SBC Tanzania Production-Distribution Logistics Network,"  The increase in customer expectation in terms of cost and services rendered,
coupled with competitive business environment and uncertainty in cost of raw
materials have posed challenges on effective supply chain engineering making it
essential to do cost-benefit analysis before making final decisions on
production distribution logistics. This paper provides a conceptual model that
provide guidance in supply chain decision making for business expansion. It
presents a mathematical model for production-distribution of an integrated
supply chain derived from current operations of SBC Tanzania Ltd which is a
major supply chain that manages products' distribution in whole of Tanzania. In
addition to finding the optimal cost, we also carried out a sensitivity
analysis on the model so as to find ways in which the company can expand at
optimal cost, while meeting customers' demands. Genetic algorithms is used to
run the simulation for their efficient in solving combinatorial problems.
",0,0,1,0,0,0
1438,Dark matter in dwarf galaxies,"  Although the cusp-core controversy for dwarf galaxies is seen as a problem, I
argue that the cored central profiles can be explained by flattened cusps
because they suffer from conflicting measurements and poor statistics and
because there is a large number of conventional processes that could have
flattened them since their creation, none of which requires new physics. Other
problems, such as ""too big to fail"", are not discussed.
",0,1,0,0,0,0
1439,A Survey of Parallel A*,"  A* is a best-first search algorithm for finding optimal-cost paths in graphs.
A* benefits significantly from parallelism because in many applications, A* is
limited by memory usage, so distributed memory implementations of A* that use
all of the aggregate memory on the cluster enable problems that can not be
solved by serial, single-machine implementations to be solved. We survey
approaches to parallel A*, focusing on decentralized approaches to A* which
partition the state space among processors. We also survey approaches to
parallel, limited-memory variants of A* such as parallel IDA*.
",1,0,0,0,0,0
1440,Large second harmonic generation enhancement in SiN waveguides by all-optically induced quasi phase matching,"  Integrated waveguides exhibiting efficient second-order nonlinearities are
crucial to obtain compact and low power optical signal processing devices.
Silicon nitride (SiN) has shown second harmonic generation (SHG) capabilities
in resonant structures and single-pass devices leveraging intermodal phase
matching, which is defined by waveguide design. Lithium niobate allows
compensating for the phase mismatch using periodically poled waveguides,
however the latter are not reconfigurable and remain difficult to integrate
with SiN or silicon (Si) circuits. Here we show the all-optical enhancement of
SHG in SiN waveguides by more than 30 dB. We demonstrate that a Watt-level
laser causes a periodic modification of the waveguide second-order
susceptibility. The resulting second order nonlinear grating has a periodicity
allowing for quasi phase matching (QPM) between the pump and SH mode. Moreover,
changing the pump wavelength or polarization updates the period, relaxing phase
matching constraints imposed by the waveguide geometry. We show that the
grating is long term inscribed in the waveguides, and we estimate a second
order nonlinearity of the order of 0.3 pm/V, while a maximum conversion
efficiency (CE) of 1.8x10-6 W-1 cm-2 is reached.
",0,1,0,0,0,0
1441,Large Scale Constrained Linear Regression Revisited: Faster Algorithms via Preconditioning,"  In this paper, we revisit the large-scale constrained linear regression
problem and propose faster methods based on some recent developments in
sketching and optimization. Our algorithms combine (accelerated) mini-batch SGD
with a new method called two-step preconditioning to achieve an approximate
solution with a time complexity lower than that of the state-of-the-art
techniques for the low precision case. Our idea can also be extended to the
high precision case, which gives an alternative implementation to the Iterative
Hessian Sketch (IHS) method with significantly improved time complexity.
Experiments on benchmark and synthetic datasets suggest that our methods indeed
outperform existing ones considerably in both the low and high precision cases.
",0,0,0,1,0,0
1442,Geometrical dependence of domain wall propagation and nucleation fields in magnetic domain wall sensor devices,"  We study the key domain wall properties in segmented nanowires loop-based
structures used in domain wall based sensors. The two reasons for device
failure, namely the distribution of domain wall propagation field (depinning)
and the nucleation field are determined with Magneto-Optical Kerr Effect (MOKE)
and Giant Magnetoresistance (GMR) measurements for thousands of elements to
obtain significant statistics. Single layers of Ni$_{81}$Fe$_{19}$, a complete
GMR stack with Co$_{90}$Fe$_{10}$/Ni$_{81}$Fe$_{19}$ as a free layer and a
single layer of Co$_{90}$Fe$_{10}$ are deposited and industrially patterned to
determine the influence of the shape anisotropy, the magnetocrystalline
anisotropy and the fabrication processes. We show that the propagation field is
little influenced by the geometry but significantly by material parameters. The
domain wall nucleation fields can be described by a typical Stoner-Wohlfarth
model related to the measured geometrical parameters of the wires and fitted by
considering the process parameters. The GMR effect is subsequently measured in
a substantial number of devices (3000), in order to accurately gauge the
variation between devices. This reveals a corrected upper limit to the
nucleation fields of the sensors that can be exploited for fast
characterization of working elements.
",0,1,0,0,0,0
1443,Faster Rates for Policy Learning,"  This article improves the existing proven rates of regret decay in optimal
policy estimation. We give a margin-free result showing that the regret decay
for estimating a within-class optimal policy is second-order for empirical risk
minimizers over Donsker classes, with regret decaying at a faster rate than the
standard error of an efficient estimator of the value of an optimal policy. We
also give a result from the classification literature that shows that faster
regret decay is possible via plug-in estimation provided a margin condition
holds. Four examples are considered. In these examples, the regret is expressed
in terms of either the mean value or the median value; the number of possible
actions is either two or finitely many; and the sampling scheme is either
independent and identically distributed or sequential, where the latter
represents a contextual bandit sampling scheme.
",0,0,1,1,0,0
1444,Anisotropic Exchange in ${\bf LiCu_2O_2}$,"  We investigate the magnetic properties of the multiferroic quantum-spin
system LiCu$_2$O$_2$ by electron spin resonance (ESR) measurements at $X$- and
$Q$-band frequencies in a wide temperature range $(T_{\rm N1} \leq T \leq
300$\,K). The observed anisotropies of the $g$ tensor and the ESR linewidth in
untwinned single crystals result from the crystal-electric field and from local
exchange geometries acting on the magnetic Cu$^{2+}$ ions in the zigzag-ladder
like structure of LiCu$_2$O$_2$. Supported by a microscopic analysis of the
exchange paths involved, we show that both the symmetric anisotropic exchange
interaction and the antisymmetric Dzyaloshinskii-Moriya interaction provide the
dominant spin-spin relaxation channels in this material.
",0,1,0,0,0,0
1445,Which friends are more popular than you? Contact strength and the friendship paradox in social networks,"  The friendship paradox states that in a social network, egos tend to have
lower degree than their alters, or, ""your friends have more friends than you
do"". Most research has focused on the friendship paradox and its implications
for information transmission, but treating the network as static and
unweighted. Yet, people can dedicate only a finite fraction of their attention
budget to each social interaction: a high-degree individual may have less time
to dedicate to individual social links, forcing them to modulate the quantities
of contact made to their different social ties. Here we study the friendship
paradox in the context of differing contact volumes between egos and alters,
finding a connection between contact volume and the strength of the friendship
paradox. The most frequently contacted alters exhibit a less pronounced
friendship paradox compared with the ego, whereas less-frequently contacted
alters are more likely to be high degree and give rise to the paradox. We argue
therefore for a more nuanced version of the friendship paradox: ""your closest
friends have slightly more friends than you do"", and in certain networks even:
""your best friend has no more friends than you do"". We demonstrate that this
relationship is robust, holding in both a social media and a mobile phone
dataset. These results have implications for information transfer and influence
in social networks, which we explore using a simple dynamical model.
",1,1,0,0,0,0
1446,Stochastic Optimization with Bandit Sampling,"  Many stochastic optimization algorithms work by estimating the gradient of
the cost function on the fly by sampling datapoints uniformly at random from a
training set. However, the estimator might have a large variance, which
inadvertently slows down the convergence rate of the algorithms. One way to
reduce this variance is to sample the datapoints from a carefully selected
non-uniform distribution. In this work, we propose a novel non-uniform sampling
approach that uses the multi-armed bandit framework. Theoretically, we show
that our algorithm asymptotically approximates the optimal variance within a
factor of 3. Empirically, we show that using this datapoint-selection technique
results in a significant reduction in the convergence time and variance of
several stochastic optimization algorithms such as SGD, SVRG and SAGA. This
approach for sampling datapoints is general, and can be used in conjunction
with any algorithm that uses an unbiased gradient estimation -- we expect it to
have broad applicability beyond the specific examples explored in this work.
",1,0,0,1,0,0
1447,Learning Robust Options,"  Robust reinforcement learning aims to produce policies that have strong
guarantees even in the face of environments/transition models whose parameters
have strong uncertainty. Existing work uses value-based methods and the usual
primitive action setting. In this paper, we propose robust methods for learning
temporally abstract actions, in the framework of options. We present a Robust
Options Policy Iteration (ROPI) algorithm with convergence guarantees, which
learns options that are robust to model uncertainty. We utilize ROPI to learn
robust options with the Robust Options Deep Q Network (RO-DQN) that solves
multiple tasks and mitigates model misspecification due to model uncertainty.
We present experimental results which suggest that policy iteration with linear
features may have an inherent form of robustness when using coarse feature
representations. In addition, we present experimental results which demonstrate
that robustness helps policy iteration implemented on top of deep neural
networks to generalize over a much broader range of dynamics than non-robust
policy iteration.
",0,0,0,1,0,0
1448,Levitation of non-magnetizable droplet inside ferrofluid,"  The central theme of this work is that a stable levitation of a denser
non-magnetizable liquid droplet, against gravity, inside a relatively lighter
ferrofluid -- a system barely considered in ferrohydrodynamics -- is possible,
and exhibits unique interfacial features; the stability of the levitation
trajectory, however, is subject to an appropriate magnetic field modulation. We
explore the shapes and the temporal dynamics of a plane non-magnetizable
droplet levitating inside ferrofluid against gravity due to a spatially
complex, but systematically generated, magnetic field in two dimensions. The
effect of the viscosity ratio, the stability of the levitation path and the
possibility of existence of multiple-stable equilibrium states is investigated.
We find, for certain conditions on the viscosity ratio, that there can be
developments of cusps and singularities at the droplet surface; this phenomenon
we also observe experimentally and compared with the simulations. Our
simulations closely replicate the singular projection on the surface of the
levitating droplet. Finally, we present an dynamical model for the vertical
trajectory of the droplet. This model reveals a condition for the onset of
levitation and the relation for the equilibrium levitation height. The
linearization of the model around the steady state captures that the nature of
the equilibrium point goes under a transition from being a spiral to a node
depending upon the control parameters, which essentially means that the
temporal route to the equilibrium can be either monotonic or undulating. The
analytical model for the droplet trajectory is in close agreement with the
detailed simulations. (See draft for full abstract).
",0,1,0,0,0,0
1449,Simultaneous Detection of H and D NMR Signals in a micro-Tesla Field,"  We present NMR spectra of remote-magnetized deuterated water, detected in an
unshielded environment by means of a differential atomic magnetometer. The
measurements are performed in a $\mu$T field, while pulsed techniques are
applied -following the sample displacement- in a 100~$\mu$T field, to tip both
D and H nuclei by controllable amounts. The broadband nature of the detection
system enables simultaneous detection of the two signals and accurate
evaluation of their decay times. The outcomes of the experiment demonstrate the
potential of ultra-low-field NMR spectroscopy in important applications where
the correlation between proton and deuteron spin-spin relaxation rates as a
function of external parameters contains significant information.
",0,1,0,0,0,0
1450,Learning Deep Networks from Noisy Labels with Dropout Regularization,"  Large datasets often have unreliable labels-such as those obtained from
Amazon's Mechanical Turk or social media platforms-and classifiers trained on
mislabeled datasets often exhibit poor performance. We present a simple,
effective technique for accounting for label noise when training deep neural
networks. We augment a standard deep network with a softmax layer that models
the label noise statistics. Then, we train the deep network and noise model
jointly via end-to-end stochastic gradient descent on the (perhaps mislabeled)
dataset. The augmented model is overdetermined, so in order to encourage the
learning of a non-trivial noise model, we apply dropout regularization to the
weights of the noise model during training. Numerical experiments on noisy
versions of the CIFAR-10 and MNIST datasets show that the proposed dropout
technique outperforms state-of-the-art methods.
",1,0,0,1,0,0
1451,On Efficiently Detecting Overlapping Communities over Distributed Dynamic Graphs,"  Modern networks are of huge sizes as well as high dynamics, which challenges
the efficiency of community detection algorithms. In this paper, we study the
problem of overlapping community detection on distributed and dynamic graphs.
Given a distributed, undirected and unweighted graph, the goal is to detect
overlapping communities incrementally as the graph is dynamically changing. We
propose an efficient algorithm, called \textit{randomized Speaker-Listener
Label Propagation Algorithm} (rSLPA), based on the \textit{Speaker-Listener
Label Propagation Algorithm} (SLPA) by relaxing the probability distribution of
label propagation. Besides detecting high-quality communities, rSLPA can
incrementally update the detected communities after a batch of edge insertion
and deletion operations. To the best of our knowledge, rSLPA is the first
algorithm that can incrementally capture the same communities as those obtained
by applying the detection algorithm from the scratch on the updated graph.
Extensive experiments are conducted on both synthetic and real-world datasets,
and the results show that our algorithm can achieve high accuracy and
efficiency at the same time.
",1,0,0,0,0,0
1452,Structured Black Box Variational Inference for Latent Time Series Models,"  Continuous latent time series models are prevalent in Bayesian modeling;
examples include the Kalman filter, dynamic collaborative filtering, or dynamic
topic models. These models often benefit from structured, non mean field
variational approximations that capture correlations between time steps. Black
box variational inference with reparameterization gradients (BBVI) allows us to
explore a rich new class of Bayesian non-conjugate latent time series models;
however, a naive application of BBVI to such structured variational models
would scale quadratically in the number of time steps. We describe a BBVI
algorithm analogous to the forward-backward algorithm which instead scales
linearly in time. It allows us to efficiently sample from the variational
distribution and estimate the gradients of the ELBO. Finally, we show results
on the recently proposed dynamic word embedding model, which was trained using
our method.
",1,0,0,1,0,0
1453,$L^p$ Norms of Eigenfunctions on Regular Graphs and on the Sphere,"  We prove upper bounds on the $L^p$ norms of eigenfunctions of the discrete
Laplacian on regular graphs. We then apply these ideas to study the $L^p$ norms
of joint eigenfunctions of the Laplacian and an averaging operator over a
finite collection of algebraic rotations of the $2$-sphere. Under mild
conditions, such joint eigenfunctions are shown to satisfy for large $p$ the
same bounds as those known for Laplace eigenfunctions on a surface of
non-positive curvature.
",0,0,1,0,0,0
1454,Spatially distributed multipartite entanglement enables Einstein-Podolsky-Rosen steering of atomic clouds,"  A key resource for distributed quantum-enhanced protocols is entanglement
between spatially separated modes. Yet, the robust generation and detection of
nonlocal entanglement between spatially separated regions of an ultracold
atomic system remains a challenge. Here, we use spin mixing in a tightly
confined Bose-Einstein condensate to generate an entangled state of
indistinguishable particles in a single spatial mode. We show experimentally
that this local entanglement can be spatially distributed by self-similar
expansion of the atomic cloud. Spatially resolved spin read-out is used to
reveal a particularly strong form of quantum correlations known as
Einstein-Podolsky-Rosen steering between distinct parts of the expanded cloud.
Based on the strength of Einstein-Podolsky-Rosen steering we construct a
witness, which testifies up to genuine five-partite entanglement.
",0,1,0,0,0,0
1455,"Multipath IP Routing on End Devices: Motivation, Design, and Performance","  Most end devices are now equipped with multiple network interfaces.
Applications can exploit all available interfaces and benefit from multipath
transmission. Recently Multipath TCP (MPTCP) was proposed to implement
multipath transmission at the transport layer and has attracted lots of
attention from academia and industry. However, MPTCP only supports TCP-based
applications and its multipath routing flexibility is limited. In this paper,
we investigate the possibility of orchestrating multipath transmission from the
network layer of end devices, and develop a Multipath IP (MPIP) design
consisting of signaling, session and path management, multipath routing, and
NAT traversal. We implement MPIP in Linux and Android kernels. Through
controlled lab experiments and Internet experiments, we demonstrate that MPIP
can effectively achieve multipath gains at the network layer. It not only
supports the legacy TCP and UDP protocols, but also works seamlessly with
MPTCP. By facilitating user-defined customized routing, MPIP can route traffic
from competing applications in a coordinated fashion to maximize the aggregate
user Quality-of-Experience.
",1,0,0,0,0,0
1456,Defense semantics of argumentation: encoding reasons for accepting arguments,"  In this paper we show how the defense relation among abstract arguments can
be used to encode the reasons for accepting arguments. After introducing a
novel notion of defenses and defense graphs, we propose a defense semantics
together with a new notion of defense equivalence of argument graphs, and
compare defense equivalence with standard equivalence and strong equivalence,
respectively. Then, based on defense semantics, we define two kinds of reasons
for accepting arguments, i.e., direct reasons and root reasons, and a notion of
root equivalence of argument graphs. Finally, we show how the notion of root
equivalence can be used in argumentation summarization.
",1,0,0,0,0,0
1457,Fast Global Convergence via Landscape of Empirical Loss,"  While optimizing convex objective (loss) functions has been a powerhouse for
machine learning for at least two decades, non-convex loss functions have
attracted fast growing interests recently, due to many desirable properties
such as superior robustness and classification accuracy, compared with their
convex counterparts. The main obstacle for non-convex estimators is that it is
in general intractable to find the optimal solution. In this paper, we study
the computational issues for some non-convex M-estimators. In particular, we
show that the stochastic variance reduction methods converge to the global
optimal with linear rate, by exploiting the statistical property of the
population loss. En route, we improve the convergence analysis for the batch
gradient method in \cite{mei2016landscape}.
",0,0,0,1,0,0
1458,Photodetector figures of merit in terms of POVMs,"  A photodetector may be characterized by various figures of merit such as
response time, bandwidth, dark count rate, efficiency, wavelength resolution,
and photon-number resolution. On the other hand, quantum theory says that any
measurement device is fully described by its POVM, which stands for
Positive-Operator-Valued Measure, and which generalizes the textbook notion of
the eigenstates of the appropriate hermitian operator (the ""observable"") as
measurement outcomes. Here we show how to define a multitude of photodetector
figures of merit in terms of a given POVM. We distinguish classical and quantum
figures of merit and issue a conjecture regarding trade-off relations between
them. We discuss the relationship between POVM elements and photodetector
clicks, and how models of photodetectors may be tested by measuring either POVM
elements or figures of merit. Finally, the POVM is advertised as a
platform-independent way of comparing different types of photodetectors, since
any such POVM refers to the Hilbert space of the incoming light, and not to any
Hilbert space internal to the detector.
",0,1,0,0,0,0
1459,Kinetics of Protein-DNA Interactions: First-Passage Analysis,"  All living systems can function only far away from equilibrium, and for this
reason chemical kinetic methods are critically important for uncovering the
mechanisms of biological processes. Here we present a new theoretical method of
investigating dynamics of protein-DNA interactions, which govern all major
biological processes. It is based on a first-passage analysis of biochemical
and biophysical transitions, and it provides a fully analytic description of
the processes. Our approach is explained for the case of a single protein
searching for a specific binding site on DNA. In addition, the application of
the method to investigations of the effect of DNA sequence heterogeneity, and
the role multiple targets and traps in the protein search dynamics are
discussed.
",0,0,0,0,1,0
1460,Jamming transitions induced by an attraction in pedestrian flow,"  We numerically study jamming transitions in pedestrian flow interacting with
an attraction, mostly based on the social force model for pedestrians who can
join the attraction. We formulate the joining probability as a function of
social influence from others, reflecting that individual choice behavior is
likely influenced by others. By controlling pedestrian influx and the social
influence parameter, we identify various pedestrian flow patterns. For the
bidirectional flow scenario, we observe a transition from the free flow phase
to the freezing phase, in which oppositely walking pedestrians reach a complete
stop and block each other. On the other hand, a different transition behavior
appears in the unidirectional flow scenario, i.e., from the free flow phase to
the localized jam phase and then to the extended jam phase. It is also observed
that the extended jam phase can end up in freezing phenomena with a certain
probability when pedestrian flux is high with strong social influence. This
study highlights that attractive interactions between pedestrians and an
attraction can trigger jamming transitions by increasing the number of
conflicts among pedestrians near the attraction. In order to avoid excessive
pedestrian jams, we suggest suppressing the number of conflicts under a certain
level by moderating pedestrian influx especially when the social influence is
strong.
",0,1,0,0,0,0
1461,A Deep Active Survival Analysis Approach for Precision Treatment Recommendations: Application of Prostate Cancer,"  Survival analysis has been developed and applied in the number of areas
including manufacturing, finance, economics and healthcare. In healthcare
domain, usually clinical data are high-dimensional, sparse and complex and
sometimes there exists few amount of time-to-event (labeled) instances.
Therefore building an accurate survival model from electronic health records is
challenging. With this motivation, we address this issue and provide a new
survival analysis framework using deep learning and active learning with a
novel sampling strategy. First, our approach provides better representation
with lower dimensions from clinical features using labeled (time-to-event) and
unlabeled (censored) instances and then actively trains the survival model by
labeling the censored data using an oracle. As a clinical assistive tool, we
introduce a simple effective treatment recommendation approach based on our
survival model. In the experimental study, we apply our approach on
SEER-Medicare data related to prostate cancer among African-Americans and white
patients. The results indicate that our approach outperforms significantly than
baseline models.
",0,0,0,1,0,0
1462,Detecting Topological Changes in Dynamic Community Networks,"  The study of time-varying (dynamic) networks (graphs) is of fundamental
importance for computer network analytics. Several methods have been proposed
to detect the effect of significant structural changes in a time series of
graphs. The main contribution of this work is a detailed analysis of a dynamic
community graph model. This model is formed by adding new vertices, and
randomly attaching them to the existing nodes. It is a dynamic extension of the
well-known stochastic blockmodel. The goal of the work is to detect the time at
which the graph dynamics switches from a normal evolution -- where balanced
communities grow at the same rate -- to an abnormal behavior -- where
communities start merging. In order to circumvent the problem of decomposing
each graph into communities, we use a metric to quantify changes in the graph
topology as a function of time. The detection of anomalies becomes one of
testing the hypothesis that the graph is undergoing a significant structural
change. In addition the the theoretical analysis of the test statistic, we
perform Monte Carlo simulations of our dynamic graph model to demonstrate that
our test can detect changes in graph topology.
",1,0,0,0,0,0
1463,Online Boosting Algorithms for Multi-label Ranking,"  We consider the multi-label ranking approach to multi-label learning.
Boosting is a natural method for multi-label ranking as it aggregates weak
predictions through majority votes, which can be directly used as scores to
produce a ranking of the labels. We design online boosting algorithms with
provable loss bounds for multi-label ranking. We show that our first algorithm
is optimal in terms of the number of learners required to attain a desired
accuracy, but it requires knowledge of the edge of the weak learners. We also
design an adaptive algorithm that does not require this knowledge and is hence
more practical. Experimental results on real data sets demonstrate that our
algorithms are at least as good as existing batch boosting algorithms.
",1,0,0,1,0,0
1464,Semisuper Efimov effect of two-dimensional bosons at a three-body resonance,"  Wave-particle duality in quantum mechanics allows for a halo bound state
whose spatial extension far exceeds a range of the interaction potential. What
is even more striking is that such quantum halos can be arbitrarily large on
special occasions. The two examples known so far are the Efimov effect and the
super Efimov effect, which predict that spatial extensions of higher excited
states grow exponentially and double exponentially, respectively. Here, we
establish yet another new class of arbitrarily large quantum halos formed by
spinless bosons with short-range interactions in two dimensions. When the
two-body interaction is absent but the three-body interaction is resonant, four
bosons exhibit an infinite tower of bound states whose spatial extensions scale
as $R_n\sim e^{(\pi n)^2/27}$ for a large $n$. The emergent scaling law is
universal and is termed a semisuper Efimov effect, which together with the
Efimov and super Efimov effects constitutes a trio of few-body universality
classes allowing for arbitrarily large quantum halos.
",0,1,0,0,0,0
1465,Free quantitative fourth moment theorems on Wigner space,"  We prove a quantitative Fourth Moment Theorem for Wigner integrals of any
order with symmetric kernels, generalizing an earlier result from Kemp et al.
(2012). The proof relies on free stochastic analysis and uses a new biproduct
formula for bi-integrals. A consequence of our main result is a
Nualart-Ortiz-Latorre type characterization of convergence in law to the
semicircular distribution for Wigner integrals. As an application, we provide
Berry-Esseen type bounds in the context of the free Breuer-Major theorem for
the free fractional Brownian motion.
",0,0,1,0,0,0
1466,Optimizing the Latent Space of Generative Networks,"  Generative Adversarial Networks (GANs) have been shown to be able to sample
impressively realistic images. GAN training consists of a saddle point
optimization problem that can be thought of as an adversarial game between a
generator which produces the images, and a discriminator, which judges if the
images are real. Both the generator and the discriminator are commonly
parametrized as deep convolutional neural networks. The goal of this paper is
to disentangle the contribution of the optimization procedure and the network
parametrization to the success of GANs. To this end we introduce and study
Generative Latent Optimization (GLO), a framework to train a generator without
the need to learn a discriminator, thus avoiding challenging adversarial
optimization problems. We show experimentally that GLO enjoys many of the
desirable properties of GANs: learning from large data, synthesizing
visually-appealing samples, interpolating meaningfully between samples, and
performing linear arithmetic with noise vectors.
",1,0,0,1,0,0
1467,Conservativity of realizations on motives of abelian type over finite fields,"  We show that the l-adic realization functor is conservative when restricted
to the Chow motives of abelian type over a finite field. A weak version of this
conservativity result extends to mixed motives of abelian type.
",0,0,1,0,0,0
1468,Towards understanding startup product development as effectual entrepreneurial behaviors,"  Software startups face with multiple technical and business challenges, which
could make the startup journey longer, or even become a failure. Little is
known about entrepreneurial decision making as a direct force to startup
development outcome. In this study, we attempted to apply a behaviour theory of
entrepreneurial firms to understand the root-cause of some software startup s
challenges. Six common challenges related to prototyping and product
development in twenty software startups were identified. We found the behaviour
theory as a useful theoretical lens to explain the technical challenges.
Software startups search for local optimal solutions, emphasise on short-run
feedback rather than long-run strategies, which results in vague prototype
planning, paradox of demonstration and evolving throw-away prototypes. The
finding implies that effectual entrepreneurial processes might require a more
suitable product development approach than the current state-of-practice.
",1,0,0,0,0,0
1469,Generalized Dirac structure beyond the linear regime in graphene,"  We show that a generalized Dirac structure survives beyond the linear regime
of the low-energy dispersion relations of graphene. A generalized uncertainty
principle of the kind compatible with specific quantum gravity scenarios with a
fundamental minimal length (here graphene lattice spacing) and Lorentz
violation (here the particle/hole asymmetry, the trigonal warping, etc.) is
naturally obtained. We then show that the corresponding emergent field theory
is a table-top realization of such scenarios, by explicitly computing the third
order Hamiltonian, and giving the general recipe for any order. Remarkably, our
results imply that going beyond the low-energy approximation does not spoil the
well-known correspondence with analogue massless quantum electrodynamics
phenomena (as usually believed), but rather it is a way to obtain experimental
signatures of quantum-gravity-like corrections to such phenomena.
",0,1,0,0,0,0
1470,Generative Mixture of Networks,"  A generative model based on training deep architectures is proposed. The
model consists of K networks that are trained together to learn the underlying
distribution of a given data set. The process starts with dividing the input
data into K clusters and feeding each of them into a separate network. After
few iterations of training networks separately, we use an EM-like algorithm to
train the networks together and update the clusters of the data. We call this
model Mixture of Networks. The provided model is a platform that can be used
for any deep structure and be trained by any conventional objective function
for distribution modeling. As the components of the model are neural networks,
it has high capability in characterizing complicated data distributions as well
as clustering data. We apply the algorithm on MNIST hand-written digits and
Yale face datasets. We also demonstrate the clustering ability of the model
using some real-world and toy examples.
",1,0,0,1,0,0
1471,Shape-dependence of the barrier for skyrmions on a two-lane racetrack,"  Single magnetic skyrmions are localized whirls in the magnetization with an
integer winding number. They have been observed on nano-meter scales up to room
temperature in multilayer structures. Due to their small size, topological
winding number, and their ability to be manipulated by extremely tiny forces,
they are often called interesting candidates for future memory devices. The
two-lane racetrack has to exhibit two lanes that are separated by an energy
barrier. The information is then encoded in the position of a skyrmion which is
located in one of these close-by lanes. The artificial barrier between the
lanes can be created by an additional nanostrip on top of the track. Here we
study the dependence of the potential barrier on the shape of the additional
nanostrip, calculating the potentials for a rectangular, triangular, and
parabolic cross section, as well as interpolations between the first two. We
find that a narrow barrier is always repulsive and that the height of the
potential strongly depends on the shape of the nanostrip, whereas the shape of
the potential is more universal. We finally show that the shape-dependence is
redundant for possible applications.
",0,1,0,0,0,0
1472,"Further Results on Size and Power of Heteroskedasticity and Autocorrelation Robust Tests, with an Application to Trend Testing","  We complement the theory developed in Preinerstorfer and P??tscher (2016)
with further finite sample results on size and power of heteroskedasticity and
autocorrelation robust tests. These allows us, in particular, to show that the
sufficient conditions for the existence of size-controlling critical values
recently obtained in P??tscher and Preinerstorfer (2016) are often also
necessary. We furthermore apply the results obtained to tests for hypotheses on
deterministic trends in stationary time series regressions, and find that many
tests currently used are strongly size-distorted.
",0,0,1,1,0,0
1473,A powerful approach to the study of moderate effect modification in observational studies,"  Effect modification means the magnitude or stability of a treatment effect
varies as a function of an observed covariate. Generally, larger and more
stable treatment effects are insensitive to larger biases from unmeasured
covariates, so a causal conclusion may be considerably firmer if this pattern
is noted if it occurs. We propose a new strategy, called the submax-method,
that combines exploratory and confirmatory efforts to determine whether there
is stronger evidence of causality - that is, greater insensitivity to
unmeasured confounding - in some subgroups of individuals. It uses the joint
distribution of test statistics that split the data in various ways based on
certain observed covariates. For $L$ binary covariates, the method splits the
population $L$ times into two subpopulations, perhaps first men and women,
perhaps then smokers and nonsmokers, computing a test statistic from each
subpopulation, and appends the test statistic for the whole population, making
$2L+1$ test statistics in total. Although $L$ binary covariates define $2^{L}$
interaction groups, only $2L+1$ tests are performed, and at least $L+1$ of
these tests use at least half of the data. The submax-method achieves the
highest design sensitivity and the highest Bahadur efficiency of its component
tests. Moreover, the form of the test is sufficiently tractable that its large
sample power may be studied analytically. The simulation suggests that the
submax method exhibits superior performance, in comparison with an approach
using CART, when there is effect modification of moderate size. Using data from
the NHANES I Epidemiologic Follow-Up Survey, an observational study of the
effects of physical activity on survival is used to illustrate the method. The
method is implemented in the $\texttt{R}$ package $\texttt{submax}$ which
contains the NHANES example.
",0,0,0,1,0,0
1474,"Ad-blocking: A Study on Performance, Privacy and Counter-measures","  Many internet ventures rely on advertising for their revenue. However, users
feel discontent by the presence of ads on the websites they visit, as the
data-size of ads is often comparable to that of the actual content. This has an
impact not only on the loading time of webpages, but also on the internet bill
of the user in some cases. In absence of a mutually-agreed procedure for opting
out of advertisements, many users resort to ad-blocking browser-extensions. In
this work, we study the performance of popular ad-blockers on a large set of
news websites. Moreover, we investigate the benefits of ad-blockers on user
privacy as well as the mechanisms used by websites to counter them. Finally, we
explore the traffic overhead due to the ad-blockers themselves.
",1,0,0,0,0,0
1475,On the quantum differentiation of smooth real-valued functions,"  Calculating the value of $C^{k\in\{1,\infty\}}$ class of smoothness
real-valued function's derivative in point of $\mathbb{R}^+$ in radius of
convergence of its Taylor polynomial (or series), applying an analog of
Newton's binomial theorem and $q$-difference operator. $(P,q)$-power difference
introduced in section 5. Additionally, by means of Newton's interpolation
formula, the discrete analog of Taylor series, interpolation using
$q$-difference and $p,q$-power difference is shown.
",0,0,1,0,0,0
1476,On recognizing shapes of polytopes from their shadows,"  Let $P$ and $Q$ be two convex polytopes both contained in the interior of an
Euclidean ball $r\textbf{B}^{d}$. We prove that $P=Q$ provided that their sight
cones from any point on the sphere $rS^{d-1}$ are congruent. We also prove an
analogous result for spherical projections.
",0,0,1,0,0,0
1477,Variational methods for steady-state Darcy/Fick flow in swollen and poroelastic solids,"  Existence of steady states in elastic media at small strains with diffusion
of a solvent or fluid due to Fick's or Darcy's laws is proved by combining
usage of variational methods inspired from static situations with Schauder's
fixed-point arguments. In the plain variant, the problem consists in the force
equilibrium coupled with the continuity equation, and the underlying operator
is non-potential and non-pseudomonotone so that conventional methods are not
applicable. In advanced variants, electrically-charged multi-component flows
through an electrically charged elastic solid are treated, employing critical
points of the saddle-point type. Eventually, anisothermal variants involving
heat-transfer equation are treated, too.
",0,0,1,0,0,0
1478,Case Studies on Plasma Wakefield Accelerator Design,"  The field of plasma-based particle accelerators has seen tremendous progress
over the past decade and experienced significant growth in the number of
activities. During this process, the involved scientific community has expanded
from traditional university-based research and is now encompassing many large
research laboratories worldwide, such as BNL, CERN, DESY, KEK, LBNL and SLAC.
As a consequence, there is a strong demand for a consolidated effort in
education at the intersection of accelerator, laser and plasma physics. The
CERN Accelerator School on Plasma Wake Acceleration has been organized as a
result of this development. In this paper, we describe the interactive
component of this one-week school, which consisted of three case studies to be
solved in 11 working groups by the participants of the CERN Accelerator School.
",0,1,0,0,0,0
1479,GANs for Biological Image Synthesis,"  In this paper, we propose a novel application of Generative Adversarial
Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy.
Compared to natural images, cells tend to have a simpler and more geometric
global structure that facilitates image generation. However, the correlation
between the spatial pattern of different fluorescent proteins reflects
important biological functions, and synthesized images have to capture these
relationships to be relevant for biological applications. We adapt GANs to the
task at hand and propose new models with casual dependencies between image
channels that can generate multi-channel images, which would be impossible to
obtain experimentally. We evaluate our approach using two independent
techniques and compare it against sensible baselines. Finally, we demonstrate
that by interpolating across the latent space we can mimic the known changes in
protein localization that occur through time during the cell cycle, allowing us
to predict temporal evolution from static images.
",1,0,0,1,0,0
1480,An objective classification of Saturn cloud features from Cassini ISS images,"  A clustering algorithm is applied to Cassini Imaging Science Subsystem
continuum and methane band images of Saturns northern hemisphere to objectively
classify regional albedo features and aid in their dynamical interpretation.
The procedure is based on a technique applied previously to visible-infrared
images of Earth. It provides a new perspective on giant planet cloud morphology
and its relationship to the dynamics and a meteorological context for the
analysis of other types of simultaneous Saturn observations. The method
identifies six clusters that exhibit distinct morphology, vertical structure,
and preferred latitudes of occurrence. These correspond to areas dominated by
deep convective cells; low contrast areas, some including thinner and thicker
clouds possibly associated with baroclinic instability; regions with possible
isolated thin cirrus clouds; darker areas due to thinner low level clouds or
clearer skies due to downwelling, or due to absorbing particles; and fields of
relatively shallow cumulus clouds. The spatial associations among these cloud
types suggest that dynamically, there are three distinct types of latitude
bands on Saturn: deep convectively disturbed latitudes in cyclonic shear
regions poleward of the eastward jets; convectively suppressed regions near and
surrounding the westward jets; and baroclinically unstable latitudes near
eastward jet cores and in the anti-cyclonic regions equatorward of them. These
are roughly analogous to some of the features of Earths tropics, subtropics,
and midlatitudes, respectively. Temporal variations of feature contrast and
cluster occurrence suggest that the upper tropospheric haze in the northern
hemisphere may have thickened by 2014.
",0,1,0,0,0,0
1481,The Peridynamic Stress Tensors and the Non-local to Local Passage,"  We re-examine the notion of stress in peridynamics. Based on the idea of
traction we define two new peridynamic stress tensors $\mathbf{P}^{\mathbf{y}}$
and $\mathbf{P}$ which stand, respectively, for analogues of the Cauchy and 1st
Piola-Kirchhoff stress tensors from classical elasticity. We show that the
tensor $\mathbf{P}$ differs from the earlier defined peridynamic stress tensor
$\nu$; though their divergence is equal. We address the question of symmetry of
the tensor $\mathbf{P}^{\mathbf{y}}$ which proves to be symmetric in case of
bond-based peridynamics; as opposed to the inverse Piola transform of $\nu$
(corresponding to the analogue of Cauchy stress tensor) which fails to be
symmetric in general. We also derive a general formula of the force-flux in
peridynamics and compute the limit of $\mathbf{P}$ for vanishing non-locality,
denoted by $\mathbf{P}_0$. We show that this tensor $\mathbf{P}_0$ surprisingly
coincides with the collapsed tensor $\nu_0$, a limit of the original tensor
$\nu$. At the end, using this flux-formula, we suggest an explanation why the
collapsed tensor $\mathbf{P}_0$ (and hence $\nu_0$) can be indeed identified
with the 1st Piola-Kirchhoff stress tensor.
",0,1,0,0,0,0
1482,Identification of Unmodeled Objects from Symbolic Descriptions,"  Successful human-robot cooperation hinges on each agent's ability to process
and exchange information about the shared environment and the task at hand.
Human communication is primarily based on symbolic abstractions of object
properties, rather than precise quantitative measures. A comprehensive robotic
framework thus requires an integrated communication module which is able to
establish a link and convert between perceptual and abstract information.
The ability to interpret composite symbolic descriptions enables an
autonomous agent to a) operate in unstructured and cluttered environments, in
tasks which involve unmodeled or never seen before objects; and b) exploit the
aggregation of multiple symbolic properties as an instance of ensemble
learning, to improve identification performance even when the individual
predicates encode generic information or are imprecisely grounded.
We propose a discriminative probabilistic model which interprets symbolic
descriptions to identify the referent object contextually w.r.t.\ the structure
of the environment and other objects. The model is trained using a collected
dataset of identifications, and its performance is evaluated by quantitative
measures and a live demo developed on the PR2 robot platform, which integrates
elements of perception, object extraction, object identification and grasping.
",1,0,0,1,0,0
1483,Balanced News Using Constrained Bandit-based Personalization,"  We present a prototype for a news search engine that presents balanced
viewpoints across liberal and conservative articles with the goal of
de-polarizing content and allowing users to escape their filter bubble. The
balancing is done according to flexible user-defined constraints, and leverages
recent advances in constrained bandit optimization. We showcase our balanced
news feed by displaying it side-by-side with the news feed produced by a
traditional (polarized) feed.
",1,0,0,0,0,0
1484,Intuitionistic Layered Graph Logic: Semantics and Proof Theory,"  Models of complex systems are widely used in the physical and social
sciences, and the concept of layering, typically building upon graph-theoretic
structure, is a common feature. We describe an intuitionistic substructural
logic called ILGL that gives an account of layering. The logic is a bunched
system, combining the usual intuitionistic connectives, together with a
non-commutative, non-associative conjunction (used to capture layering) and its
associated implications. We give soundness and completeness theorems for a
labelled tableaux system with respect to a Kripke semantics on graphs. We then
give an equivalent relational semantics, itself proven equivalent to an
algebraic semantics via a representation theorem. We utilise this result in two
ways. First, we prove decidability of the logic by showing the finite
embeddability property holds for the algebraic semantics. Second, we prove a
Stone-type duality theorem for the logic. By introducing the notions of ILGL
hyperdoctrine and indexed layered frame we are able to extend this result to a
predicate version of the logic and prove soundness and completeness theorems
for an extension of the layered graph semantics . We indicate the utility of
predicate ILGL with a resource-labelled bigraph model.
",1,0,0,0,0,0
1485,Learning Efficient Image Representation for Person Re-Identification,"  Color names based image representation is successfully used in person
re-identification, due to the advantages of being compact, intuitively
understandable as well as being robust to photometric variance. However, there
exists the diversity between underlying distribution of color names' RGB values
and that of image pixels' RGB values, which may lead to inaccuracy when
directly comparing them in Euclidean space. In this paper, we propose a new
method named soft Gaussian mapping (SGM) to address this problem. We model the
discrepancies between color names and pixels using a Gaussian and utilize the
inverse of covariance matrix to bridge the gap between them. Based on SGM, an
image could be converted to several soft Gaussian maps. In each soft Gaussian
map, we further seek to establish stable and robust descriptors within a local
region through a max pooling operation. Then, a robust image representation
based on color names is obtained by concatenating the statistical descriptors
in each stripe. When labeled data are available, one discriminative subspace
projection matrix is learned to build efficient representations of an image via
cross-view coupling learning. Experiments on the public datasets - VIPeR,
PRID450S and CUHK03, demonstrate the effectiveness of our method.
",1,0,0,0,0,0
1486,Exciting Nucleons in Compton Scattering and Hydrogen-Like Atoms,"  This PhD thesis is devoted to the low-energy structure of the nucleon (proton
and neutron) as seen through electromagnetic probes, e.g., electron and Compton
scattering. The research presented here is based primarily on dispersion theory
and chiral effective-field theory. The main motivation is the recent proton
radius puzzle, which is the discrepancy between the classic proton charge
radius determinations (based on electron-proton scattering and normal hydrogen
spectroscopy) and the highly precise extraction based on first muonic-hydrogen
experiments by the CREMA Collaboration. The precision of muonic-hydrogen
experiments is presently limited by the knowledge of proton structure effects
beyond the charge radius. A major part of this thesis is devoted to calculating
these effects using everything we know about the nucleon electromagnetic
structure from both theory and experiment.
The thesis consists of eight chapters. The first and last are, respectively,
the introduction and conclusion. The remainder of this thesis can roughly be
divided into the following three topics: finite-size effects in hydrogen-like
atoms, real and virtual Compton scattering, and two-photon-exchange effects.
",0,1,0,0,0,0
1487,Multiple universalities in order-disorder magnetic phase transitions,"  Phase transitions in isotropic quantum antiferromagnets are associated with
the condensation of bosonic triplet excitations. In three dimensional quantum
antiferromagnets, such as TlCuCl$_3$, condensation can be either pressure or
magnetic field induced. The corresponding magnetic order obeys universal
scaling with thermal critical exponent $\phi$. Employing a relativistic quantum
field theory, the present work predicts the emergence of multiple (three)
universalities under combined pressure and field tuning. Changes of
universality are signalled by changes of the critical exponent $\phi$.
Explicitly, we predict the existence of two new exponents $\phi=1$ and $1/2$ as
well as recovering the known exponent $\phi=3/2$. We also predict logarithmic
corrections to the power law scaling.
",0,1,0,0,0,0
1488,Exact Inference of Causal Relations in Dynamical Systems,"  From philosophers of ancient times to modern economists, biologists and other
researchers are engaged in revealing causal relations. The most challenging
problem is inferring the type of the causal relationship: whether it is uni- or
bi-directional or only apparent - implied by a hidden common cause only. Modern
technology provides us tools to record data from complex systems such as the
ecosystem of our planet or the human brain, but understanding their functioning
needs detection and distinction of causal relationships of the system
components without interventions. Here we present a new method, which
distinguishes and assigns probabilities to the presence of all the possible
causal relations between two or more time series from dynamical systems. The
new method is validated on synthetic datasets and applied to EEG
(electroencephalographic) data recorded in epileptic patients. Given the
universality of our method, it may find application in many fields of science.
",0,0,0,0,1,0
1489,Privacy-Preserving Deep Inference for Rich User Data on The Cloud,"  Deep neural networks are increasingly being used in a variety of machine
learning applications applied to rich user data on the cloud. However, this
approach introduces a number of privacy and efficiency challenges, as the cloud
operator can perform secondary inferences on the available data. Recently,
advances in edge processing have paved the way for more efficient, and private,
data processing at the source for simple tasks and lighter models, though they
remain a challenge for larger, and more complicated models. In this paper, we
present a hybrid approach for breaking down large, complex deep models for
cooperative, privacy-preserving analytics. We do this by breaking down the
popular deep architectures and fine-tune them in a particular way. We then
evaluate the privacy benefits of this approach based on the information exposed
to the cloud service. We also asses the local inference cost of different
layers on a modern handset for mobile applications. Our evaluations show that
by using certain kind of fine-tuning and embedding techniques and at a small
processing costs, we can greatly reduce the level of information available to
unintended tasks applied to the data feature on the cloud, and hence achieving
the desired tradeoff between privacy and performance.
",1,0,0,0,0,0
1490,Gradient Method With Inexact Oracle for Composite Non-Convex Optimization,"  In this paper, we develop new first-order method for composite non-convex
minimization problems with simple constraints and inexact oracle. The objective
function is given as a sum of ""`hard""', possibly non-convex part, and
""`simple""' convex part. Informally speaking, oracle inexactness means that, for
the ""`hard""' part, at any point we can approximately calculate the value of the
function and construct a quadratic function, which approximately bounds this
function from above. We give several examples of such inexactness: smooth
non-convex functions with inexact H??lder-continuous gradient, functions given
by auxiliary uniformly concave maximization problem, which can be solved only
approximately. For the introduced class of problems, we propose a gradient-type
method, which allows to use different proximal setup to adapt to geometry of
the feasible set, adaptively chooses controlled oracle error, allows for
inexact proximal mapping. We provide convergence rate for our method in terms
of the norm of generalized gradient mapping and show that, in the case of
inexact H??lder-continuous gradient, our method is universal with respect to
H??lder parameters of the problem. Finally, in a particular case, we show that
small value of the norm of generalized gradient mapping at a point means that a
necessary condition of local minimum approximately holds at that point.
",0,0,1,0,0,0
1491,Kernel Implicit Variational Inference,"  Recent progress in variational inference has paid much attention to the
flexibility of variational posteriors. One promising direction is to use
implicit distributions, i.e., distributions without tractable densities as the
variational posterior. However, existing methods on implicit posteriors still
face challenges of noisy estimation and computational infeasibility when
applied to models with high-dimensional latent variables. In this paper, we
present a new approach named Kernel Implicit Variational Inference that
addresses these challenges. As far as we know, for the first time implicit
variational inference is successfully applied to Bayesian neural networks,
which shows promising results on both regression and classification tasks.
",1,0,0,1,0,0
1492,The Ringel dual of the Auslander-Dlab-Ringel algebra,"  The ADR algebra $R_A$ of a finite-dimensional algebra $A$ is a
quasihereditary algebra. In this paper we study the Ringel dual
$\mathcal{R}(R_A)$ of $R_A$. We prove that $\mathcal{R}(R_A)$ can be identified
with $(R_{A^{op}})^{op}$, under certain 'minimal' regularity conditions for
$A$. We also give necessary and sufficient conditions for the ADR algebra to be
Ringel selfdual.
",0,0,1,0,0,0
1493,"The socle filtrations of principal series representations of $SL(3,\mathbb{R})$ and $Sp(2,\mathbb{R})$","  We study the structure of the $(\mathfrak{g},K)$-modules of the principal
series representations of $SL(3,\mathbb{R})$ and $Sp(2,\mathbb{R})$ induced
from minimal parabolic subgroups, in the case when the infinitesimal character
is nonsingular. The composition factors of these modules are known by
Kazhdan-Lusztig-Vogan conjecture. In this paper, we give complete descriptions
of the socle filtrations of these modules.
",0,0,1,0,0,0
1494,Improving the phase response of an atom interferometer by means of temporal pulse shaping,"  We study theoretically and experimentally the influence of temporally shaping
the light pulses in an atom interferometer, with a focus on the phase response
of the interferometer. We show that smooth light pulse shapes allow rejecting
high frequency phase fluctuations (above the Rabi frequency) and thus relax the
requirements on the phase noise or frequency noise of the interrogation lasers
driving the interferometer. The light pulse shape is also shown to modify the
scale factor of the interferometer, which has to be taken into account in the
evaluation of its accuracy budget. We discuss the trade-offs to operate when
choosing a particular pulse shape, by taking into account phase noise
rejection, velocity selectivity, and applicability to large momentum transfer
atom interferometry.
",0,1,0,0,0,0
1495,Helium-like and Lithium-like ions: Ground state energy,"  It is shown that the non-relativistic ground state energy of helium-like and
lithium-like ions with static nuclei can be interpolated in full physics range
of nuclear charges $Z$ with accuracy of not less than 6 decimal digits (d.d.)
or 7-8 significant digits (s.d.) using a meromorphic function in appropriate
variable with a few free parameters. It is demonstrated that finite nuclear
mass effects do not change 4-5 s.d. for $Z \in [1,50]$ for 2-,3-electron
systems and the leading relativistic and QED corrections leave unchanged 3-4
s.d. for $Z \in [1,12]$ in the ground state energy for 2-electron system, thus,
the interpolation reproduces definitely those figures. A meaning of proposed
interpolation is in a construction of unified, {\it two-point} Pade approximant
(for both small and large $Z$ expansions) with fitting some parameters at
intermediate $Z$.
",0,1,0,0,0,0
1496,Improvement of training set structure in fusion data cleaning using Time-Domain Global Similarity method,"  Traditional data cleaning identifies dirty data by classifying original data
sequences, which is a class$-$imbalanced problem since the proportion of
incorrect data is much less than the proportion of correct ones for most
diagnostic systems in Magnetic Confinement Fusion (MCF) devices. When using
machine learning algorithms to classify diagnostic data based on
class$-$imbalanced training set, most classifiers are biased towards the major
class and show very poor classification rates on the minor class. By
transforming the direct classification problem about original data sequences
into a classification problem about the physical similarity between data
sequences, the class$-$balanced effect of Time$-$Domain Global Similarity
(TDGS) method on training set structure is investigated in this paper.
Meanwhile, the impact of improved training set structure on data cleaning
performance of TDGS method is demonstrated with an application example in EAST
POlarimetry$-$INTerferometry (POINT) system.
",1,0,0,0,0,0
1497,Eigenvalue Solvers for Modeling Nuclear Reactors on Leadership Class Machines,"  Three complementary methods have been implemented in the code Denovo that
accelerate neutral particle transport calculations with methods that use
leadership-class computers fully and effectively: a multigroup block (MG)
Krylov solver, a Rayleigh Quotient Iteration (RQI) eigenvalue solver, and a
multigrid in energy (MGE) preconditioner. The MG Krylov solver converges more
quickly than Gauss Seidel and enables energy decomposition such that Denovo can
scale to hundreds of thousands of cores. RQI should converge in fewer
iterations than power iteration (PI) for large and challenging problems. RQI
creates shifted systems that would not be tractable without the MG Krylov
solver. It also creates ill-conditioned matrices. The MGE preconditioner
reduces iteration count significantly when used with RQI and takes advantage of
the new energy decomposition such that it can scale efficiently. Each
individual method has been described before, but this is the first time they
have been demonstrated to work together effectively.
The combination of solvers enables the RQI eigenvalue solver to work better
than the other available solvers for large reactors problems on leadership
class machines. Using these methods together, RQI converged in fewer iterations
and in less time than PI for a full pressurized water reactor core. These
solvers also performed better than an Arnoldi eigenvalue solver for a reactor
benchmark problem when energy decomposition is needed. The MG Krylov, MGE
preconditioner, and RQI solver combination also scales well in energy. This
solver set is a strong choice for very large and challenging problems.
",1,1,0,0,0,0
1498,"Thermoregulation in mice, rats and humans: An insight into the evolution of human hairlessness","  The thermoregulation system in animals removes body heat in hot temperatures
and retains body heat in cold temperatures. The better the animal removes heat,
the worse the animal retains heat and visa versa. It is the balance between
these two conflicting goals that determines the mammal's size, heart rate and
amount of hair. The rat's loss of tail hair and human's loss of its body hair
are responses to these conflicting thermoregulation needs as these animals
evolved to larger size over time.
",0,0,0,0,1,0
1499,Koszul A-infinity algebras and free loop space homology,"  We introduce a notion of Koszul A-infinity algebra that generalizes Priddy's
notion of a Koszul algebra and we use it to construct small A-infinity algebra
models for Hochschild cochains. As an application, this yields new techniques
for computing free loop space homology algebras of manifolds that are either
formal or coformal (over a field or over the integers). We illustrate these
techniques in two examples.
",0,0,1,0,0,0
1500,Learning RBM with a DC programming Approach,"  By exploiting the property that the RBM log-likelihood function is the
difference of convex functions, we formulate a stochastic variant of the
difference of convex functions (DC) programming to minimize the negative
log-likelihood. Interestingly, the traditional contrastive divergence algorithm
is a special case of the above formulation and the hyperparameters of the two
algorithms can be chosen such that the amount of computation per mini-batch is
identical. We show that for a given computational budget the proposed algorithm
almost always reaches a higher log-likelihood more rapidly, compared to the
standard contrastive divergence algorithm. Further, we modify this algorithm to
use the centered gradients and show that it is more efficient and effective
compared to the standard centered gradient algorithm on benchmark datasets.
",1,0,0,1,0,0
1501,Duality and Universal Transport in a Mixed-Dimension Electrodynamics,"  We consider a theory of a two-component Dirac fermion localized on a (2+1)
dimensional brane coupled to a (3+1) dimensional bulk. Using the fermionic
particle-vortex duality, we show that the theory has a strong-weak duality that
maps the coupling $e$ to $\tilde e=(8\pi)/e$. We explore the theory at
$e^2=8\pi$ where it is self-dual. The electrical conductivity of the theory is
a constant independent of frequency. When the system is at finite density and
magnetic field at filling factor $\nu=\frac12$, the longitudinal and Hall
conductivity satisfies a semicircle law, and the ratio of the longitudinal and
Hall thermal electric coefficients is completely determined by the Hall angle.
The thermal Hall conductivity is directly related to the thermal electric
coefficients.
",0,1,0,0,0,0
1502,Beyond similarity assessment: Selecting the optimal model for sequence alignment via the Factorized Asymptotic Bayesian algorithm,"  Pair Hidden Markov Models (PHMMs) are probabilistic models used for pairwise
sequence alignment, a quintessential problem in bioinformatics. PHMMs include
three types of hidden states: match, insertion and deletion. Most previous
studies have used one or two hidden states for each PHMM state type. However,
few studies have examined the number of states suitable for representing
sequence data or improving alignment accuracy.We developed a novel method to
select superior models (including the number of hidden states) for PHMM. Our
method selects models with the highest posterior probability using Factorized
Information Criteria (FIC), which is widely utilised in model selection for
probabilistic models with hidden variables. Our simulations indicated this
method has excellent model selection capabilities with slightly improved
alignment accuracy. We applied our method to DNA datasets from 5 and 28
species, ultimately selecting more complex models than those used in previous
studies.
",0,0,0,1,0,0
1503,Experimental evidence for Glycolaldehyde and Ethylene Glycol formation by surface hydrogenation of CO molecules under dense molecular cloud conditions,"  This study focuses on the formation of two molecules of astrobiological
importance - glycolaldehyde (HC(O)CH2OH) and ethylene glycol (H2C(OH)CH2OH) -
by surface hydrogenation of CO molecules. Our experiments aim at simulating the
CO freeze-out stage in interstellar dark cloud regions, well before thermal and
energetic processing become dominant. It is shown that along with the formation
of H2CO and CH3OH - two well established products of CO hydrogenation - also
molecules with more than one carbon atom form. The key step in this process is
believed to be the recombination of two HCO radicals followed by the formation
of a C-C bond. The experimentally established reaction pathways are implemented
into a continuous-time random-walk Monte Carlo model, previously used to model
the formation of CH3OH on astrochemical time-scales, to study their impact on
the solid-state abundances in dense interstellar clouds of glycolaldehyde and
ethylene glycol.
",0,1,0,0,0,0
1504,New Methods of Enhancing Prediction Accuracy in Linear Models with Missing Data,"  In this paper, prediction for linear systems with missing information is
investigated. New methods are introduced to improve the Mean Squared Error
(MSE) on the test set in comparison to state-of-the-art methods, through
appropriate tuning of Bias-Variance trade-off. First, the use of proposed Soft
Weighted Prediction (SWP) algorithm and its efficacy are depicted and compared
to previous works for non-missing scenarios. The algorithm is then modified and
optimized for missing scenarios. It is shown that controlled over-fitting by
suggested algorithms will improve prediction accuracy in various cases.
Simulation results approve our heuristics in enhancing the prediction accuracy.
",1,0,0,1,0,0
1505,Revisiting Imidazolium Based Ionic Liquids: Effect of the Conformation Bias of the [NTf$_{2}$] Anion Studied By Molecular Dynamics Simulations,"  We study ionic liquids composed 1-alkyl-3-methylimidazolium cations and
bis(trifluoromethyl-sulfonyl)imide anions ([C$_n$MIm][NTf$_2$]) with varying
chain-length $n\!=\!2, 4, 6, 8$ by using molecular dynamics simulations. We
show that a reparametrization of the dihedral potentials as well as charges of
the [NTf$_2$] anion leads to an improvment of the force field model introduced
by K??ddermann {\em et al.} [ChemPhysChem, \textbf{8}, 2464 (2007)] (KPL-force
field). A crucial advantage of the new parameter set is that the minimum energy
conformations of the anion ({\em trans} and {\em gauche}), as deduced from {\em
ab initio} calculations and {\sc Raman} experiments, are now both well
represented by our model. In addition, the results for [C$_n$MIm][NTf$_2$] show
that this modification leads to an even better agreement between experiment and
molecular dynamics simulation as demonstrated for densities, diffusion
coefficients, vaporization enthalpies, reorientational correlation times, and
viscosities. Even though we focused on a better representation of the anion
conformation, also the alkyl chain-length dependence of the cation behaves
closer to the experiment. We strongly encourage to use the new NGKPL force
field for the [NTf$_2$] anion instead of the earlier KPL parameter set for
computer simulations aiming to describe the thermodynamics, dynamics and also
structure of imidazolium based ionic liquids.
",0,1,0,0,0,0
1506,"Tick: a Python library for statistical learning, with a particular emphasis on time-dependent modelling","  Tick is a statistical learning library for Python~3, with a particular
emphasis on time-dependent models, such as point processes, and tools for
generalized linear models and survival analysis. The core of the library is an
optimization module providing model computational classes, solvers and proximal
operators for regularization. tick relies on a C++ implementation and
state-of-the-art optimization algorithms to provide very fast computations in a
single node multi-core setting. Source code and documentation can be downloaded
from this https URL
",0,0,0,1,0,0
1507,An energy method for rough partial differential equations,"  We present a well-posedness and stability result for a class of nondegenerate
linear parabolic equations driven by rough paths. More precisely, we introduce
a notion of weak solution that satisfies an intrinsic formulation of the
equation in a suitable Sobolev space of negative order. Weak solutions are then
shown to satisfy the corresponding en- ergy estimates which are deduced
directly from the equation. Existence is obtained by showing compactness of a
suitable sequence of approximate solutions whereas unique- ness relies on a
doubling of variables argument and a careful analysis of the passage to the
diagonal. Our result is optimal in the sense that the assumptions on the
deterministic part of the equation as well as the initial condition are the
same as in the classical PDEs theory.
",0,0,1,0,0,0
1508,Sparse Inverse Covariance Estimation for Chordal Structures,"  In this paper, we consider the Graphical Lasso (GL), a popular optimization
problem for learning the sparse representations of high-dimensional datasets,
which is well-known to be computationally expensive for large-scale problems.
Recently, we have shown that the sparsity pattern of the optimal solution of GL
is equivalent to the one obtained from simply thresholding the sample
covariance matrix, for sparse graphs under different conditions. We have also
derived a closed-form solution that is optimal when the thresholded sample
covariance matrix has an acyclic structure. As a major generalization of the
previous result, in this paper we derive a closed-form solution for the GL for
graphs with chordal structures. We show that the GL and thresholding
equivalence conditions can significantly be simplified and are expected to hold
for high-dimensional problems if the thresholded sample covariance matrix has a
chordal structure. We then show that the GL and thresholding equivalence is
enough to reduce the GL to a maximum determinant matrix completion problem and
drive a recursive closed-form solution for the GL when the thresholded sample
covariance matrix has a chordal structure. For large-scale problems with up to
450 million variables, the proposed method can solve the GL problem in less
than 2 minutes, while the state-of-the-art methods converge in more than 2
hours.
",0,0,0,1,0,0
1509,Orthogonal free quantum group factors are strongly 1-bounded,"  We prove that the orthogonal free quantum group factors
$\mathcal{L}(\mathbb{F}O_N)$ are strongly $1$-bounded in the sense of Jung. In
particular, they are not isomorphic to free group factors. This result is
obtained by establishing a spectral regularity result for the edge reversing
operator on the quantum Cayley tree associated to $\mathbb{F}O_N$, and
combining this result with a recent free entropy dimension rank theorem of Jung
and Shlyakhtenko.
",0,0,1,0,0,0
1510,Enhanced spin ordering temperature in ultrathin FeTe films grown on a topological insulator,"  We studied the temperature dependence of the diagonal double-stripe spin
order in one and two unit cell thick layers of FeTe grown on the topological
insulator Bi_2Te_3 via spin-polarized scanning tunneling microscopy. The spin
order persists up to temperatures which are higher than the transition
temperature reported for bulk Fe_1+yTe with lowest possible excess Fe content
y. The enhanced spin order stability is assigned to a strongly decreased y with
respect to the lowest values achievable in bulk crystal growth, and effects due
to the interface between the FeTe and the topological insulator. The result is
relevant for understanding the recent observation of a coexistence of
superconducting correlations and spin order in this system.
",0,1,0,0,0,0
1511,High Order Hierarchical Divergence-free Constrained Transport $H(div)$ Finite Element Method for Magnetic Induction Equation,"  In this paper, we will use the interior functions of an hierarchical basis
for high order $BDM_p$ elements to enforce the divergence-free condition of a
magnetic field $B$ approximated by the H(div) $BDM_p$ basis. The resulting
constrained finite element method can be used to solve magnetic induction
equation in MHD equations. The proposed procedure is based on the fact that the
scalar $(p-1)$-th order polynomial space on each element can be decomposed as
an orthogonal sum of the subspace defined by the divergence of the interior
functions of the $p$-th order $BDM_p$ basis and the constant function.
Therefore, the interior functions can be used to remove element-wise all higher
order terms except the constant in the divergence error of the finite element
solution of $B$-field. The constant terms from each element can be then easily
corrected using a first order H(div) basis globally. Numerical results for a
3-D magnetic induction equation show the effectiveness of the proposed method
in enforcing divergence-free condition of the magnetic field.
",0,0,1,0,0,0
1512,REMOTEGATE: Incentive-Compatible Remote Configuration of Security Gateways,"  Imagine that a malicious hacker is trying to attack a server over the
Internet and the server wants to block the attack packets as close to their
point of origin as possible. However, the security gateway ahead of the source
of attack is untrusted. How can the server block the attack packets through
this gateway? In this paper, we introduce REMOTEGATE, a trustworthy mechanism
for allowing any party (server) on the Internet to configure a security gateway
owned by a second party, at a certain agreed upon reward that the former pays
to the latter for its service. We take an interactive incentive-compatible
approach, for the case when both the server and the gateway are rational, to
devise a protocol that will allow the server to help the security gateway
generate and deploy a policy rule that filters the attack packets before they
reach the server. The server will reward the gateway only when the latter can
successfully verify that it has generated and deployed the correct rule for the
issue. This mechanism will enable an Internet-scale approach to improving
security and privacy, backed by digital payment incentives.
",1,0,0,0,0,0
1513,Distributed Event-Triggered Control for Global Consensus of Multi-Agent Systems with Input Saturation,"  We consider the global consensus problem for multi-agent systems with input
saturation over digraphs. Under a mild connectivity condition that the
underlying digraph has a directed spanning tree, we use Lyapunov methods to
show that the widely used distributed consensus protocol, which solves the
consensus problem for the case without input saturation constraints, also
solves the global consensus problem for the case with input saturation
constraints. In order to reduce the overall need of communication and system
updates, we then propose a distributed event-triggered control law. Global
consensus is still realized and Zeno behavior is excluded. Numerical
simulations are provided to illustrate the effectiveness of the theoretical
results.
",0,0,1,0,0,0
1514,Autocommuting probability of a finite group relative to its subgroups,"  Let $H \subseteq K$ be two subgroups of a finite group $G$ and Aut$(K)$ the
automorphism group of $K$. The autocommuting probability of $G$ relative to its
subgroups $H$ and $K$, denoted by ${\rm Pr}(H, {\rm Aut}(K))$, is the
probability that the autocommutator of a randomly chosen pair of elements, one
from $H$ and the other from Aut$(K)$, is equal to the identity element of $G$.
In this paper, we study ${\rm Pr}(H, {\rm Aut}(K))$ through a generalization.
",0,0,1,0,0,0
1515,Total variation regularization with variable Lebesgue prior,"  This work proposes the variable exponent Lebesgue modular as a replacement
for the 1-norm in total variation (TV) regularization. It allows the exponent
to vary with spatial location and thus enables users to locally select whether
to preserve edges or smooth intensity variations. In contrast to earlier work
using TV-like methods with variable exponents, the exponent function is here
computed offline as a fixed parameter of the final optimization problem,
resulting in a convex goal functional. The obtained formulas for the convex
conjugate and the proximal operators are simple in structure and can be
evaluated very efficiently, an important property for practical usability.
Numerical results with variable $L^p$ TV prior in denoising and tomography
problems on synthetic data compare favorably to total generalized variation
(TGV) and TV.
",0,0,1,0,0,0
1516,Radio observations confirm young stellar populations in local analogues to $z\sim5$ Lyman break galaxies,"  We present radio observations at 1.5 GHz of 32 local objects selected to
reproduce the physical properties of $z\sim5$ star-forming galaxies. We also
report non-detections of five such sources in the sub-millimetre. We find a
radio-derived star formation rate which is typically half that derived from
H$\alpha$ emission for the same objects. These observations support previous
indications that we are observing galaxies with a young dominant stellar
population, which has not yet established a strong supernova-driven synchrotron
continuum. We stress caution when applying star formation rate calibrations to
stellar populations younger than 100 Myr. We calibrate the conversions for
younger galaxies, which are dominated by a thermal radio emission component. We
improve the size constraints for these sources, compared to previous unresolved
ground-based optical observations. Their physical size limits indicate very
high star formation rate surface densities, several orders of magnitude higher
than the local galaxy population. In typical nearby galaxies, this would imply
the presence of galaxy-wide winds. Given the young stellar populations, it is
unclear whether a mechanism exists in our sources that can deposit sufficient
kinetic energy into the interstellar medium to drive such outflows.
",0,1,0,0,0,0
1517,Sparse Deep Neural Network Exact Solutions,"  Deep neural networks (DNNs) have emerged as key enablers of machine learning.
Applying larger DNNs to more diverse applications is an important challenge.
The computations performed during DNN training and inference are dominated by
operations on the weight matrices describing the DNN. As DNNs incorporate more
layers and more neurons per layers, these weight matrices may be required to be
sparse because of memory limitations. Sparse DNNs are one possible approach,
but the underlying theory is in the early stages of development and presents a
number of challenges, including determining the accuracy of inference and
selecting nonzero weights for training. Associative array algebra has been
developed by the big data community to combine and extend database, matrix, and
graph/network concepts for use in large, sparse data problems. Applying this
mathematics to DNNs simplifies the formulation of DNN mathematics and reveals
that DNNs are linear over oscillating semirings. This work uses associative
array DNNs to construct exact solutions and corresponding perturbation models
to the rectified linear unit (ReLU) DNN equations that can be used to construct
test vectors for sparse DNN implementations over various precisions. These
solutions can be used for DNN verification, theoretical explorations of DNN
properties, and a starting point for the challenge of sparse training.
",0,0,0,1,0,0
1518,Variation formulas for an extended Gompf invariant,"  In 1998, R. Gompf defined a homotopy invariant $\theta_G$ of oriented 2-plane
fields in 3-manifolds. This invariant is defined for oriented 2-plane fields
$\xi$ in a closed oriented 3-manifold $M$ when the first Chern class $c_1(\xi)$
is a torsion element of $H^2(M;\mathbb{Z})$. In this article, we define an
extension of the Gompf invariant for all compact oriented 3-manifolds with
boundary and we study its iterated variations under Lagrangian-preserving
surgeries. It follows that the extended Gompf invariant is a degree two
invariant with respect to a suitable finite type invariant theory.
",0,0,1,0,0,0
1519,A SAT+CAS Approach to Finding Good Matrices: New Examples and Counterexamples,"  We enumerate all circulant good matrices with odd orders divisible by 3 up to
order 70. As a consequence of this we find a previously overlooked set of good
matrices of order 27 and a new set of good matrices of order 57. We also find
that circulant good matrices do not exist in the orders 51, 63, and 69, thereby
finding three new counterexamples to the conjecture that such matrices exist in
all odd orders. Additionally, we prove a new relationship between the entries
of good matrices and exploit this relationship in our enumeration algorithm.
Our method applies the SAT+CAS paradigm of combining computer algebra
functionality with modern SAT solvers to efficiently search large spaces which
are specified by both algebraic and logical constraints.
",1,0,0,0,0,0
1520,An Exploration of Mimic Architectures for Residual Network Based Spectral Mapping,"  Spectral mapping uses a deep neural network (DNN) to map directly from noisy
speech to clean speech. Our previous study found that the performance of
spectral mapping improves greatly when using helpful cues from an acoustic
model trained on clean speech. The mapper network learns to mimic the input
favored by the spectral classifier and cleans the features accordingly. In this
study, we explore two new innovations: we replace a DNN-based spectral mapper
with a residual network that is more attuned to the goal of predicting clean
speech. We also examine how integrating long term context in the mimic
criterion (via wide-residual biLSTM networks) affects the performance of
spectral mapping compared to DNNs. Our goal is to derive a model that can be
used as a preprocessor for any recognition system; the features derived from
our model are passed through the standard Kaldi ASR pipeline and achieve a WER
of 9.3%, which is the lowest recorded word error rate for CHiME-2 dataset using
only feature adaptation.
",1,0,0,0,0,0
1521,Deep Neural Networks to Enable Real-time Multimessenger Astrophysics,"  Gravitational wave astronomy has set in motion a scientific revolution. To
further enhance the science reach of this emergent field, there is a pressing
need to increase the depth and speed of the gravitational wave algorithms that
have enabled these groundbreaking discoveries. To contribute to this effort, we
introduce Deep Filtering, a new highly scalable method for end-to-end
time-series signal processing, based on a system of two deep convolutional
neural networks, which we designed for classification and regression to rapidly
detect and estimate parameters of signals in highly noisy time-series data
streams. We demonstrate a novel training scheme with gradually increasing noise
levels, and a transfer learning procedure between the two networks. We showcase
the application of this method for the detection and parameter estimation of
gravitational waves from binary black hole mergers. Our results indicate that
Deep Filtering significantly outperforms conventional machine learning
techniques, achieves similar performance compared to matched-filtering while
being several orders of magnitude faster thus allowing real-time processing of
raw big data with minimal resources. More importantly, Deep Filtering extends
the range of gravitational wave signals that can be detected with ground-based
gravitational wave detectors. This framework leverages recent advances in
artificial intelligence algorithms and emerging hardware architectures, such as
deep-learning-optimized GPUs, to facilitate real-time searches of gravitational
wave sources and their electromagnetic and astro-particle counterparts.
",1,1,0,0,0,0
1522,Contribution of cellular automata to the understanding of corrosion phenomena,"  We present a stochastic CA modelling approach of corrosion based on spatially
separated electrochemical half-reactions, diffusion, acido-basic neutralization
in solution and passive properties of the oxide layers. Starting from different
initial conditions, a single framework allows one to describe generalised
corrosion, localised corrosion, reactive and passive surfaces, including
occluded corrosion phenomena as well. Spontaneous spatial separation of anodic
and cathodic zones is associated with bare metal and passivated metal on the
surface. This separation is also related to local acidification of the
solution. This spontaneous change is associated with a much faster corrosion
rate. Material morphology is closely related to corrosion kinetics, which can
be used for technological applications.
",0,1,0,0,0,0
1523,Involutive bordered Floer homology,"  We give a bordered extension of involutive HF-hat and use it to give an
algorithm to compute involutive HF-hat for general 3-manifolds. We also explain
how the mapping class group action on HF-hat can be computed using bordered
Floer homology. As applications, we prove that involutive HF-hat satisfies a
surgery exact triangle and compute HFI-hat of the branched double covers of all
10-crossing knots.
",0,0,1,0,0,0
1524,"Orbital Evolution, Activity, and Mass Loss of Comet C/1995 O1 (Hale-Bopp). I. Close Encounter with Jupiter in Third Millennium BCE and Effects of Outgassing on the Comet's Motion and Physical Properties","  This comprehensive study of comet C/1995 O1 focuses first on investigating
its orbital motion over a period of 17.6 yr (1993-2010). The comet is suggested
to have approached Jupiter to 0.005 AU on -2251 November 7, in general
conformity with Marsden's (1999) proposal of a Jovian encounter nearly 4300 yr
ago. The variations of sizable nongravitational effects with heliocentric
distance correlate with the evolution of outgassing, asymmetric relative to
perihelion. The future orbital period will shorten to ~1000 yr because of
orbital-cascade resonance effects. We find that the sublimation curves of
parent molecules are fitted with the type of a law used for the
nongravitational acceleration, determine their orbit-integrated mass loss, and
conclude that the share of water ice was at most 57%, and possibly less than
50%, of the total outgassed mass. Even though organic parent molecules (many
still unidentified) had very low abundances relative to water individually,
their high molar mass and sheer number made them, summarily, important
potential mass contributors to the total production of gas. The mass loss of
dust per orbit exceeded that of water ice by a factor of ~12, a dust loading
high enough to imply a major role for heavy organic molecules of low volatility
in accelerating the minuscule dust particles in the expanding halos to terminal
velocities as high as 0.7 km s^{-1}. In Part II, the comet's nucleus will be
modeled as a compact cluster of massive fragments to conform to the integrated
nongravitational effect.
",0,1,0,0,0,0
1525,A Note on Property Testing Sum of Squares and Multivariate Polynomial Interpolation,"  In this paper, we investigate property testing whether or not a degree d
multivariate poly- nomial is a sum of squares or is far from a sum of squares.
We show that if we require that the property tester always accepts YES
instances and uses random samples, $n^{\Omega(d)}$ samples are required, which
is not much fewer than it would take to completely determine the polynomial. To
prove this lower bound, we show that with high probability, multivariate
polynomial in- terpolation matches arbitrary values on random points and the
resulting polynomial has small norm. We then consider a particular polynomial
which is non-negative yet not a sum of squares and use pseudo-expectation
values to prove it is far from being a sum of squares.
",1,0,0,0,0,0
1526,Closed-form mathematical expressions for the exponentiated Cauchy-Rayleigh distribution,"  The Cauchy-Rayleigh (CR) distribution has been successfully used to describe
asymmetric and heavy-tail events from radar imagery. Employing such model to
describe lifetime data may then seem attractive, but some drawbacks arise: its
probability density function does not cover non-modal behavior as well as the
CR hazard rate function (hrf) assumes only one form. To outperform this
difficulty, we introduce an extended CR model, called exponentiated
Cauchy-Rayleigh (ECR) distribution. This model has two parameters and hrf with
decreasing, decreasing-increasing-decreasing and upside-down bathtub forms. In
this paper, several closed-form mathematical expressions for the ECR model are
proposed: median, mode, probability weighted, log-, incomplete and order
statistic moments and Fisher information matrix. We propose three estimation
procedures for the ECR parameters: maximum likelihood (ML), bias corrected ML
and percentile-based methods. A simulation study is done to assess the
performance of estimators. An application to survival time of heart problem
patients illustrates the usefulness of the ECR model. Results point out that
the ECR distribution may outperform classical lifetime models, such as the
gamma, Birnbaun-Saunders, Weibull and log-normal laws, before heavy-tail data.
",0,0,1,1,0,0
1527,"HTEM data improve 3D modelling of aquifers in Paris Basin, France","  In Paris Basin, we evaluate how HTEM data complement the usual borehole,
geological and deep seismic data used for modelling aquifer geometries. With
these traditional data, depths between ca. 50 to 300m are often relatively
ill-constrained, as most boreholes lie within the first tens of meters of the
underground and petroleum seismic is blind shallower than ca. 300m. We have
fully reprocessed and re-inverted 540km of flight lines of a SkyTEM survey of
2009, acquired on a 40x12km zone with 400m line spacing. The resistivity model
is first ""calibrated"" with respect to ca. 50 boreholes available on the study
area. Overall, the correlation between EM resistivity models and the
hydrogeological horizons clearly shows that the geological units in which the
aquifers are developed almost systematically correspond to relative increase of
resistivity, whatever the ""background"" resistivity environment and the
lithology of the aquifer. In 3D Geomodeller software, this allows interpreting
11 aquifer/aquitar layers along the flight lines and then jointly interpolating
them in 3D along with the borehole data. The resulting model displays 3D
aquifer geometries consistent with the SIGES ""reference"" regional
hydrogeological model and improves it in between the boreholes and on the
50-300m depth range.
",0,1,0,0,0,0
1528,"Implementing GraphQL as a Query Language for Deductive Databases in SWI-Prolog Using DCGs, Quasi Quotations, and Dicts","  The methods to access large relational databases in a distributed system are
well established: the relational query language SQL often serves as a language
for data access and manipulation, and in addition public interfaces are exposed
using communication protocols like REST. Similarly to REST, GraphQL is the
query protocol of an application layer developed by Facebook. It provides a
unified interface between the client and the server for data fetching and
manipulation. Using GraphQL's type system, it is possible to specify data
handling of various sources and to combine, e.g., relational with NoSQL
databases. In contrast to REST, GraphQL provides a single API endpoint and
supports flexible queries over linked data.
GraphQL can also be used as an interface for deductive databases. In this
paper, we give an introduction of GraphQL and a comparison to REST. Using
language features recently added to SWI-Prolog 7, we have developed the Prolog
library GraphQL.pl, which implements the GraphQL type system and query syntax
as a domain-specific language with the help of definite clause grammars (DCG),
quasi quotations, and dicts. Using our library, the type system created for a
deductive database can be validated, while the query system provides a unified
interface for data access and introspection.
",1,0,0,0,0,0
1529,Social Network based Short-Term Stock Trading System,"  This paper proposes a novel adaptive algorithm for the automated short-term
trading of financial instrument. The algorithm adopts a semantic sentiment
analysis technique to inspect the Twitter posts and to use them to predict the
behaviour of the stock market. Indeed, the algorithm is specifically developed
to take advantage of both the sentiment and the past values of a certain
financial instrument in order to choose the best investment decision. This
allows the algorithm to ensure the maximization of the obtainable profits by
trading on the stock market. We have conducted an investment simulation and
compared the performance of our proposed with a well-known benchmark (DJTATO
index) and the optimal results, in which an investor knows in advance the
future price of a product. The result shows that our approach outperforms the
benchmark and achieves the performance score close to the optimal result.
",1,0,0,0,0,1
1530,New Determinant Expressions of the Multi-indexed Orthogonal Polynomials in Discrete Quantum Mechanics,"  The multi-indexed orthogonal polynomials (the Meixner, little $q$-Jacobi
(Laguerre), ($q$-)Racah, Wilson, Askey-Wilson types) satisfying second order
difference equations were constructed in discrete quantum mechanics. They are
polynomials in the sinusoidal coordinates $\eta(x)$ ($x$ is the coordinate of
quantum system) and expressed in terms of the Casorati determinants whose
matrix elements are functions of $x$ at various points. By using shape
invariance properties, we derive various equivalent determinant expressions,
especially those whose matrix elements are functions of the same point $x$.
Except for the ($q$-)Racah case, they can be expressed in terms of $\eta$ only,
without explicit $x$-dependence.
",0,1,1,0,0,0
1531,Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior,"  We describe an approach to understand the peculiar and counterintuitive
generalization properties of deep neural networks. The approach involves going
beyond worst-case theoretical capacity control frameworks that have been
popular in machine learning in recent years to revisit old ideas in the
statistical mechanics of neural networks. Within this approach, we present a
prototypical Very Simple Deep Learning (VSDL) model, whose behavior is
controlled by two control parameters, one describing an effective amount of
data, or load, on the network (that decreases when noise is added to the
input), and one with an effective temperature interpretation (that increases
when algorithms are early stopped). Using this model, we describe how a very
simple application of ideas from the statistical mechanics theory of
generalization provides a strong qualitative description of recently-observed
empirical results regarding the inability of deep neural networks not to
overfit training data, discontinuous learning and sharp transitions in the
generalization properties of learning algorithms, etc.
",1,0,0,1,0,0
1532,Towards an Understanding of the Effects of Augmented Reality Games on Disaster Management,"  Location-based augmented reality games have entered the mainstream with the
nearly overnight success of Niantic's Pok??mon Go. Unlike traditional video
games, the fact that players of such games carry out actions in the external,
physical world to accomplish in-game objectives means that the large-scale
adoption of such games motivate people, en masse, to do things and go places
they would not have otherwise done in unprecedented ways. The social
implications of such mass-mobilisation of individual players are, in general,
difficult to anticipate or characterise, even for the short-term. In this work,
we focus on disaster relief, and the short- and long-term implications that a
proliferation of AR games like Pok??mon Go, may have in disaster-prone regions
of the world. We take a distributed cognition approach and focus on one natural
disaster-prone region of New Zealand, the city of Wellington.
",1,0,0,0,0,0
1533,Integrating a Global Induction Mechanism into a Sequent Calculus,"  Most interesting proofs in mathematics contain an inductive argument which
requires an extension of the LK-calculus to formalize. The most commonly used
calculi for induction contain a separate rule or axiom which reduces the valid
proof theoretic properties of the calculus. To the best of our knowledge, there
are no such calculi which allow cut-elimination to a normal form with the
subformula property, i.e. every formula occurring in the proof is a subformula
of the end sequent. Proof schemata are a variant of LK-proofs able to simulate
induction by linking proofs together. There exists a schematic normal form
which has comparable proof theoretic behaviour to normal forms with the
subformula property. However, a calculus for the construction of proof schemata
does not exist. In this paper, we introduce a calculus for proof schemata and
prove soundness and completeness with respect to a fragment of the inductive
arguments formalizable in Peano arithmetic.
",1,0,0,0,0,0
1534,An analytic resolution to the competition between Lyman-Werner radiation and metal winds in direct collapse black hole hosts,"  A near pristine atomic cooling halo close to a star forming galaxy offers a
natural pathway for forming massive direct collapse black hole (DCBH) seeds
which could be the progenitors of the $z>6$ redshift quasars. The close
proximity of the haloes enables a sufficient Lyman-Werner flux to effectively
dissociate H$_2$ in the core of the atomic cooling halo. A mild background may
also be required to delay star formation in the atomic cooling halo, often
attributed to distant background galaxies. In this letter we investigate the
impact of metal enrichment from both the background galaxies and the close star
forming galaxy under extremely unfavourable conditions such as instantaneous
metal mixing. We find that within the time window of DCBH formation, the level
of enrichment never exceeds the critical threshold (Z$_{cr} \sim 1 \times
10^{-5} \ \rm Z_{\odot})$, and attains a maximum metallicity of Z $\sim 2
\times 10^{-6} \ \rm Z_{\odot}$. As the system evolves, the metallicity
eventually exceeds the critical threshold, long after the DCBH has formed.
",0,1,0,0,0,0
1535,Driving Interactive Graph Exploration Using 0-Dimensional Persistent Homology Features,"  Graphs are commonly used to encode relationships among entities, yet, their
abstractness makes them incredibly difficult to analyze. Node-link diagrams are
a popular method for drawing graphs. Classical techniques for the node-link
diagrams include various layout methods that rely on derived information to
position points, which often lack interactive exploration functionalities; and
force-directed layouts, which ignore global structures of the graph. This paper
addresses the graph drawing challenge by leveraging topological features of a
graph as derived information for interactive graph drawing. We first discuss
extracting topological features from a graph using persistent homology. We then
introduce an interactive persistence barcodes to study the substructures of a
force-directed graph layout; in particular, we add contracting and repulsing
forces guided by the 0-dimensional persistent homology features. Finally, we
demonstrate the utility of our approach across three datasets.
",1,0,0,0,0,0
1536,Identification of Conduit Countries and Community Structures in the Withholding Tax Networks,"  Due to economic globalization, each country's economic law, including tax
laws and tax treaties, has been forced to work as a single network. However,
each jurisdiction (country or region) has not made its economic law under the
assumption that its law functions as an element of one network, so it has
brought unexpected results. We thought that the results are exactly
international tax avoidance. To contribute to the solution of international tax
avoidance, we tried to investigate which part of the network is vulnerable.
Specifically, focusing on treaty shopping, which is one of international tax
avoidance methods, we attempt to identified which jurisdiction are likely to be
used for treaty shopping from tax liabilities and the relationship between
jurisdictions which are likely to be used for treaty shopping and others. For
that purpose, based on withholding tax rates imposed on dividends, interest,
and royalties by jurisdictions, we produced weighted multiple directed graphs,
computed the centralities and detected the communities. As a result, we
clarified the jurisdictions that are likely to be used for treaty shopping and
pointed out that there are community structures. The results of this study
suggested that fewer jurisdictions need to introduce more regulations for
prevention of treaty abuse worldwide.
",0,0,0,0,0,1
1537,A comprehensive study of batch construction strategies for recurrent neural networks in MXNet,"  In this work we compare different batch construction methods for mini-batch
training of recurrent neural networks. While popular implementations like
TensorFlow and MXNet suggest a bucketing approach to improve the
parallelization capabilities of the recurrent training process, we propose a
simple ordering strategy that arranges the training sequences in a stochastic
alternatingly sorted way. We compare our method to sequence bucketing as well
as various other batch construction strategies on the CHiME-4 noisy speech
recognition corpus. The experiments show that our alternated sorting approach
is able to compete both in training time and recognition performance while
being conceptually simpler to implement.
",1,0,0,1,0,0
1538,On a class of shift-invariant subspaces of the Drury-Arveson space,"  In the Drury-Arveson space, we consider the subspace of functions whose
Taylor coefficients are supported in the complement of a set
$Y\subset\mathbb{N}^d$ with the property that $Y+e_j\subset Y$ for all
$j=1,\dots,d$. This is an easy example of shift-invariant subspace, which can
be considered as a RKHS in is own right, with a kernel that can be explicitely
calculated. Moreover, every such a space can be seen as an intersection of
kernels of Hankel operators, whose symbols can be explicity calcuated as well.
Finally, this is the right space on which Drury's inequality can be optimally
adapted to a sub-family of the commuting and contractive operators originally
considered by Drury.
",0,0,1,0,0,0
1539,Airborne gamma-ray spectroscopy for modeling cosmic radiation and effective dose in the lower atmosphere,"  In this paper we present the results of a $\sim$5 hour airborne gamma-ray
survey carried out over the Tyrrhenian sea in which the height range (77-3066)
m has been investigated. Gamma-ray spectroscopy measurements have been
performed by using the AGRS_16L detector, a module of four 4L NaI(Tl) crystals.
The experimental setup was mounted on the Radgyro, a prototype aircraft
designed for multisensorial acquisitions in the field of proximal remote
sensing. By acquiring high-statistics spectra over the sea (i.e. in the absence
of signals having geological origin) and by spanning a wide spectrum of
altitudes it has been possible to split the measured count rate into a constant
aircraft component and a cosmic component exponentially increasing with
increasing height. The monitoring of the count rate having pure cosmic origin
in the >3 MeV energy region allowed to infer the background count rates in the
$^{40}$K, $^{214}$Bi and $^{208}$Tl photopeaks, which need to be subtracted in
processing airborne gamma-ray data in order to estimate the potassium, uranium
and thorium abundances in the ground. Moreover, a calibration procedure has
been carried out by implementing the CARI-6P and EXPACS dosimetry tools,
according to which the annual cosmic effective dose to human population has
been linearly related to the measured cosmic count rates.
",0,1,0,0,0,0
1540,Search for axions in streaming dark matter,"  A new search strategy for the detection of the elusive dark matter (DM) axion
is proposed. The idea is based on streaming DM axions, whose flux might get
temporally enormously enhanced due to gravitational lensing. This can happen if
the Sun or some planet (including the Moon) is found along the direction of a
DM stream propagating towards the Earth location. The experimental requirements
to the axion haloscope are a wide-band performance combined with a fast axion
rest mass scanning mode, which are feasible. Once both conditions have been
implemented in a haloscope, the axion search can continue parasitically almost
as before. Interestingly, some new DM axion detectors are operating wide-band
by default. In order not to miss the actually unpredictable timing of a
potential short duration signal, a network of co-ordinated axion antennae is
required, preferentially distributed world-wide. The reasoning presented here
for the axions applies to some degree also to any other DM candidates like the
WIMPs.
",0,1,0,0,0,0
1541,Faster integer and polynomial multiplication using cyclotomic coefficient rings,"  We present an algorithm that computes the product of two n-bit integers in
O(n log n (4\sqrt 2)^{log^* n}) bit operations. Previously, the best known
bound was O(n log n 6^{log^* n}). We also prove that for a fixed prime p,
polynomials in F_p[X] of degree n may be multiplied in O(n log n 4^{log^* n})
bit operations; the previous best bound was O(n log n 8^{log^* n}).
",1,0,0,0,0,0
1542,Reward Maximization Under Uncertainty: Leveraging Side-Observations on Networks,"  We study the stochastic multi-armed bandit (MAB) problem in the presence of
side-observations across actions that occur as a result of an underlying
network structure. In our model, a bipartite graph captures the relationship
between actions and a common set of unknowns such that choosing an action
reveals observations for the unknowns that it is connected to. This models a
common scenario in online social networks where users respond to their friends'
activity, thus providing side information about each other's preferences. Our
contributions are as follows: 1) We derive an asymptotic lower bound (with
respect to time) as a function of the bi-partite network structure on the
regret of any uniformly good policy that achieves the maximum long-term average
reward. 2) We propose two policies - a randomized policy; and a policy based on
the well-known upper confidence bound (UCB) policies - both of which explore
each action at a rate that is a function of its network position. We show,
under mild assumptions, that these policies achieve the asymptotic lower bound
on the regret up to a multiplicative factor, independent of the network
structure. Finally, we use numerical examples on a real-world social network
and a routing example network to demonstrate the benefits obtained by our
policies over other existing policies.
",1,0,0,1,0,0
1543,Verifying Safety of Functional Programs with Rosette/Unbound,"  The goal of unbounded program verification is to discover an inductive
invariant that safely over-approximates all possible program behaviors.
Functional languages featuring higher order and recursive functions become more
popular due to the domain-specific needs of big data analytics, web, and
security. We present Rosette/Unbound, the first program verifier for Racket
exploiting the automated constrained Horn solver on its backend. One of the key
features of Rosette/Unbound is the ability to synchronize recursive
computations over the same inputs allowing to verify programs that iterate over
unbounded data streams multiple times. Rosette/Unbound is successfully
evaluated on a set of non-trivial recursive and higher order functional
programs.
",1,0,0,0,0,0
1544,Extended Formulations for Polytopes of Regular Matroids,"  We present a simple proof of the fact that the base (and independence)
polytope of a rank $n$ regular matroid over $m$ elements has an extension
complexity $O(mn)$.
",1,0,1,0,0,0
1545,Multiscale Change-point Segmentation: Beyond Step Functions,"  Modern multiscale type segmentation methods are known to detect multiple
change-points with high statistical accuracy, while allowing for fast
computation. Underpinning theory has been developed mainly for models that
assume the signal as a piecewise constant function. In this paper this will be
extended to certain function classes beyond such step functions in a
nonparametric regression setting, revealing certain multiscale segmentation
methods as robust to deviation from such piecewise constant functions. Our main
finding is the adaptation over such function classes for a universal
thresholding, which includes bounded variation functions, and (piecewise)
H??lder functions of smoothness order $ 0 < \alpha \le1$ as special cases.
From this we derive statistical guarantees on feature detection in terms of
jumps and modes. Another key finding is that these multiscale segmentation
methods perform nearly (up to a log-factor) as well as the oracle piecewise
constant segmentation estimator (with known jump locations), and the best
piecewise constant approximants of the (unknown) true signal. Theoretical
findings are examined by various numerical simulations.
",0,0,1,1,0,0
1546,Data Motif-based Proxy Benchmarks for Big Data and AI Workloads,"  For the architecture community, reasonable simulation time is a strong
requirement in addition to performance data accuracy. However, emerging big
data and AI workloads are too huge at binary size level and prohibitively
expensive to run on cycle-accurate simulators. The concept of data motif, which
is identified as a class of units of computation performed on initial or
intermediate data, is the first step towards building proxy benchmark to mimic
the real-world big data and AI workloads. However, there is no practical way to
construct a proxy benchmark based on the data motifs to help simulation-based
research. In this paper, we embark on a study to bridge the gap between data
motif and a practical proxy benchmark. We propose a data motif-based proxy
benchmark generating methodology by means of machine learning method, which
combine data motifs with different weights to mimic the big data and AI
workloads. Furthermore, we implement various data motifs using light-weight
stacks and apply the methodology to five real-world workloads to construct a
suite of proxy benchmarks, considering the data types, patterns, and
distributions. The evaluation results show that our proxy benchmarks shorten
the execution time by 100s times on real systems while maintaining the average
system and micro-architecture performance data accuracy above 90%, even
changing the input data sets or cluster configurations. Moreover, the generated
proxy benchmarks reflect consistent performance trends across different
architectures. To facilitate the community, we will release the proxy
benchmarks on the project homepage this http URL.
",1,0,0,0,0,0
1547,The neighborhood lattice for encoding partial correlations in a Hilbert space,"  Neighborhood regression has been a successful approach in graphical and
structural equation modeling, with applications to learning undirected and
directed graphical models. We extend these ideas by defining and studying an
algebraic structure called the neighborhood lattice based on a generalized
notion of neighborhood regression. We show that this algebraic structure has
the potential to provide an economic encoding of all conditional independence
statements in a Gaussian distribution (or conditional uncorrelatedness in
general), even in the cases where no graphical model exists that could
""perfectly"" encode all such statements. We study the computational complexity
of computing these structures and show that under a sparsity assumption, they
can be computed in polynomial time, even in the absence of the assumption of
perfectness to a graph. On the other hand, assuming perfectness, we show how
these neighborhood lattices may be ""graphically"" computed using the separation
properties of the so-called partial correlation graph. We also draw connections
with directed acyclic graphical models and Bayesian networks. We derive these
results using an abstract generalization of partial uncorrelatedness, called
partial orthogonality, which allows us to use algebraic properties of
projection operators on Hilbert spaces to significantly simplify and extend
existing ideas and arguments. Consequently, our results apply to a wide range
of random objects and data structures, such as random vectors, data matrices,
and functions.
",1,0,1,1,0,0
1548,The 2-adic complexity of a class of binary sequences with almost optimal autocorrelation,"  Pseudo-random sequences with good statistical property, such as low
autocorrelation, high linear complexity and large 2-adic complexity, have been
applied in stream cipher. In general, it is difficult to give both the linear
complexity and 2-adic complexity of a periodic binary sequence. Cai and Ding
\cite{Cai Ying} gave a class of sequences with almost optimal autocorrelation
by constructing almost difference sets. Wang \cite{Wang Qi} proved that one
type of those sequences by Cai and Ding has large linear complexity. Sun et al.
\cite{Sun Yuhua} showed that another type of sequences by Cai and Ding has also
large linear complexity. Additionally, Sun et al. also generalized the
construction by Cai and Ding using $d$-form function with difference-balanced
property. In this paper, we first give the detailed autocorrelation
distribution of the sequences was generalized from Cai and Ding \cite{Cai Ying}
by Sun et al. \cite{Sun Yuhua}. Then, inspired by the method of Hu \cite{Hu
Honggang}, we analyse their 2-adic complexity and give a lower bound on the
2-adic complexity of these sequences. Our result show that the 2-adic
complexity of these sequences is at least $N-\mathrm{log}_2\sqrt{N+1}$ and that
it reach $N-1$ in many cases, which are large enough to resist the rational
approximation algorithm (RAA) for feedback with carry shift registers (FCSRs).
",1,0,1,0,0,0
1549,Nesterov's Smoothing Technique and Minimizing Differences of Convex Functions for Hierarchical Clustering,"  A bilevel hierarchical clustering model is commonly used in designing optimal
multicast networks. In this paper, we consider two different formulations of
the bilevel hierarchical clustering problem, a discrete optimization problem
which can be shown to be NP-hard. Our approach is to reformulate the problem as
a continuous optimization problem by making some relaxations on the
discreteness conditions. Then Nesterov's smoothing technique and a numerical
algorithm for minimizing differences of convex functions called the DCA are
applied to cope with the nonsmoothness and nonconvexity of the problem.
Numerical examples are provided to illustrate our method.
",0,0,1,0,0,0
1550,Minimal solutions to generalized Lambda-semiflows and gradient flows in metric spaces,"  Generalized Lambda-semiflows are an abstraction of semiflows with
non-periodic solutions, for which there may be more than one solution
corresponding to given initial data. A select class of solutions to generalized
Lambda-semiflows is introduced. It is proved that such minimal solutions are
unique corresponding to given ranges and generate all other solutions by time
reparametrization. Special qualities of minimal solutions are shown. The
concept of minimal solutions is applied to gradient flows in metric spaces and
generalized semiflows. Generalized semiflows have been introduced by Ball.
",0,0,1,0,0,0
1551,Newton-Type Methods for Non-Convex Optimization Under Inexact Hessian Information,"  We consider variants of trust-region and cubic regularization methods for
non-convex optimization, in which the Hessian matrix is approximated. Under
mild conditions on the inexact Hessian, and using approximate solution of the
corresponding sub-problems, we provide iteration complexity to achieve $
\epsilon $-approximate second-order optimality which have shown to be tight.
Our Hessian approximation conditions constitute a major relaxation over the
existing ones in the literature. Consequently, we are able to show that such
mild conditions allow for the construction of the approximate Hessian through
various random sampling methods. In this light, we consider the canonical
problem of finite-sum minimization, provide appropriate uniform and non-uniform
sub-sampling strategies to construct such Hessian approximations, and obtain
optimal iteration complexity for the corresponding sub-sampled trust-region and
cubic regularization methods.
",1,0,0,1,0,0
1552,$G 1$-smooth splines on quad meshes with 4-split macro-patch elements,"  We analyze the space of differentiable functions on a quad-mesh $\cM$, which
are composed of 4-split spline macro-patch elements on each quadrangular face.
We describe explicit transition maps across shared edges, that satisfy
conditions which ensure that the space of differentiable functions is ample on
a quad-mesh of arbitrary topology. These transition maps define a finite
dimensional vector space of $G^{1}$ spline functions of bi-degree $\le (k,k)$
on each quadrangular face of $\cM$. We determine the dimension of this space of
$G^{1}$ spline functions for $k$ big enough and provide explicit constructions
of basis functions attached respectively to vertices, edges and faces. This
construction requires the analysis of the module of syzygies of univariate
b-spline functions with b-spline function coefficients. New results on their
generators and dimensions are provided. Examples of bases of $G^{1}$ splines of
small degree for simple topological surfaces are detailed and illustrated by
parametric surface constructions.
",0,0,1,0,0,0
1553,BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character Dataset,"  Bangla handwriting recognition is becoming a very important issue nowadays.
It is potentially a very important task specially for Bangla speaking
population of Bangladesh and West Bengal. By keeping that in our mind we are
introducing a comprehensive Bangla handwritten character dataset named
BanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic
characters and compound characters. This dataset was collected from multiple
geographical location within Bangladesh and includes sample collected from a
variety of aged groups. This dataset can also be used for other classification
problems i.e: gender, age, district. This is the largest dataset on Bangla
handwritten characters yet.
",1,0,0,0,0,0
1554,Estimates for maximal functions associated to hypersurfaces in $\Bbb R^3$ with height $h<2:$ Part I,"  In this article, we continue the study of the problem of $L^p$-boundedness of
the maximal operator $M$ associated to averages along isotropic dilates of a
given, smooth hypersurface $S$ of finite type in 3-dimensional Euclidean space.
An essentially complete answer to this problem had been given about seven years
ago by the last named two authors in joint work with M. Kempe for the case
where the height h of the given surface is at least two. In the present
article, we turn to the case $h<2.$ More precisely, in this Part I, we study
the case where $h<2,$ assuming that $S$ is contained in a sufficiently small
neighborhood of a given point $x^0\in S$ at which both principal curvatures of
$S$ vanish. Under these assumptions and a natural transversality assumption, we
show that, as in the case where $h\ge 2,$ the critical Lebesgue exponent for
the boundedness of $M$ remains to be $p_c=h,$ even though the proof of this
result turns out to require new methods, some of which are inspired by the more
recent work by the last named two authors on Fourier restriction to S. Results
on the case where $h<2$ and exactly one principal curvature of $S$ does not
vanish at $x^0$ will appear elsewhere.
",0,0,1,0,0,0
1555,Using Inertial Sensors for Position and Orientation Estimation,"  In recent years, MEMS inertial sensors (3D accelerometers and 3D gyroscopes)
have become widely available due to their small size and low cost. Inertial
sensor measurements are obtained at high sampling rates and can be integrated
to obtain position and orientation information. These estimates are accurate on
a short time scale, but suffer from integration drift over longer time scales.
To overcome this issue, inertial sensors are typically combined with additional
sensors and models. In this tutorial we focus on the signal processing aspects
of position and orientation estimation using inertial sensors. We discuss
different modeling choices and a selected number of important algorithms. The
algorithms include optimization-based smoothing and filtering as well as
computationally cheaper extended Kalman filter and complementary filter
implementations. The quality of their estimates is illustrated using both
experimental and simulated data.
",1,0,0,0,0,0
1556,Heisenberg equation for a nonrelativistic particle on a hypersurface: from the centripetal force to a curvature induced force,"  In classical mechanics, a nonrelativistic particle constrained on an $N-1$
curved hypersurface embedded in $N$ flat space experiences the centripetal
force only. In quantum mechanics, the situation is totally different for the
presence of the geometric potential. We demonstrate that the motion of the
quantum particle is ""driven"" by not only the the centripetal force, but also a
curvature induced force proportional to the Laplacian of the mean curvature,
which is fundamental in the interface physics, causing curvature driven
interface evolution.
",0,1,0,0,0,0
1557,On constraining projections of future climate using observations and simulations from multiple climate models,"  A new Bayesian framework is presented that can constrain projections of
future climate using historical observations by exploiting robust estimates of
emergent relationships between multiple climate models. We argue that emergent
relationships can be interpreted as constraints on model inadequacy, but that
projections may be biased if we do not account for internal variability in
climate model projections. We extend the previously proposed coexchangeable
framework to account for natural variability in the Earth system and internal
variability simulated by climate models. A detailed theoretical comparison with
previous multi-model projection frameworks is provided.
The proposed framework is applied to projecting surface temperature in the
Arctic at the end of the 21st century. A subset of available climate models are
selected in order to satisfy the assumptions of the framework. All available
initial condition runs from each model are utilized in order maximize the
utility of the data. Projected temperatures in some regions are more than 2C
lower when constrained by historical observations. The uncertainty about the
climate response is reduced by up to 30% where strong constraints exist.
",0,0,0,1,0,0
1558,Higher order molecular organisation as a source of biological function,"  Molecular interactions have widely been modelled as networks. The local
wiring patterns around molecules in molecular networks are linked with their
biological functions. However, networks model only pairwise interactions
between molecules and cannot explicitly and directly capture the higher order
molecular organisation, such as protein complexes and pathways. Hence, we ask
if hypergraphs (hypernetworks), that directly capture entire complexes and
pathways along with protein-protein interactions (PPIs), carry additional
functional information beyond what can be uncovered from networks of pairwise
molecular interactions. The mathematical formalism of a hypergraph has long
been known, but not often used in studying molecular networks due to the lack
of sophisticated algorithms for mining the underlying biological information
hidden in the wiring patterns of molecular systems modelled as hypernetworks.
We propose a new, multi-scale, protein interaction hypernetwork model that
utilizes hypergraphs to capture different scales of protein organization,
including PPIs, protein complexes and pathways. In analogy to graphlets, we
introduce hypergraphlets, small, connected, non-isomorphic, induced
sub-hypergraphs of a hypergraph, to quantify the local wiring patterns of these
multi-scale molecular hypergraphs and to mine them for new biological
information. We apply them to model the multi-scale protein networks of baker
yeast and human and show that the higher order molecular organisation captured
by these hypergraphs is strongly related to the underlying biology.
Importantly, we demonstrate that our new models and data mining tools reveal
different, but complementary biological information compared to classical PPI
networks. We apply our hypergraphlets to successfully predict biological
functions of uncharacterised proteins.
",0,0,0,0,1,0
1559,The Massive CO White Dwarf in the Symbiotic Recurrent Nova RS Ophiuchi,"  If accreting white dwarfs (WD) in binary systems are to produce type Ia
supernovae (SNIa), they must grow to nearly the Chandrasekhar mass and ignite
carbon burning. Proving conclusively that a WD has grown substantially since
its birth is a challenging task. Slow accretion of hydrogen inevitably leads to
the erosion, rather than the growth of WDs. Rapid hydrogen accretion does lead
to growth of a helium layer, due to both decreased degeneracy and the
inhibition of mixing of the accreted hydrogen with the underlying WD. However,
until recently, simulations of helium-accreting WDs all claimed to show the
explosive ejection of a helium envelope once it exceeded $\sim 10^{-1}\, \rm
M_{\odot}$. Because CO WDs cannot be born with masses in excess of $\sim 1.1\,
\rm M_{\odot}$, any such object, in excess of $\sim 1.2\, \rm M_{\odot}$, must
have grown substantially. We demonstrate that the WD in the symbiotic nova RS
Oph is in the mass range 1.2-1.4\,M$_{\odot}$. We compare UV spectra of RS Oph
with those of novae with ONe WDs, and with novae erupting on CO WDs. The RS Oph
WD is clearly made of CO, demonstrating that it has grown substantially since
birth. It is a prime candidate to eventually produce an SNIa.
",0,1,0,0,0,0
1560,Semi-equivelar maps on the torus are Archimedean,"  If the face-cycles at all the vertices in a map on a surface are of same type
then the map is called semi-equivelar. There are eleven types of Archimedean
tilings on the plane. All the Archimedean tilings are semi-equivelar maps. If a
map $X$ on the torus is a quotient of an Archimedean tiling on the plane then
the map $X$ is semi-equivelar. We show that each semi-equivelar map on the
torus is a quotient of an Archimedean tiling on the plane.
Vertex-transitive maps are semi-equivelar maps. We know that four types of
semi-equivelar maps on the torus are always vertex-transitive and there are
examples of other seven types of semi-equivelar maps which are not
vertex-transitive. We show that the number of ${\rm Aut}(Y)$-orbits of vertices
for any semi-equivelar map $Y$ on the torus is at most six. In fact, the number
of orbits is at most three except one type of semi-equivelar maps. Our bounds
on the number of orbits are sharp.
",0,0,1,0,0,0
1561,Dynamics of Porous Dust Aggregates and Gravitational Instability of Their Disk,"  We consider the dynamics of porous icy dust aggregates in a turbulent gas
disk and investigate the stability of the disk. We evaluate the random velocity
of porous dust aggregates by considering their self-gravity, collisions,
aerodynamic drag, turbulent stirring and scattering due to gas. We extend our
previous work by introducing the anisotropic velocity dispersion and the
relaxation time of the random velocity. We find the minimum mass solar nebular
model to be gravitationally unstable if the turbulent viscosity parameter
$\alpha$ is less than about $4 \times 10^{-3}$. The upper limit of $\alpha$ for
the onset of gravitational instability is derived as a function of the disk
parameters. We discuss the implications of the gravitational instability for
planetesimal formation.
",0,1,0,0,0,0
1562,Localization landscape theory of disorder in semiconductors II: Urbach tails of disordered quantum well layers,"  Urbach tails in semiconductors are often associated to effects of
compositional disorder. The Urbach tail observed in InGaN alloy quantum wells
of solar cells and LEDs by biased photocurrent spectroscopy is shown to be
characteristic of the ternary alloy disorder. The broadening of the absorption
edge observed for quantum wells emitting from violet to green (indium content
ranging from 0 to 28\%) corresponds to a typical Urbach energy of 20~meV. A 3D
absorption model is developed based on a recent theory of disorder-induced
localization which provides the effective potential seen by the localized
carriers without having to resort to the solution of the Schr??dinger equation
in a disordered potential. This model incorporating compositional disorder
accounts well for the experimental broadening of the Urbach tail of the
absorption edge. For energies below the Urbach tail of the InGaN quantum wells,
type-II well-to-barrier transitions are observed and modeled. This contribution
to the below bandgap absorption is particularly efficient in near-UV emitting
quantum wells. When reverse biasing the device, the well-to-barrier below
bandgap absorption exhibits a red shift, while the Urbach tail corresponding to
the absorption within the quantum wells is blue shifted, due to the partial
compensation of the internal piezoelectric fields by the external bias. The
good agreement between the measured Urbach tail and its modeling by the new
localization theory demonstrates the applicability of the latter to
compositional disorder effects in nitride semiconductors.
",0,1,0,0,0,0
1563,Global teleconnectivity structures of the El Ni?ño-Southern Oscillation and large volcanic eruptions -- An evolving network perspective,"  Recent work has provided ample evidence that global climate dynamics at
time-scales between multiple weeks and several years can be severely affected
by the episodic occurrence of both, internal (climatic) and external
(non-climatic) perturbations. Here, we aim to improve our understanding on how
regional to local disruptions of the ""normal"" state of the global surface air
temperature field affect the corresponding global teleconnectivity structure.
Specifically, we present an approach to quantify teleconnectivity based on
different characteristics of functional climate network analysis. Subsequently,
we apply this framework to study the impacts of different phases of the El
Ni?ño-Southern Oscillation (ENSO) as well as the three largest volcanic
eruptions since the mid 20th century on the dominating spatiotemporal
co-variability patterns of daily surface air temperatures. Our results confirm
the existence of global effects of ENSO which result in episodic breakdowns of
the hierarchical organization of the global temperature field. This is
associated with the emergence of strong teleconnections. At more regional
scales, similar effects are found after major volcanic eruptions. Taken
together, the resulting time-dependent patterns of network connectivity allow a
tracing of the spatial extents of the dominating effects of both types of
climate disruptions. We discuss possible links between these observations and
general aspects of atmospheric circulation.
",0,1,0,0,0,0
1564,Unbiased Shrinkage Estimation,"  Shrinkage estimation usually reduces variance at the cost of bias. But when
we care only about some parameters of a model, I show that we can reduce
variance without incurring bias if we have additional information about the
distribution of covariates. In a linear regression model with homoscedastic
Normal noise, I consider shrinkage estimation of the nuisance parameters
associated with control variables. For at least three control variables and
exogenous treatment, I establish that the standard least-squares estimator is
dominated with respect to squared-error loss in the treatment effect even among
unbiased estimators and even when the target parameter is low-dimensional. I
construct the dominating estimator by a variant of James-Stein shrinkage in a
high-dimensional Normal-means problem. It can be interpreted as an invariant
generalized Bayes estimator with an uninformative (improper) Jeffreys prior in
the target parameter.
",0,0,1,1,0,0
1565,Characterizing The Influence of Continuous Integration. Empirical Results from 250+ Open Source and Proprietary Projects,"  Continuous integration (CI) tools integrate code changes by automatically
compiling, building, and executing test cases upon submission of code changes.
Use of CI tools is getting increasingly popular, yet how proprietary projects
reap the benefits of CI remains unknown. To investigate the influence of CI on
software development, we analyze 150 open source software (OSS) projects, and
123 proprietary projects. For OSS projects, we observe the expected benefits
after CI adoption, e.g., improvements in bug and issue resolution. However, for
the proprietary projects, we cannot make similar observations. Our findings
indicate that only adoption of CI might not be enough to the improve software
development process. CI can be effective for software development if
practitioners use CI's feedback mechanism efficiently, by applying the practice
of making frequent commits. For our set of proprietary projects we observe
practitioners commit less frequently, and hence not use CI effectively for
obtaining feedback on the submitted code changes. Based on our findings we
recommend industry practitioners to adopt the best practices of CI to reap the
benefits of CI tools for example, making frequent commits.
",1,0,0,0,0,0
1566,Why Abeta42 Is Much More Toxic Than Abeta40,"  Amyloid precursor with 770 amino acids dimerizes and aggregates, as do its c
terminal 99 amino acids and amyloid 40,42 amino acids fragments. The titled
question has been discussed extensively, and here it is addressed further using
thermodynamic scaling theory to analyze mutational trends in structural factors
and kinetics. Special attention is given to Family Alzheimer's Disease
mutations outside amyloid 42. The scaling analysis is connected to extensive
docking simulations which included membranes, thereby confirming their results
and extending them to Amyloid precursor.
",0,0,0,0,1,0
1567,A Polynomial Time Algorithm for Spatio-Temporal Security Games,"  An ever-important issue is protecting infrastructure and other valuable
targets from a range of threats from vandalism to theft to piracy to terrorism.
The ""defender"" can rarely afford the needed resources for a 100% protection.
Thus, the key question is, how to provide the best protection using the limited
available resources. We study a practically important class of security games
that is played out in space and time, with targets and ""patrols"" moving on a
real line. A central open question here is whether the Nash equilibrium (i.e.,
the minimax strategy of the defender) can be computed in polynomial time. We
resolve this question in the affirmative. Our algorithm runs in time polynomial
in the input size, and only polylogarithmic in the number of possible patrol
locations (M). Further, we provide a continuous extension in which patrol
locations can take arbitrary real values. Prior work obtained polynomial-time
algorithms only under a substantial assumption, e.g., a constant number of
rounds. Further, all these algorithms have running times polynomial in M, which
can be very large.
",1,0,0,0,0,0
1568,TIDBD: Adapting Temporal-difference Step-sizes Through Stochastic Meta-descent,"  In this paper, we introduce a method for adapting the step-sizes of temporal
difference (TD) learning. The performance of TD methods often depends on well
chosen step-sizes, yet few algorithms have been developed for setting the
step-size automatically for TD learning. An important limitation of current
methods is that they adapt a single step-size shared by all the weights of the
learning system. A vector step-size enables greater optimization by specifying
parameters on a per-feature basis. Furthermore, adapting parameters at
different rates has the added benefit of being a simple form of representation
learning. We generalize Incremental Delta Bar Delta (IDBD)---a vectorized
adaptive step-size method for supervised learning---to TD learning, which we
name TIDBD. We demonstrate that TIDBD is able to find appropriate step-sizes in
both stationary and non-stationary prediction tasks, outperforming ordinary TD
methods and TD methods with scalar step-size adaptation; we demonstrate that it
can differentiate between features which are relevant and irrelevant for a
given task, performing representation learning; and we show on a real-world
robot prediction task that TIDBD is able to outperform ordinary TD methods and
TD methods augmented with AlphaBound and RMSprop.
",0,0,0,1,0,0
1569,Enhanced clustering tendency of Cu-impurities with a number of oxygen vacancies in heavy carbon-loaded TiO2 - the bulk and surface morphologies,"  The over threshold carbon-loadings (~50 at.%) of initial TiO2-hosts and
posterior Cu-sensitization (~7 at.%) was made using pulsed ion-implantation
technique in sequential mode with 1 hour vacuum-idle cycle between sequential
stages of embedding. The final Cx-TiO2:Cu samples were qualified using XPS
wide-scan elemental analysis, core-levels and valence band mappings. The
results obtained were discussed on the theoretic background employing
DFT-calculations. The combined XPS and DFT analysis allows to establish and
prove the final formula of the synthesized samples as Cx-TiO2:[Cu+][Cu2+] for
the bulk and Cx-TiO2:[Cu+][Cu0] for thin-films. It was demonstrated the in the
mode of heavy carbon-loadings the remaining majority of neutral C-C bonds
(sp3-type) is dominating and only a lack of embedded carbon is fabricating the
O-C=O clusters. No valence base-band width altering was established after
sequential carbon-copper modification of the atomic structure of initial
TiO2-hosts except the dominating majority of Cu 3s states after
Cu-sensitization. The crucial role of neutral carbon low-dimensional impurities
as the precursors for the new phases growth was shown for Cu-sensitized Cx-TiO2
intermediate-state hosts.
",0,1,0,0,0,0
1570,On Controllable Abundance Of Saturated-input Linear Discrete Systems,"  Several theorems on the volume computing of the polyhedron spanned by a
n-dimensional vector set with the finite-interval parameters are presented and
proved firstly, and then are used in the analysis of the controllable regions
of the linear discrete time-invariant systems with saturated inputs. A new
concept and continuous measure on the control ability, control efficiency of
the input variables, and the diversity of the control laws, named as the
controllable abundance, is proposed based on the volume computing of the
regions and is applied to the actuator placing and configuring problems, the
optimizing problems of dynamics and kinematics of the controlled plants, etc..
The numerical experiments show the effectiveness of the new concept and methods
for investigating and optimizing the control ability and efficiency.
",1,0,1,0,0,0
1571,Localization and dynamics of sulfur-oxidizing microbes in natural sediment,"  Organic material in anoxic sediment represents a globally significant carbon
reservoir that acts to stabilize Earth's atmospheric composition. The dynamics
by which microbes organize to consume this material remain poorly understood.
Here we observe the collective dynamics of a microbial community, collected
from a salt marsh, as it comes to steady state in a two-dimensional ecosystem,
covered by flowing water and under constant illumination. Microbes form a very
thin front at the oxic-anoxic interface that moves towards the surface with
constant velocity and comes to rest at a fixed depth. Fronts are stable to all
perturbations while in the sediment, but develop bioconvective plumes in water.
We observe the transient formation of parallel fronts. We model these dynamics
to understand how they arise from the coupling between metabolism, aerotaxis,
and diffusion. These results identify the typical timescale for the oxygen flux
and penetration depth to reach steady state.
",0,1,0,0,0,0
1572,Probabilistic Surfel Fusion for Dense LiDAR Mapping,"  With the recent development of high-end LiDARs, more and more systems are
able to continuously map the environment while moving and producing spatially
redundant information. However, none of the previous approaches were able to
effectively exploit this redundancy in a dense LiDAR mapping problem. In this
paper, we present a new approach for dense LiDAR mapping using probabilistic
surfel fusion. The proposed system is capable of reconstructing a high-quality
dense surface element (surfel) map from spatially redundant multiple views.
This is achieved by a proposed probabilistic surfel fusion along with a
geometry considered data association. The proposed surfel data association
method considers surface resolution as well as high measurement uncertainty
along its beam direction which enables the mapping system to be able to control
surface resolution without introducing spatial digitization. The proposed
fusion method successfully suppresses the map noise level by considering
measurement noise caused by laser beam incident angle and depth distance in a
Bayesian filtering framework. Experimental results with simulated and real data
for the dense surfel mapping prove the ability of the proposed method to
accurately find the canonical form of the environment without further
post-processing.
",1,0,0,0,0,0
1573,Quantum Paramagnet and Frustrated Quantum Criticality in a Spin-One Diamond Lattice Antiferromagnet,"  Motivated by the proposal of topological quantum paramagnet in the diamond
lattice antiferromagnet NiRh$_2$O$_4$, we propose a minimal model to describe
the magnetic interaction and properties of the diamond material with the
spin-one local moments. Our model includes the first and second neighbor
Heisenberg interactions as well as a local single-ion spin anisotropy that is
allowed by the spin-one nature of the local moment and the tetragonal symmetry
of the system. We point out that there exists a quantum phase transition from a
trivial quantum paramagnet when the single-ion spin anisotropy is dominant to
the magnetic ordered states when the exchange is dominant. Due to the
frustrated spin interaction, the magnetic excitation in the quantum
paramagnetic state supports extensively degenerate band minima in the spectra.
As the system approaches the transition, extensively degenerate bosonic modes
become critical at the criticality, giving rise to unusual magnetic properties.
Our phase diagram and experimental predictions for different phases provide a
guildeline for the identification of the ground state for NiRh$_2$O$_4$.
Although our results are fundamentally different from the proposal of
topological quantum paramagnet, it represents interesting possibilities for
spin-one diamond lattice antiferromagnets.
",0,1,0,0,0,0
1574,Characterizations of minimal dominating sets and the well-dominated property in lexicographic product graphs,"  A graph is said to be well-dominated if all its minimal dominating sets are
of the same size. The class of well-dominated graphs forms a subclass of the
well studied class of well-covered graphs. While the recognition problem for
the class of well-covered graphs is known to be co-NP-complete, the recognition
complexity of well-dominated graphs is open.
In this paper we introduce the notion of an irreducible dominating set, a
variant of dominating set generalizing both minimal dominating sets and minimal
total dominating sets. Based on this notion, we characterize the family of
minimal dominating sets in a lexicographic product of two graphs and derive a
characterization of the well-dominated lexicographic product graphs. As a side
result motivated by this study, we give a polynomially testable
characterization of well-dominated graphs with domination number two, and show,
more generally, that well-dominated graphs can be recognized in polynomial time
in any class of graphs with bounded domination number. Our results include a
characterization of dominating sets in lexicographic product graphs, which
generalizes the expression for the domination number of such graphs following
from works of Zhang et al. (2011) and of ÿumenjak et al. (2012).
",1,0,1,0,0,0
1575,To the Acceleration of Charged Particles with Travelling Laser Focus,"  We describe here the latest results of calculations with FlexPDE code of
wake-fields induced by the bunch in micro-structures. These structures,
illuminated by swept laser bust, serve for acceleration of charged particles.
The basis of the scheme is a fast sweeping device for the laser bunch. After
sweeping, the laser bunch has a slope ~45o with respect to the direction of
propagation. So the every cell of the microstructure becomes excited locally
only for the moment when the particles are there. Self-consistent parameters of
collider based on this idea allow consideration this type of collider as a
candidate for the near-future accelerator era.
",0,1,0,0,0,0
1576,Affiliation networks with an increasing degree sequence,"  Affiliation network is one kind of two-mode social network with two different
sets of nodes (namely, a set of actors and a set of social events) and edges
representing the affiliation of the actors with the social events. Although a
number of statistical models are proposed to analyze affiliation networks, the
asymptotic behaviors of the estimator are still unknown or have not been
properly explored. In this paper, we study an affiliation model with the degree
sequence as the exclusively natural sufficient statistic in the exponential
family distributions. We establish the uniform consistency and asymptotic
normality of the maximum likelihood estimator when the numbers of actors and
events both go to infinity. Simulation studies and a real data example
demonstrate our theoretical results.
",0,0,1,1,0,0
1577,Coarse Grained Parallel Selection,"  We analyze the running time of the Saukas-Song algorithm for selection on a
coarse grained multicomputer without expressing the running time in terms of
communication rounds. This shows that while in the best case the Saukas-Song
algorithm runs in asymptotically optimal time, in general it does not. We
propose other algorithms for coarse grained selection that have optimal
expected running time.
",1,0,0,0,0,0
1578,An IoT Analytics Embodied Agent Model based on Context-Aware Machine Learning,"  Agent-based Internet of Things (IoT) applications have recently emerged as
applications that can involve sensors, wireless devices, machines and software
that can exchange data and be accessed remotely. Such applications have been
proposed in several domains including health care, smart cities and
agriculture. However, despite their increased adoption, deploying these
applications in specific settings has been very challenging because of the
complex static and dynamic variability of the physical devices such as sensors
and actuators, the software application behavior and the environment in which
the application is embedded. In this paper, we propose a modeling approach for
IoT analytics based on learning embodied agents (i.e. situated agents). The
approach involves: (i) a variability model of IoT embodied agents; (ii)
feedback evaluative machine learning; and (iii) reconfiguration of a group of
agents in accordance with environmental context. The proposed approach advances
the state of the art in that it facilitates the development of Agent-based IoT
applications by explicitly capturing their complex and dynamic variabilities
and supporting their self-configuration based on an context-aware and machine
learning-based approach.
",1,0,0,0,0,0
1579,Aggregation of Classifiers: A Justifiable Information Granularity Approach,"  In this study, we introduce a new approach to combine multi-classifiers in an
ensemble system. Instead of using numeric membership values encountered in
fixed combining rules, we construct interval membership values associated with
each class prediction at the level of meta-data of observation by using
concepts of information granules. In the proposed method, uncertainty
(diversity) of findings produced by the base classifiers is quantified by
interval-based information granules. The discriminative decision model is
generated by considering both the bounds and the length of the obtained
intervals. We select ten and then fifteen learning algorithms to build a
heterogeneous ensemble system and then conducted the experiment on a number of
UCI datasets. The experimental results demonstrate that the proposed approach
performs better than the benchmark algorithms including six fixed combining
methods, one trainable combining method, AdaBoost, Bagging, and Random
Subspace.
",1,0,0,1,0,0
1580,FRET-based nanocommunication with luciferase and channelrhodopsin molecules for in-body medical systems,"  The paper is concerned with an in-body system gathering data for medical
purposes. It is focused on communication between the following two components
of the system: liposomes gathering the data inside human veins and a detector
collecting the data from liposomes. Foerster Resonance Energy Transfer (FRET)
is considered as a mechanism for communication between the system components.
The usage of bioluminescent molecules as an energy source for generating FRET
signals is suggested and the performance evaluation of this approach is given.
FRET transmission may be initiated without an aid of an external laser, which
is crucial in case of communication taking place inside of human body. It is
also shown how to solve the problem of FRET signals recording. The usage of
channelrhodopsin molecules, able to receive FRET signals and convert them into
voltage, is proposed. The communication system is modelled with molecular
structures and spectral characteristics of the proposed molecules and further
validated by using Monte Carlo computer simulations, calculating the data
throughput and the bit error rate.
",0,0,0,0,1,0
1581,FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High Dimensional Similarity Search,"  We present FLASH (\textbf{F}ast \textbf{L}SH \textbf{A}lgorithm for
\textbf{S}imilarity search accelerated with \textbf{H}PC), a similarity search
system for ultra-high dimensional datasets on a single machine, that does not
require similarity computations and is tailored for high-performance computing
platforms. By leveraging a LSH style randomized indexing procedure and
combining it with several principled techniques, such as reservoir sampling,
recent advances in one-pass minwise hashing, and count based estimations, we
reduce the computational and parallelization costs of similarity search, while
retaining sound theoretical guarantees.
We evaluate FLASH on several real, high-dimensional datasets from different
domains, including text, malicious URL, click-through prediction, social
networks, etc. Our experiments shed new light on the difficulties associated
with datasets having several million dimensions. Current state-of-the-art
implementations either fail on the presented scale or are orders of magnitude
slower than FLASH. FLASH is capable of computing an approximate k-NN graph,
from scratch, over the full webspam dataset (1.3 billion nonzeros) in less than
10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam
dataset, using brute-force ($n^2D$), will require at least 20 teraflops. We
provide CPU and GPU implementations of FLASH for replicability of our results.
",1,0,0,0,0,0
1582,Neural Sequence Model Training via $?ñ$-divergence Minimization,"  We propose a new neural sequence model training method in which the objective
function is defined by $\alpha$-divergence. We demonstrate that the objective
function generalizes the maximum-likelihood (ML)-based and reinforcement
learning (RL)-based objective functions as special cases (i.e., ML corresponds
to $\alpha \to 0$ and RL to $\alpha \to1$). We also show that the gradient of
the objective function can be considered a mixture of ML- and RL-based
objective gradients. The experimental results of a machine translation task
show that minimizing the objective function with $\alpha > 0$ outperforms
$\alpha \to 0$, which corresponds to ML-based methods.
",1,0,0,1,0,0
1583,Output Range Analysis for Deep Neural Networks,"  Deep neural networks (NN) are extensively used for machine learning tasks
such as image classification, perception and control of autonomous systems.
Increasingly, these deep NNs are also been deployed in high-assurance
applications. Thus, there is a pressing need for developing techniques to
verify neural networks to check whether certain user-expected properties are
satisfied. In this paper, we study a specific verification problem of computing
a guaranteed range for the output of a deep neural network given a set of
inputs represented as a convex polyhedron. Range estimation is a key primitive
for verifying deep NNs. We present an efficient range estimation algorithm that
uses a combination of local search and linear programming problems to
efficiently find the maximum and minimum values taken by the outputs of the NN
over the given input set. In contrast to recently proposed ""monolithic""
optimization approaches, we use local gradient descent to repeatedly find and
eliminate local minima of the function. The final global optimum is certified
using a mixed integer programming instance. We implement our approach and
compare it with Reluplex, a recently proposed solver for deep neural networks.
We demonstrate the effectiveness of the proposed approach for verification of
NNs used in automated control as well as those used in classification.
",1,0,0,1,0,0
1584,Projection Theorems of Divergences and Likelihood Maximization Methods,"  Projection theorems of divergences enable us to find reverse projection of a
divergence on a specific statistical model as a forward projection of the
divergence on a different but rather ""simpler"" statistical model, which, in
turn, results in solving a system of linear equations. Reverse projection of
divergences are closely related to various estimation methods such as the
maximum likelihood estimation or its variants in robust statistics. We consider
projection theorems of three parametric families of divergences that are widely
used in robust statistics, namely the R??nyi divergences (or the Cressie-Reed
power divergences), density power divergences, and the relative
$\alpha$-entropy (or the logarithmic density power divergences). We explore
these projection theorems from the usual likelihood maximization approach and
from the principle of sufficiency. In particular, we show the equivalence of
solving the estimation problems by the projection theorems of the respective
divergences and by directly solving the corresponding estimating equations. We
also derive the projection theorem for the density power divergences.
",0,0,1,1,0,0
1585,Optimal Timing in Dynamic and Robust Attacker Engagement During Advanced Persistent Threats,"  Advanced persistent threats (APTs) are stealthy attacks which make use of
social engineering and deception to give adversaries insider access to
networked systems. Against APTs, active defense technologies aim to create and
exploit information asymmetry for defenders. In this paper, we study a scenario
in which a powerful defender uses honeynets for active defense in order to
observe an attacker who has penetrated the network. Rather than immediately
eject the attacker, the defender may elect to gather information. We introduce
an undiscounted, infinite-horizon Markov decision process on a continuous state
space in order to model the defender's problem. We find a threshold of
information that the defender should gather about the attacker before ejecting
him. Then we study the robustness of this policy using a Stackelberg game.
Finally, we simulate the policy for a conceptual network. Our results provide a
quantitative foundation for studying optimal timing for attacker engagement in
network defense.
",1,0,0,0,0,0
1586,The Mismeasure of Mergers: Revised Limits on Self-interacting Dark Matter in Merging Galaxy Clusters,"  In an influential recent paper, Harvey et al (2015) derive an upper limit to
the self-interaction cross section of dark matter ($\sigma_{\rm DM} < 0.47$
cm$^2$/g at 95\% confidence) by averaging the dark matter-galaxy offsets in a
sample of merging galaxy clusters. Using much more comprehensive data on the
same clusters, we identify several substantial errors in their offset
measurements. Correcting these errors relaxes the upper limit on $\sigma_{\rm
DM}$ to $\lesssim 2$ cm$^2$/g, following the Harvey et al prescription for
relating offsets to cross sections in a simple solid body scattering model.
Furthermore, many clusters in the sample violate the assumptions behind this
prescription, so even this revised upper limit should be used with caution.
Although this particular sample does not tightly constrain self-interacting
dark matter models when analyzed this way, we discuss how merger ensembles may
be used more effectively in the future. We conclude that errors inherent in
using single-band imaging to identify mass and light peaks do not necessarily
average out in a sample of this size, particularly when a handful of
substructures constitute a majority of the weight in the ensemble.
",0,1,0,0,0,0
1587,International crop trade networks: The impact of shocks and cascades,"  Analyzing available FAO data from 176 countries over 21 years, we observe an
increase of complexity in the international trade of maize, rice, soy, and
wheat. A larger number of countries play a role as producers or intermediaries,
either for trade or food processing. In consequence, we find that the trade
networks become more prone to failure cascades caused by exogenous shocks. In
our model, countries compensate for demand deficits by imposing export
restrictions. To capture these, we construct higher-order trade dependency
networks for the different crops and years. These networks reveal hidden
dependencies between countries and allow to discuss policy implications.
",0,0,0,0,0,1
1588,Winning on the Merits: The Joint Effects of Content and Style on Debate Outcomes,"  Debate and deliberation play essential roles in politics and government, but
most models presume that debates are won mainly via superior style or agenda
control. Ideally, however, debates would be won on the merits, as a function of
which side has the stronger arguments. We propose a predictive model of debate
that estimates the effects of linguistic features and the latent persuasive
strengths of different topics, as well as the interactions between the two.
Using a dataset of 118 Oxford-style debates, our model's combination of content
(as latent topics) and style (as linguistic features) allows us to predict
audience-adjudicated winners with 74% accuracy, significantly outperforming
linguistic features alone (66%). Our model finds that winning sides employ
stronger arguments, and allows us to identify the linguistic features
associated with strong or weak arguments.
",1,0,0,0,0,0
1589,Gene regulatory networks: a primer in biological processes and statistical modelling,"  Modelling gene regulatory networks not only requires a thorough understanding
of the biological system depicted but also the ability to accurately represent
this system from a mathematical perspective. Throughout this chapter, we aim to
familiarise the reader with the biological processes and molecular factors at
play in the process of gene expression regulation.We first describe the
different interactions controlling each step of the expression process, from
transcription to mRNA and protein decay. In the second section, we provide
statistical tools to accurately represent this biological complexity in the
form of mathematical models. Amongst other considerations, we discuss the
topological properties of biological networks, the application of deterministic
and stochastic frameworks and the quantitative modelling of regulation. We
particularly focus on the use of such models for the simulation of expression
data that can serve as a benchmark for the testing of network inference
algorithms.
",0,0,0,1,1,0
1590,Mathematical Knowledge and the Role of an Observer: Ontological and epistemological aspects,"  As David Berlinski writes (1997), the existence and nature of mathematics is
a more compelling and far deeper problem than any of the problems raised by
mathematics itself. Here we analyze the essence of mathematics making the main
emphasis on mathematics as an advanced system of knowledge. This knowledge
consists of structures and represents structures, existence of which depends on
observers in a nonstandard way. Structural nature of mathematics explains its
reasonable effectiveness.
",0,0,1,0,0,0
1591,Persuasive Technology For Human Development: Review and Case Study,"  Technology is an extremely potent tool that can be leveraged for human
development and social good. Owing to the great importance of environment and
human psychology in driving human behavior, and the ubiquity of technology in
modern life, there is a need to leverage the insights and capabilities of both
fields together for nudging people towards a behavior that is optimal in some
sense (personal or social). In this regard, the field of persuasive technology,
which proposes to infuse technology with appropriate design and incentives
using insights from psychology, behavioral economics, and human-computer
interaction holds a lot of promise. Whilst persuasive technology is already
being developed and is at play in many commercial applications, it can have the
great social impact in the field of Information and Communication Technology
for Development (ICTD) which uses Information and Communication Technology
(ICT) for human developmental ends such as education and health. In this paper
we will explore what persuasive technology is and how it can be used for the
ends of human development. To develop the ideas in a concrete setting, we
present a case study outlining how persuasive technology can be used for human
development in Pakistan, a developing South Asian country, that suffers from
many of the problems that plague typical developing country.
",1,0,0,0,0,0
1592,Variable Prioritization in Nonlinear Black Box Methods: A Genetic Association Case Study,"  The central aim in this paper is to address variable selection questions in
nonlinear and nonparametric regression. Motivated by statistical genetics,
where nonlinear interactions are of particular interest, we introduce a novel
and interpretable way to summarize the relative importance of predictor
variables. Methodologically, we develop the ""RelATive cEntrality"" (RATE)
measure to prioritize candidate genetic variants that are not just marginally
important, but whose associations also stem from significant covarying
relationships with other variants in the data. We illustrate RATE through
Bayesian Gaussian process regression, but the methodological innovations apply
to other ""black box"" methods. It is known that nonlinear models often exhibit
greater predictive accuracy than linear models, particularly for phenotypes
generated by complex genetic architectures. With detailed simulations and two
real data association mapping studies, we show that applying RATE enables an
explanation for this improved performance.
",0,0,0,1,1,0
1593,Activit{??} motrice des truies en groupes dans les diff{??}rents syst{??}mes de logement,"  Assessment of the motor activity of group-housed sows in commercial farms.
The objective of this study was to specify the level of motor activity of
pregnant sows housed in groups in different housing systems. Eleven commercial
farms were selected for this study. Four housing systems were represented:
small groups of five to seven sows (SG), free access stalls (FS) with exercise
area, electronic sow feeder with a stable group (ESFsta) or a dynamic group
(ESFdyn). Ten sows in mid-gestation were observed in each farm. The
observations of motor activity were made for 6 hours at the first meal or at
the start of the feeding sequence, two consecutive days and at regular
intervals of 4 minutes. The results show that the motor activity of
group-housed sows depends on the housing system. The activity is higher with
the ESFdyn system (standing: 55.7%), sows are less active in the SG system
(standing: 26.5%), and FS system is intermediate. The distance traveled by sows
in ESF system is linked to a larger area available. Thus, sows travel an
average of 362 m $\pm$ 167 m in the ESFdyn system with an average available
surface of 446 m${}^2$ whereas sows in small groups travel 50 m $\pm$ 15 m for
15 m${}^2$ available.
",0,0,0,0,1,0
1594,Linking High-Energy Cosmic Particles by Black-Hole Jets Embedded in Large-Scale Structures,"  The origin of ultrahigh-energy cosmic rays (UHECRs) is a half-century old
enigma (Linsley 1963). The mystery has been deepened by an intriguing
coincidence: over ten orders of magnitude in energy, the energy generation
rates of UHECRs, PeV neutrinos, and isotropic sub-TeV gamma rays are
comparable, which hints at a grand-unified picture (Murase and Waxman 2016).
Here we report that powerful black hole jets in aggregates of galaxies can
supply the common origin of all of these phenomena. Once accelerated by a jet,
low-energy cosmic rays confined in the radio lobe are adiabatically cooled;
higher-energy cosmic rays leaving the source interact with the magnetized
cluster environment and produce neutrinos and gamma rays; the highest-energy
particles escape from the host cluster and contribute to the observed cosmic
rays above 100 PeV. The model is consistent with the spectrum, composition, and
isotropy of the observed UHECRs, and also explains the IceCube neutrinos and
the non-blazar component of the Fermi gamma-ray background, assuming a
reasonable energy output from black hole jets in clusters.
",0,1,0,0,0,0
1595,Quantifying and suppressing ranking bias in a large citation network,"  It is widely recognized that citation counts for papers from different fields
cannot be directly compared because different scientific fields adopt different
citation practices. Citation counts are also strongly biased by paper age since
older papers had more time to attract citations. Various procedures aim at
suppressing these biases and give rise to new normalized indicators, such as
the relative citation count. We use a large citation dataset from Microsoft
Academic Graph and a new statistical framework based on the Mahalanobis
distance to show that the rankings by well known indicators, including the
relative citation count and Google's PageRank score, are significantly biased
by paper field and age. We propose a general normalization procedure motivated
by the $z$-score which produces much less biased rankings when applied to
citation count and PageRank score.
",1,1,0,1,0,0
1596,Posterior Concentration for Bayesian Regression Trees and Forests,"  Since their inception in the 1980's, regression trees have been one of the
more widely used non-parametric prediction methods. Tree-structured methods
yield a histogram reconstruction of the regression surface, where the bins
correspond to terminal nodes of recursive partitioning. Trees are powerful, yet
susceptible to over-fitting. Strategies against overfitting have traditionally
relied on pruning greedily grown trees. The Bayesian framework offers an
alternative remedy against overfitting through priors. Roughly speaking, a good
prior charges smaller trees where overfitting does not occur. While the
consistency of random histograms, trees and their ensembles has been studied
quite extensively, the theoretical understanding of the Bayesian counterparts
has been missing. In this paper, we take a step towards understanding why/when
do Bayesian trees and their ensembles not overfit. To address this question, we
study the speed at which the posterior concentrates around the true smooth
regression function. We propose a spike-and-tree variant of the popular
Bayesian CART prior and establish new theoretical results showing that
regression trees (and their ensembles) (a) are capable of recovering smooth
regression surfaces, achieving optimal rates up to a log factor, (b) can adapt
to the unknown level of smoothness and (c) can perform effective dimension
reduction when p>n. These results provide a piece of missing theoretical
evidence explaining why Bayesian trees (and additive variants thereof) have
worked so well in practice.
",0,0,1,1,0,0
1597,Predicting Oral Disintegrating Tablet Formulations by Neural Network Techniques,"  Oral Disintegrating Tablets (ODTs) is a novel dosage form that can be
dissolved on the tongue within 3min or less especially for geriatric and
pediatric patients. Current ODT formulation studies usually rely on the
personal experience of pharmaceutical experts and trial-and-error in the
laboratory, which is inefficient and time-consuming. The aim of current
research was to establish the prediction model of ODT formulations with direct
compression process by Artificial Neural Network (ANN) and Deep Neural Network
(DNN) techniques. 145 formulation data were extracted from Web of Science. All
data sets were divided into three parts: training set (105 data), validation
set (20) and testing set (20). ANN and DNN were compared for the prediction of
the disintegrating time. The accuracy of the ANN model has reached 85.60%,
80.00% and 75.00% on the training set, validation set and testing set
respectively, whereas that of the DNN model was 85.60%, 85.00% and 80.00%,
respectively. Compared with the ANN, DNN showed the better prediction for ODT
formulations. It is the first time that deep neural network with the improved
dataset selection algorithm is applied to formulation prediction on small data.
The proposed predictive approach could evaluate the critical parameters about
quality control of formulation, and guide research and process development. The
implementation of this prediction model could effectively reduce drug product
development timeline and material usage, and proactively facilitate the
development of a robust drug product.
",0,0,0,1,0,0
1598,HNCcorr: A Novel Combinatorial Approach for Cell Identification in Calcium-Imaging Movies,"  Calcium imaging has emerged as a workhorse method in neuroscience to
investigate patterns of neuronal activity. Instrumentation to acquire calcium
imaging movies has rapidly progressed and has become standard across labs.
Still, algorithms to automatically detect and extract activity signals from
calcium imaging movies are highly variable from~lab~to~lab and more advanced
algorithms are continuously being developed. Here we present HNCcorr, a novel
algorithm for cell identification in calcium imaging movies based on
combinatorial optimization. The algorithm identifies cells by finding distinct
groups of highly similar pixels in correlation space, where a pixel is
represented by the vector of correlations to a set of other pixels. The HNCcorr
algorithm achieves the best known results for the cell identification benchmark
of Neurofinder, and guarantees an optimal solution to the underlying
deterministic optimization model resulting in a transparent mapping from input
data to outcome.
",0,0,1,0,0,0
1599,Intense cross-tail field-aligned currents in the plasma sheet at lunar distances,"  Field-aligned currents in the Earth's magnetotail are traditionally
associated with transient plasma flows and strong plasma pressure gradients in
the near-Earth side. In this paper we demonstrate a new field-aligned current
system present at the lunar orbit tail. Using magnetotail current sheet
observations by two ARTEMIS probes at $\sim60 R_E$, we analyze statistically
the current sheet structure and current density distribution closest to the
neutral sheet. For about half of our 130 current sheet crossings, the
equatorial magnetic field component across-the tail (along the main, cross-tail
current) contributes significantly to the vertical pressure balance. This
magnetic field component peaks at the equator, near the cross-tail current
maximum. For those cases, a significant part of the tail current, having an
intensity in the range 1-10nA/m$^2$, flows along the magnetic field lines (it
is both field-aligned and cross-tail). We suggest that this current system
develops in order to compensate the thermal pressure by particles that on its
own is insufficient to fend off the lobe magnetic pressure.
",0,1,0,0,0,0
1600,First non-icosahedral boron allotrope synthesized at high pressure and high temperature,"  Theoretical predictions of pressure-induced phase transformations often
become long-standing enigmas because of limitations of contemporary available
experimental possibilities. Hitherto the existence of a non-icosahedral boron
allotrope has been one of them. Here we report on the first non-icosahedral
boron allotrope, which we denoted as {\zeta}-B, with the orthorhombic
{\alpha}-Ga-type structure (space group Cmce) synthesized in a diamond anvil
cell at extreme high-pressure high-temperature conditions (115 GPa and 2100 K).
The structure of {\zeta}-B was solved using single-crystal synchrotron X-ray
diffraction and its compressional behavior was studied in the range of very
high pressures (115 GPa to 135 GPa). Experimental validation of theoretical
predictions reveals the degree of our up-to-date comprehension of condensed
matter and promotes further development of the solid state physics and
chemistry.
",0,1,0,0,0,0
1601,A Result of Uniqueness of Solutions of the Shigesada-Kawasaki-Teramoto Equations,"  We derive the uniqueness of weak solutions to the Shigesada-Kawasaki-Teramoto
(SKT) systems using the adjoint problem argument. Combining with [PT17] we then
derive the well-posedness for the SKT systems in space dimension $d\le 4$
",0,0,1,0,0,0
1602,Mind the Gap: A Well Log Data Analysis,"  The main task in oil and gas exploration is to gain an understanding of the
distribution and nature of rocks and fluids in the subsurface. Well logs are
records of petro-physical data acquired along a borehole, providing direct
information about what is in the subsurface. The data collected by logging
wells can have significant economic consequences, due to the costs inherent to
drilling wells, and the potential return of oil deposits. In this paper, we
describe preliminary work aimed at building a general framework for well log
prediction.
First, we perform a descriptive and exploratory analysis of the gaps in the
neutron porosity logs of more than a thousand wells in the North Sea. Then, we
generate artificial gaps in the neutron logs that reflect the statistics
collected before. Finally, we compare Artificial Neural Networks, Random
Forests, and three algorithms of Linear Regression in the prediction of missing
gaps on a well-by-well basis.
",1,0,0,1,0,0
1603,Inconsistency of Template Estimation with the Fr{??}chet mean in Quotient Space,"  We tackle the problem of template estimation when data have been randomly
transformed under an isometric group action in the presence of noise. In order
to estimate the template, one often minimizes the variance when the influence
of the transformations have been removed (computation of the Fr{??}chet mean
in quotient space). The consistency bias is defined as the distance (possibly
zero) between the orbit of the template and the orbit of one element which
minimizes the variance. In this article we establish an asymptotic behavior of
the consistency bias with respect to the noise level. This behavior is linear
with respect to the noise level. As a result the inconsistency is unavoidable
as soon as the noise is large enough. In practice, the template estimation with
a finite sample is often done with an algorithm called max-max. We show the
convergence of this algorithm to an empirical Karcher mean. Finally, our
numerical experiments show that the bias observed in practice cannot be
attributed to the small sample size or to a convergence problem but is indeed
due to the previously studied inconsistency.
",0,0,1,1,0,0
1604,All-optical switching and unidirectional plasmon launching with electron-hole plasma driven silicon nanoantennas,"  High-index dielectric nanoparticles have become a powerful platform for
modern light science, enabling various fascinating applications, especially in
nonlinear nanophotonics for which they enable special types of optical
nonlinearity, such as electron-hole plasma photoexcitation, which are not
inherent to plasmonic nanostructures. Here, we propose a novel geometry for
highly tunable all-dielectric nanoantennas, consisting of a chain of silicon
nanoparticles excited by an electric dipole source, which allows tuning their
radiation properties via electron-hole plasma photoexcitation. We show that the
slowly guided modes determining the Van Hove singularity of the nanoantenna are
very sensitive to the nanoparticle permittivity, opening up the ability to
utilize this effect for efficient all-optical modulation. We show that by
pumping several boundary nanoparticles with relatively low intensities may
cause dramatic variations in the nanoantenna radiation power patterns and
Purcell factor. We also demonstrate that ultrafast pumping of the designed
nanoantenna allows unidirectional launching of surface plasmon-polaritons, with
interesting implications for modern nonlinear nanophotonics.
",0,1,0,0,0,0
1605,Group chasing tactics: how to catch a faster prey?,"  We propose a bio-inspired, agent-based approach to describe the natural
phenomenon of group chasing in both two and three dimensions. Using a set of
local interaction rules we created a continuous-space and discrete-time model
with time delay, external noise and limited acceleration. We implemented a
unique collective chasing strategy, optimized its parameters and studied its
properties when chasing a much faster, erratic escaper. We show that collective
chasing strategies can significantly enhance the chasers' success rate. Our
realistic approach handles group chasing within closed, soft boundaries -
contrasting most of those published in the literature with periodic ones -- and
resembles several properties of pursuits observed in nature, such as the
emergent encircling or the escaper's zigzag motion.
",0,1,0,1,0,0
1606,On solving a restricted linear congruence using generalized Ramanujan sums,"  Consider the linear congruence equation $x_1+\ldots+x_k \equiv b\,(\text{mod
} n)$ for $b,n\in\mathbb{Z}$. By $(a,b)_s$, we mean the largest
$l^s\in\mathbb{N}$ which divides $a$ and $b$ simultaneously. For each $d_j|n$,
define $\mathcal{C}_{j,s} = \{1\leq x\leq n^s | (x,n^s)_s = d^s_j\}$. Bibak et
al. gave a formula using Ramanujan sums for the number of solutions of the
above congruence equation with some gcd restrictions on $x_i$. We generalize
their result with generalized gcd restrictions on $x_i$ by proving that for the
above linear congruence, the number of solutions is
$$\frac{1}{n^s}\sum\limits_{d|n}c_{d,s}(b)\prod\limits_{j=1}^{\tau(n)}\left(c_{\frac{n}{d_j},s}(\frac{n^s}{d^s})\right)^{g_j}$$
where $g_j = |\{x_1,\ldots, x_k\}\cap \mathcal{C}_{j,s}|$ for $j=1,\ldots
\tau(n)$ and $c_{d,s}$ denote the generalized ramanujan sum defined by E.
Cohen.
",0,0,1,0,0,0
1607,Magnetization spin dynamics in a (LuBi)3Fe5O12 (BLIG) epitaxial film,"  Bismuth substituted lutetium iron garnet (BLIG) films exhibit larger Faraday
rotation, and have a higher Curie temperature than yttrium iron garnet. We have
observed magnetic stripe domains and measured domain widths of 1.4 {\mu}{\mu}m
using Fourier domain polarization microscopy, Faraday rotation experiments
yield a coercive field of 5 Oe. These characterizations form the basis of
micromagnetic simulations that allow us to estimate and compare spin wave
excitations in BLIG films. We observed that these films support thermal magnons
with a precessional frequency of 7 GHz with a line width of 400 MHz. Further,
we studied the dependence of precessional frequency on the externally applied
magnetic field. Brillouin light scattering experiments and precession
frequencies predicted by simulations show similar trend with increasing field.
",0,1,0,0,0,0
1608,Probing for sparse and fast variable selection with model-based boosting,"  We present a new variable selection method based on model-based gradient
boosting and randomly permuted variables. Model-based boosting is a tool to fit
a statistical model while performing variable selection at the same time. A
drawback of the fitting lies in the need of multiple model fits on slightly
altered data (e.g. cross-validation or bootstrap) to find the optimal number of
boosting iterations and prevent overfitting. In our proposed approach, we
augment the data set with randomly permuted versions of the true variables, so
called shadow variables, and stop the step-wise fitting as soon as such a
variable would be added to the model. This allows variable selection in a
single fit of the model without requiring further parameter tuning. We show
that our probing approach can compete with state-of-the-art selection methods
like stability selection in a high-dimensional classification benchmark and
apply it on gene expression data for the estimation of riboflavin production of
Bacillus subtilis.
",0,0,0,1,0,0
1609,A surface-hopping method for semiclassical calculations of cross sections for radiative association with electronic transitions,"  A semicalssical method based on surface-hopping techniques is developed to
model the dynamics of radiative association with electronic transitions in
arbitrary polyatomic systems. It can be proven that our method is an extension
of the established semiclassical formula used in the characterization of
diatomic molecule- formation. Our model is tested for diatomic molecules. It
gives the same cross sections as the former semiclassical formula, but contrary
to the former method it allows us to follow the fate of the trajectories after
the emission of a photon. This means that we can characterize the rovibrational
states of the stabilized molecules: using semiclassial quantization we can
obtain quantum state resolved cross sections or emission spectra for the
radiative association process. The calculated semiclassical state resolved
spectra show good agreement with the result of quantum mechanical perturbation
theory. Furthermore our surface-hopping model is not only applicable for the
description of radiative association but it can be use for semiclassical
characterization of any molecular process where spontaneous emission occurs.
",0,1,0,0,0,0
1610,Holographic coherent states from random tensor networks,"  Random tensor networks provide useful models that incorporate various
important features of holographic duality. A tensor network is usually defined
for a fixed graph geometry specified by the connection of tensors. In this
paper, we generalize the random tensor network approach to allow quantum
superposition of different spatial geometries. We set up a framework in which
all possible bulk spatial geometries, characterized by weighted adjacent
matrices of all possible graphs, are mapped to the boundary Hilbert space and
form an overcomplete basis of the boundary. We name such an overcomplete basis
as holographic coherent states. A generic boundary state can be expanded on
this basis, which describes the state as a superposition of different spatial
geometries in the bulk. We discuss how to define distinct classical geometries
and small fluctuations around them. We show that small fluctuations around
classical geometries define ""code subspaces"" which are mapped to the boundary
Hilbert space isometrically with quantum error correction properties. In
addition, we also show that the overlap between different geometries is
suppressed exponentially as a function of the geometrical difference between
the two geometries. The geometrical difference is measured in an area law
fashion, which is a manifestation of the holographic nature of the states
considered.
",0,1,0,0,0,0
1611,Persistent Spread Measurement for Big Network Data Based on Register Intersection,"  Persistent spread measurement is to count the number of distinct elements
that persist in each network flow for predefined time periods. It has many
practical applications, including detecting long-term stealthy network
activities in the background of normal-user activities, such as stealthy DDoS
attack, stealthy network scan, or faked network trend, which cannot be detected
by traditional flow cardinality measurement. With big network data, one
challenge is to measure the persistent spreads of a massive number of flows
without incurring too much memory overhead as such measurement may be performed
at the line speed by network processors with fast but small on-chip memory. We
propose a highly compact Virtual Intersection HyperLogLog (VI-HLL) architecture
for this purpose. It achieves far better memory efficiency than the best prior
work of V-Bitmap, and in the meantime drastically extends the measurement
range. Theoretical analysis and extensive experiments demonstrate that VI-HLL
provides good measurement accuracy even in very tight memory space of less than
1 bit per flow.
",1,0,0,0,0,0
1612,High-dimensional Linear Regression for Dependent Observations with Application to Nowcasting,"  In the last few years, an extensive literature has been focused on the
$\ell_1$ penalized least squares (Lasso) estimators of high dimensional linear
regression when the number of covariates $p$ is considerably larger than the
sample size $n$. However, there is limited attention paid to the properties of
the estimators when the errors or/and the covariates are serially dependent. In
this study, we investigate the theoretical properties of the Lasso estimators
for linear regression with random design under serially dependent and/or
non-sub-Gaussian errors and covariates. In contrast to the traditional case in
which the errors are i.i.d and have finite exponential moments, we show that
$p$ can at most be a power of $n$ if the errors have only polynomial moments.
In addition, the rate of convergence becomes slower due to the serial
dependencies in errors and the covariates. We also consider sign consistency
for model selection via Lasso when there are serial correlations in the errors
or the covariates or both. Adopting the framework of functional dependence
measure, we provide a detailed description on how the rates of convergence and
the selection consistencies of the estimators depend on the dependence measures
and moment conditions of the errors and the covariates. Simulation results show
that Lasso regression can be substantially more powerful than the mixed
frequency data sampling regression (MIDAS) in the presence of irrelevant
variables. We apply the results obtained for the Lasso method to nowcasting
mixing frequency data in which serially correlated errors and a large number of
covariates are common. In real examples, the Lasso procedure outperforms the
MIDAS in both forecasting and nowcasting.
",0,0,1,1,0,0
1613,Dynamic control of the optical emission from GaN/InGaN nanowire quantum dots by surface acoustic waves,"  The optical emission of InGaN quantum dots embedded in GaN nanowires is
dynamically controlled by a surface acoustic wave (SAW). The emission energy of
both the exciton and biexciton lines is modulated over a 1.5 meV range at ~330
MHz. A small but systematic difference in the exciton and biexciton spectral
modulation reveals a linear change of the biexciton binding energy with the SAW
amplitude. The present results are relevant for the dynamic control of
individual single photon emitters based on nitride semiconductors.
",0,1,0,0,0,0
1614,Robust Tracking and Behavioral Modeling of Movements of Biological Collectives from Ordinary Video Recordings,"  We propose a novel computational method to extract information about
interactions among individuals with different behavioral states in a biological
collective from ordinary video recordings. Assuming that individuals are acting
as finite state machines, our method first detects discrete behavioral states
of those individuals and then constructs a model of their state transitions,
taking into account the positions and states of other individuals in the
vicinity. We have tested the proposed method through applications to two
real-world biological collectives: termites in an experimental setting and
human pedestrians in a university campus. For each application, a robust
tracking system was developed in-house, utilizing interactive human
intervention (for termite tracking) or online agent-based simulation (for
pedestrian tracking). In both cases, significant interactions were detected
between nearby individuals with different states, demonstrating the
effectiveness of the proposed method.
",1,1,0,0,0,0
1615,Understanding Membership Inferences on Well-Generalized Learning Models,"  Membership Inference Attack (MIA) determines the presence of a record in a
machine learning model's training data by querying the model. Prior work has
shown that the attack is feasible when the model is overfitted to its training
data or when the adversary controls the training algorithm. However, when the
model is not overfitted and the adversary does not control the training
algorithm, the threat is not well understood. In this paper, we report a study
that discovers overfitting to be a sufficient but not a necessary condition for
an MIA to succeed. More specifically, we demonstrate that even a
well-generalized model contains vulnerable instances subject to a new
generalized MIA (GMIA). In GMIA, we use novel techniques for selecting
vulnerable instances and detecting their subtle influences ignored by
overfitting metrics. Specifically, we successfully identify individual records
with high precision in real-world datasets by querying black-box machine
learning models. Further we show that a vulnerable record can even be
indirectly attacked by querying other related records and existing
generalization techniques are found to be less effective in protecting the
vulnerable instances. Our findings sharpen the understanding of the fundamental
cause of the problem: the unique influences the training instance may have on
the model.
",0,0,0,1,0,0
1616,Identification of Near-Infrared [Se III] and [Kr VI] Emission Lines in Planetary Nebulae,"  We identify [Se III] 1.0994 micron in the planetary nebula (PN) NGC 5315 and
[Kr VI] 1.2330 micron in three PNe, from spectra obtained with the FIRE
spectrometer on the 6.5-m Baade Telescope. Se and Kr are the two most
widely-detected neutron-capture elements in astrophysical nebulae, and can be
enriched by s-process nucleosynthesis in PN progenitor stars. The detection of
[Se III] 1.0994 micron is particularly valuable when paired with observations
of [Se IV] 2.2858 micron, as it can be used to improve the accuracy of nebular
Se abundance determinations, and allows Se ionization correction factor (ICF)
schemes to be empirically tested for the first time. We present new effective
collision strength calculations for Se^{2+} and Kr^{5+}, which we use to
compute ionic abundances. In NGC 5315, we find that the Se abundance computed
from Se^{3+}/H^+ is lower than that determined with ICFs that incorporate
Se^{2+}/H^+. We compute new Kr ICFs that take Kr^{5+}/H^+ into account, by
fitting correlations found in grids of Cloudy models between Kr ionic fractions
and those of more abundant elements, and use these to derive Kr abundances in
four PNe. Observations of [Se III] and [Kr VI] in a larger sample of PNe, with
a range of excitation levels, are needed to rigorously test the ICF
prescriptions for Se and our new Kr ICFs.
",0,1,0,0,0,0
1617,On reduction of differential inclusions and Lyapunov stability,"  In this paper, locally Lipschitz regular functions are utilized to identify
and remove infeasible directions from differential inclusions. The resulting
reduced differential inclusion is point-wise smaller (in the sense of set
containment) than the original differential inclusion. The reduced inclusion is
utilized to develop a generalized notion of a derivative in the direction(s) of
a set-valued map for locally Lipschitz candidate Lyapunov functions. The
developed generalized derivative yields less conservative statements of
Lyapunov stability results, invariance-like results, and Matrosov results for
differential inclusions. Illustrative examples are included to demonstrate the
utility of the developed stability theorems.
",1,0,0,0,0,0
1618,Deep Generative Networks For Sequence Prediction,"  This thesis investigates unsupervised time series representation learning for
sequence prediction problems, i.e. generating nice-looking input samples given
a previous history, for high dimensional input sequences by decoupling the
static input representation from the recurrent sequence representation. We
introduce three models based on Generative Stochastic Networks (GSN) for
unsupervised sequence learning and prediction. Experimental results for these
three models are presented on pixels of sequential handwritten digit (MNIST)
data, videos of low-resolution bouncing balls, and motion capture data. The
main contribution of this thesis is to provide evidence that GSNs are a viable
framework to learn useful representations of complex sequential input data, and
to suggest a new framework for deep generative models to learn complex
sequences by decoupling static input representations from dynamic time
dependency representations.
",0,0,0,1,0,0
1619,Composite Behavioral Modeling for Identity Theft Detection in Online Social Networks,"  In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.
",1,0,0,0,0,0
1620,Asymptotic properties of the set of systoles of arithmetic Riemann surfaces,"  The purpose this article is to try to understand the mysterious coincidence
between the asymptotic behavior of the volumes of the Moduli Space of closed
hyperbolic surfaces of genus $g$ with respect to the Weil-Petersson metric and
the asymptotic behavior of the number of arithmetic closed hyperbolic surfaces
of genus $g$. If the set of arithmetic surfaces is well distributed then its
image for any interesting function should be well distributed too. We
investigate the distribution of the function systole. We give several results
indicating that the systoles of arithmetic surfaces can not be concentrated,
consequently the same holds for the set of arithmetic surfaces. The proofs are
based in different techniques: combinatorics (obtaining regular graphs with any
girth from results of B. Bollobas and constructions with cages and Ramanujan
graphs), group theory (constructing finite index subgroups of surface groups
from finite index subgroups of free groups using results of G. Baumslag) and
geometric group theory (linking the geometry of graphs with the geometry of
coverings of a surface).
",0,0,1,0,0,0
1621,Nonlinear elliptic equations on Carnot groups,"  This article concerns a class of elliptic equations on Carnot groups
depending on one real positive parameter and involving a subcritical
nonlinearity (for the critical case we refer to G. Molica Bisci and D.
Repov­, Yamabe-type equations on Carnot groups, Potential Anal. 46:2
(2017), 369-383; arXiv:1705.10100 [math.AP]). As a special case of our results
we prove the existence of at least one nontrivial solution for a subelliptic
equation defined on a smooth and bounded domain $D$ of the Heisenberg group
$\mathbb{H}^n=\mathbb{C}^n\times \mathbb{R}$. The main approach is based on
variational methods.
",0,0,1,0,0,0
1622,Raptor Codes for Higher-Order Modulation Using a Multi-Edge Framework,"  In this paper, we represent Raptor codes as multi-edge type low-density
parity-check (MET-LDPC) codes, which gives a general framework to design them
for higher-order modulation using MET density evolution. We then propose an
efficient Raptor code design method for higher-order modulation, where we
design distinct degree distributions for distinct bit levels. We consider a
joint decoding scheme based on belief propagation for Raptor codes and also
derive an exact expression for the stability condition. In several examples, we
demonstrate that the higher-order modulated Raptor codes designed using the
multi-edge framework outperform previously reported higher-order modulation
codes in literature.
",1,0,1,0,0,0
1623,A Tidy Data Model for Natural Language Processing using cleanNLP,"  The package cleanNLP provides a set of fast tools for converting a textual
corpus into a set of normalized tables. The underlying natural language
processing pipeline utilizes Stanford's CoreNLP library, exposing a number of
annotation tasks for text written in English, French, German, and Spanish.
Annotators include tokenization, part of speech tagging, named entity
recognition, entity linking, sentiment analysis, dependency parsing,
coreference resolution, and information extraction.
",1,0,0,1,0,0
1624,Relaxation of the EM Algorithm via Quantum Annealing for Gaussian Mixture Models,"  We propose a modified expectation-maximization algorithm by introducing the
concept of quantum annealing, which we call the deterministic quantum annealing
expectation-maximization (DQAEM) algorithm. The expectation-maximization (EM)
algorithm is an established algorithm to compute maximum likelihood estimates
and applied to many practical applications. However, it is known that EM
heavily depends on initial values and its estimates are sometimes trapped by
local optima. To solve such a problem, quantum annealing (QA) was proposed as a
novel optimization approach motivated by quantum mechanics. By employing QA, we
then formulate DQAEM and present a theorem that supports its stability.
Finally, we demonstrate numerical simulations to confirm its efficiency.
",0,1,0,1,0,0
1625,Multiband Superconductivity in the time reversal symmetry broken superconductor Re6Zr,"  We report point contact Andreev Reflection (PCAR) measurements on a
high-quality single crystal of the non-centrosymmetric superconductor Re6Zr. We
observe that the PCAR spectra can be fitted by taking two isotropic
superconducting gaps with Delta_1 ~ 0.79 meV and Delta_2 ~ 0.22 meV
respectively, suggesting that there are at least two bands which contribute to
superconductivity. Combined with the observation of time reversal symmetry
breaking at the superconducting transition from muon spin relaxation
measurements (Phys. Rev. Lett. 112, 107002 (2014)), our results imply an
unconventional superconducting order in this compound: A multiband singlet
state that breaks time reversal symmetry or a triplet state dominated by
interband pairing.
",0,1,0,0,0,0
1626,Influence of broken-pair excitations on the exact pair wavefunction,"  Doubly occupied configuration interaction (DOCI), the exact diagonalization
of the Hamiltonian in the paired (seniority zero) sector of the Hilbert space,
is a combinatorial cost wave function that can be very efficiently approximated
by pair coupled cluster doubles (pCCD) at mean-field computational cost. As
such, it is a very interesting candidate as a starting point for building the
full configuration interaction (FCI) ground state eigenfunction belonging to
all (not just paired) seniority sectors. The true seniority zero sector of FCI
(referred to here as FCI${}_0$) includes the effect of coupling between all
seniority sectors rather than just seniority zero, and is, in principle,
different from DOCI. We here study the accuracy with which DOCI approximates
FCI${}_0$. Using a set of small Hubbard lattices, where FCI is possible, we
show that DOCI $\sim$ FCI${}_0$ under weak correlation. However, in the strong
correlation regime, the nature of the FCI${}_0$ wavefunction can change
significantly, rendering DOCI and pCCD a less than ideal starting point for
approximating FCI.
",0,1,0,0,0,0
1627,Dynamical regularities of US equities opening and closing auctions,"  We first investigate the evolution of opening and closing auctions volumes of
US equities along the years. We then report dynamical properties of pre-auction
periods: the indicative match price is strongly mean-reverting because the
imbalance is; the final auction price reacts to a single auction order
placement or cancellation in markedly different ways in the opening and closing
auctions when computed conditionally on imbalance improving or worsening
events; the indicative price reverts towards the mid price of the regular limit
order book but is not especially bound to the spread.
",0,0,0,0,0,1
1628,"Comment on ""Laser cooling of $^{173}$Yb for isotope separation and precision hyperfine spectroscopy""","  We present measurements of the hyperfine splitting in the Yb-173
$6s6p~^1P_1^{\rm o} (F^{\prime}=3/2,7/2)$ states that disagree significantly
with those measured previously by Das and Natarajan [Phys. Rev. A 76, 062505
(2007)]. We point out inconsistencies in their measurements and suggest that
their error is due to optical pumping and improper determination of the atomic
line center. Our measurements are made using an optical frequency comb. We use
an optical pumping scheme to improve the signal-to-background ratio for the
$F^{\prime}=3/2$ component.
",0,1,0,0,0,0
1629,Training GANs with Optimism,"  We address the issue of limit cycling behavior in training Generative
Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for
training Wasserstein GANs. Recent theoretical results have shown that
optimistic mirror decent (OMD) can enjoy faster regret rates in the context of
zero-sum games. WGANs is exactly a context of solving a zero-sum game with
simultaneous no-regret dynamics. Moreover, we show that optimistic mirror
decent addresses the limit cycling problem in training WGANs. We formally show
that in the case of bi-linear zero-sum games the last iterate of OMD dynamics
converges to an equilibrium, in contrast to GD dynamics which are bound to
cycle. We also portray the huge qualitative difference between GD and OMD
dynamics with toy examples, even when GD is modified with many adaptations
proposed in the recent literature, such as gradient penalty or momentum. We
apply OMD WGAN training to a bioinformatics problem of generating DNA
sequences. We observe that models trained with OMD achieve consistently smaller
KL divergence with respect to the true underlying distribution, than models
trained with GD variants. Finally, we introduce a new algorithm, Optimistic
Adam, which is an optimistic variant of Adam. We apply it to WGAN training on
CIFAR10 and observe improved performance in terms of inception score as
compared to Adam.
",1,0,0,1,0,0
1630,New concepts of inertial measurements with multi-species atom interferometry,"  In the field of cold atom inertial sensors, we present and analyze innovative
configurations for improving their measurement range and sensitivity,
especially attracting for onboard applications. These configurations rely on
multi-species atom interferometry, involving the simultaneous manipulation of
different atomic species in a unique instrument to deduce inertial
measurements. Using a dual-species atom accelerometer manipulating
simultaneously both isotopes of rubidium, we report a preliminary experimental
realization of original concepts involving the implementation of two atom
interferometers first with different interrogation times and secondly in phase
quadrature. These results open the door to a new generation of atomic sensors
relying on high performance multi-species atom interferometric measurements.
",0,1,0,0,0,0
1631,LAMN in a class of parametric models for null recurrent diffusion,"  We study statistical models for one-dimensional diffusions which are
recurrent null. A first parameter in the drift is the principal one, and
determines regular varying rates of convergence for the score and the
information process. A finite number of other parameters, of secondary
importance, introduces additional flexibility for the modelization of the
drift, and does not perturb the null recurrent behaviour. Under time-continuous
observation we obtain local asymptotic mixed normality (LAMN), state a local
asymptotic minimax bound, and specify asymptotically optimal estimators.
",0,0,1,1,0,0
1632,A recipe for topological observables of density matrices,"  Meaningful topological invariants for mixed quantum states are challenging to
identify as there is no unique way to define them, and most choices do not
directly relate to physical observables. Here, we propose a simple pragmatic
approach to construct topological invariants of mixed states while preserving a
connection to physical observables, by continuously deforming known topological
invariants for pure (ground) states. Our approach relies on expectation values
of many-body operators, with no reference to single-particle (e.g., Bloch)
wavefunctions. To illustrate it, we examine extensions to mixed states of
$U(1)$ geometric (Berry) phases and their corresponding topological invariant
(winding or Chern number). We discuss measurement schemes, and provide a
detailed construction of invariants for thermal or more general mixed states of
quantum systems with (at least) $U(1)$ charge-conservation symmetry, such as
quantum Hall insulators.
",0,1,0,0,0,0
1633,Generalization Tower Network: A Novel Deep Neural Network Architecture for Multi-Task Learning,"  Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by
incorporating deep neural networks in learning representations from the input
to RL. However, the conventional deep neural network architecture is limited in
learning representations for multi-task RL (MT-RL), as multiple tasks can refer
to different kinds of representations. In this paper, we thus propose a novel
deep neural network architecture, namely generalization tower network (GTN),
which can achieve MT-RL within a single learned model. Specifically, the
architecture of GTN is composed of both horizontal and vertical streams. In our
GTN architecture, horizontal streams are used to learn representation shared in
similar tasks. In contrast, the vertical streams are introduced to be more
suitable for handling diverse tasks, which encodes hierarchical shared
knowledge of these tasks. The effectiveness of the introduced vertical stream
is validated by experimental results. Experimental results further verify that
our GTN architecture is able to advance the state-of-the-art MT-RL, via being
tested on 51 Atari games.
",1,0,0,1,0,0
1634,On a class of infinitely differentiable functions in ${\mathbb R}^n$ admitting holomorphic extension in ${\mathbb C}^n$,"  A space $G(M, \varPhi)$ of infinitely differentiable functions in ${\mathbb
R}^n$ constructed with a help of a family
$\varPhi=\{\varphi_m\}_{m=1}^{\infty}$ of real-valued functions $\varphi_m
\in~C({\mathbb R}^n)$ and a logarithmically convex sequence $M$ of positive
numbers is considered in the article. In view of conditions on $M$ each
function of $G(M, \varPhi)$ can be extended to an entire function in ${\mathbb
C}^n$. Imposed conditions on $M$ and $\varPhi$ allow to describe the space of
such extensions.
",0,0,1,0,0,0
1635,"Spatially resolved, energy-filtered imaging of core level and valence band photoemission of highly p and n doped silicon patterns","  An accurate description of spatial variations in the energy levels of
patterned semiconductor substrates on the micron and sub-micron scale as a
function of local doping is an important technological challenge for the
microelectronics industry. Spatially resolved surface analysis by photoelectron
spectromicroscopy can provide an invaluable contribution thanks to the
relatively non-destructive, quantitative analysis. We present results on highly
doped n and p type patterns on, respectively, p and n type silicon substrates.
Using synchrotron radiation and spherical aberration-corrected energy
filtering, we have obtained a spectroscopic image series at the Si 2p core
level and across the valence band. Local band alignments are extracted,
accounting for doping, band bending and surface photovoltage.
",0,1,0,0,0,0
1636,Effects of Degree Correlations in Interdependent Security: Good or Bad?,"  We study the influence of degree correlations or network mixing in
interdependent security. We model the interdependence in security among agents
using a dependence graph and employ a population game model to capture the
interaction among many agents when they are strategic and have various security
measures they can choose to defend themselves. The overall network security is
measured by what we call the average risk exposure (ARE) from neighbors, which
is proportional to the total (expected) number of attacks in the network.
We first show that there exists a unique pure-strategy Nash equilibrium of a
population game. Then, we prove that as the agents with larger degrees in the
dependence graph see higher risks than those with smaller degrees, the overall
network security deteriorates in that the ARE experienced by agents increases
and there are more attacks in the network. Finally, using this finding, we
demonstrate that the effects of network mixing on ARE depend on the (cost)
effectiveness of security measures available to agents; if the security
measures are not effective, increasing assortativity of dependence graph
results in higher ARE. On the other hand, if the security measures are
effective at fending off the damages and losses from attacks, increasing
assortativity reduces the ARE experienced by agents.
",1,1,0,0,0,0
1637,Towards more Reliable Transfer Learning,"  Multi-source transfer learning has been proven effective when within-target
labeled data is scarce. Previous work focuses primarily on exploiting domain
similarities and assumes that source domains are richly or at least comparably
labeled. While this strong assumption is never true in practice, this paper
relaxes it and addresses challenges related to sources with diverse labeling
volume and diverse reliability. The first challenge is combining domain
similarity and source reliability by proposing a new transfer learning method
that utilizes both source-target similarities and inter-source relationships.
The second challenge involves pool-based active learning where the oracle is
only available in source domains, resulting in an integrated active transfer
learning framework that incorporates distribution matching and uncertainty
sampling. Extensive experiments on synthetic and two real-world datasets
clearly demonstrate the superiority of our proposed methods over several
baselines including state-of-the-art transfer learning methods.
",0,0,0,1,0,0
1638,Forming short-period Wolf-Rayet X-ray binaries and double black holes through stable mass transfer,"  We show that black-hole High-Mass X-ray Binaries (HMXBs) with O- or B-type
donor stars and relatively short orbital periods, of order one week to several
months may survive spiral in, to then form Wolf-Rayet (WR) X-ray binaries with
orbital periods of order a day to a few days; while in systems where the
compact star is a neutron star, HMXBs with these orbital periods never survive
spiral-in. We therefore predict that WR X-ray binaries can only harbor black
holes. The reason why black-hole HMXBs with these orbital periods may survive
spiral in is: the combination of a radiative envelope of the donor star, and a
high mass of the compact star. In this case, when the donor begins to overflow
its Roche lobe, the systems are able to spiral in slowly with stable Roche-lobe
overflow, as is shown by the system SS433. In this case the transferred mass is
ejected from the vicinity of the compact star (so-called ""isotropic
re-emission"" mass loss mode, or ""SS433-like mass loss""), leading to gradual
spiral-in. If the mass ratio of donor and black hole is $>3.5$, these systems
will go into CE evolution and are less likely to survive. If they survive, they
produce WR X-ray binaries with orbital periods of a few hours to one day.
Several of the well-known WR+O binaries in our Galaxy and the Magellanic
Clouds, with orbital periods in the range between a week and several months,
are expected to evolve into close WR-Black-Hole binaries,which may later
produce close double black holes. The galactic formation rate of double black
holes resulting from such systems is still uncertain, as it depends on several
poorly known factors in this evolutionary picture. It might possibly be as high
as $\sim 10^{-5}$ per year.
",0,1,0,0,0,0
1639,Syzygies of Cohen-Macaulay modules over one dimensional Cohen-Macaulay local rings,"  We study syzygies of (maximal) Cohen-Macaulay modules over one dimensional
Cohen-Macaulay local rings. We compare these modules to Cohen-Macaulay modules
over the endomorphism ring of the maximal ideal. After this comparison, we give
several characterizations of almost Gorenstein rings in terms of syzygies of
Cohen-Macaulay modules.
",0,0,1,0,0,0
1640,Chirality provides a direct fitness advantage and facilitates intermixing in cellular aggregates,"  Chirality in shape and motility can evolve rapidly in microbes and cancer
cells. To determine how chirality affects cell fitness, we developed a model of
chiral growth in compact aggregates such as microbial colonies and solid
tumors. Our model recapitulates previous experimental findings and shows that
mutant cells can invade by increasing their chirality or switching their
handedness. The invasion results either in a takeover or stable coexistence
between the mutant and the ancestor depending on their relative chirality. For
large chiralities, the coexistence is accompanied by strong intermixing between
the cells, while spatial segregation occurs otherwise. We show that the
competition within the aggregate is mediated by bulges in regions where the
cells with different chiralities meet. The two-way coupling between aggregate
shape and natural selection is described by the chiral Kardar-Parisi-Zhang
equation coupled to the Burgers' equation with multiplicative noise. We solve
for the key features of this theory to explain the origin of selection on
chirality. Overall, our work suggests that chirality could be an important
ecological trait that mediates competition, invasion, and spatial structure in
cellular populations.
",0,1,0,0,0,0
1641,Learning to Draw Samples with Amortized Stein Variational Gradient Descent,"  We propose a simple algorithm to train stochastic neural networks to draw
samples from given target distributions for probabilistic inference. Our method
is based on iteratively adjusting the neural network parameters so that the
output changes along a Stein variational gradient direction (Liu & Wang, 2016)
that maximally decreases the KL divergence with the target distribution. Our
method works for any target distribution specified by their unnormalized
density function, and can train any black-box architectures that are
differentiable in terms of the parameters we want to adapt. We demonstrate our
method with a number of applications, including variational autoencoder (VAE)
with expressive encoders to model complex latent space structures, and
hyper-parameter learning of MCMC samplers that allows Bayesian inference to
adaptively improve itself when seeing more data.
",0,0,0,1,0,0
1642,Preconditioned dynamic mode decomposition and mode selection algorithms for large datasets using incremental proper orthogonal decomposition,"  This note proposes a simple and general framework of dynamic mode
decomposition (DMD) and a mode selection for large datasets. The proposed
framework explicitly introduces a preconditioning step using an incremental
proper orthogonal decomposition to DMD and mode selection algorithms. By
performing the preconditioning step, the DMD and the mode selection can be
performed with low memory consumption and small computational complexity and
can be applied to large datasets. In addition, a simple mode selection
algorithm based on a greedy method is proposed. The proposed framework is
applied to the analysis of a three-dimensional flows around a circular
cylinder.
",0,1,0,0,0,0
1643,A functional perspective on emergent supersymmetry,"  We investigate the emergence of ${\cal N}=1$ supersymmetry in the long-range
behavior of three-dimensional parity-symmetric Yukawa systems. We discuss a
renormalization approach that manifestly preserves supersymmetry whenever such
symmetry is realized, and use it to prove that supersymmetry-breaking operators
are irrelevant, thus proving that such operators are suppressed in the
infrared. All our findings are illustrated with the aid of the
$\epsilon$-expansion and a functional variant of perturbation theory, but we
provide numerical estimates of critical exponents that are based on the
non-perturbative functional renormalization group.
",0,1,0,0,0,0
1644,Variants of RMSProp and Adagrad with Logarithmic Regret Bounds,"  Adaptive gradient methods have become recently very popular, in particular as
they have been shown to be useful in the training of deep neural networks. In
this paper we have analyzed RMSProp, originally proposed for the training of
deep neural networks, in the context of online convex optimization and show
$\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and
SC-RMSProp for which we show logarithmic regret bounds for strongly convex
functions. Finally, we demonstrate in the experiments that these new variants
outperform other adaptive gradient techniques or stochastic gradient descent in
the optimization of strongly convex functions as well as in training of deep
neural networks.
",1,0,0,1,0,0
1645,"Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions","  We study connections between Dykstra's algorithm for projecting onto an
intersection of convex sets, the augmented Lagrangian method of multipliers or
ADMM, and block coordinate descent. We prove that coordinate descent for a
regularized regression problem, in which the (separable) penalty functions are
seminorms, is exactly equivalent to Dykstra's algorithm applied to the dual
problem. ADMM on the dual problem is also seen to be equivalent, in the special
case of two sets, with one being a linear subspace. These connections, aside
from being interesting in their own right, suggest new ways of analyzing and
extending coordinate descent. For example, from existing convergence theory on
Dykstra's algorithm over polyhedra, we discern that coordinate descent for the
lasso problem converges at an (asymptotically) linear rate. We also develop two
parallel versions of coordinate descent, based on the Dykstra and ADMM
connections.
",0,0,1,1,0,0
1646,A Topological Perspective on Interacting Algebraic Theories,"  Techniques from higher categories and higher-dimensional rewriting are
becoming increasingly important for understanding the finer, computational
properties of higher algebraic theories that arise, among other fields, in
quantum computation. These theories have often the property of containing
simpler sub-theories, whose interaction is regulated in a limited number of
ways, which reveals a topological substrate when pictured by string diagrams.
By exploring the double nature of computads as presentations of higher
algebraic theories, and combinatorial descriptions of ""directed spaces"", we
develop a basic language of directed topology for the compositional study of
algebraic theories. We present constructions of computads, all with clear
analogues in standard topology, that capture in great generality such notions
as homomorphisms and actions, and the interactions of monoids and comonoids
that lead to the theory of Frobenius algebras and of bialgebras. After a number
of examples, we describe how a fragment of the ZX calculus can be reconstructed
in this framework.
",1,0,1,0,0,0
1647,Dynamic nested sampling: an improved algorithm for parameter estimation and evidence calculation,"  We introduce dynamic nested sampling: a generalisation of the nested sampling
algorithm in which the number of ""live points"" varies to allocate samples more
efficiently. In empirical tests the new method significantly improves
calculation accuracy compared to standard nested sampling with the same number
of samples; this increase in accuracy is equivalent to speeding up the
computation by factors of up to ~72 for parameter estimation and ~7 for
evidence calculations. We also show that the accuracy of both parameter
estimation and evidence calculations can be improved simultaneously. In
addition, unlike in standard nested sampling, more accurate results can be
obtained by continuing the calculation for longer. Popular standard nested
sampling implementations can be easily adapted to perform dynamic nested
sampling, and several dynamic nested sampling software packages are now
publicly available.
",0,1,0,1,0,0
1648,Multistage Adaptive Testing of Sparse Signals,"  Multistage design has been used in a wide range of scientific fields. By
allocating sensing resources adaptively, one can effectively eliminate null
locations and localize signals with a smaller study budget. We formulate a
decision-theoretic framework for simultaneous multi- stage adaptive testing and
study how to minimize the total number of measurements while meeting
pre-specified constraints on both the false positive rate (FPR) and missed
discovery rate (MDR). The new procedure, which effectively pools information
across individual tests using a simultaneous multistage adaptive ranking and
thresholding (SMART) approach, can achieve precise error rates control and lead
to great savings in total study costs. Numerical studies confirm the
effectiveness of SMART for FPR and MDR control and show that it achieves
substantial power gain over existing methods. The SMART procedure is
demonstrated through the analysis of high-throughput screening data and spatial
imaging data.
",0,0,0,1,0,0
1649,On the commutativity of the powerspace constructions,"  We investigate powerspace constructions on topological spaces, with a
particular focus on the category of quasi-Polish spaces. We show that the upper
and lower powerspaces commute on all quasi-Polish spaces, and show more
generally that this commutativity is equivalent to the topological property of
consonance. We then investigate powerspace constructions on the open set
lattices of quasi-Polish spaces, and provide a complete characterization of how
the upper and lower powerspaces distribute over the open set lattice
construction.
",1,0,1,0,0,0
1650,Bounds on poloidal kinetic energy in plane layer convection,"  A numerical method is presented which conveniently computes upper bounds on
heat transport and poloidal energy in plane layer convection for infinite and
finite Prandtl numbers. The bounds obtained for the heat transport coincide
with earlier results. These bounds imply upper bounds for the poloidal energy
which follow directly from the definitions of dissipation and energy. The same
constraints used for computing upper bounds on the heat transport lead to
improved bounds for the poloidal energy.
",0,1,0,0,0,0
1651,Concentration of Multilinear Functions of the Ising Model with Applications to Network Data,"  We prove near-tight concentration of measure for polynomial functions of the
Ising model under high temperature. For any degree $d$, we show that a
degree-$d$ polynomial of a $n$-spin Ising model exhibits exponential tails that
scale as $\exp(-r^{2/d})$ at radius $r=\tilde{\Omega}_d(n^{d/2})$. Our
concentration radius is optimal up to logarithmic factors for constant $d$,
improving known results by polynomial factors in the number of spins. We
demonstrate the efficacy of polynomial functions as statistics for testing the
strength of interactions in social networks in both synthetic and real world
data.
",1,0,1,1,0,0
1652,Switching Isotropic and Directional Exploration with Parameter Space Noise in Deep Reinforcement Learning,"  This paper proposes an exploration method for deep reinforcement learning
based on parameter space noise. Recent studies have experimentally shown that
parameter space noise results in better exploration than the commonly used
action space noise. Previous methods devised a way to update the diagonal
covariance matrix of a noise distribution and did not consider the direction of
the noise vector and its correlation. In addition, fast updates of the noise
distribution are required to facilitate policy learning. We propose a method
that deforms the noise distribution according to the accumulated returns and
the noises that have led to the returns. Moreover, this method switches
isotropic exploration and directional exploration in parameter space with
regard to obtained rewards. We validate our exploration strategy in the OpenAI
Gym continuous environments and modified environments with sparse rewards. The
proposed method achieves results that are competitive with a previous method at
baseline tasks. Moreover, our approach exhibits better performance in sparse
reward environments by exploration with the switching strategy.
",0,0,0,1,0,0
1653,Fast and high-accuracy measuring technique for transmittance spectrum in VIS-NIR,"  In this paper, based on the framework of traditional spectrophotometry, we
put forward a novel fast and high-accuracy technique for measuring
transmittance spectrum in VIS-NIR wave range, its key feature is that during
the measurement procedure, the output wavelength of the grating monochromator
is kept increasing continuously and at the same time, the photoelectric
detectors execute a concurrently continuous data acquisition routine. Initial
experiment result shows that the newly proposed technique could shorten the
time consumed for measuring the transmittance spectrum down to 50% that of the
conventional spectrophotometric method, a relative error of 0.070% and a
repeatability error of 0.042% are generated. Compared with the current mostly
used techniques (spectrophotometry, methods based on multi-channel spectrometer
and strategy using Fourier transform spectrometer) for obtaining transmittance
spectrum in VIS-NIR, the new strategy has at all once the following advantages,
firstly the measuring speed could be greatly quicken, fast measurement of
transmittance spectrum in VIS-NIR is therefore promising, which would find wide
application in dynamic environment, secondly high measuring accuracy
(0.1%-0.3%) is available, and finally the measuring system has high mechanical
stability because the motor of the grating monochromator is rotating
continuously during the measurement.
",0,1,0,0,0,0
1654,The solution to the initial value problem for the ultradiscrete Somos-4 and 5 equations,"  We propose a method to solve the initial value problem for the ultradiscrete
Somos-4 and Somos-5 equations by expressing terms in the equations as convex
polygons and regarding max-plus algebras as those on polygons.
",0,1,0,0,0,0
1655,Width-tuned magnetic order oscillation on zigzag edges of honeycomb nanoribbons,"  Quantum confinement and interference often generate exotic properties in
nanostructures. One recent highlight is the experimental indication of a
magnetic phase transition in zigzag-edged graphene nanoribbons at the critical
ribbon width of about 7 nm [G. Z. Magda et al., Nature \textbf{514}, 608
(2014)]. Here we show theoretically that with further increase in the ribbon
width, the magnetic correlation of the two edges can exhibit an intriguing
oscillatory behavior between antiferromagnetic and ferromagnetic, driven by
acquiring the positive coherence between the two edges to lower the free
energy. The oscillation effect is readily tunable in applied magnetic fields.
These novel properties suggest new experimental manifestation of the edge
magnetic orders in graphene nanoribbons, and enhance the hopes of graphene-like
spintronic nanodevices functioning at room temperature.
",0,1,0,0,0,0
1656,Fast and accurate approximation of the full conditional for gamma shape parameters,"  The gamma distribution arises frequently in Bayesian models, but there is not
an easy-to-use conjugate prior for the shape parameter of a gamma. This
inconvenience is usually dealt with by using either Metropolis-Hastings moves,
rejection sampling methods, or numerical integration. However, in models with a
large number of shape parameters, these existing methods are slower or more
complicated than one would like, making them burdensome in practice. It turns
out that the full conditional distribution of the gamma shape parameter is well
approximated by a gamma distribution, even for small sample sizes, when the
prior on the shape parameter is also a gamma distribution. This article
introduces a quick and easy algorithm for finding a gamma distribution that
approximates the full conditional distribution of the shape parameter. We
empirically demonstrate the speed and accuracy of the approximation across a
wide range of conditions. If exactness is required, the approximation can be
used as a proposal distribution for Metropolis-Hastings.
",0,0,0,1,0,0
1657,Two variants of the Froiduire-Pin Algorithm for finite semigroups,"  In this paper, we present two algorithms based on the Froidure-Pin Algorithm
for computing the structure of a finite semigroup from a generating set. As was
the case with the original algorithm of Froidure and Pin, the algorithms
presented here produce the left and right Cayley graphs, a confluent
terminating rewriting system, and a reduced word of the rewriting system for
every element of the semigroup.
If $U$ is any semigroup, and $A$ is a subset of $U$, then we denote by
$\langle A\rangle$ the least subsemigroup of $U$ containing $A$. If $B$ is any
other subset of $U$, then, roughly speaking, the first algorithm we present
describes how to use any information about $\langle A\rangle$, that has been
found using the Froidure-Pin Algorithm, to compute the semigroup $\langle A\cup
B\rangle$. More precisely, we describe the data structure for a finite
semigroup $S$ given by Froidure and Pin, and how to obtain such a data
structure for $\langle A\cup B\rangle$ from that for $\langle A\rangle$. The
second algorithm is a lock-free concurrent version of the Froidure-Pin
Algorithm.
",1,0,1,0,0,0
1658,Holon Wigner Crystal in a Lightly Doped Kagome Quantum Spin Liquid,"  We address the problem of a lightly doped spin-liquid through a large-scale
density-matrix renormalization group (DMRG) study of the $t$-$J$ model on a
Kagome lattice with a small but non-zero concentration, $\delta$, of doped
holes. It is now widely accepted that the undoped ($\delta=0$) spin 1/2
Heisenberg antiferromagnet has a spin-liquid groundstate. Theoretical arguments
have been presented that light doping of such a spin-liquid could give rise to
a high temperature superconductor or an exotic topological Fermi liquid metal
(FL$^\ast$). Instead, we infer that the doped holes form an insulating
charge-density wave state with one doped-hole per unit cell - i.e. a Wigner
crystal (WC). Spin correlations remain short-ranged, as in the spin-liquid
parent state, from which we infer that the state is a crystal of spinless
holons (WC$^\ast$), rather than of holes. Our results may be relevant to Kagome
lattice Herbertsmithite $\rm ZnCu_3(OH)_6Cl_2$ upon doping.
",0,1,0,0,0,0
1659,Accelerated Block Coordinate Proximal Gradients with Applications in High Dimensional Statistics,"  Nonconvex optimization problems arise in different research fields and arouse
lots of attention in signal processing, statistics and machine learning. In
this work, we explore the accelerated proximal gradient method and some of its
variants which have been shown to converge under nonconvex context recently. We
show that a novel variant proposed here, which exploits adaptive momentum and
block coordinate update with specific update rules, further improves the
performance of a broad class of nonconvex problems. In applications to sparse
linear regression with regularizations like Lasso, grouped Lasso, capped
$\ell_1$ and SCAP, the proposed scheme enjoys provable local linear
convergence, with experimental justification.
",1,0,0,1,0,0
1660,Phase retrieval using alternating minimization in a batch setting,"  This paper considers the problem of phase retrieval, where the goal is to
recover a signal $z\in C^n$ from the observations $y_i=|a_i^* z|$,
$i=1,2,\cdots,m$. While many algorithms have been proposed, the alternating
minimization algorithm has been one of the most commonly used methods, and it
is very simple to implement. Current work has proved that when the observation
vectors $\{a_i\}_{i=1}^m$ are sampled from a complex Gaussian distribution
$N(0, I)$, it recovers the underlying signal with a good initialization when
$m=O(n)$, or with random initialization when $m=O(n^2)$, and it conjectured
that random initialization succeeds with $m=O(n)$. This work proposes a
modified alternating minimization method in a batch setting, and proves that
when $m=O(n\log^{3}n)$, the proposed algorithm with random initialization
recovers the underlying signal with high probability. The proof is based on the
observation that after each iteration of alternating minimization, with high
probability, the angle between the estimated signal and the underlying signal
is reduced.
",0,0,1,1,0,0
1661,Inverse statistical problems: from the inverse Ising problem to data science,"  Inverse problems in statistical physics are motivated by the challenges of
`big data' in different fields, in particular high-throughput experiments in
biology. In inverse problems, the usual procedure of statistical physics needs
to be reversed: Instead of calculating observables on the basis of model
parameters, we seek to infer parameters of a model based on observations. In
this review, we focus on the inverse Ising problem and closely related
problems, namely how to infer the coupling strengths between spins given
observed spin correlations, magnetisations, or other data. We review
applications of the inverse Ising problem, including the reconstruction of
neural connections, protein structure determination, and the inference of gene
regulatory networks. For the inverse Ising problem in equilibrium, a number of
controlled and uncontrolled approximate solutions have been developed in the
statistical mechanics community. A particularly strong method,
pseudolikelihood, stems from statistics. We also review the inverse Ising
problem in the non-equilibrium case, where the model parameters must be
reconstructed based on non-equilibrium statistics.
",0,1,0,0,0,0
1662,Static structure of chameleon dark Matter as an explanation of dwarf spheroidal galactic core,"  We propose a novel mechanism which explains cored dark matter density profile
in recently observed dark matter rich dwarf spheroidal galaxies. In our
scenario, dark matter particle mass decreases gradually as function of distance
towards the center of a dwarf galaxy due to its interaction with a chameleon
scalar. At closer distance towards galactic center the strength of attractive
scalar fifth force becomes much stronger than gravity and is balanced by the
Fermi pressure of dark matter cloud, thus an equilibrium static configuration
of dark matter halo is obtained. Like the case of soliton star or fermion
Q-star, the stability of the dark matter halo is obtained as the scalar
achieves a static profile and reaches an asymptotic value away from the
galactic center. For simple scalar-dark matter interaction and quadratic scalar
self interaction potential, we show that dark matter behaves exactly like cold
dark matter (CDM) beyond few $\rm{kpc}$ away from galactic center but at closer
distance it becomes lighter and fermi pressure cannot be ignored anymore. Using
Thomas-Fermi approximation, we numerically solve the radial static profile of
the scalar field, fermion mass and dark matter energy density as a function of
distance. We find that for fifth force mediated by an ultra light scalar, it is
possible to obtain a flattened dark matter density profile towards galactic
center. In our scenario, the fifth force can be neglected at distance $ r \geq
1\, \rm{kpc}$ from galactic center and dark matter can be simply treated as
heavy non-relativistic particles beyond this distance, thus reproducing the
success of CDM at large scales.
",0,1,0,0,0,0
1663,Multi-Stakeholder Recommendation: Applications and Challenges,"  Recommender systems have been successfully applied to assist decision making
by producing a list of item recommendations tailored to user preferences.
Traditional recommender systems only focus on optimizing the utility of the end
users who are the receiver of the recommendations. By contrast,
multi-stakeholder recommendation attempts to generate recommendations that
satisfy the needs of both the end users and other parties or stakeholders. This
paper provides an overview and discussion about the multi-stakeholder
recommendations from the perspective of practical applications, available data
sets, corresponding research challenges and potential solutions.
",1,0,0,0,0,0
1664,Unbalancing Sets and an Almost Quadratic Lower Bound for Syntactically Multilinear Arithmetic Circuits,"  We prove a lower bound of $\Omega(n^2/\log^2 n)$ on the size of any
syntactically multilinear arithmetic circuit computing some explicit
multilinear polynomial $f(x_1, \ldots, x_n)$. Our approach expands and improves
upon a result of Raz, Shpilka and Yehudayoff ([RSY08]), who proved a lower
bound of $\Omega(n^{4/3}/\log^2 n)$ for the same polynomial. Our improvement
follows from an asymptotically optimal lower bound for a generalized version of
Galvin's problem in extremal set theory.
",1,0,0,0,0,0
1665,Saturated absorption competition microscopy,"  We introduce the concept of saturated absorption competition (SAC) microscopy
as a means of providing sub-diffraction spatial resolution in fluorescence
imaging. Unlike the post-competition process between stimulated and spontaneous
emission that is used in stimulated emission depletion (STED) microscopy, SAC
microscopy breaks the diffraction limit by emphasizing a pre-competition
process that occurs in the fluorescence absorption stage in a manner that
shares similarities with ground-state depletion (GSD) microscopy. Moreover,
unlike both STED and GSD microscopy, SAC microscopy offers a reduction in
complexity and cost by utilizing only a single continuous-wave laser diode and
an illumination intensity that is ~ 20x smaller than that used in STED. Our
approach can be physically implemented in a confocal microscope by dividing the
input laser source into a time-modulated primary excitation beam and a
doughnut-shaped saturation beam, and subsequently employing a homodyne
detection scheme to select the modulated fluorescence signal. Herein, we
provide both a physico-chemical model of SAC and experimentally demonstrate by
way of a proof-of-concept experiment a transverse spatial resolution of
~lambda/6.
",0,1,0,0,0,0
1666,Topological and non inertial effects on the interbank light absorption,"  In this work, we investigate the combined influence of the nontrivial
topology introduced by a disclination and non inertial effects due to rotation,
in the energy levels and the wave functions of a noninteracting electron gas
confined to a two-dimensional pseudoharmonic quantum dot, under the influence
of an external uniform magnetic field. The exact solutions for energy
eigenvalues and wave functions are computed as functions of the applied
magnetic field strength, the disclination topological charge, magnetic quantum
number and the rotation speed of the sample. We investigate the modifications
on the light interband absorption coefficient and absorption threshold
frequency. We observe novel features in the system, including a range of
magnetic field without corresponding absorption phenomena, which is due to a
tripartite term of the Hamiltonian, involving magnetic field, the topological
charge of the defect and the rotation frequency.
",0,1,0,0,0,0
1667,Evolution of antiferromagnetic domains in the all-in-all-out ordered pyrochlore Nd$_2$Zr$_2$O$_7$,"  We report the observation of magnetic domains in the exotic,
antiferromagnetically ordered all-in-all-out state of Nd$_2$Zr$_2$O$_7$,
induced by spin canting. The all-in-all-out state can be realized by Ising-like
spins on a pyrochlore lattice and is established in Nd$_2$Zr$_2$O$_7$ below
0.31 K for external magnetic fields up to 0.14 T. Two different spin
arrangements can fulfill this configuration which leads to the possibility of
magnetic domains. The all-in-all-out domain structure can be controlled by an
external magnetic field applied parallel to the [111] direction. This is a
result of different spin canting mechanism for the two all-in-all-out
configurations for such a direction of the magnetic field. The change of the
domain structure is observed through a hysteresis in the magnetic
susceptibility. No hysteresis occurs, however, in case the external magnetic
field is applied along [100].
",0,1,0,0,0,0
1668,Phylogenetic networks that are their own fold-ups,"  Phylogenetic networks are becoming of increasing interest to evolutionary
biologists due to their ability to capture complex non-treelike evolutionary
processes. From a combinatorial point of view, such networks are certain types
of rooted directed acyclic graphs whose leaves are labelled by, for example,
species. A number of mathematically interesting classes of phylogenetic
networks are known. These include the biologically relevant class of stable
phylogenetic networks whose members are defined via certain fold-up and un-fold
operations that link them with concepts arising within the theory of, for
example, graph fibrations. Despite this exciting link, the structural
complexity of stable phylogenetic networks is still relatively poorly
understood. Employing the popular tree-based, reticulation-visible, and
tree-child properties which allow one to gauge this complexity in one way or
another, we provide novel characterizations for when a stable phylogenetic
network satisfies either one of these three properties.
",0,0,0,0,1,0
1669,Hyperbolic inverse mean curvature flow,"  In this paper, we prove the short-time existence of hyperbolic inverse (mean)
curvature flow (with or without the specified forcing term) under the
assumption that the initial compact smooth hypersurface of $\mathbb{R}^{n+1}$
($n\geqslant2$) is mean convex and star-shaped. Several interesting examples
and some hyperbolic evolution equations for geometric quantities of the
evolving hypersurfaces have been shown. Besides, under different assumptions
for the initial velocity, we can get the expansion and the convergence results
of a hyperbolic inverse mean curvature flow in the plane $\mathbb{R}^2$, whose
evolving curves move normally.
",0,0,1,0,0,0
1670,Theoretically Principled Trade-off between Robustness and Accuracy,"  We identify a trade-off between robustness and accuracy that serves as a
guiding principle in the design of defenses against adversarial examples.
Although the problem has been widely studied empirically, much remains unknown
concerning the theory underlying this trade-off. In this work, we quantify the
trade-off in terms of the gap between the risk for adversarial examples and the
risk for non-adversarial examples. The challenge is to provide tight bounds on
this quantity in terms of a surrogate loss. We give an optimal upper bound on
this quantity in terms of classification-calibrated loss, which matches the
lower bound in the worst case. Inspired by our theoretical analysis, we also
design a new defense method, TRADES, to trade adversarial robustness off
against accuracy. Our proposed algorithm performs well experimentally in
real-world datasets. The methodology is the foundation of our entry to the
NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of
1,995 submissions in the robust model track, surpassing the runner-up approach
by $11.41\%$ in terms of mean $\ell_2$ perturbation distance.
",1,0,0,1,0,0
1671,A new charge reconstruction algorithm for the DAMPE silicon microstrip detector,"  The DArk Matter Particle Explorer (DAMPE) is one of the four satellites
within the Strategic Pioneer Research Program in Space Science of the Chinese
Academy of Science (CAS). The Silicon-Tungsten Tracker (STK), which is composed
of 768 singled-sided silicon microstrip detectors, is one of the four
subdetectors in DAMPE, providing track reconstruction and charge identification
for relativistic charged particles. The charge response of DAMPE silicon
microstrip detectors is complicated, depending on the incident angle and impact
position. A new charge reconstruction algorithm for the DAMPE silicon
microstrip detector is introduced in this paper. This algorithm can correct the
complicated charge response, and was proved applicable by the ion test beam.
",0,1,0,0,0,0
1672,The Structure Transfer Machine Theory and Applications,"  Representation learning is a fundamental but challenging problem, especially
when the distribution of data is unknown. We propose a new representation
learning method, termed Structure Transfer Machine (STM), which enables feature
learning process to converge at the representation expectation in a
probabilistic way. We theoretically show that such an expected value of the
representation (mean) is achievable if the manifold structure can be
transferred from the data space to the feature space. The resulting structure
regularization term, named manifold loss, is incorporated into the loss
function of the typical deep learning pipeline. The STM architecture is
constructed to enforce the learned deep representation to satisfy the intrinsic
manifold structure from the data, which results in robust features that suit
various application scenarios, such as digit recognition, image classification
and object tracking. Compared to state-of-the-art CNN architectures, we achieve
the better results on several commonly used benchmarks\footnote{The source code
is available. this https URL }.
",0,0,0,1,0,0
1673,Inverse Fractional Knapsack Problem with Profits and Costs Modification,"  We address in this paper the problem of modifying both profits and costs of a
fractional knapsack problem optimally such that a prespecified solution becomes
an optimal solution with prespect to new parameters. This problem is called the
inverse fractional knapsack problem. Concerning the $l_1$-norm, we first prove
that the problem is NP-hard. The problem can be however solved in quadratic
time if we only modify profit parameters. Additionally, we develop a
quadratic-time algorithm that solves the inverse fractional knapsack problem
under $l_\infty$-norm.
",1,0,1,0,0,0
1674,Large dimensional analysis of general margin based classification methods,"  Margin-based classifiers have been popular in both machine learning and
statistics for classification problems. Since a large number of classifiers are
available, one natural question is which type of classifiers should be used
given a particular classification task. We aim to answering this question by
investigating the asymptotic performance of a family of large-margin
classifiers in situations where the data dimension $p$ and the sample $n$ are
both large. This family covers a broad range of classifiers including support
vector machine, distance weighted discrimination, penalized logistic
regression, and large-margin unified machine as special cases. The asymptotic
results are described by a set of nonlinear equations and we observe a close
match of them with Monte Carlo simulation on finite data samples. Our
analytical studies shed new light on how to select the best classifier among
various classification methods as well as on how to choose the optimal tuning
parameters for a given method.
",1,0,0,1,0,0
1675,"Complete reducibility, Kulshammer's question, conjugacy classes: a D_4 example","  Let $k$ be a nonperfect separably closed field. Let $G$ be a connected
reductive algebraic group defined over $k$. We study rationality problems for
Serre's notion of complete reducibility of subgroups of $G$. In particular, we
present a new example of subgroup $H$ of $G$ of type $D_4$ in characteristic
$2$ such that $H$ is $G$-completely reducible but not $G$-completely reducible
over $k$ (or vice versa). This is new: all known such examples are for $G$ of
exceptional type. We also find a new counterexample for K?¬lshammer's question
on representations of finite groups for $G$ of type $D_4$. A problem concerning
the number of conjugacy classes is also considered. The notion of nonseparable
subgroups plays a crucial role in all our constructions.
",0,0,1,0,0,0
1676,Covariance structure associated with an equality between two general ridge estimators,"  In a general linear model, this paper derives a necessary and sufficient
condition under which two general ridge estimators coincide with each other.
The condition is given as a structure of the dispersion matrix of the error
term. Since the class of estimators considered here contains linear unbiased
estimators such as the ordinary least squares estimator and the best linear
unbiased estimator, our result can be viewed as a generalization of the
well-known theorems on the equality between these two estimators, which have
been fully studied in the literature. Two related problems are also considered:
equality between two residual sums of squares, and classification of dispersion
matrices by a perturbation approach.
",0,0,1,1,0,0
1677,Three natural subgroups of the Brauer-Picard group of a Hopf algebra with applications,"  In this article we construct three explicit natural subgroups of the
Brauer-Picard group of the category of representations of a finite-dimensional
Hopf algebra. In examples the Brauer Picard group decomposes into an ordered
product of these subgroups, somewhat similar to a Bruhat decomposition.
Our construction returns for any Hopf algebra three types of braided
autoequivalences and correspondingly three families of invertible bimodule
categories. This gives examples of so-called (2-)Morita equivalences and
defects in topological field theories. We have a closer look at the case of
quantum groups and Nichols algebras and give interesting applications. Finally,
we briefly discuss the three families of group-theoretic extensions.
",0,0,1,0,0,0
1678,UAV Visual Teach and Repeat Using Only Semantic Object Features,"  We demonstrate the use of semantic object detections as robust features for
Visual Teach and Repeat (VTR). Recent CNN-based object detectors are able to
reliably detect objects of tens or hundreds of categories in a video at frame
rates. We show that such detections are repeatable enough to use as landmarks
for VTR, without any low-level image features. Since object detections are
highly invariant to lighting and surface appearance changes, our VTR can cope
with global lighting changes and local movements of the landmark objects. In
the teaching phase, we build a series of compact scene descriptors: a list of
detected object labels and their image-plane locations. In the repeating phase,
we use Seq-SLAM-like relocalization to identify the most similar learned scene,
then use a motion control algorithm based on the funnel lane theory to navigate
the robot along the previously piloted trajectory. We evaluate the method on a
commodity UAV, examining the robustness of the algorithm to new viewpoints,
lighting conditions, and movements of landmark objects. The results suggest
that semantic object features could be useful due to their invariance to
superficial appearance changes compared to low-level image features.
",1,0,0,0,0,0
1679,Towards a Deep Reinforcement Learning Approach for Tower Line Wars,"  There have been numerous breakthroughs with reinforcement learning in the
recent years, perhaps most notably on Deep Reinforcement Learning successfully
playing and winning relatively advanced computer games. There is undoubtedly an
anticipation that Deep Reinforcement Learning will play a major role when the
first AI masters the complicated game plays needed to beat a professional
Real-Time Strategy game player. For this to be possible, there needs to be a
game environment that targets and fosters AI research, and specifically Deep
Reinforcement Learning. Some game environments already exist, however, these
are either overly simplistic such as Atari 2600 or complex such as Starcraft II
from Blizzard Entertainment. We propose a game environment in between Atari
2600 and Starcraft II, particularly targeting Deep Reinforcement Learning
algorithm research. The environment is a variant of Tower Line Wars from
Warcraft III, Blizzard Entertainment. Further, as a proof of concept that the
environment can harbor Deep Reinforcement algorithms, we propose and apply a
Deep Q-Reinforcement architecture. The architecture simplifies the state space
so that it is applicable to Q-learning, and in turn improves performance
compared to current state-of-the-art methods. Our experiments show that the
proposed architecture can learn to play the environment well, and score 33%
better than standard Deep Q-learning which in turn proves the usefulness of the
game environment.
",1,0,0,0,0,0
1680,A Case for an Atmosphere on Super-Earth 55 Cancri e,"  One of the primary questions when characterizing Earth-sized and
super-Earth-sized exoplanets is whether they have a substantial atmosphere like
Earth and Venus or a bare-rock surface like Mercury. Phase curves of the
planets in thermal emission provide clues to this question, because a
substantial atmosphere would transport heat more efficiently than a bare-rock
surface. Analyzing phase curve photometric data around secondary eclipse has
previously been used to study energy transport in the atmospheres of hot
Jupiters. Here we use phase curve, Spitzer time-series photometry to study the
thermal emission properties of the super-Earth exoplanet 55 Cancri e. We
utilize a semi-analytical framework to fit a physical model to the infrared
photometric data at 4.5 micron. The model uses parameters of planetary
properties including Bond albedo, heat redistribution efficiency (i.e., ratio
between radiative timescale and advective timescale of the atmosphere), and
atmospheric greenhouse factor. The phase curve of 55 Cancri e is dominated by
thermal emission with an eastward-shifted hot spot. We determine the heat
redistribution efficiency to be ~1.47, which implies that the advective
timescale is on the same order as the radiative timescale. This requirement
cannot be met by the bare-rock planet scenario because heat transport by
currents of molten lava would be too slow. The phase curve thus favors the
scenario with a substantial atmosphere. Our constraints on the heat
redistribution efficiency translate to an atmospheric pressure of ~1.4 bar. The
Spitzer 4.5-micron band is thus a window into the deep atmosphere of the planet
55 Cancri e.
",0,1,0,0,0,0
1681,From LiDAR to Underground Maps via 5G - Business Models Enabling a System-of-Systems Approach to Mapping the Kankberg Mine,"  With ever-increasing productivity targets in mining operations, there is a
growing interest in mining automation. The PIMM project addresses the
fundamental challenge of network communication by constructing a pilot 5G
network in the underground mine Kankberg. In this report, we discuss how such a
5G network could constitute the essential infrastructure to organize existing
systems in Kankberg into a system-of-systems (SoS). In this report, we analyze
a scenario in which LiDAR equipped vehicles operating in the mine are connected
to existing mine mapping and positioning solutions. The approach is motivated
by the approaching era of remote controlled, or even autonomous, vehicles in
mining operations. The proposed SoS could ensure continuously updated maps of
Kankberg, rendered in unprecedented detail, supporting both productivity and
safety in the underground mine. We present four different SoS solutions from an
organizational point of view, discussing how development and operations of the
constituent systems could be distributed among Boliden and external
stakeholders, e.g., the vehicle suppliers, the hauling company, and the
developers of the mapping software. The four scenarios are compared from both
technical and business perspectives, and based on trade-off discussions and
SWOT analyses. We conclude our report by recommending continued research along
two future paths, namely a closer cooperation with the vehicle suppliers, and
further feasibility studies regarding establishing a Kankberg software
ecosystem.
",1,0,0,0,0,0
1682,The number of realizations of a Laman graph,"  Laman graphs model planar frameworks that are rigid for a general choice of
distances between the vertices. There are finitely many ways, up to isometries,
to realize a Laman graph in the plane. Such realizations can be seen as
solutions of systems of quadratic equations prescribing the distances between
pairs of points. Using ideas from algebraic and tropical geometry, we provide a
recursive formula for the number of complex solutions of such systems.
",1,0,1,0,0,0
1683,UV Detector based on InAlN/GaN-on-Si HEMT Stack with Photo-to-Dark Current Ratio > 107,"  We demonstrate an InAlN/GaN-on-Si HEMT based UV detector with photo to dark
current ratio > 107. Ti/Al/Ni/Au metal stack was evaporated and rapid thermal
annealed for Ohmic contacts to the 2D electron gas (2DEG) at the InAlN/GaN
interface while the channel + barrier was recess etched to a depth of 20 nm to
pinch-off the 2DEG between Source-Drain pads. Spectral responsivity (SR) of 34
A/W at 367 nm was measured at 5 V in conjunction with very high photo to dark
current ratio of > 10^7. The photo to dark current ratio at a fixed bias was
found to be decreasing with increase in recess length of the PD. The fabricated
devices were found to exhibit a UV-to-visible rejection ratio of >103 with a
low dark current < 32 pA at 5 V. Transient measurements showed rise and fall
times in the range of 3-4 ms. The gain mechanism was investigated and carrier
lifetimes were estimated which matched well with those reported elsewhere.
",0,1,0,0,0,0
1684,Deep Reinforcement Learning for Inquiry Dialog Policies with Logical Formula Embeddings,"  This paper is the first attempt to learn the policy of an inquiry dialog
system (IDS) by using deep reinforcement learning (DRL). Most IDS frameworks
represent dialog states and dialog acts with logical formulae. In order to make
learning inquiry dialog policies more effective, we introduce a logical formula
embedding framework based on a recursive neural network. The results of
experiments to evaluate the effect of 1) the DRL and 2) the logical formula
embedding framework show that the combination of the two are as effective or
even better than existing rule-based methods for inquiry dialog policies.
",1,0,0,0,0,0
1685,Proof of Time's Arrow with Perfectly Chaotic Superdiffusion,"  The problem of Time's Arrow is rigorously solved in a certain microscopic
system associated with a Hamiltonian using only information about the
microscopic system. This microscopic system obeys an equation with time
reversal symmetry. In detail, we prove that a symplectic map with time reversal
symmetry is an Anosov diffeomorphism. This result guarantees that any initial
density function defined except for a zero volume set converges to the unique
invariant density (uniform distribution) in the sense of mixing. In addition,
we discover that there is a mathematical structure which connects Time's Arrow
(Anosov diffeomorphism) with superdiffusion in our system. In particular, the
mechanism of this superdiffusion in our system is different from those
previously found.
",0,1,0,0,0,0
1686,Using Phone Sensors and an Artificial Neural Network to Detect Gait Changes During Drinking Episodes in the Natural Environment,"  Phone sensors could be useful in assessing changes in gait that occur with
alcohol consumption. This study determined (1) feasibility of collecting
gait-related data during drinking occasions in the natural environment, and (2)
how gait-related features measured by phone sensors relate to estimated blood
alcohol concentration (eBAC). Ten young adult heavy drinkers were prompted to
complete a 5-step gait task every hour from 8pm to 12am over four consecutive
weekends. We collected 3-xis accelerometer, gyroscope, and magnetometer data
from phone sensors, and computed 24 gait-related features using a sliding
window technique. eBAC levels were calculated at each time point based on
Ecological Momentary Assessment (EMA) of alcohol use. We used an artificial
neural network model to analyze associations between sensor features and eBACs
in training (70% of the data) and validation and test (30% of the data)
datasets. We analyzed 128 data points where both eBAC and gait-related sensor
data was captured, either when not drinking (n=60), while eBAC was ascending
(n=55) or eBAC was descending (n=13). 21 data points were captured at times
when the eBAC was greater than the legal limit (0.08 mg/dl). Using a Bayesian
regularized neural network, gait-related phone sensor features showed a high
correlation with eBAC (Pearson's r > 0.9), and >95% of estimated eBAC would
fall between -0.012 and +0.012 of actual eBAC. It is feasible to collect
gait-related data from smartphone sensors during drinking occasions in the
natural environment. Sensor-based features can be used to infer gait changes
associated with elevated blood alcohol content.
",1,0,0,1,0,0
1687,A Lattice Model of Charge-Pattern-Dependent Polyampholyte Phase Separation,"  In view of recent intense experimental and theoretical interests in the
biophysics of liquid-liquid phase separation (LLPS) of intrinsically disordered
proteins (IDPs), heteropolymer models with chain molecules configured as
self-avoiding walks on the simple cubic lattice are constructed to study how
phase behaviors depend on the sequence of monomers along the chains. To address
pertinent general principles, we focus primarily on two fully charged
50-monomer sequences with significantly different charge patterns. Each monomer
in our models occupies a single lattice site and all monomers interact via a
screened pairwise Coulomb potential. Phase diagrams are obtained by extensive
Monte Carlo sampling performed at multiple temperatures on ensembles of 300
chains in boxes of sizes ranging from $52\times 52\times 52$ to $246\times
246\times 246$ to simulate a large number of different systems with the overall
polymer volume fraction $\phi$ in each system varying from $0.001$ to $0.1$.
Phase separation in the model systems is characterized by the emergence of a
large cluster connected by inter-monomer nearest-neighbor lattice contacts and
by large fluctuations in local polymer density. The simulated critical
temperatures, $T_{\rm cr}$, of phase separation for the two sequences differ
significantly, whereby the sequence with a more ""blocky"" charge pattern
exhibits a substantially higher propensity to phase separate. The trend is
consistent with our sequence-specific random-phase-approximation (RPA) polymer
theory, but the variation of the simulated $T_{\rm cr}$ with a previously
proposed ""sequence charge decoration"" pattern parameter is milder than that
predicted by RPA. Ramifications of our findings for the development of
analytical theory and simulation protocols of IDP LLPS are discussed.
",0,0,0,0,1,0
1688,"""Noiseless"" thermal noise measurement of atomic force microscopy cantilevers","  When measuring quadratic values representative of random fluctuations, such
as the thermal noise of Atomic Force Microscopy (AFM) cantilevers, the
background measurement noise cannot be averaged to zero. We present a signal
processing method that allows to get rid of this limitation using the
ubiquitous optical beam deflection sensor of standard AFMs. We demonstrate a
two orders of magnitude enhancement of the signal to noise ratio in our
experiment, allowing the calibration of stiff cantilevers or easy
identification of higher order modes from thermal noise measurements.
",0,1,0,0,0,0
1689,A Forward Model at Purkinje Cell Synapses Facilitates Cerebellar Anticipatory Control,"  How does our motor system solve the problem of anticipatory control in spite
of a wide spectrum of response dynamics from different musculo-skeletal
systems, transport delays as well as response latencies throughout the central
nervous system? To a great extent, our highly-skilled motor responses are a
result of a reactive feedback system, originating in the brain-stem and spinal
cord, combined with a feed-forward anticipatory system, that is adaptively
fine-tuned by sensory experience and originates in the cerebellum. Based on
that interaction we design the counterfactual predictive control (CFPC)
architecture, an anticipatory adaptive motor control scheme in which a
feed-forward module, based on the cerebellum, steers an error feedback
controller with counterfactual error signals. Those are signals that trigger
reactions as actual errors would, but that do not code for any current or
forthcoming errors. In order to determine the optimal learning strategy, we
derive a novel learning rule for the feed-forward module that involves an
eligibility trace and operates at the synaptic level. In particular, our
eligibility trace provides a mechanism beyond co-incidence detection in that it
convolves a history of prior synaptic inputs with error signals. In the context
of cerebellar physiology, this solution implies that Purkinje cell synapses
should generate eligibility traces using a forward model of the system being
controlled. From an engineering perspective, CFPC provides a general-purpose
anticipatory control architecture equipped with a learning rule that exploits
the full dynamics of the closed-loop system.
",1,0,1,0,0,0
1690,De Rham and twisted cohomology of Oeljeklaus-Toma manifolds,"  Oeljeklaus-Toma (OT) manifolds are complex non-K??hler manifolds whose
construction arises from specific number fields. In this note, we compute their
de Rham cohomology in terms of invariants associated to the background number
field. This is done by two distinct approaches, one using invariant cohomology
and the other one using the Leray-Serre spectral sequence. In addition, we
compute also their Morse-Novikov cohomology. As an application, we show that
the low degree Chern classes of any complex vector bundle on an OT manifold
vanish in the real cohomology. Other applications concern the OT manifolds
admitting locally conformally K??hler (LCK) metrics: we show that there is
only one possible Lee class of an LCK metric, and we determine all the possible
Morse-Novikov classes of an LCK metric, which implies the nondegeneracy of
certain Lefschetz maps in cohomology.
",0,0,1,0,0,0
1691,Episode-Based Active Learning with Bayesian Neural Networks,"  We investigate different strategies for active learning with Bayesian deep
neural networks. We focus our analysis on scenarios where new, unlabeled data
is obtained episodically, such as commonly encountered in mobile robotics
applications. An evaluation of different strategies for acquisition, updating,
and final training on the CIFAR-10 dataset shows that incremental network
updates with final training on the accumulated acquisition set are essential
for best performance, while limiting the amount of required human labeling
labor.
",1,0,0,1,0,0
1692,Origin of X-ray and gamma-ray emission from the Galactic central region,"  We study a possible connection between different non-thermal emissions from
the inner few parsecs of the Galaxy. We analyze the origin of the gamma-ray
source 2FGL J1745.6-2858 (or 3FGL J1745.6-2859c) in the Galactic Center and the
diffuse hard X-ray component recently found by NuSTAR, as well as the radio
emission and processes of hydrogen ionization from this area. We assume that a
source in the GC injected energetic particles with power-law spectrum into the
surrounding medium in the past or continues to inject until now. The energetic
particles may be protons, electrons or a combination of both. These particles
diffuse to the surrounding medium and interact with gas, magnetic field and
background photons to produce non-thermal emissions. We study the spectral and
spatial features of the hard X-ray emission and gamma-ray emission by the
particles from the central source. Our goal is to examine whether the hard
X-ray and gamma-ray emissions have a common origin. Our estimations show that
in the case of pure hadronic models the expected flux of hard X-ray emission is
too low. Despite protons can produce a non-zero contribution in gamma-ray
emission, it is unlikely that they and their secondary electrons can make a
significant contribution in hard X-ray flux. In the case of pure leptonic
models it is possible to reproduce both X-ray and gamma-ray emissions for both
transient and continuous supply models. However, in the case of continuous
supply model the ionization rate of molecular hydrogen may significantly exceed
the observed value.
",0,1,0,0,0,0
1693,Freeness and The Partial Transposes of Wishart Random Matrices,"  We show that the partial transposes of complex Wishart random matrices are
asymptotically free. We also investigate regimes where the number of blocks is
fixed but the size of the blocks increases. This gives a example where the
partial transpose produces freeness at the operator level. Finally we
investigate the case of real Wishart matrices.
",0,0,1,0,0,0
1694,Coset space construction for the conformal group. I. Unbroken phase,"  The technique for constructing conformally invariant theories within the
coset space construction is developed. It reproduces all consequences of the
conformal invariance and Lagrangians of widely-known conformal field theories.
The method of induced representations, which plays the key role in the
construction, allows to reveal a special role of the ""Nambu-Goldstone fields""
for special conformal transformations. Namely, their dependence on the
coordinates turns out to be fixed by the symmetries. This results in the
appearance of the constraints on possible forms of Lagrangians, which ensure
that discrete symmetries are indeed symmetries of the theory.
",0,0,1,0,0,0
1695,Fixed points of polarity type operators,"  A well-known result says that the Euclidean unit ball is the unique fixed
point of the polarity operator. This result implies that if, in $\mathbb{R}^n$,
the unit ball of some norm is equal to the unit ball of the dual norm, then the
norm must be Euclidean. Motivated by these results and by relatively recent
results in convex analysis and convex geometry regarding various properties of
order reversing operators, we consider, in a real Hilbert space setting, a more
general fixed point equation in which the polarity operator is composed with a
continuous invertible linear operator. We show that if the linear operator is
positive definite, then the considered equation is uniquely solvable by an
ellipsoid. Otherwise, the equation can have several (possibly infinitely many)
solutions or no solution at all. Our analysis yields a few by-products of
possible independent interest, among them results related to coercive bilinear
forms (essentially a quantitative convex analytic converse to the celebrated
Lax-Milgram theorem from partial differential equations) and a characterization
of real Hilbertian spaces.
",0,0,1,0,0,0
1696,Multiple Improvements of Multiple Imputation Likelihood Ratio Tests,"  Multiple imputation (MI) inference handles missing data by first properly
imputing the missing values $m$ times, and then combining the $m$ analysis
results from applying a complete-data procedure to each of the completed
datasets. However, the existing method for combining likelihood ratio tests has
multiple defects: (i) the combined test statistic can be negative in practice
when the reference null distribution is a standard $F$ distribution; (ii) it is
not invariant to re-parametrization; (iii) it fails to ensure monotonic power
due to its use of an inconsistent estimator of the fraction of missing
information (FMI) under the alternative hypothesis; and (iv) it requires
non-trivial access to the likelihood ratio test statistic as a function of
estimated parameters instead of datasets. This paper shows, via both
theoretical derivations and empirical investigations, that essentially all of
these problems can be straightforwardly addressed if we are willing to perform
an additional likelihood ratio test by stacking the $m$ completed datasets as
one big completed dataset. A particularly intriguing finding is that the FMI
itself can be estimated consistently by a likelihood ratio statistic for
testing whether the $m$ completed datasets produced by MI can be regarded
effectively as samples coming from a common model. Practical guidelines are
provided based on an extensive comparison of existing MI tests.
",0,0,1,1,0,0
1697,Estimating the unseen from multiple populations,"  Given samples from a distribution, how many new elements should we expect to
find if we continue sampling this distribution? This is an important and
actively studied problem, with many applications ranging from unseen species
estimation to genomics. We generalize this extrapolation and related unseen
estimation problems to the multiple population setting, where population $j$
has an unknown distribution $D_j$ from which we observe $n_j$ samples. We
derive an optimal estimator for the total number of elements we expect to find
among new samples across the populations. Surprisingly, we prove that our
estimator's accuracy is independent of the number of populations. We also
develop an efficient optimization algorithm to solve the more general problem
of estimating multi-population frequency distributions. We validate our methods
and theory through extensive experiments. Finally, on a real dataset of human
genomes across multiple ancestries, we demonstrate how our approach for unseen
estimation can enable cohort designs that can discover interesting mutations
with greater efficiency.
",1,0,0,1,0,0
1698,Continued Fractions and $q$-Series Generating Functions for the Generalized Sum-of-Divisors Functions,"  We construct new continued fraction expansions of Jacobi-type J-fractions in
$z$ whose power series expansions generate the ratio of the $q$-Pochhamer
symbols, $(a; q)_n / (b; q)_n$, for all integers $n \geq 0$ and where $a,b,q
\in \mathbb{C}$ are non-zero and defined such that $|q| < 1$ and $|b/a| < |z| <
1$. If we set the parameters $(a, b) := (q, q^2)$ in these generalized series
expansions, then we have a corresponding J-fraction enumerating the sequence of
terms $(1-q) / (1-q^{n+1})$ over all integers $n \geq 0$. Thus we are able to
define new $q$-series expansions which correspond to the Lambert series
generating the divisor function, $d(n)$, when we set $z \mapsto q$ in our new
J-fraction expansions. By repeated differentiation with respect to $z$, we also
use these generating functions to formulate new $q$-series expansions of the
generating functions for the sums-of-divisors functions, $\sigma_{\alpha}(n)$,
when $\alpha \in \mathbb{Z}^{+}$. To expand the new $q$-series generating
functions for these special arithmetic functions we define a generalized
classes of so-termed Stirling-number-like ""$q$-coefficients"", or Stirling
$q$-coefficients, whose properties, relations to elementary symmetric
polynomials, and relations to the convergents to our infinite J-fractions are
also explored within the results proved in the article.
",0,0,1,0,0,0
1699,Implications of a wavelength dependent PSF for weak lensing measurements,"  The convolution of galaxy images by the point-spread function (PSF) is the
dominant source of bias for weak gravitational lensing studies, and an accurate
estimate of the PSF is required to obtain unbiased shape measurements. The PSF
estimate for a galaxy depends on its spectral energy distribution (SED),
because the instrumental PSF is generally a function of the wavelength. In this
paper we explore various approaches to determine the resulting `effective' PSF
using broad-band data. Considering the Euclid mission as a reference, we find
that standard SED template fitting methods result in biases that depend on
source redshift, although this may be remedied if the algorithms can be
optimised for this purpose. Using a machine-learning algorithm we show that, at
least in principle, the required accuracy can be achieved with the current
survey parameters. It is also possible to account for the correlations between
photometric redshift and PSF estimates that arise from the use of the same
photometry. We explore the impact of errors in photometric calibration, errors
in the assumed wavelength dependence of the PSF model and limitations of the
adopted template libraries. Our results indicate that the required accuracy for
Euclid can be achieved using the data that are planned to determine photometric
redshifts.
",0,1,0,0,0,0
1700,Improving TSP tours using dynamic programming over tree decomposition,"  Given a traveling salesman problem (TSP) tour $H$ in graph $G$ a $k$-move is
an operation which removes $k$ edges from $H$, and adds $k$ edges of $G$ so
that a new tour $H'$ is formed. The popular $k$-OPT heuristics for TSP finds a
local optimum by starting from an arbitrary tour $H$ and then improving it by a
sequence of $k$-moves.
Until 2016, the only known algorithm to find an improving $k$-move for a
given tour was the naive solution in time $O(n^k)$. At ICALP'16 de Berg,
Buchin, Jansen and Woeginger showed an $O(n^{\lfloor 2/3k \rfloor+1})$-time
algorithm.
We show an algorithm which runs in $O(n^{(1/4+\epsilon_k)k})$ time, where
$\lim \epsilon_k = 0$. We are able to show that it improves over the state of
the art for every $k=5,\ldots,10$. For the most practically relevant case $k=5$
we provide a slightly refined algorithm running in $O(n^{3.4})$ time. We also
show that for the $k=4$ case, improving over the $O(n^3)$-time algorithm of de
Berg et al. would be a major breakthrough: an $O(n^{3-\epsilon})$-time
algorithm for any $\epsilon>0$ would imply an $O(n^{3-\delta})$-time algorithm
for the ALL PAIRS SHORTEST PATHS problem, for some $\delta>0$.
",1,0,0,0,0,0
1701,Semi-Supervised Approaches to Efficient Evaluation of Model Prediction Performance,"  In many modern machine learning applications, the outcome is expensive or
time-consuming to collect while the predictor information is easy to obtain.
Semi-supervised learning (SSL) aims at utilizing large amounts of `unlabeled'
data along with small amounts of `labeled' data to improve the efficiency of a
classical supervised approach. Though numerous SSL classification and
prediction procedures have been proposed in recent years, no methods currently
exist to evaluate the prediction performance of a working regression model. In
the context of developing phenotyping algorithms derived from electronic
medical records (EMR), we present an efficient two-step estimation procedure
for evaluating a binary classifier based on various prediction performance
measures in the semi-supervised (SS) setting. In step I, the labeled data is
used to obtain a non-parametrically calibrated estimate of the conditional risk
function. In step II, SS estimates of the prediction accuracy parameters are
constructed based on the estimated conditional risk function and the unlabeled
data. We demonstrate that under mild regularity conditions the proposed
estimators are consistent and asymptotically normal. Importantly, the
asymptotic variance of the SS estimators is always smaller than that of the
supervised counterparts under correct model specification. We also correct for
potential overfitting bias in the SS estimators in finite sample with
cross-validation and develop a perturbation resampling procedure to approximate
their distributions. Our proposals are evaluated through extensive simulation
studies and illustrated with two real EMR studies aiming to develop phenotyping
algorithms for rheumatoid arthritis and multiple sclerosis.
",0,0,0,1,0,0
1702,Linearization of the box-ball system: an elementary approach,"  Kuniba, Okado, Takagi and Yamada have found that the time-evolution of the
Takahashi-Satsuma box-ball system can be linearized by considering rigged
configurations associated with states of the box-ball system. We introduce a
simple way to understand the rigged configuration of $\mathfrak{sl}_2$-type,
and give an elementary proof of the linearization property. Our approach can be
applied to a box-ball system with finite carrier, which is related to a
discrete modified KdV equation, and also to the combinatorial $R$-matrix of
$A_1^{(1)}$-type. We also discuss combinatorial statistics and related
fermionic formulas associated with the states of the box-ball systems. A
fermionic-type formula we obtain for the finite carrier case seems to be new.
",0,1,0,0,0,0
1703,Controlling Sources of Inaccuracy in Stochastic Kriging,"  Scientists and engineers commonly use simulation models to study real systems
for which actual experimentation is costly, difficult, or impossible. Many
simulations are stochastic in the sense that repeated runs with the same input
configuration will result in different outputs. For expensive or time-consuming
simulations, stochastic kriging \citep{ankenman} is commonly used to generate
predictions for simulation model outputs subject to uncertainty due to both
function approximation and stochastic variation. Here, we develop and justify a
few guidelines for experimental design, which ensure accuracy of stochastic
kriging emulators. We decompose error in stochastic kriging predictions into
nominal, numeric, parameter estimation and parameter estimation numeric
components and provide means to control each in terms of properties of the
underlying experimental design. The design properties implied for each source
of error are weakly conflicting and broad principles are proposed. In brief,
space-filling properties ""small fill distance"" and ""large separation distance""
should balance with replication at distinct input configurations, with number
of replications depending on the relative magnitudes of stochastic and process
variability. Non-stationarity implies higher input density in more active
regions, while regression functions imply a balance with traditional design
properties. A few examples are presented to illustrate the results.
",0,0,1,1,0,0
1704,Implications of Decentralized Q-learning Resource Allocation in Wireless Networks,"  Reinforcement Learning is gaining attention by the wireless networking
community due to its potential to learn good-performing configurations only
from the observed results. In this work we propose a stateless variation of
Q-learning, which we apply to exploit spatial reuse in a wireless network. In
particular, we allow networks to modify both their transmission power and the
channel used solely based on the experienced throughput. We concentrate in a
completely decentralized scenario in which no information about neighbouring
nodes is available to the learners. Our results show that although the
algorithm is able to find the best-performing actions to enhance aggregate
throughput, there is high variability in the throughput experienced by the
individual networks. We identify the cause of this variability as the
adversarial setting of our setup, in which the most played actions provide
intermittent good/poor performance depending on the neighbouring decisions. We
also evaluate the effect of the intrinsic learning parameters of the algorithm
on this variability.
",1,0,0,0,0,0
1705,Exponential Ergodicity of the Bouncy Particle Sampler,"  Non-reversible Markov chain Monte Carlo schemes based on piecewise
deterministic Markov processes have been recently introduced in applied
probability, automatic control, physics and statistics. Although these
algorithms demonstrate experimentally good performance and are accordingly
increasingly used in a wide range of applications, geometric ergodicity results
for such schemes have only been established so far under very restrictive
assumptions. We give here verifiable conditions on the target distribution
under which the Bouncy Particle Sampler algorithm introduced in \cite{P_dW_12}
is geometrically ergodic. This holds whenever the target satisfies a curvature
condition and has tails decaying at least as fast as an exponential and at most
as fast as a Gaussian distribution. This allows us to provide a central limit
theorem for the associated ergodic averages. When the target has tails thinner
than a Gaussian distribution, we propose an original modification of this
scheme that is geometrically ergodic. For thick-tailed target distributions,
such as $t$-distributions, we extend the idea pioneered in \cite{J_G_12} in a
random walk Metropolis context. We apply a change of variable to obtain a
transformed target satisfying the tail conditions for geometric ergodicity. By
sampling the transformed target using the Bouncy Particle Sampler and mapping
back the Markov process to the original parameterization, we obtain a
geometrically ergodic algorithm.
",0,0,0,1,0,0
1706,Analysis and X-ray tomography,"  These are lecture notes for the course ""MATS4300 Analysis and X-ray
tomography"" given at the University of Jyv??skyl?? in Fall 2017. The course
is a broad overview of various tools in analysis that can be used to study
X-ray tomography. The focus is on tools and ideas, not so much on technical
details and minimal assumptions. Only very basic functional analysis is assumed
as background. Exercise problems are included.
",0,0,1,0,0,0
1707,Spatially Transformed Adversarial Examples,"  Recent studies show that widely used deep neural networks (DNNs) are
vulnerable to carefully crafted adversarial examples. Many advanced algorithms
have been proposed to generate adversarial examples by leveraging the
$\mathcal{L}_p$ distance for penalizing perturbations. Researchers have
explored different defense methods to defend against such adversarial attacks.
While the effectiveness of $\mathcal{L}_p$ distance as a metric of perceptual
quality remains an active research area, in this paper we will instead focus on
a different type of perturbation, namely spatial transformation, as opposed to
manipulating the pixel values directly as in prior works. Perturbations
generated through spatial transformation could result in large $\mathcal{L}_p$
distance measures, but our extensive experiments show that such spatially
transformed adversarial examples are perceptually realistic and more difficult
to defend against with existing defense systems. This potentially provides a
new direction in adversarial example generation and the design of corresponding
defenses. We visualize the spatial transformation based perturbation for
different examples and show that our technique can produce realistic
adversarial examples with smooth image deformation. Finally, we visualize the
attention of deep networks with different types of adversarial examples to
better understand how these examples are interpreted.
",0,0,0,1,0,0
1708,Arrow Categories of Monoidal Model Categories,"  We prove that the arrow category of a monoidal model category, equipped with
the pushout product monoidal structure and the projective model structure, is a
monoidal model category. This answers a question posed by Mark Hovey, and has
the important consequence that it allows for the consideration of a monoidal
product in cubical homotopy theory. As illustrations we include numerous
examples of non-cofibrantly generated monoidal model categories, including
chain complexes, small categories, topological spaces, and pro-categories.
",0,0,1,0,0,0
1709,Differential-operator representations of Weyl group and singular vectors,"  Given a suitable ordering of the positive root system associated with a
semisimple Lie algebra, there exists a natural correspondence between Verma
modules and related polynomial algebras. With this, the Lie algebra action on a
Verma module can be interpreted as a differential operator action on
polynomials, and thus on the corresponding truncated formal power series. We
prove that the space of truncated formal power series is a
differential-operator representation of the Weyl group $W$. We also introduce a
system of partial differential equations to investigate singular vectors in the
Verma module. It is shown that the solution space of the system in the space of
truncated formal power series is the span of $\{w(1)\ |\ w\in W\}$. Those
$w(1)$ that are polynomials correspond to singular vectors in the Verma module.
This elementary approach by partial differential equations also gives a new
proof of the well-known BGG-Verma Theorem.
",0,0,1,0,0,0
1710,Faithful Semitoric Systems,"  This paper consists of two parts. The first provides a review of the basic
properties of integrable and almost-toric systems, with a particular emphasis
on the integral affine structure associated to an integrable system. The second
part introduces faithful semitoric systems, a generalization of semitoric
systems (introduced by Vu Ngoc and classified by Pelayo and Vu Ngoc) that
provides the language to develop surgeries on almost-toric systems in dimension
4. We prove that faithful semitoric systems are natural building blocks of
almost-toric systems. Moreover, we show that they enjoy many of the properties
that their (proper) semitoric counterparts do.
",0,1,1,0,0,0
1711,HoloScope: Topology-and-Spike Aware Fraud Detection,"  As online fraudsters invest more resources, including purchasing large pools
of fake user accounts and dedicated IPs, fraudulent attacks become less obvious
and their detection becomes increasingly challenging. Existing approaches such
as average degree maximization suffer from the bias of including more nodes
than necessary, resulting in lower accuracy and increased need for manual
verification. Hence, we propose HoloScope, which uses information from graph
topology and temporal spikes to more accurately detect groups of fraudulent
users. In terms of graph topology, we introduce ""contrast suspiciousness,"" a
dynamic weighting approach, which allows us to more accurately detect
fraudulent blocks, particularly low-density blocks. In terms of temporal
spikes, HoloScope takes into account the sudden bursts and drops of fraudsters'
attacking patterns. In addition, we provide theoretical bounds for how much
this increases the time cost needed for fraudsters to conduct adversarial
attacks. Additionally, from the perspective of ratings, HoloScope incorporates
the deviation of rating scores in order to catch fraudsters more accurately.
Moreover, HoloScope has a concise framework and sub-quadratic time complexity,
making the algorithm reproducible and scalable. Extensive experiments showed
that HoloScope achieved significant accuracy improvements on synthetic and real
data, compared with state-of-the-art fraud detection methods.
",1,0,0,0,0,0
1712,On Approximation Guarantees for Greedy Low Rank Optimization,"  We provide new approximation guarantees for greedy low rank matrix estimation
under standard assumptions of restricted strong convexity and smoothness. Our
novel analysis also uncovers previously unknown connections between the low
rank estimation and combinatorial optimization, so much so that our bounds are
reminiscent of corresponding approximation bounds in submodular maximization.
Additionally, we also provide statistical recovery guarantees. Finally, we
present empirical comparison of greedy estimation with established baselines on
two important real-world problems.
",1,0,0,1,0,0
1713,Topology Estimation in Bulk Power Grids: Guarantees on Exact Recovery,"  The topology of a power grid affects its dynamic operation and settlement in
the electricity market. Real-time topology identification can enable faster
control action following an emergency scenario like failure of a line. This
article discusses a graphical model framework for topology estimation in bulk
power grids (both loopy transmission and radial distribution) using
measurements of voltage collected from the grid nodes. The graphical model for
the probability distribution of nodal voltages in linear power flow models is
shown to include additional edges along with the operational edges in the true
grid. Our proposed estimation algorithms first learn the graphical model and
subsequently extract the operational edges using either thresholding or a
neighborhood counting scheme. For grid topologies containing no three-node
cycles (two buses do not share a common neighbor), we prove that an exact
extraction of the operational topology is theoretically guaranteed. This
includes a majority of distribution grids that have radial topologies. For
grids that include cycles of length three, we provide sufficient conditions
that ensure existence of algorithms for exact reconstruction. In particular,
for grids with constant impedance per unit length and uniform injection
covariances, this observation leads to conditions on geographical placement of
the buses. The performance of algorithms is demonstrated in test case
simulations.
",1,0,1,1,0,0
1714,Wasserstein Introspective Neural Networks,"  We present Wasserstein introspective neural networks (WINN) that are both a
generator and a discriminator within a single model. WINN provides a
significant improvement over the recent introspective neural networks (INN)
method by enhancing INN's generative modeling capability. WINN has three
interesting properties: (1) A mathematical connection between the formulation
of the INN algorithm and that of Wasserstein generative adversarial networks
(WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN
results in a large enhancement to INN, achieving compelling results even with a
single classifier --- e.g., providing nearly a 20 times reduction in model size
over INN for unsupervised generative modeling. (3) When applied to supervised
classification, WINN also gives rise to improved robustness against adversarial
examples in terms of the error reduction. In the experiments, we report
encouraging results on unsupervised learning problems including texture, face,
and object modeling, as well as a supervised classification task against
adversarial attacks.
",1,0,0,0,0,0
1715,Convexity of level lines of Martin functions and applications,"  Let $\Omega$ be an unbounded domain in $\mathbb{R}\times\mathbb{R}^{d}.$ A
positive harmonic function $u$ on $\Omega$ that vanishes on the boundary of
$\Omega$ is called a Martin function. In this note, we show that, when $\Omega$
is convex, the superlevel sets of a Martin function are also convex. As a
consequence we obtain that if in addition $\Omega$ is symmetric, then the
maximum of any Martin function along a slice $\Omega\cap
(\{t\}\times\mathbb{R}^d)$ is attained at $(t,0).$
",0,0,1,0,0,0
1716,New skein invariants of links,"  We introduce new skein invariants of links based on a procedure where we
first apply the skein relation only to crossings of distinct components, so as
to produce collections of unlinked knots. We then evaluate the resulting knots
using a given invariant. A skein invariant can be computed on each link solely
by the use of skein relations and a set of initial conditions. The new
procedure, remarkably, leads to generalizations of the known skein invariants.
We make skein invariants of classical links, $H[R]$, $K[Q]$ and $D[T]$, based
on the invariants of knots, $R$, $Q$ and $T$, denoting the regular isotopy
version of the Homflypt polynomial, the Kauffman polynomial and the Dubrovnik
polynomial. We provide skein theoretic proofs of the well-definedness of these
invariants. These invariants are also reformulated into summations of the
generating invariants ($R$, $Q$, $T$) on sublinks of a given link $L$, obtained
by partitioning $L$ into collections of sublinks.
",0,0,1,0,0,0
1717,HSTREAM: A directive-based language extension for heterogeneous stream computing,"  Big data streaming applications require utilization of heterogeneous parallel
computing systems, which may comprise multiple multi-core CPUs and many-core
accelerating devices such as NVIDIA GPUs and Intel Xeon Phis. Programming such
systems require advanced knowledge of several hardware architectures and
device-specific programming models, including OpenMP and CUDA. In this paper,
we present HSTREAM, a compiler directive-based language extension to support
programming stream computing applications for heterogeneous parallel computing
systems. HSTREAM source-to-source compiler aims to increase the programming
productivity by enabling programmers to annotate the parallel regions for
heterogeneous execution and generate target specific code. The HSTREAM runtime
automatically distributes the workload across CPUs and accelerating devices. We
demonstrate the usefulness of HSTREAM language extension with various
applications from the STREAM benchmark. Experimental evaluation results show
that HSTREAM can keep the same programming simplicity as OpenMP, and the
generated code can deliver performance beyond what CPUs-only and GPUs-only
executions can deliver.
",1,0,0,0,0,0
1718,Faddeev-Jackiw approach of the noncommutative spacetime Podolsky electromagnetic theory,"  The interest in higher derivatives field theories has its origin mainly in
their influence concerning the renormalization properties of physical models
and to remove ultraviolet divergences. The noncommutative Podolsky theory is a
constrained system that cannot by directly quantized by the canonical way. In
this work we have used the Faddeev-Jackiw method in order to obtain the Dirac
brackets of the NC Podolsky theory.
",0,1,0,0,0,0
1719,Spin Transport and Accumulation in 2D Weyl Fermion System,"  In this work, we study the spin Hall effect and Rashba-Edelstein effect of a
2D Weyl fermion system in the clean limit using the Kubo formalism. Spin
transport is solely due to the spin-torque current in this strongly spin-orbit
coupled (SOC) system, and chiral spin-flip scattering off non-SOC scalar
impurities, with potential strength $V$ and size $a$, gives rise to a
skew-scattering mechanism for the spin Hall effect. The key result is that the
resultant spin-Hall angle has a fixed sign, with $\theta^{SH} \sim O
\left(\tfrac{V^2}{v_F^2/a^2} (k_F a)^4 \right)$ being a strongly-dependent
function of $k_F a$, with $k_F$ and $v_F$ being the Fermi wave-vector and Fermi
velocity respectively. This, therefore, allows for the possibility of tuning
the SHE by adjusting the Fermi energy or impurity size.
",0,1,0,0,0,0
1720,Model-Based Control Using Koopman Operators,"  This paper explores the application of Koopman operator theory to the control
of robotic systems. The operator is introduced as a method to generate
data-driven models that have utility for model-based control methods. We then
motivate the use of the Koopman operator towards augmenting model-based
control. Specifically, we illustrate how the operator can be used to obtain a
linearizable data-driven model for an unknown dynamical process that is useful
for model-based control synthesis. Simulated results show that with increasing
complexity in the choice of the basis functions, a closed-loop controller is
able to invert and stabilize a cart- and VTOL-pendulum systems. Furthermore,
the specification of the basis function are shown to be of importance when
generating a Koopman operator for specific robotic systems. Experimental
results with the Sphero SPRK robot explore the utility of the Koopman operator
in a reduced state representation setting where increased complexity in the
basis function improve open- and closed-loop controller performance in various
terrains, including sand.
",1,0,0,0,0,0
1721,Turbulence Hierarchy in a Random Fibre Laser,"  Turbulence is a challenging feature common to a wide range of complex
phenomena. Random fibre lasers are a special class of lasers in which the
feedback arises from multiple scattering in a one-dimensional disordered
cavity-less medium. Here, we report on statistical signatures of turbulence in
the distribution of intensity fluctuations in a continuous-wave-pumped
erbium-based random fibre laser, with random Bragg grating scatterers. The
distribution of intensity fluctuations in an extensive data set exhibits three
qualitatively distinct behaviours: a Gaussian regime below threshold, a mixture
of two distributions with exponentially decaying tails near the threshold, and
a mixture of distributions with stretched-exponential tails above threshold.
All distributions are well described by a hierarchical stochastic model that
incorporates Kolmogorov's theory of turbulence, which includes energy cascade
and the intermittence phenomenon. Our findings have implications for explaining
the remarkably challenging turbulent behaviour in photonics, using a random
fibre laser as the experimental platform.
",0,1,0,0,0,0
1722,Optimal Rates for Learning with Nystr??m Stochastic Gradient Methods,"  In the setting of nonparametric regression, we propose and study a
combination of stochastic gradient methods with Nystr??m subsampling, allowing
multiple passes over the data and mini-batches. Generalization error bounds for
the studied algorithm are provided. Particularly, optimal learning rates are
derived considering different possible choices of the step-size, the mini-batch
size, the number of iterations/passes, and the subsampling level. In comparison
with state-of-the-art algorithms such as the classic stochastic gradient
methods and kernel ridge regression with Nystr??m, the studied algorithm has
advantages on the computational complexity, while achieving the same optimal
learning rates. Moreover, our results indicate that using mini-batches can
reduce the total computational cost while achieving the same optimal
statistical results.
",1,0,1,1,0,0
1723,"Run Procrustes, Run! On the convergence of accelerated Procrustes Flow","  In this work, we present theoretical results on the convergence of non-convex
accelerated gradient descent in matrix factorization models. The technique is
applied to matrix sensing problems with squared loss, for the estimation of a
rank $r$ optimal solution $X^\star \in \mathbb{R}^{n \times n}$. We show that
the acceleration leads to linear convergence rate, even under non-convex
settings where the variable $X$ is represented as $U U^\top$ for $U \in
\mathbb{R}^{n \times r}$. Our result has the same dependence on the condition
number of the objective --and the optimal solution-- as that of the recent
results on non-accelerated algorithms. However, acceleration is observed in
practice, both in synthetic examples and in two real applications: neuronal
multi-unit activities recovery from single electrode recordings, and quantum
state tomography on quantum computing simulators.
",0,0,0,1,0,0
1724,A note on the bijectivity of antipode of a Hopf algebra and its applications,"  Certain sufficient homological and ring-theoretical conditions are given for
a Hopf algebra to have bijective antipode with applications to noetherian Hopf
algebras regarding their homological behaviors.
",0,0,1,0,0,0
1725,Perfect Edge Domination: Hard and Solvable Cases,"  Let $G$ be an undirected graph. An edge of $G$ dominates itself and all edges
adjacent to it. A subset $E'$ of edges of $G$ is an edge dominating set of $G$,
if every edge of the graph is dominated by some edge of $E'$. We say that $E'$
is a perfect edge dominating set of $G$, if every edge not in $E'$ is dominated
by exactly one edge of $E'$. The perfect edge dominating problem is to
determine a least cardinality perfect edge dominating set of $G$. For this
problem, we describe two NP-completeness proofs, for the classes of claw-free
graphs of degree at most 3, and for bounded degree graphs, of maximum degree at
most $d \geq 3$ and large girth. In contrast, we prove that the problem admits
an $O(n)$ time solution, for cubic claw-free graphs. In addition, we prove a
complexity dichotomy theorem for the perfect edge domination problem, based on
the results described in the paper. Finally, we describe a linear time
algorithm for finding a minimum weight perfect edge dominating set of a
$P_5$-free graph. The algorithm is robust, in the sense that, given an
arbitrary graph $G$, either it computes a minimum weight perfect edge
dominating set of $G$, or it exhibits an induced subgraph of $G$, isomorphic to
a $P_5$.
",1,0,0,0,0,0
1726,On the presentation of Hecke-Hopf algebras for non-simply-laced type,"  Hecke-Hopf algebras were defined by A. Berenstein and D. Kazhdan. We give an
explicit presentation of an Hecke-Hopf algebra when the parameter $m_{ij},$
associated to any two distinct vertices $i$ and $j$ in the presentation of a
Coxeter group, equals $4,$ $5$ or $6$. As an application, we give a proof of a
conjecture of Berenstein and Kazhdan when the Coxeter group is crystallographic
and non-simply-laced. As another application, we show that another conjecture
of Berenstein and Kazhdan holds when $m_{ij},$ associated to any two distinct
vertices $i$ and $j,$ equals $4$ and that the conjecture does not hold when
some $m_{ij}$ equals $6$ by giving a counterexample to it.
",0,0,1,0,0,0
1727,ILP-based Alleviation of Dense Meander Segments with Prioritized Shifting and Progressive Fixing in PCB Routing,"  Length-matching is an important technique to bal- ance delays of bus signals
in high-performance PCB routing. Existing routers, however, may generate very
dense meander segments. Signals propagating along these meander segments
exhibit a speedup effect due to crosstalk between the segments of the same
wire, thus leading to mismatch of arrival times even under the same physical
wire length. In this paper, we present a post-processing method to enlarge the
width and the distance of meander segments and hence distribute them more
evenly on the board so that crosstalk can be reduced. In the proposed
framework, we model the sharing of available routing areas after removing dense
meander segments from the initial routing, as well as the generation of relaxed
meander segments and their groups for wire length compensation. This model is
transformed into an ILP problem and solved for a balanced distribution of wire
patterns. In addition, we adjust the locations of long wire segments according
to wire priorities to swap free spaces toward critical wires that need much
length compensation. To reduce the problem space of the ILP model, we also
introduce a progressive fixing technique so that wire patterns are grown
gradually from the edge of the routing toward the center area. Experimental
results show that the proposed method can expand meander segments significantly
even under very tight area constraints, so that the speedup effect can be
alleviated effectively in high- performance PCB designs.
",1,0,0,0,0,0
1728,Membrane Trafficking in the Yeast Saccharomyces cerevisiae Model,"  The yeast Saccharomyces cerevisiae is one of the best characterized
eukaryotic models. The secretory pathway was the first trafficking pathway
clearly understood mainly thanks to the work done in the laboratory of Randy
Schekman in the 1980s. They have isolated yeast sec mutants unable to secrete
an extracellular enzyme and these SEC genes were identified as encoding key
effectors of the secretory machinery. For this work, the 2013 Nobel Prize in
Physiology and Medicine has been awarded to Randy Schekman; the prize is shared
with James Rothman and Thomas S{?¬}dhof. Here, we present the different
trafficking pathways of yeast S. cerevisiae. At the Golgi apparatus newly
synthesized proteins are sorted between those transported to the plasma
membrane (PM), or the external medium, via the exocytosis or secretory pathway
(SEC), and those targeted to the vacuole either through endosomes (vacuolar
protein sorting or VPS pathway) or directly (alkaline phosphatase or ALP
pathway). Plasma membrane proteins can be internalized by endocytosis (END) and
transported to endosomes where they are sorted between those targeted for
vacuolar degradation and those redirected to the Golgi (recycling or RCY
pathway). Studies in yeast S. cerevisiae allowed the identification of most of
the known effectors, protein complexes, and trafficking pathways in eukaryotic
cells, and most of them are conserved among eukaryotes.
",0,0,0,0,1,0
1729,Synthesizing Programs for Images using Reinforced Adversarial Learning,"  Advances in deep generative networks have led to impressive results in recent
years. Nevertheless, such models can often waste their capacity on the minutiae
of datasets, presumably due to weak inductive biases in their decoders. This is
where graphics engines may come in handy since they abstract away low-level
details and represent images as high-level programs. Current methods that
combine deep learning and renderers are limited by hand-crafted likelihood or
distance functions, a need for large amounts of supervision, or difficulties in
scaling their inference algorithms to richer datasets. To mitigate these
issues, we present SPIRAL, an adversarially trained agent that generates a
program which is executed by a graphics engine to interpret and sample images.
The goal of this agent is to fool a discriminator network that distinguishes
between real and rendered data, trained with a distributed reinforcement
learning setup without any supervision. A surprising finding is that using the
discriminator's output as a reward signal is the key to allow the agent to make
meaningful progress at matching the desired output rendering. To the best of
our knowledge, this is the first demonstration of an end-to-end, unsupervised
and adversarial inverse graphics agent on challenging real world (MNIST,
Omniglot, CelebA) and synthetic 3D datasets.
",0,0,0,1,0,0
1730,A Correspondence Between Random Neural Networks and Statistical Field Theory,"  A number of recent papers have provided evidence that practical design
questions about neural networks may be tackled theoretically by studying the
behavior of random networks. However, until now the tools available for
analyzing random neural networks have been relatively ad-hoc. In this work, we
show that the distribution of pre-activations in random neural networks can be
exactly mapped onto lattice models in statistical physics. We argue that
several previous investigations of stochastic networks actually studied a
particular factorial approximation to the full lattice model. For random linear
networks and random rectified linear networks we show that the corresponding
lattice models in the wide network limit may be systematically approximated by
a Gaussian distribution with covariance between the layers of the network. In
each case, the approximate distribution can be diagonalized by Fourier
transformation. We show that this approximation accurately describes the
results of numerical simulations of wide random neural networks. Finally, we
demonstrate that in each case the large scale behavior of the random networks
can be approximated by an effective field theory.
",1,1,0,1,0,0
1731,The nature of the progenitor of the M31 North-western stream: globular clusters as milestones of its orbit,"  We examine the nature, possible orbits and physical properties of the
progenitor of the North-western stellar stream (NWS) in the halo of the
Andromeda galaxy (M31). The progenitor is assumed to be an accreting dwarf
galaxy with globular clusters (GCs). It is, in general, difficult to determine
the progenitor's orbit precisely because of many necessary parameters.
Recently, Veljanoski et al. 2014 reported five GCs whose positions and radial
velocities suggest an association with the stream. We use this data to
constrain the orbital motions of the progenitor using test-particle
simulations. Our simulations split the orbit solutions into two branches
according to whether the stream ends up in the foreground or in the background
of M31. Upcoming observations that will determine the distance to the NWS will
be able to reject one of the two branches. In either case, the solutions
require that the pericentric radius of any possible orbit be over 2 kpc. We
estimate the efficiency of the tidal disruption and confirm the consistency
with the assumption for the progenitor being a dwarf galaxy. The progenitor
requires the mass $\ga 2\times10^6 M_{\sun}$ and half-light radius $\ga 30$ pc.
In addition, $N$-body simulations successfully reproduce the basic observed
features of the NWS and the GCs' line-of-sight velocities.
",0,1,0,0,0,0
1732,On codimension two flats in Fermat-type arrangements,"  In the present note we study certain arrangements of codimension $2$ flats in
projective spaces, we call them ""Fermat arrangements"". We describe algebraic
properties of their defining ideals. In particular, we show that they provide
counterexamples to an expected containment relation between ordinary and
symbolic powers of homogeneous ideals.
",0,0,1,0,0,0
1733,Invariant Causal Prediction for Sequential Data,"  We investigate the problem of inferring the causal predictors of a response
$Y$ from a set of $d$ explanatory variables $(X^1,\dots,X^d)$. Classical
ordinary least squares regression includes all predictors that reduce the
variance of $Y$. Using only the causal predictors instead leads to models that
have the advantage of remaining invariant under interventions, loosely speaking
they lead to invariance across different ""environments"" or ""heterogeneity
patterns"". More precisely, the conditional distribution of $Y$ given its causal
predictors remains invariant for all observations. Recent work exploits such a
stability to infer causal relations from data with different but known
environments. We show that even without having knowledge of the environments or
heterogeneity pattern, inferring causal relations is possible for time-ordered
(or any other type of sequentially ordered) data. In particular, this allows
detecting instantaneous causal relations in multivariate linear time series
which is usually not the case for Granger causality. Besides novel methodology,
we provide statistical confidence bounds and asymptotic detection results for
inferring causal predictors, and present an application to monetary policy in
macroeconomics.
",0,0,1,1,0,0
1734,Smoothing with Couplings of Conditional Particle Filters,"  In state space models, smoothing refers to the task of estimating a latent
stochastic process given noisy measurements related to the process. We propose
an unbiased estimator of smoothing expectations. The lack-of-bias property has
methodological benefits: independent estimators can be generated in parallel,
and confidence intervals can be constructed from the central limit theorem to
quantify the approximation error. To design unbiased estimators, we combine a
generic debiasing technique for Markov chains with a Markov chain Monte Carlo
algorithm for smoothing. The resulting procedure is widely applicable and we
show in numerical experiments that the removal of the bias comes at a
manageable increase in variance. We establish the validity of the proposed
estimators under mild assumptions. Numerical experiments are provided on toy
models, including a setting of highly-informative observations, and a realistic
Lotka-Volterra model with an intractable transition density.
",0,0,0,1,0,0
1735,Formation of High Pressure Gradients at the Free Surface of a Liquid Dielectric in a Tangential Electric Field,"  Nonlinear dynamics of the free surface of an ideal incompressible
non-conducting fluid with high dielectric constant subjected by strong
horizontal electric field is simulated on the base of the method of conformal
transformations. It is demonstrated that interaction of counter-propagating
waves leads to formation of regions with steep wave front at the fluid surface;
angles of the boundary inclination tend to {\pi}/2, and the curvature of
surface extremely increases. A significant concentration of the energy of the
system occurs at these points. From the physical point of view, the appearance
of these singularities corresponds to formation of regions at the fluid surface
where pressure exerted by electric field undergoes a discontinuity and
dynamical pressure increases almost an order of magnitude.
",0,1,0,0,0,0
1736,Subsampling for Ridge Regression via Regularized Volume Sampling,"  Given $n$ vectors $\mathbf{x}_i\in \mathbb{R}^d$, we want to fit a linear
regression model for noisy labels $y_i\in\mathbb{R}$. The ridge estimator is a
classical solution to this problem. However, when labels are expensive, we are
forced to select only a small subset of vectors $\mathbf{x}_i$ for which we
obtain the labels $y_i$. We propose a new procedure for selecting the subset of
vectors, such that the ridge estimator obtained from that subset offers strong
statistical guarantees in terms of the mean squared prediction error over the
entire dataset of $n$ labeled vectors. The number of labels needed is
proportional to the statistical dimension of the problem which is often much
smaller than $d$. Our method is an extension of a joint subsampling procedure
called volume sampling. A second major contribution is that we speed up volume
sampling so that it is essentially as efficient as leverage scores, which is
the main i.i.d. subsampling procedure for this task. Finally, we show
theoretically and experimentally that volume sampling has a clear advantage
over any i.i.d. sampling when labels are expensive.
",1,0,0,0,0,0
1737,Ab initio calculations of the concentration dependent band gap reduction in dilute nitrides,"  While being of persistent interest for the integration of lattice-matched
laser devices with silicon circuits, the electronic structure of dilute nitride
III/V-semiconductors has presented a challenge to ab initio computational
approaches. The root of this lies in the strong distortion N atoms exert on
most host materials. Here, we resolve these issues by combining density
functional theory calculations based on the meta-GGA functional presented by
Tran and Blaha (TB09) with a supercell approach for the dilute nitride Ga(NAs).
Exploring the requirements posed to supercells, we show that the distortion
field of a single N atom must be allowed to decrease so far, that it does not
overlap with its periodic images. This also prevents spurious electronic
interactions between translational symmetric atoms, allowing to compute band
gaps in very good agreement with experimentally derived reference values. These
results open up the field of dilute nitride compound semiconductors to
predictive ab initio calculations.
",0,1,0,0,0,0
1738,Outliers and related problems,"  We define outliers as a set of observations which contradicts the proposed
mathematical (statistical) model and we discuss the frequently observed types
of the outliers. Further we explore what changes in the model have to be made
in order to avoid the occurance of the outliers. We observe that some variants
of the outliers lead to classical results in probability, such as the law of
large numbers and the concept of heavy tailed distributions.
Key words: outlier; the law of large numbers; heavy tailed distributions;
model rejection.
",0,0,1,1,0,0
1739,On Quadratic Convergence of DC Proximal Newton Algorithm for Nonconvex Sparse Learning in High Dimensions,"  We propose a DC proximal Newton algorithm for solving nonconvex regularized
sparse learning problems in high dimensions. Our proposed algorithm integrates
the proximal Newton algorithm with multi-stage convex relaxation based on the
difference of convex (DC) programming, and enjoys both strong computational and
statistical guarantees. Specifically, by leveraging a sophisticated
characterization of sparse modeling structures/assumptions (i.e., local
restricted strong convexity and Hessian smoothness), we prove that within each
stage of convex relaxation, our proposed algorithm achieves (local) quadratic
convergence, and eventually obtains a sparse approximate local optimum with
optimal statistical properties after only a few convex relaxations. Numerical
experiments are provided to support our theory.
",1,0,1,1,0,0
1740,Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning,"  In reinforcement learning, agents learn by performing actions and observing
their outcomes. Sometimes, it is desirable for a human operator to
\textit{interrupt} an agent in order to prevent dangerous situations from
happening. Yet, as part of their learning process, agents may link these
interruptions, that impact their reward, to specific states and deliberately
avoid them. The situation is particularly challenging in a multi-agent context
because agents might not only learn from their own past interruptions, but also
from those of other agents. Orseau and Armstrong defined \emph{safe
interruptibility} for one learner, but their work does not naturally extend to
multi-agent systems. This paper introduces \textit{dynamic safe
interruptibility}, an alternative definition more suited to decentralized
learning problems, and studies this notion in two learning frameworks:
\textit{joint action learners} and \textit{independent learners}. We give
realistic sufficient conditions on the learning algorithm to enable dynamic
safe interruptibility in the case of joint action learners, yet show that these
conditions are not sufficient for independent learners. We show however that if
agents can detect interruptions, it is possible to prune the observations to
ensure dynamic safe interruptibility even for independent learners.
",1,0,0,1,0,0
1741,Heuristic Optimization for Automated Distribution System Planning in Network Integration Studies,"  Network integration studies try to assess the impact of future developments,
such as the increase of Renewable Energy Sources or the introduction of Smart
Grid Technologies, on large-scale network areas. Goals can be to support
strategic alignment in the regulatory framework or to adapt the network
planning principles of Distribution System Operators. This study outlines an
approach for the automated distribution system planning that can calculate
network reconfiguration, reinforcement and extension plans in a fully automated
fashion. This allows the estimation of the expected cost in massive
probabilistic simulations of large numbers of real networks and constitutes a
core component of a framework for large-scale network integration studies.
Exemplary case study results are presented that were performed in cooperation
with different major distribution system operators. The case studies cover the
estimation of expected network reinforcement costs, technical and economical
assessment of smart grid technologies and structural network optimisation.
",1,0,0,0,0,0
1742,The Sizes and Depletions of the Dust and Gas Cavities in the Transitional Disk J160421.7-213028,"  We report ALMA Cycle 2 observations of 230 GHz (1.3 mm) dust continuum
emission, and $^{12}$CO, $^{13}$CO, and C$^{18}$O J = 2-1 line emission, from
the Upper Scorpius transitional disk [PZ99] J160421.7-213028, with an angular
resolution of ~0"".25 (35 AU). Armed with these data and existing H-band
scattered light observations, we measure the size and depth of the disk's
central cavity, and the sharpness of its outer edge, in three components:
sub-$\mu$m-sized ""small"" dust traced by scattered light, millimeter-sized ""big""
dust traced by the millimeter continuum, and gas traced by line emission. Both
dust populations feature a cavity of radius $\sim$70 AU that is depleted by
factors of at least 1000 relative to the dust density just outside. The
millimeter continuum data are well explained by a cavity with a sharp edge.
Scattered light observations can be fitted with a cavity in small dust that has
either a sharp edge at 60 AU, or an edge that transitions smoothly over an
annular width of 10 AU near 60 AU. In gas, the data are consistent with a
cavity that is smaller, about 15 AU in radius, and whose surface density at 15
AU is $10^{3\pm1}$ times smaller than the surface density at 70 AU; the gas
density grades smoothly between these two radii. The CO isotopologue
observations rule out a sharp drop in gas surface density at 30 AU or a
double-drop model as found by previous modeling. Future observations are needed
to assess the nature of these gas and dust cavities, e.g., whether they are
opened by multiple as-yet-unseen planets or photoevaporation.
",0,1,0,0,0,0
1743,Demystifying AlphaGo Zero as AlphaGo GAN,"  The astonishing success of AlphaGo Zero\cite{Silver_AlphaGo} invokes a
worldwide discussion of the future of our human society with a mixed mood of
hope, anxiousness, excitement and fear. We try to dymystify AlphaGo Zero by a
qualitative analysis to indicate that AlphaGo Zero can be understood as a
specially structured GAN system which is expected to possess an inherent good
convergence property. Thus we deduct the success of AlphaGo Zero may not be a
sign of a new generation of AI.
",1,0,0,1,0,0
1744,Effects of pressure and magnetic field on the re-entrant superconductor Eu(Fe$_{0.93}$Rh$_{0.07}$)$_2$As$_2$,"  Electron-doped Eu(Fe$_{0.93}$Rh$_{0.07}$)$_2$As$_2$ has been systematically
studied by high pressure investigations of the magnetic and electrical
transport properties, in order to unravel the complex interplay of
superconductivity and magnetism. The compound reveals an exceedingly broad
re-entrant transition to the superconducting state between $T_{\rm{c,on}} =
19.8$ K and $T_{\rm{c,0}} = 5.2$ K due to a canted A-type antiferromagnetic
ordering of the Eu$^{2+}$ moments at $T_{\rm{N}} = 16.6$ K and a re-entrant
spin glass transition at $T_{\rm{SG}} = 14.1$ K. At ambient pressure evidences
for the coexistence of superconductivity and ferromagnetism could be observed,
as well as a magnetic-field-induced enhancement of the zero-resistance
temperature $T_{\rm{c,0}}$ up to $7.2$ K with small magnetic fields applied
parallel to the \textit{ab}-plane of the crystal. We attribute the
field-induced-enhancement of superconductivity to the suppression of the
ferromagnetic component of the Eu$^{2+}$ moments along the \textit{c}-axis,
which leads to a reduction of the orbital pair breaking effect. Application of
hydrostatic pressure suppresses the superconducting state around $14$ kbar
along with a linear temperature dependence of the resistivity, implying that a
non-Fermi liquid region is located at the boundary of the superconducting
phase. At intermediate pressure, an additional feature in the resistivity
curves is identified, which can be suppressed by external magnetic fields and
competes with the superconducting phase. We suggest that the effect of negative
pressure by the chemical Rh substitution in
Eu(Fe$_{0.93}$Rh$_{0.07}$)$_2$As$_2$ is partially reversed, leading to a
re-activation of the spin density wave.
",0,1,0,0,0,0
1745,Commissioning and Operation,"  Chapter 16 in High-Luminosity Large Hadron Collider (HL-LHC) : Preliminary
Design Report. The Large Hadron Collider (LHC) is one of the largest scientific
instruments ever built. Since opening up a new energy frontier for exploration
in 2010, it has gathered a global user community of about 7,000 scientists
working in fundamental particle physics and the physics of hadronic matter at
extreme temperature and density. To sustain and extend its discovery potential,
the LHC will need a major upgrade in the 2020s. This will increase its
luminosity (rate of collisions) by a factor of five beyond the original design
value and the integrated luminosity (total collisions created) by a factor ten.
The LHC is already a highly complex and exquisitely optimised machine so this
upgrade must be carefully conceived and will require about ten years to
implement. The new configuration, known as High Luminosity LHC (HL-LHC), will
rely on a number of key innovations that push accelerator technology beyond its
present limits. Among these are cutting-edge 11-12 tesla superconducting
magnets, compact superconducting cavities for beam rotation with ultra-precise
phase control, new technology and physical processes for beam collimation and
300 metre-long high-power superconducting links with negligible energy
dissipation. The present document describes the technologies and components
that will be used to realise the project and is intended to serve as the basis
for the detailed engineering design of HL-LHC.
",0,1,0,0,0,0
1746,Only in the standard representation the Dirac theory is a quantum theory of a single fermion,"  It is shown that the relativistic quantum mechanics of a single fermion can
be developed only on the basis of the standard representation of the Dirac
bispinor. As in the nonrelativistic quantum mechanics, the arbitrariness in
defining the bispinor, as a four-component wave function, is restricted by its
multiplication by an arbitrary phase factor. We reveal the role of the large
and small components of the bispinor, establish their link in the
nonrelativistic limit with the Pauli spinor, as well as explain the role of
states with negative energies. The Klein tunneling is treated here as a
physical phenomenon analogous to the propagation of the electromagnetic wave in
a medium with negative dielectric permittivity and permeability. For the case
of localized stationary states we define the effective one-particle operators
which act in the space of the large component but contain the contributions of
both components. The effective operator of energy is presented in a compact
analytical form.
",0,1,0,0,0,0
1747,Stable absorbing boundary conditions for molecular dynamics in general domains,"  A new type of absorbing boundary conditions for molecular dynamics
simulations are presented. The exact boundary conditions for crystalline solids
with harmonic approximation are expressed as a dynamic Dirichlet- to-Neumann
(DtN) map. It connects the displacement of the atoms at the boundary to the
traction on these atoms. The DtN map is valid for a domain with general
geometry. To avoid evaluating the time convo- lution of the dynamic DtN map, we
approximate the associated kernel function by rational functions in the Laplace
domain. The parameters in the approximations are determined by interpolations.
The explicit forms of the zeroth, first, and second order approximations will
be presented. The stability of the molecular dynamics model, supplemented with
these absorbing boundary conditions is established. Two numerical simulations
are performed to demonstrate the effectiveness of the methods.
",0,1,0,0,0,0
1748,Algebraic operads up to homotopy,"  This paper deals with the homotopy theory of differential graded operads. We
endow the Koszul dual category of curved conilpotent cooperads, where the
notion of quasi-isomorphism barely makes sense, with a model category structure
Quillen equivalent to that of operads. This allows us to describe the homotopy
properties of differential graded operads in a simpler and richer way, using
obstruction methods.
",0,0,1,0,0,0
1749,Nonlinear Kalman Filtering with Divergence Minimization,"  We consider the nonlinear Kalman filtering problem using Kullback-Leibler
(KL) and $\alpha$-divergence measures as optimization criteria. Unlike linear
Kalman filters, nonlinear Kalman filters do not have closed form Gaussian
posteriors because of a lack of conjugacy due to the nonlinearity in the
likelihood. In this paper we propose novel algorithms to optimize the forward
and reverse forms of the KL divergence, as well as the alpha-divergence which
contains these two as limiting cases. Unlike previous approaches, our
algorithms do not make approximations to the divergences being optimized, but
use Monte Carlo integration techniques to derive unbiased algorithms for direct
optimization. We assess performance on radar and sensor tracking, and options
pricing problems, showing general improvement over the UKF and EKF, as well as
competitive performance with particle filtering.
",0,0,1,1,0,0
1750,On Chern number inequality in dimension 3,"  We prove that if $X---> X^+$ is a threefold terminal flip, then
$c_1(X).c_2(X)\leq c_1(X^+).c_2(X^+)$ where $c_1(X)$ and $c_2(X)$ denote the
Chern classes. This gives the affirmative answer to a Question by Xie
\cite{Xie2}. We obtain the similar but weaker result in the case of divisorial
contraction to curves.
",0,0,1,0,0,0
1751,Enhancing SDO/HMI images using deep learning,"  The Helioseismic and Magnetic Imager (HMI) provides continuum images and
magnetograms with a cadence better than one per minute. It has been
continuously observing the Sun 24 hours a day for the past 7 years. The obvious
trade-off between full disk observations and spatial resolution makes HMI not
enough to analyze the smallest-scale events in the solar atmosphere. Our aim is
to develop a new method to enhance HMI data, simultaneously deconvolving and
super-resolving images and magnetograms. The resulting images will mimic
observations with a diffraction-limited telescope twice the diameter of HMI.
Our method, which we call Enhance, is based on two deep fully convolutional
neural networks that input patches of HMI observations and output deconvolved
and super-resolved data. The neural networks are trained on synthetic data
obtained from simulations of the emergence of solar active regions. We have
obtained deconvolved and supper-resolved HMI images. To solve this ill-defined
problem with infinite solutions we have used a neural network approach to add
prior information from the simulations. We test Enhance against Hinode data
that has been degraded to a 28 cm diameter telescope showing very good
consistency. The code is open source.
",1,1,0,0,0,0
1752,Suppression of the superconductivity in ultrathin amorphous Mo$_{78}$Ge$_{22}$ thin films observed by STM,"  In contact with a superconductor, a normal metal modifies its properties due
to Andreev reflection. In the current work, the local density of states (LDOS)
of superconductor - normal metal Mo$_{78}$Ge$_{22}$ - Au bilayers are studied
by means of STM applied from the Au side. Three bilayers have been prepared on
silicate glass substrate consisting of 100, 10 and 5 nm MoGe thin films covered
always by 5 nm Au layer. The tunneling spectra were measured at temperatures
from 0.5 K to 7 K. The two-dimensional cross-correlation between topography and
normalized zero-bias conductance (ZBC) indicates a proximity effect between 100
and 10 nm MoGe thin films and Au layer where a superconducting gap slightly
smaller than that of bulk MoGe is observed. The effect of the thinnest 5 nm
MoGe layer on Au leads to much smaller gap moreover the LDOS reveals almost
completely suppressed coherence peaks. This is attributed to a strong
pair-breaking effect of spin-flip processes at the interface between MoGe films
and the substrate.
",0,1,0,0,0,0
1753,Functional data analysis in the Banach space of continuous functions,"  Functional data analysis is typically conducted within the $L^2$-Hilbert
space framework. There is by now a fully developed statistical toolbox allowing
for the principled application of the functional data machinery to real-world
problems, often based on dimension reduction techniques such as functional
principal component analysis. At the same time, there have recently been a
number of publications that sidestep dimension reduction steps and focus on a
fully functional $L^2$-methodology. This paper goes one step further and
develops data analysis methodology for functional time series in the space of
all continuous functions. The work is motivated by the fact that objects with
rather different shapes may still have a small $L^2$-distance and are therefore
identified as similar when using an $L^2$-metric. However, in applications it
is often desirable to use metrics reflecting the visualization of the curves in
the statistical analysis. The methodological contributions are focused on
developing two-sample and change-point tests as well as confidence bands, as
these procedures appear do be conducive to the proposed setting. Particular
interest is put on relevant differences; that is, on not trying to test for
exact equality, but rather for pre-specified deviations under the null
hypothesis.
The procedures are justified through large-sample theory. To ensure
practicability, non-standard bootstrap procedures are developed and
investigated addressing particular features that arise in the problem of
testing relevant hypotheses. The finite sample properties are explored through
a simulation study and an application to annual temperature profiles.
",0,0,1,1,0,0
1754,Bayesian Recurrent Neural Networks,"  In this work we explore a straightforward variational Bayes scheme for
Recurrent Neural Networks. Firstly, we show that a simple adaptation of
truncated backpropagation through time can yield good quality uncertainty
estimates and superior regularisation at only a small extra computational cost
during training, also reducing the amount of parameters by 80\%. Secondly, we
demonstrate how a novel kind of posterior approximation yields further
improvements to the performance of Bayesian RNNs. We incorporate local gradient
information into the approximate posterior to sharpen it around the current
batch statistics. We show how this technique is not exclusive to recurrent
neural networks and can be applied more widely to train Bayesian neural
networks. We also empirically demonstrate how Bayesian RNNs are superior to
traditional RNNs on a language modelling benchmark and an image captioning
task, as well as showing how each of these methods improve our model over a
variety of other schemes for training them. We also introduce a new benchmark
for studying uncertainty for language models so future methods can be easily
compared.
",1,0,0,1,0,0
1755,Cross-Correlation Redshift Calibration Without Spectroscopic Calibration Samples in DES Science Verification Data,"  Galaxy cross-correlations with high-fidelity redshift samples hold the
potential to precisely calibrate systematic photometric redshift uncertainties
arising from the unavailability of complete and representative training and
validation samples of galaxies. However, application of this technique in the
Dark Energy Survey (DES) is hampered by the relatively low number density,
small area, and modest redshift overlap between photometric and spectroscopic
samples. We propose instead using photometric catalogs with reliable
photometric redshifts for photo-z calibration via cross-correlations. We verify
the viability of our proposal using redMaPPer clusters from the Sloan Digital
Sky Survey (SDSS) to successfully recover the redshift distribution of SDSS
spectroscopic galaxies. We demonstrate how to combine photo-z with
cross-correlation data to calibrate photometric redshift biases while
marginalizing over possible clustering bias evolution in either the calibration
or unknown photometric samples. We apply our method to DES Science Verification
(DES SV) data in order to constrain the photometric redshift distribution of a
galaxy sample selected for weak lensing studies, constraining the mean of the
tomographic redshift distributions to a statistical uncertainty of $\Delta z
\sim \pm 0.01$. We forecast that our proposal can in principle control
photometric redshift uncertainties in DES weak lensing experiments at a level
near the intrinsic statistical noise of the experiment over the range of
redshifts where redMaPPer clusters are available. Our results provide strong
motivation to launch a program to fully characterize the systematic errors from
bias evolution and photo-z shapes in our calibration procedure.
",0,1,0,0,0,0
1756,Completely bounded bimodule maps and spectral synthesis,"  We initiate the study of the completely bounded multipliers of the Haagerup
tensor product $A(G)\otimes_{\rm h} A(G)$ of two copies of the Fourier algebra
$A(G)$ of a locally compact group $G$. If $E$ is a closed subset of $G$ we let
$E^{\sharp} = \{(s,t) : st\in E\}$ and show that if $E^{\sharp}$ is a set of
spectral synthesis for $A(G)\otimes_{\rm h} A(G)$ then $E$ is a set of local
spectral synthesis for $A(G)$. Conversely, we prove that if $E$ is a set of
spectral synthesis for $A(G)$ and $G$ is a Moore group then $E^{\sharp}$ is a
set of spectral synthesis for $A(G)\otimes_{\rm h} A(G)$. Using the natural
identification of the space of all completely bounded weak* continuous
$VN(G)'$-bimodule maps with the dual of $A(G)\otimes_{\rm h} A(G)$, we show
that, in the case $G$ is weakly amenable, such a map leaves the multiplication
algebra of $L^{\infty}(G)$ invariant if and only if its support is contained in
the antidiagonal of $G$.
",0,0,1,0,0,0
1757,Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation,"  Black-box risk scoring models permeate our lives, yet are typically
proprietary or opaque. We propose Distill-and-Compare, a model distillation and
comparison approach to audit such models. To gain insight into black-box
models, we treat them as teachers, training transparent student models to mimic
the risk scores assigned by black-box models. We compare the student model
trained with distillation to a second un-distilled transparent model trained on
ground-truth outcomes, and use differences between the two models to gain
insight into the black-box model. Our approach can be applied in a realistic
setting, without probing the black-box model API. We demonstrate the approach
on four public data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending
Club. We also propose a statistical test to determine if a data set is missing
key features used to train the black-box model. Our test finds that the
ProPublica data is likely missing key feature(s) used in COMPAS.
",1,0,0,1,0,0
1758,An influence-based fast preceding questionnaire model for elderly assessments,"  To improve the efficiency of elderly assessments, an influence-based fast
preceding questionnaire model (FPQM) is proposed. Compared with traditional
assessments, the FPQM optimizes questionnaires by reordering their attributes.
The values of low-ranking attributes can be predicted by the values of the
high-ranking attributes. Therefore, the number of attributes can be reduced
without redesigning the questionnaires. A new function for calculating the
influence of the attributes is proposed based on probability theory. Reordering
and reducing algorithms are given based on the attributes' influences. The
model is verified through a practical application. The practice in an
elderly-care company shows that the FPQM can reduce the number of attributes by
90.56% with a prediction accuracy of 98.39%. Compared with other methods, such
as the Expert Knowledge, Rough Set and C4.5 methods, the FPQM achieves the best
performance. In addition, the FPQM can also be applied to other questionnaires.
",1,0,0,0,0,0
1759,A GPU Accelerated Discontinuous Galerkin Incompressible Flow Solver,"  We present a GPU-accelerated version of a high-order discontinuous Galerkin
discretization of the unsteady incompressible Navier-Stokes equations. The
equations are discretized in time using a semi-implicit scheme with explicit
treatment of the nonlinear term and implicit treatment of the split Stokes
operators. The pressure system is solved with a conjugate gradient method
together with a fully GPU-accelerated multigrid preconditioner which is
designed to minimize memory requirements and to increase overall performance. A
semi-Lagrangian subcycling advection algorithm is used to shift the
computational load per timestep away from the pressure Poisson solve by
allowing larger timestep sizes in exchange for an increased number of advection
steps. Numerical results confirm we achieve the design order accuracy in time
and space. We optimize the performance of the most time-consuming kernels by
tuning the fine-grain parallelism, memory utilization, and maximizing
bandwidth. To assess overall performance we present an empirically calibrated
roofline performance model for a target GPU to explain the achieved efficiency.
We demonstrate that, in the most cases, the kernels used in the solver are
close to their empirically predicted roofline performance.
",1,0,0,0,0,0
1760,The Auger Engineering Radio Array and multi-hybrid cosmic ray detection (TAUP 2015),"  The Auger Engineering Radio Array (AERA) aims at the detection of air showers
induced by high-energy cosmic rays. As an extension of the Pierre Auger
Observatory, it measures complementary information to the particle detectors,
fluorescence telescopes and to the muon scintillators of the Auger Muons and
Infill for the Ground Array (AMIGA). AERA is sensitive to all fundamental
parameters of an extensive air shower such as the arrival direction, energy and
depth of shower maximum. Since the radio emission is induced purely by the
electromagnetic component of the shower, in combination with the AMIGA muon
counters, AERA is perfect for separate measurements of the electrons and muons
in the shower, if combined with a muon counting detector like AMIGA. In
addition to the depth of the shower maximum, the ratio of the electron and muon
number serves as a measure of the primary particle mass.
",0,1,0,0,0,0
1761,Historic Emergence of Diversity in Painting: Heterogeneity in Chromatic Distance in Images and Characterization of Massive Painting Data Set,"  Painting is an art form that has long functioned as a major channel for the
creative expression and communication of humans, its evolution taking place
under an interplay with the science, technology, and social environments of the
times. Therefore, understanding the process based on comprehensive data could
shed light on how humans acted and manifested creatively under changing
conditions. Yet, there exist few systematic frameworks that characterize the
process for painting, which would require robust statistical methods for
defining painting characteristics and identifying human's creative
developments, and data of high quality and sufficient quantity. Here we propose
that the color contrast of a painting image signifying the heterogeneity in
inter-pixel chromatic distance can be a useful representation of its style,
integrating both the color and geometry. From the color contrasts of paintings
from a large-scale, comprehensive archive of 179,853 high-quality images
spanning several centuries we characterize the temporal evolutionary patterns
of paintings, and present a deep study of an extraordinary expansion in
creative diversity and individuality that came to define the modern era.
",1,1,0,0,0,0
1762,Cage Size and Jump Precursors in Glass-Forming Liquids: Experiment and Simulations,"  Glassy dynamics is intermittent, as particles suddenly jump out of the cage
formed by their neighbours, and heterogeneous, as these jumps are not uniformly
distributed across the system. Relating these features of the dynamics to the
diverse local environments explored by the particles is essential to
rationalize the relaxation process. Here we investigate this issue
characterizing the local environment of a particle with the amplitude of its
short time vibrational motion, as determined by segmenting in cages and jumps
the particle trajectories. Both simulations of supercooled liquids and
experiments on colloidal suspensions show that particles in large cages are
likely to jump after a small time-lag, and that, on average, the cage enlarges
shortly before the particle jumps. At large time-lags, the cage has essentially
a constant value, which is smaller for longer-lasting cages. Finally, we
clarify how this coupling between cage size and duration controls the average
behaviour and opens the way to a better understanding of the relaxation process
in glass--forming liquids.
",0,1,0,0,0,0
1763,A Comprehensive Survey on Bengali Phoneme Recognition,"  Hidden Markov model based various phoneme recognition methods for Bengali
language is reviewed. Automatic phoneme recognition for Bengali language using
multilayer neural network is reviewed. Usefulness of multilayer neural network
over single layer neural network is discussed. Bangla phonetic feature table
construction and enhancement for Bengali speech recognition is also discussed.
Comparison among these methods is discussed.
",1,0,0,0,0,0
1764,Factorization of arithmetic automorphic periods,"  In this paper, we prove that the arithmetic automorphic periods for $GL_{n}$
over a CM field factorize through the infinite places. This generalizes a
conjecture of Shimura in 1983, and is predicted by the Langlands correspondence
between automorphic representations and motives.
",0,0,1,0,0,0
1765,Multielectronic processes in particle and antiparticle collisions with rare gases,"  In this chapter we analyze the multiple ionization by impact of |Z|=1
projectiles: electrons, positrons, protons and antiprotons. Differences and
similarities among the cross sections by these four projectiles allows us to
have an insight on the physics involved. Mass and charge effects, energy
thresholds, and relative importance of collisional and post-collisional
processes are discussed. For this purpose, we performed a detailed
theoretical-experimental comparison for single up to quintuple ionization of
Ne, Ar, Kr and Xe by particles and antiparticles. We include an extensive
compilation of the available data for the sixteen collisional systems, and the
theoretical cross sections by means of the continuum distorted wave eikonal
initial state approximation. We underline here that post-collisional ionization
is decisive to describe multiple ionization by light projectiles, covering
almost the whole energy range, from threshold to high energies. The
normalization of positron and antiproton measurements to electron impact ones,
the lack of data in certain cases, and the future prospects are presented and
discussed.
",0,1,0,0,0,0
1766,A Floating Cylinder on An Unbounded Bath,"  In this paper, we reconsider a circular cylinder horizontally floating on an
unbounded reservoir in a gravitational field directed downwards, which was
studied by Bhatnargar and Finn in 2006. We follow their approach but with some
modifications. We establish the relation between the total energy relative to
the undisturbed state and the total force. There is a monotone relation between
the height of the centre and the wetting angle. We study the number of
equilibria, the floating configurations and their stability for all parameter
values. We find that the system admits at most two equilibrium points for
arbitrary contact angle, the one with smaller wetting angle is stable and the
one with larger wetting angle is unstable. The initial model has a limitation
that the fluid interfaces may intersect. We show that the stable equilibrium
point never lies in the intersection region, while the unstable equilibrium
point may lie in the intersection region.
",0,1,1,0,0,0
1767,Switching between Limit Cycles in a Model of Running Using Exponentially Stabilizing Discrete Control Lyapunov Function,"  This paper considers the problem of switching between two periodic motions,
also known as limit cycles, to create agile running motions. For each limit
cycle, we use a control Lyapunov function to estimate the region of attraction
at the apex of the flight phase. We switch controllers at the apex, only if the
current state of the robot is within the region of attraction of the subsequent
limit cycle. If the intersection between two limit cycles is the null set, then
we construct additional limit cycles till we are able to achieve sufficient
overlap of the region of attraction between sequential limit cycles.
Additionally, we impose an exponential convergence condition on the control
Lyapunov function that allows us to rapidly transition between limit cycles.
Using the approach we demonstrate switching between 5 limit cycles in about 5
steps with the speed changing from 2 m/s to 5 m/s.
",1,0,0,0,0,0
1768,Locally Private Bayesian Inference for Count Models,"  As more aspects of social interaction are digitally recorded, there is a
growing need to develop privacy-preserving data analysis methods. Social
scientists will be more likely to adopt these methods if doing so entails
minimal change to their current methodology. Toward that end, we present a
general and modular method for privatizing Bayesian inference for Poisson
factorization, a broad class of models that contains some of the most widely
used models in the social sciences. Our method satisfies local differential
privacy, which ensures that no single centralized server need ever store the
non-privatized data. To formulate our local-privacy guarantees, we introduce
and focus on limited-precision local privacy---the local privacy analog of
limited-precision differential privacy (Flood et al., 2013). We present two
case studies, one involving social networks and one involving text corpora,
that test our method's ability to form the posterior distribution over latent
variables under different levels of noise, and demonstrate our method's utility
over a na??ve approach, wherein inference proceeds as usual, treating the
privatized data as if it were not privatized.
",1,0,0,1,0,0
1769,Carrier Diffusion in Thin-Film CH3NH3PbI3 Perovskite Measured using Four-Wave Mixing,"  We report the application of femtosecond four-wave mixing (FWM) to the study
of carrier transport in solution-processed CH3NH3PbI3. The diffusion
coefficient was extracted through direct detection of the lateral diffusion of
carriers utilizing the transient grating technique, coupled with simultaneous
measurement of decay kinetics exploiting the versatility of the boxcar
excitation beam geometry. The observation of exponential decay of the transient
grating versus interpulse delay indicates diffusive transport with negligible
trapping within the first nanosecond following excitation. The in-plane
transport geometry in our experiments enabled the diffusion length to be
compared directly with the grain size, indicating that carriers move across
multiple grain boundaries prior to recombination. Our experiments illustrate
the broad utility of FWM spectroscopy for rapid characterization of macroscopic
film transport properties.
",0,1,0,0,0,0
1770,On effective Birkhoff's ergodic theorem for computable actions of amenable groups,"  We introduce computable actions of computable groups and prove the following
versions of effective Birkhoff's ergodic theorem. Let $\Gamma$ be a computable
amenable group, then there always exists a canonically computable tempered
two-sided F{\o}lner sequence $(F_n)_{n \geq
1}$ in $\Gamma$. For a computable, measure-preserving, ergodic action of
$\Gamma$ on a Cantor space $\{0,1\}^{\mathbb N}$ endowed with a computable
probability measure $\mu$, it is shown that for every bounded lower
semicomputable function $f$ on $\{0,1\}^{\mathbb N}$ and for every Martin-L??f
random $\omega \in \{0,1\}^{\mathbb N}$ the equality \[ \lim\limits_{n \to
\infty} \frac{1}{|F_n|} \sum\limits_{g \in F_n} f(g \cdot \omega) = \int\limits
f d \mu \] holds, where the averages are taken with respect to a canonically
computable tempered two-sided F{\o}lner sequence $(F_n)_{n \geq
1}$. We also prove the same identity for all lower semicomputable $f$'s in
the special case when $\Gamma$ is a computable group of polynomial growth and
$F_n:=\mathrm{B}(n)$ is the F{\o}lner sequence of balls around the neutral
element of $\Gamma$.
",0,0,1,0,0,0
1771,Gender Differences in Participation and Reward on Stack Overflow,"  Programming is a valuable skill in the labor market, making the
underrepresentation of women in computing an increasingly important issue.
Online question and answer platforms serve a dual purpose in this field: they
form a body of knowledge useful as a reference and learning tool, and they
provide opportunities for individuals to demonstrate credible, verifiable
expertise. Issues, such as male-oriented site design or overrepresentation of
men among the site's elite may therefore compound the issue of women's
underrepresentation in IT. In this paper we audit the differences in behavior
and outcomes between men and women on Stack Overflow, the most popular of these
Q&A sites. We observe significant differences in how men and women participate
in the platform and how successful they are. For example, the average woman has
roughly half of the reputation points, the primary measure of success on the
site, of the average man. Using an Oaxaca-Blinder decomposition, an econometric
technique commonly applied to analyze differences in wages between groups, we
find that most of the gap in success between men and women can be explained by
differences in their activity on the site and differences in how these
activities are rewarded. Specifically, 1) men give more answers than women and
2) are rewarded more for their answers on average, even when controlling for
possible confounders such as tenure or buy-in to the site. Women ask more
questions and gain more reward per question. We conclude with a hypothetical
redesign of the site's scoring system based on these behavioral differences,
cutting the reputation gap in half.
",1,0,0,0,0,0
1772,Invariant surface area functionals and singular Yamabe problem in 3-dimensional CR geometry,"  We express two CR invariant surface area elements in terms of quantities in
pseudohermitian geometry. We deduce the Euler-Lagrange equations of the
associated energy functionals. Many solutions are given and discussed. In
relation to the singular CR Yamabe problem, we show that one of the energy
functionals appears as the coefficient (up to a constant multiple) of the log
term in the associated volume renormalization.
",0,0,1,0,0,0
1773,"Dynamic dipole polarizabilities of heteronuclear alkali dimers: optical response, trapping and control of ultracold molecules","  In this article we address the general approach for calculating dynamical
dipole polarizabilities of small quantum systems, based on a sum-over-states
formula involving in principle the entire energy spectrum of the system. We
complement this method by a few-parameter model involving a limited number of
effective transitions, allowing for a compact and accurate representation of
both the isotropic and anisotropic components of the polarizability. We apply
the method to the series of ten heteronuclear molecules composed of two of
($^7$Li,$^{23}$Na,$^{39}$K,$^{87}$Rb,$^{133}$Cs) alkali-metal atoms. We rely on
both up-to-date spectroscopically-determined potential energy curves for the
lowest electronic states, and on our systematic studies of these systems
performed during the last decade for higher excited states and for permanent
and transition dipole moments. Such a compilation is timely for the
continuously growing researches on ultracold polar molecules. Indeed the
knowledge of the dynamic dipole polarizabilities is crucial to model the
optical response of molecules when trapped in optical lattices, and to
determine optimal lattice frequencies ensuring optimal transfer to the absolute
ground state of initially weakly-bound molecules. When they exist, we determine
the so-called ""magic frequencies"" where the ac-Stark shift and thus the viewed
trap depth, is the same for both weakly-bound and ground-state molecules.
",0,1,0,0,0,0
1774,Minor-free graphs have light spanners,"  We show that every $H$-minor-free graph has a light $(1+\epsilon)$-spanner,
resolving an open problem of Grigni and Sissokho and proving a conjecture of
Grigni and Hung. Our lightness bound is
\[O\left(\frac{\sigma_H}{\epsilon^3}\log \frac{1}{\epsilon}\right)\] where
$\sigma_H = |V(H)|\sqrt{\log |V(H)|}$ is the sparsity coefficient of
$H$-minor-free graphs. That is, it has a practical dependency on the size of
the minor $H$. Our result also implies that the polynomial time approximation
scheme (PTAS) for the Travelling Salesperson Problem (TSP) in $H$-minor-free
graphs by Demaine, Hajiaghayi and Kawarabayashi is an efficient PTAS whose
running time is $2^{O_H\left(\frac{1}{\epsilon^4}\log
\frac{1}{\epsilon}\right)}n^{O(1)}$ where $O_H$ ignores dependencies on the
size of $H$. Our techniques significantly deviate from existing lines of
research on spanners for $H$-minor-free graphs, but build upon the work of
Chechik and Wulff-Nilsen for spanners of general graphs.
",1,0,0,0,0,0
1775,Adversarial Generation of Natural Language,"  Generative Adversarial Networks (GANs) have gathered a lot of attention from
the computer vision community, yielding impressive results for image
generation. Advances in the adversarial generation of natural language from
noise however are not commensurate with the progress made in generating images,
and still lag far behind likelihood based methods. In this paper, we take a
step towards generating natural language with a GAN objective alone. We
introduce a simple baseline that addresses the discrete output space problem
without relying on gradient estimators and show that it is able to achieve
state-of-the-art results on a Chinese poem generation dataset. We present
quantitative results on generating sentences from context-free and
probabilistic context-free grammars, and qualitative language modeling results.
A conditional version is also described that can generate sequences conditioned
on sentence characteristics.
",1,0,0,1,0,0
1776,Likely Transiting Exocomets Detected by Kepler,"  We present the first good evidence for exocomet transits of a host star in
continuum light in data from the Kepler mission. The Kepler star in question,
KIC 3542116, is of spectral type F2V and is quite bright at K_p = 10. The
transits have a distinct asymmetric shape with a steeper ingress and slower
egress that can be ascribed to objects with a trailing dust tail passing over
the stellar disk. There are three deeper transits with depths of ~0.1% that
last for about a day, and three that are several times more shallow and of
shorter duration. The transits were found via an exhaustive visual search of
the entire Kepler photometric data set, which we describe in some detail. We
review the methods we use to validate the Kepler data showing the comet
transits, and rule out instrumental artefacts as sources of the signals. We fit
the transits with a simple dust-tail model, and find that a transverse comet
speed of ~35-50 km/s and a minimum amount of dust present in the tail of ~10^16
g are required to explain the larger transits. For a dust replenishment time of
~10 days, and a comet lifetime of only ~300 days, this implies a total cometary
mass of > 3 x 10^17 g, or about the mass of Halley's comet. We also discuss the
number of comets and orbital geometry that would be necessary to explain the
six transits detected over the four years of Kepler prime-field observations.
Finally, we also report the discovery of a single comet-shaped transit in KIC
11084727 with very similar transit and host-star properties.
",0,1,0,0,0,0
1777,Origins of bond and spin order in rare-earth nickelate bulk and heterostructures,"  We analyze the charge- and spin response functions of rare-earth nickelates
RNiO3 and their heterostructures using random-phase approximation in a two-band
Hubbard model. The inter-orbital charge fluctuation is found to be the driving
mechanism for the rock-salt type bond order in bulk RNiO3, and good agreement
of the ordering temperature with experimental values is achieved for all RNiO3
using realistic crystal structures and interaction parameters. We further show
that magnetic ordering in bulk is not driven by the spin fluctuation and should
be instead explained as ordering of localized moments. This picture changes for
low-dimensional heterostructures, where the charge fluctuation is suppressed
and overtaken by the enhanced spin instability, which results in a
spin-density-wave ground state observed in recent experiments. Predictions for
spectroscopy allow for further experimental testing of our claims.
",0,1,0,0,0,0
1778,Canonical Truth,"  We introduce and study a notion of canonical set theoretical truth, which
means truth in a `canonical model', i.e. a transitive class model that is
uniquely characterized by some $\in$-formula. We show that this notion of truth
is `informative', i.e. there are statements that hold in all canonical models
but do not follow from ZFC, such as Reitz' ground model axiom or the
nonexistence of measurable cardinals. We also show that ZF+$V=L[\mathbb{R}]$+AD
has no canonical models. On the other hand, we show that there are canonical
models for `every real has sharp'. Moreover, we consider `theory-canonical'
statements that only fix a transitive class model of ZFC up to elementary
equivalence and show that it is consistent relative to large cardinals that
there are theory-canonical models with measurable cardinals and that
theory-canonicity is still informative in the sense explained above.
",0,0,1,0,0,0
1779,AACT: Application-Aware Cooperative Time Allocation for Internet of Things,"  As the number of Internet of Things (IoT) devices keeps increasing, data is
required to be communicated and processed by these devices at unprecedented
rates. Cooperation among wireless devices by exploiting Device-to-Device (D2D)
connections is promising, where aggregated resources in a cooperative setup can
be utilized by all devices, which would increase the total utility of the
setup. In this paper, we focus on the resource allocation problem for
cooperating IoT devices with multiple heterogeneous applications. In
particular, we develop Application-Aware Cooperative Time allocation (AACT)
framework, which optimizes the time that each application utilizes the
aggregated system resources by taking into account heterogeneous device
constraints and application requirements. AACT is grounded on the concept of
Rolling Horizon Control (RHC) where decisions are made by iteratively solving a
convex optimization problem over a moving control window of estimated system
parameters. The simulation results demonstrate significant performance gains.
",1,0,0,0,0,0
1780,COPA: Constrained PARAFAC2 for Sparse & Large Datasets,"  PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.
",0,0,0,1,0,0
1781,Effect of Composition Gradient on Magnetothermal Instability Modified by Shear and Rotation,"  We model the intracluster medium as a weakly collisional plasma that is a
binary mixture of the hydrogen and the helium ions, along with free electrons.
When, owing to the helium sedimentation, the gradient of the mean molecular
weight (or equivalently, composition or helium ions' concentration) of the
plasma is not negligible, it can have appreciable influence on the stability
criteria of the thermal convective instabilities, e.g., the heat-flux-buoyancy
instability and the magnetothermal instability (MTI). These instabilities are
consequences of the anisotropic heat conduction occurring preferentially along
the magnetic field lines. In this paper, without ignoring the magnetic tension,
we first present the mathematical criterion for the onset of composition
gradient modified MTI. Subsequently, we relax the commonly adopted equilibrium
state in which the plasma is at rest, and assume that the plasma is in a
sheared state which may be due to differential rotation. We discuss how the
concentration gradient affects the coupling between the Kelvin--Helmholtz
instability and the MTI in rendering the plasma unstable or stable. We derive
exact stability criterion by working with the sharp boundary case in which the
physical variables---temperature, mean molecular weight, density, and magnetic
field---change discontinuously from one constant value to another on crossing
the boundary. Finally, we perform the linear stability analysis for the case of
the differentially rotating plasma that is thermally and compositionally
stratified as well. By assuming axisymmetric perturbations, we find the
corresponding dispersion relation and the explicit mathematical expression
determining the onset of the modified MTI.
",0,1,0,0,0,0
1782,A Verified Algorithm Enumerating Event Structures,"  An event structure is a mathematical abstraction modeling concepts as
causality, conflict and concurrency between events. While many other
mathematical structures, including groups, topological spaces, rings, abound
with algorithms and formulas to generate, enumerate and count particular sets
of their members, no algorithm or formulas are known to generate or count all
the possible event structures over a finite set of events. We present an
algorithm to generate such a family, along with a functional implementation
verified using Isabelle/HOL. As byproducts, we obtain a verified enumeration of
all possible preorders and partial orders. While the integer sequences counting
preorders and partial orders are already listed on OEIS (On-line Encyclopedia
of Integer Sequences), the one counting event structures is not. We therefore
used our algorithm to submit a formally verified addition, which has been
successfully reviewed and is now part of the OEIS.
",1,0,0,0,0,0
1783,Missing Data as Part of the Social Behavior in Real-World Financial Complex Systems,"  Many real-world networks are known to exhibit facts that counter our
knowledge prescribed by the theories on network creation and communication
patterns. A common prerequisite in network analysis is that information on
nodes and links will be complete because network topologies are extremely
sensitive to missing information of this kind. Therefore, many real-world
networks that fail to meet this criterion under random sampling may be
discarded.
In this paper we offer a framework for interpreting the missing observations
in network data under the hypothesis that these observations are not missing at
random. We demonstrate the methodology with a case study of a financial trade
network, where the awareness of agents to the data collection procedure by a
self-interested observer may result in strategic revealing or withholding of
information. The non-random missingness has been overlooked despite the
possibility of this being an important feature of the processes by which the
network is generated. The analysis demonstrates that strategic information
withholding may be a valid general phenomenon in complex systems. The evidence
is sufficient to support the existence of an influential observer and to offer
a compelling dynamic mechanism for the creation of the network.
",0,0,0,1,0,0
1784,Quantum Monte Carlo simulation of a two-dimensional Majorana lattice model,"  We study interacting Majorana fermions in two dimensions as a low-energy
effective model of a vortex lattice in two-dimensional time-reversal-invariant
topological superconductors. For that purpose, we implement ab-initio quantum
Monte Carlo simulation to the Majorana fermion system in which the
path-integral measure is given by a semi-positive Pfaffian. We discuss
spontaneous breaking of time-reversal symmetry at finite temperature.
",0,1,0,0,0,0
1785,Geometric Rescaling Algorithms for Submodular Function Minimization,"  We present a new class of polynomial-time algorithms for submodular function
minimization (SFM), as well as a unified framework to obtain strongly
polynomial SFM algorithms. Our new algorithms are based on simple iterative
methods for the minimum-norm problem, such as the conditional gradient and the
Fujishige-Wolfe algorithms. We exhibit two techniques to turn simple iterative
methods into polynomial-time algorithms.
Firstly, we use the geometric rescaling technique, which has recently gained
attention in linear programming. We adapt this technique to SFM and obtain a
weakly polynomial bound $O((n^4\cdot EO + n^5)\log (n L))$.
Secondly, we exhibit a general combinatorial black-box approach to turn any
strongly polynomial $\varepsilon L$-approximate SFM oracle into a strongly
polynomial exact SFM algorithm. This framework can be applied to a wide range
of combinatorial and continuous algorithms, including pseudo-polynomial ones.
In particular, we can obtain strongly polynomial algorithms by a repeated
application of the conditional gradient or of the Fujishige-Wolfe algorithm.
Combined with the geometric rescaling technique, the black-box approach
provides a $O((n^5\cdot EO + n^6)\log^2 n)$ algorithm. Finally, we show that
one of the techniques we develop in the paper can also be combined with the
cutting-plane method of Lee, Sidford, and Wong, yielding a simplified variant
of their $O(n^3 \log^2 n \cdot EO + n^4\log^{O(1)} n)$ algorithm.
",1,0,1,0,0,0
1786,Statistical PT-symmetric lasing in an optical fiber network,"  PT-symmetry in optics is a condition whereby the real and imaginary parts of
the refractive index across a photonic structure are deliberately balanced.
This balance can lead to a host of novel optical phenomena, such as
unidirectional invisibility, loss-induced lasing, single-mode lasing from
multimode resonators, and non-reciprocal effects in conjunction with
nonlinearities. Because PT-symmetry has been thought of as fragile,
experimental realizations to date have been usually restricted to on-chip
micro-devices. Here, we demonstrate that certain features of PT-symmetry are
sufficiently robust to survive the statistical fluctuations associated with a
macroscopic optical cavity. We construct optical-fiber-based coupled-cavities
in excess of a kilometer in length (the free spectral range is less than 0.8
fm) with balanced gain and loss in two sub-cavities and examine the lasing
dynamics. In such a macroscopic system, fluctuations can lead to a
cavity-detuning exceeding the free spectral range. Nevertheless, by varying the
gain-loss contrast, we observe that both the lasing threshold and the growth of
the laser power follow the predicted behavior of a stable PT-symmetric
structure. Furthermore, a statistical symmetry-breaking point is observed upon
varying the cavity loss. These findings indicate that PT-symmetry is a more
robust optical phenomenon than previously expected, and points to potential
applications in optical fiber networks and fiber lasers.
",0,1,0,0,0,0
1787,Transforming Single Domain Magnetic CoFe2O4 Nanoparticles from Hydrophobic to Hydrophilic By Novel Mechanochemical Ligand Exchange,"  Single phase, uniform size (~9 nm) Cobalt Ferrite (CFO) nanoparticles have
been synthesized by hydrothermal synthesis using oleic acid as a surfactant.
The as synthesized oleic acid coated CFO (OA-CFO) nanoparticles were well
dispersible in nonpolar solvents but not dispersible in water. The OA-CFO
nanoparticles have been successfully transformed to highly water dispersible
citric acid coated CFO (CA-CFO) nanoparticles using a novel single step ligand
exchange process by mechanochemical milling, in which small chain citric acid
molecules replace the original large chain oleic acid molecules available on
CFO nanoparticles. The contact angle measurement shows that OA-CFO
nanoparticles are hydrophobic whereas CA-CFO nanoparticles are superhydrophilic
in nature. The potentiality of as synthesized OA-CFO and mechanochemically
transformed CA-CFO nanoparticles for the demulsification of highly stabilized
water-in-oil and oil-in-water emulsions has been demonstrated.
",0,1,0,0,0,0
1788,Probing the dusty stellar populations of the Local Volume Galaxies with JWST/MIRI,"  The Mid-Infrared Instrument (MIRI) for the {\em James Webb Space Telescope}
(JWST) will revolutionize our understanding of infrared stellar populations in
the Local Volume. Using the rich {\em Spitzer}-IRS spectroscopic data-set and
spectral classifications from the Surveying the Agents of Galaxy Evolution
(SAGE)-Spectroscopic survey of over a thousand objects in the Magellanic
Clouds, the Grid of Red supergiant and Asymptotic giant branch star ModelS
({\sc grams}), and the grid of YSO models by Robitaille et al. (2006), we
calculate the expected flux-densities and colors in the MIRI broadband filters
for prominent infrared stellar populations. We use these fluxes to explore the
{\em JWST}/MIRI colours and magnitudes for composite stellar population studies
of Local Volume galaxies. MIRI colour classification schemes are presented;
these diagrams provide a powerful means of identifying young stellar objects,
evolved stars and extragalactic background galaxies in Local Volume galaxies
with a high degree of confidence. Finally, we examine which filter combinations
are best for selecting populations of sources based on their JWST colours.
",0,1,0,0,0,0
1789,PythonRobotics: a Python code collection of robotics algorithms,"  This paper describes an Open Source Software (OSS) project: PythonRobotics.
This is a collection of robotics algorithms implemented in the Python
programming language. The focus of the project is on autonomous navigation, and
the goal is for beginners in robotics to understand the basic ideas behind each
algorithm. In this project, the algorithms which are practical and widely used
in both academia and industry are selected. Each sample code is written in
Python3 and only depends on some standard modules for readability and ease of
use. It includes intuitive animations to understand the behavior of the
simulation.
",1,0,0,0,0,0
1790,A Liouville theorem for the Euler equations in the plane,"  This paper is concerned with qualitative properties of bounded steady flows
of an ideal incompressible fluid with no stagnation point in the
two-dimensional plane R^2. We show that any such flow is a shear flow, that is,
it is parallel to some constant vector. The proof of this Liouville-type result
is firstly based on the study of the geometric properties of the level curves
of the stream function and secondly on the derivation of some estimates on the
at most logarithmic growth of the argument of the flow. These estimates lead to
the conclusion that the streamlines of the flow are all parallel lines.
",0,0,1,0,0,0
1791,First On-Site True Gamma-Ray Imaging-Spectroscopy of Contamination near Fukushima Plant,"  We have developed an Electron Tracking Compton Camera (ETCC), which provides
a well-defined Point Spread Function (PSF) by reconstructing a direction of
each gamma as a point and realizes simultaneous measurement of brightness and
spectrum of MeV gamma-rays for the first time. Here, we present the results of
our on-site pilot gamma-imaging-spectroscopy with ETCC at three contaminated
locations in the vicinity of the Fukushima Daiichi Nuclear Power Plants in
Japan in 2014. The obtained distribution of brightness (or emissivity) with
remote-sensing observations is unambiguously converted into the dose
distribution. We confirm that the dose distribution is consistent with the one
taken by conventional mapping measurements with a dosimeter physically placed
at each grid point. Furthermore, its imaging spectroscopy, boosted by
Compton-edge-free spectra, reveals complex radioactive features in a
quantitative manner around each individual target point in the
background-dominated environment. Notably, we successfully identify a ""micro
hot spot"" of residual caesium contamination even in an already decontaminated
area. These results show that the ETCC performs exactly as the geometrical
optics predicts, demonstrates its versatility in the field radiation
measurement, and reveals potentials for application in many fields, including
the nuclear industry, medical field, and astronomy.
",0,1,0,0,0,0
1792,Weak Versus Strong Disorder Superfluid-Bose Glass Transition in One Dimension,"  Using large-scale simulations based on matrix product state and quantum Monte
Carlo techniques, we study the superfluid to Bose glass-transition for
one-dimensional attractive hard-core bosons at zero temperature, across the
full regime from weak to strong disorder. As a function of interaction and
disorder strength, we identify a Berezinskii-Kosterlitz-Thouless critical line
with two different regimes. At small attraction where critical disorder is weak
compared to the bandwidth, the critical Luttinger parameter $K_c$ takes its
universal Giamarchi-Schulz value $K_{c}=3/2$. Conversely, a non-universal
$K_c>3/2$ emerges for stronger attraction where weak-link physics is relevant.
In this strong disorder regime, the transition is characterized by self-similar
power-law distributed weak links with a continuously varying characteristic
exponent $\alpha$.
",0,1,0,0,0,0
1793,Quantum Structures in Human Decision-making: Towards Quantum Expected Utility,"  {\it Ellsberg thought experiments} and empirical confirmation of Ellsberg
preferences pose serious challenges to {\it subjective expected utility theory}
(SEUT). We have recently elaborated a quantum-theoretic framework for human
decisions under uncertainty which satisfactorily copes with the Ellsberg
paradox and other puzzles of SEUT. We apply here the quantum-theoretic
framework to the {\it Ellsberg two-urn example}, showing that the paradox can
be explained by assuming a state change of the conceptual entity that is the
object of the decision ({\it decision-making}, or {\it DM}, {\it entity}) and
representing subjective probabilities by quantum probabilities. We also model
the empirical data we collected in a DM test on human participants within the
theoretic framework above. The obtained results are relevant, as they provide a
line to model real life, e.g., financial and medical, decisions that show the
same empirical patterns as the two-urn experiment.
",0,0,0,0,1,1
1794,Fine-grained Event Learning of Human-Object Interaction with LSTM-CRF,"  Event learning is one of the most important problems in AI. However,
notwithstanding significant research efforts, it is still a very complex task,
especially when the events involve the interaction of humans or agents with
other objects, as it requires modeling human kinematics and object movements.
This study proposes a methodology for learning complex human-object interaction
(HOI) events, involving the recording, annotation and classification of event
interactions. For annotation, we allow multiple interpretations of a motion
capture by slicing over its temporal span, for classification, we use
Long-Short Term Memory (LSTM) sequential models with Conditional Randon Field
(CRF) for constraints of outputs. Using a setup involving captures of
human-object interaction as three dimensional inputs, we argue that this
approach could be used for event types involving complex spatio-temporal
dynamics.
",1,0,0,0,0,0
1795,Book Review Interferometry and Synthesis in Radio Astronomy - 3rd Ed,"  Review of the third edition of ""Interferometry and Synthesis in Radio
Astronomy"" by Thompson, Moran and Swenson
",0,1,0,0,0,0
1796,A Kernel Theory of Modern Data Augmentation,"  Data augmentation, a technique in which a training set is expanded with
class-preserving transformations, is ubiquitous in modern machine learning
pipelines. In this paper, we seek to establish a theoretical framework for
understanding modern data augmentation techniques. We start by showing that for
kernel classifiers, data augmentation can be approximated by first-order
feature averaging and second-order variance regularization components. We
connect this general approximation framework to prior work in invariant
kernels, tangent propagation, and robust optimization. Next, we explicitly
tackle the compositional aspect of modern data augmentation techniques,
proposing a novel model of data augmentation as a Markov process. Under this
model, we show that performing $k$-nearest neighbors with data augmentation is
asymptotically equivalent to a kernel classifier. Finally, we illustrate ways
in which our theoretical framework can be leveraged to accelerate machine
learning workflows in practice, including reducing the amount of computation
needed to train on augmented data, and predicting the utility of a
transformation prior to training.
",0,0,0,1,0,0
1797,Carrier driven coupling in ferromagnetic oxide heterostructures,"  Transition metal oxides are well known for their complex magnetic and
electrical properties. When brought together in heterostructure geometries,
they show particular promise for spintronics and colossal magnetoresistance
applications. In this letter, we propose a new mechanism for the coupling
between layers of itinerant ferromagnetic materials in heterostructures. The
coupling is mediated by charge carriers that strive to maximally delocalize
through the heterostructure to gain kinetic energy. In doing so, they force a
ferromagnetic or antiferromagnetic coupling between the constituent layers. To
illustrate this, we focus on heterostructures composed of SrRuO$_3$ and
La$_{1-x}$A$_{x}$MnO$_3$ (A=Ca/Sr). Our mechanism is consistent with
antiferromagnetic alignment that is known to occur in multilayers of
SrRuO$_3$-La$_{1-x}$A$_{x}$MnO$_3$. To support our assertion, we present a
minimal Kondo-lattice model which reproduces the known magnetization properties
of such multilayers. In addition, we discuss a quantum well model for
heterostructures and argue that the spin-dependent density of states determines
the nature of the coupling. As a smoking gun signature, we propose that
bilayers with the same constituents will oscillate between ferromagnetic and
antiferromagnetic coupling upon tuning the relative thicknesses of the layers.
",0,1,0,0,0,0
1798,Data Dropout in Arbitrary Basis for Deep Network Regularization,"  An important problem in training deep networks with high capacity is to
ensure that the trained network works well when presented with new inputs
outside the training dataset. Dropout is an effective regularization technique
to boost the network generalization in which a random subset of the elements of
the given data and the extracted features are set to zero during the training
process. In this paper, a new randomized regularization technique in which we
withhold a random part of the data without necessarily turning off the
neurons/data-elements is proposed. In the proposed method, of which the
conventional dropout is shown to be a special case, random data dropout is
performed in an arbitrary basis, hence the designation Generalized Dropout. We
also present a framework whereby the proposed technique can be applied
efficiently to convolutional neural networks. The presented numerical
experiments demonstrate that the proposed technique yields notable performance
gain. Generalized Dropout provides new insight into the idea of dropout, shows
that we can achieve different performance gains by using different bases
matrices, and opens up a new research question as of how to choose optimal
bases matrices that achieve maximal performance gain.
",1,0,0,1,0,0
1799,Efficient algorithms to discover alterations with complementary functional association in cancer,"  Recent large cancer studies have measured somatic alterations in an
unprecedented number of tumours. These large datasets allow the identification
of cancer-related sets of genetic alterations by identifying relevant
combinatorial patterns. Among such patterns, mutual exclusivity has been
employed by several recent methods that have shown its effectivenes in
characterizing gene sets associated to cancer. Mutual exclusivity arises
because of the complementarity, at the functional level, of alterations in
genes which are part of a group (e.g., a pathway) performing a given function.
The availability of quantitative target profiles, from genetic perturbations or
from clinical phenotypes, provides additional information that can be leveraged
to improve the identification of cancer related gene sets by discovering groups
with complementary functional associations with such targets.
In this work we study the problem of finding groups of mutually exclusive
alterations associated with a quantitative (functional) target. We propose a
combinatorial formulation for the problem, and prove that the associated
computation problem is computationally hard. We design two algorithms to solve
the problem and implement them in our tool UNCOVER. We provide analytic
evidence of the effectiveness of UNCOVER in finding high-quality solutions and
show experimentally that UNCOVER finds sets of alterations significantly
associated with functional targets in a variety of scenarios. In addition, our
algorithms are much faster than the state-of-the-art, allowing the analysis of
large datasets of thousands of target profiles from cancer cell lines. We show
that on one such dataset from project Achilles our methods identify several
significant gene sets with complementary functional associations with targets.
",0,0,0,0,1,0
1800,Laplace operators on holomorphic Lie algebroids,"  The paper introduces Laplace-type operators for functions defined on the
tangent space of a Finsler Lie algebroid, using a volume form on the
prolongation of the algebroid. It also presents the construction of a
horizontal Laplace operator for forms defined on the prolongation of the
algebroid. All of the Laplace operators considered in the paper are also
locally expressed using the Chern-Finsler connection of the algebroid.
",0,0,1,0,0,0
1801,Composing Differential Privacy and Secure Computation: A case study on scaling private record linkage,"  Private record linkage (PRL) is the problem of identifying pairs of records
that are similar as per an input matching rule from databases held by two
parties that do not trust one another. We identify three key desiderata that a
PRL solution must ensure: 1) perfect precision and high recall of matching
pairs, 2) a proof of end-to-end privacy, and 3) communication and computational
costs that scale subquadratically in the number of input records. We show that
all of the existing solutions for PRL - including secure 2-party computation
(S2PC), and their variants that use non-private or differentially private (DP)
blocking to ensure subquadratic cost - violate at least one of the three
desiderata. In particular, S2PC techniques guarantee end-to-end privacy but
have either low recall or quadratic cost. In contrast, no end-to-end privacy
guarantee has been formalized for solutions that achieve subquadratic cost.
This is true even for solutions that compose DP and S2PC: DP does not permit
the release of any exact information about the databases, while S2PC algorithms
for PRL allow the release of matching records.
In light of this deficiency, we propose a novel privacy model, called output
constrained differential privacy, that shares the strong privacy protection of
DP, but allows for the truthful release of the output of a certain function
applied to the data. We apply this to PRL, and show that protocols satisfying
this privacy model permit the disclosure of the true matching records, but
their execution is insensitive to the presence or absence of a single
non-matching record. We find that prior work that combine DP and S2PC
techniques even fail to satisfy this end-to-end privacy model. Hence, we
develop novel protocols that provably achieve this end-to-end privacy
guarantee, together with the other two desiderata of PRL.
",1,0,0,0,0,0
1802,Recurrent Neural Filters: Learning Independent Bayesian Filtering Steps for Time Series Prediction,"  Despite the recent popularity of deep generative state space models, few
comparisons have been made between network architectures and the inference
steps of the Bayesian filtering framework -- with most models simultaneously
approximating both state transition and update steps with a single recurrent
neural network (RNN). In this paper, we introduce the Recurrent Neural Filter
(RNF), a novel recurrent variational autoencoder architecture that learns
distinct representations for each Bayesian filtering step, captured by a series
of encoders and decoders. Testing this on three real-world time series
datasets, we demonstrate that decoupling representations not only improves the
accuracy of one-step-ahead forecasts while providing realistic uncertainty
estimates, but also facilitates multistep prediction through the separation of
encoder stages.
",1,0,0,1,0,0
1803,A Submodularity-Based Approach for Multi-Agent Optimal Coverage Problems,"  We consider the optimal coverage problem where a multi-agent network is
deployed in an environment with obstacles to maximize a joint event detection
probability. The objective function of this problem is non-convex and no global
optimum is guaranteed by gradient-based algorithms developed to date. We first
show that the objective function is monotone submodular, a class of functions
for which a simple greedy algorithm is known to be within 0.63 of the optimal
solution. We then derive two tighter lower bounds by exploiting the curvature
information (total curvature and elemental curvature) of the objective
function. We further show that the tightness of these lower bounds is
complementary with respect to the sensing capabilities of the agents. The
greedy algorithm solution can be subsequently used as an initial point for a
gradient-based algorithm to obtain solutions even closer to the global optimum.
Simulation results show that this approach leads to significantly better
performance relative to previously used algorithms.
",1,0,1,0,0,0
1804,A GPU-based Multi-level Algorithm for Boundary Value Problems,"  A novel and scalable geometric multi-level algorithm is presented for the
numerical solution of elliptic partial differential equations, specially
designed to run with high occupancy of streaming processors inside Graphics
Processing Units(GPUs). The algorithm consists of iterative, superposed
operations on a single grid, and it is composed of two simple full-grid
routines: a restriction and a coarsened interpolation-relaxation. The
restriction is used to collect sources using recursive coarsened averages, and
the interpolation-relaxation simultaneously applies coarsened finite-difference
operators and interpolations. The routines are scheduled in a saw-like refining
cycle. Convergence to machine precision is achieved repeating the full cycle
using accumulated residuals and successively collecting the solution. Its total
number of operations scale linearly with the number of nodes. It provides an
attractive fast solver for Boundary Value Problems (BVPs), specially for
simulations running entirely in the GPU. Applications shown in this work
include the deformation of two-dimensional grids, the computation of
three-dimensional streamlines for a singular trifoil-knot vortex and the
calculation of three-dimensional electric potentials in heterogeneous
dielectric media.
",1,1,0,0,0,0
1805,Counterexample-Guided k-Induction Verification for Fast Bug Detection,"  Recently, the k-induction algorithm has proven to be a successful approach
for both finding bugs and proving correctness. However, since the algorithm is
an incremental approach, it might waste resources trying to prove incorrect
programs. In this paper, we propose to extend the k-induction algorithm in
order to shorten the number of steps required to find a property violation. We
convert the algorithm into a meet-in-the-middle bidirectional search algorithm,
using the counterexample produced from over-approximating the program. The
preliminary results show that the number of steps required to find a property
violation is reduced to $\lfloor\frac{k}{2} + 1\rfloor$ and the verification
time for programs with large state space is reduced considerably.
",1,0,0,0,0,0
1806,Cautious Model Predictive Control using Gaussian Process Regression,"  Gaussian process (GP) regression has been widely used in supervised machine
learning due to its flexibility and inherent ability to describe uncertainty in
function estimation. In the context of control, it is seeing increasing use for
modeling of nonlinear dynamical systems from data, as it allows the direct
assessment of residual model uncertainty. We present a model predictive control
(MPC) approach that integrates a nominal system with an additive nonlinear part
of the dynamics modeled as a GP. Approximation techniques for propagating the
state distribution are reviewed and we describe a principled way of formulating
the chance constrained MPC problem, which takes into account residual
uncertainties provided by the GP model to enable cautious control. Using
additional approximations for efficient computation, we finally demonstrate the
approach in a simulation example, as well as in a hardware implementation for
autonomous racing of remote controlled race cars, highlighting improvements
with regard to both performance and safety over a nominal controller.
",1,0,1,0,0,0
1807,Probabilistic Trajectory Segmentation by Means of Hierarchical Dirichlet Process Switching Linear Dynamical Systems,"  Using movement primitive libraries is an effective means to enable robots to
solve more complex tasks. In order to build these movement libraries, current
algorithms require a prior segmentation of the demonstration trajectories. A
promising approach is to model the trajectory as being generated by a set of
Switching Linear Dynamical Systems and inferring a meaningful segmentation by
inspecting the transition points characterized by the switching dynamics. With
respect to the learning, a nonparametric Bayesian approach is employed
utilizing a Gibbs sampler.
",1,0,0,1,0,0
1808,Radio Frequency Interference Mitigation,"  Radio astronomy observational facilities are under constant upgradation and
development to achieve better capabilities including increasing the time and
frequency resolutions of the recorded data, and increasing the receiving and
recording bandwidth. As only a limited spectrum resource has been allocated to
radio astronomy by the International Telecommunication Union, this results in
the radio observational instrumentation being inevitably exposed to undesirable
radio frequency interference (RFI) signals which originate mainly from
terrestrial human activity and are becoming stronger with time. RFIs degrade
the quality of astronomical data and even lead to data loss. The impact of RFIs
on scientific outcome is becoming progressively difficult to manage. In this
article, we motivate the requirement for RFI mitigation, and review the RFI
characteristics, mitigation techniques and strategies. Mitigation strategies
adopted at some representative observatories, telescopes and arrays are also
introduced. We also discuss and present advantages and shortcomings of the four
classes of RFI mitigation strategies, applicable at the connected causal
stages: preventive, pre-detection, pre-correlation and post-correlation. The
proper identification and flagging of RFI is key to the reduction of data loss
and improvement in data quality, and is also the ultimate goal of developing
RFI mitigation techniques. This can be achieved through a strategy involving a
combination of the discussed techniques in stages. Recent advances in high
speed digital signal processing and high performance computing allow for
performing RFI excision of large data volumes generated from large telescopes
or arrays in both real time and offline modes, aiding the proposed strategy.
",0,1,0,0,0,0
1809,Online Calibration of Phasor Measurement Unit Using Density-Based Spatial Clustering,"  Data quality of Phasor Measurement Unit (PMU) is receiving increasing
attention as it has been identified as one of the limiting factors that affect
many wide-area measurement system (WAMS) based applications. In general,
existing PMU calibration methods include offline testing and model based
approaches. However, in practice, the effectiveness of both is limited due to
the very strong assumptions employed. This paper presents a novel framework for
online bias error detection and calibration of PMU measurement using
density-based spatial clustering of applications with noise (DBSCAN) based on
much relaxed assumptions. With a new problem formulation, the proposed data
mining based methodology is applicable across a wide spectrum of practical
conditions and one side-product of it is more accurate transmission line
parameters for EMS database and protective relay settings. Case studies
demonstrate the effectiveness of the proposed approach.
",1,0,0,0,0,0
1810,Some basic properties of bounded solutions of parabolic equations with p-Laplacian diffusion,"  We provide a detailed (and fully rigorous) derivation of several fundamental
properties of bounded weak solutions to initial-value problems for general
conservative 2nd-order parabolic equations with p-Laplacian diffusion and
(arbitrary) bounded and integrable initial data.
",0,0,1,0,0,0
1811,Andreev Reflection without Fermi surface alignment in High T$_{c}$-Topological heterostructures,"  We address the controversy over the proximity effect between topological
materials and high T$_{c}$ superconductors. Junctions are produced between
Bi$_{2}$Sr$_{2}$CaCu$_{2}$O$_{8+\delta}$ and materials with different Fermi
surfaces (Bi$_{2}$Te$_{3}$ \& graphite). Both cases reveal tunneling spectra
consistent with Andreev reflection. This is confirmed by magnetic field that
shifts features via the Doppler effect. This is modeled with a single parameter
that accounts for tunneling into a screening supercurrent. Thus the tunneling
involves Cooper pairs crossing the heterostructure, showing the Fermi surface
mis-match does not hinder the ability to form transparent interfaces, which is
accounted for by the extended Brillouin zone and different lattice symmetries.
",0,1,0,0,0,0
1812,Structural Data Recognition with Graph Model Boosting,"  This paper presents a novel method for structural data recognition using a
large number of graph models. In general, prevalent methods for structural data
recognition have two shortcomings: 1) Only a single model is used to capture
structural variation. 2) Naive recognition methods are used, such as the
nearest neighbor method. In this paper, we propose strengthening the
recognition performance of these models as well as their ability to capture
structural variation. The proposed method constructs a large number of graph
models and trains decision trees using the models. This paper makes two main
contributions. The first is a novel graph model that can quickly perform
calculations, which allows us to construct several models in a feasible amount
of time. The second contribution is a novel approach to structural data
recognition: graph model boosting. Comprehensive structural variations can be
captured with a large number of graph models constructed in a boosting
framework, and a sophisticated classifier can be formed by aggregating the
decision trees. Consequently, we can carry out structural data recognition with
powerful recognition capability in the face of comprehensive structural
variation. The experiments shows that the proposed method achieves impressive
results and outperforms existing methods on datasets of IAM graph database
repository.
",1,0,0,1,0,0
1813,Exceptional points in two simple textbook examples,"  We propose to introduce the concept of exceptional points in intermediate
courses on mathematics and classical mechanics by means of simple textbook
examples. The first one is an ordinary second-order differential equation with
constant coefficients. The second one is the well known damped harmonic
oscillator. They enable one to connect the occurrence of linearly dependent
exponential solutions with a defective matrix that cannot be diagonalized but
can be transformed into a Jordan canonical form.
",0,1,0,0,0,0
1814,Bootstrap of residual processes in regression: to smooth or not to smooth ?,"  In this paper we consider a location model of the form $Y = m(X) +
\varepsilon$, where $m(\cdot)$ is the unknown regression function, the error
$\varepsilon$ is independent of the $p$-dimensional covariate $X$ and
$E(\varepsilon)=0$. Given i.i.d. data $(X_1,Y_1),\ldots,(X_n,Y_n)$ and given an
estimator $\hat m(\cdot)$ of the function $m(\cdot)$ (which can be parametric
or nonparametric of nature), we estimate the distribution of the error term
$\varepsilon$ by the empirical distribution of the residuals $Y_i-\hat m(X_i)$,
$i=1,\ldots,n$. To approximate the distribution of this estimator, Koul and
Lahiri (1994) and Neumeyer (2008, 2009) proposed bootstrap procedures, based on
smoothing the residuals either before or after drawing bootstrap samples. So
far it has been an open question whether a classical non-smooth residual
bootstrap is asymptotically valid in this context. In this paper we solve this
open problem, and show that the non-smooth residual bootstrap is consistent. We
illustrate this theoretical result by means of simulations, that show the
accuracy of this bootstrap procedure for various models, testing procedures and
sample sizes.
",0,0,1,1,0,0
1815,Polynomiality for the Poisson centre of truncated maximal parabolic subalgebras,"  We show that the Poisson centre of truncated maximal parabolic subalgebras of
a simple Lie algebra of type B, D and E_6 is a polynomial algebra.
In roughly half of the cases the polynomiality of the Poisson centre was
already known by a completely different method.
For the rest of the cases, our approach is to construct an algebraic slice in
the sense of Kostant given by an adapted pair and the computation of an
improved upper bound for the Poisson centre.
",0,0,1,0,0,0
1816,Row-Centric Lossless Compression of Markov Images,"  Motivated by the question of whether the recently introduced Reduced Cutset
Coding (RCC) offers rate-complexity performance benefits over conventional
context-based conditional coding for sources with two-dimensional Markov
structure, this paper compares several row-centric coding strategies that vary
in the amount of conditioning as well as whether a model or an empirical table
is used in the encoding of blocks of rows. The conclusion is that, at least for
sources exhibiting low-order correlations, 1-sided model-based conditional
coding is superior to the method of RCC for a given constraint on complexity,
and conventional context-based conditional coding is nearly as good as the
1-sided model-based coding.
",1,0,0,0,0,0
1817,Planetesimal formation by the streaming instability in a photoevaporating disk,"  Recent years have seen growing interest in the streaming instability as a
candidate mechanism to produce planetesimals. However, these investigations
have been limited to small-scale simulations. We now present the results of a
global protoplanetary disk evolution model that incorporates planetesimal
formation by the streaming instability, along with viscous accretion,
photoevaporation by EUV, FUV, and X-ray photons, dust evolution, the water ice
line, and stratified turbulence. Our simulations produce massive (60-130
$M_\oplus$) planetesimal belts beyond 100 au and up to $\sim 20 M_\oplus$ of
planetesimals in the middle regions (3-100 au). Our most comprehensive model
forms 8 $M_\oplus$ of planetesimals inside 3 au, where they can give rise to
terrestrial planets. The planetesimal mass formed in the inner disk depends
critically on the timing of the formation of an inner cavity in the disk by
high-energy photons. Our results show that the combination of photoevaporation
and the streaming instability are efficient at converting the solid component
of protoplanetary disks into planetesimals. Our model, however, does not form
enough early planetesimals in the inner and middle regions of the disk to give
rise to giant planets and super-Earths with gaseous envelopes. Additional
processes such as particle pileups and mass loss driven by MHD winds may be
needed to drive the formation of early planetesimal generations in the planet
forming regions of protoplanetary disks.
",0,1,0,0,0,0
1818,Fault Tolerant Thermal Control of Steam Turbine Shell Deflections,"  The metal-to-metal clearances of a steam turbine during full or part load
operation are among the main drivers of efficiency. The requirement to add
clearances is driven by a number of factors including the relative movements of
the steam turbine shell and rotor during transient conditions such as startup
and shutdown. This paper includes a description of a control algorithm to
manage external heating blankets for the thermal control of the shell
deflections during turbine shutdown. The proposed method is tolerant of changes
in the heat loss characteristics of the system as well as simultaneous
component failures.
",1,0,0,0,0,0
1819,Causal Mediation Analysis Leveraging Multiple Types of Summary Statistics Data,"  Summary statistics of genome-wide association studies (GWAS) teach causal
relationship between millions of genetic markers and tens and thousands of
phenotypes. However, underlying biological mechanisms are yet to be elucidated.
We can achieve necessary interpretation of GWAS in a causal mediation
framework, looking to establish a sparse set of mediators between genetic and
downstream variables, but there are several challenges. Unlike existing methods
rely on strong and unrealistic assumptions, we tackle practical challenges
within a principled summary-based causal inference framework. We analyzed the
proposed methods in extensive simulations generated from real-world genetic
data. We demonstrated only our approach can accurately redeem causal genes,
even without knowing actual individual-level data, despite the presence of
competing non-causal trails.
",1,0,0,1,1,0
1820,Causal Queries from Observational Data in Biological Systems via Bayesian Networks: An Empirical Study in Small Networks,"  Biological networks are a very convenient modelling and visualisation tool to
discover knowledge from modern high-throughput genomics and postgenomics data
sets. Indeed, biological entities are not isolated, but are components of
complex multi-level systems. We go one step further and advocate for the
consideration of causal representations of the interactions in living
systems.We present the causal formalism and bring it out in the context of
biological networks, when the data is observational. We also discuss its
ability to decipher the causal information flow as observed in gene expression.
We also illustrate our exploration by experiments on small simulated networks
as well as on a real biological data set.
",0,0,0,1,1,0
1821,Hierarchical Bloom Filter Trees for Approximate Matching,"  Bytewise approximate matching algorithms have in recent years shown
significant promise in de- tecting files that are similar at the byte level.
This is very useful for digital forensic investigators, who are regularly faced
with the problem of searching through a seized device for pertinent data. A
common scenario is where an investigator is in possession of a collection of
""known-illegal"" files (e.g. a collection of child abuse material) and wishes to
find whether copies of these are stored on the seized device. Approximate
matching addresses shortcomings in traditional hashing, which can only find
identical files, by also being able to deal with cases of merged files,
embedded files, partial files, or if a file has been changed in any way.
Most approximate matching algorithms work by comparing pairs of files, which
is not a scalable approach when faced with large corpora. This paper
demonstrates the effectiveness of using a ""Hierarchical Bloom Filter Tree""
(HBFT) data structure to reduce the running time of
collection-against-collection matching, with a specific focus on the MRSH-v2
algorithm. Three experiments are discussed, which explore the effects of
different configurations of HBFTs. The proposed approach dramatically reduces
the number of pairwise comparisons required, and demonstrates substantial speed
gains, while maintaining effectiveness.
",1,0,0,0,0,0
1822,GANDALF - Graphical Astrophysics code for N-body Dynamics And Lagrangian Fluids,"  GANDALF is a new hydrodynamics and N-body dynamics code designed for
investigating planet formation, star formation and star cluster problems.
GANDALF is written in C++, parallelised with both OpenMP and MPI and contains a
python library for analysis and visualisation. The code has been written with a
fully object-oriented approach to easily allow user-defined implementations of
physics modules or other algorithms. The code currently contains
implementations of Smoothed Particle Hydrodynamics, Meshless Finite-Volume and
collisional N-body schemes, but can easily be adapted to include additional
particle schemes. We present in this paper the details of its implementation,
results from the test suite, serial and parallel performance results and
discuss the planned future development. The code is freely available as an open
source project on the code-hosting website github at
this https URL and is available under the GPLv2
license.
",0,1,0,0,0,0
1823,Pre-freezing transition in Boltzmann-Gibbs measures associated with log-correlated fields,"  We consider Boltzmann-Gibbs measures associated with log-correlated Gaussian
fields as potentials and study their multifractal properties which exhibit
phase transitions. In particular, the pre-freezing and freezing phenomena of
the annealed exponent, predicted by Fyodorov using a modified
replica-symmetry-breaking ansatz, are generalised to arbitrary dimension and
verified using results from Gaussian multiplicative chaos theory.
",0,1,0,0,0,0
1824,Learning Combinatorial Optimization Algorithms over Graphs,"  The design of good heuristics or approximation algorithms for NP-hard
combinatorial optimization problems often requires significant specialized
knowledge and trial-and-error. Can we automate this challenging, tedious
process, and learn the algorithms instead? In many real-world applications, it
is typically the case that the same optimization problem is solved again and
again on a regular basis, maintaining the same problem structure but differing
in the data. This provides an opportunity for learning heuristic algorithms
that exploit the structure of such recurring problems. In this paper, we
propose a unique combination of reinforcement learning and graph embedding to
address this challenge. The learned greedy policy behaves like a meta-algorithm
that incrementally constructs a solution, and the action is determined by the
output of a graph embedding network capturing the current state of the
solution. We show that our framework can be applied to a diverse range of
optimization problems over graphs, and learns effective algorithms for the
Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.
",1,0,0,1,0,0
1825,Optimal Oil Production and Taxation in Presence of Global Disruptions,"  This paper studies the optimal extraction policy of an oil field as well as
the efficient taxation of the revenues generated. Taking into account the fact
that the oil price in worldwide commodity markets fluctuates randomly following
global and seasonal macroeconomic parameters, we model the evolution of the oil
price as a mean reverting regime-switching jump diffusion process. Given that
oil producing countries rely on oil sale revenues as well as taxes levied on
oil companies for a good portion of the revenue side of their budgets, we
formulate this problem as a differential game where the two players are the
mining company whose aim is to maximize the revenues generated from its
extracting activities and the government agency in charge of regulating and
taxing natural resources. We prove the existence of a Nash equilibrium and the
convergence of an approximating scheme for the value functions. Furthermore,
optimal extraction and fiscal policies that should be applied when the
equilibrium is reached are derived.A numerical example is presented to
illustrate these results.
",0,0,1,0,0,0
1826,Critical well-posedness and scattering results for fractional Hartree-type equations,"  Scattering for the mass-critical fractional Schr??dinger equation with a
cubic Hartree-type nonlinearity for initial data in a small ball in the
scale-invariant space of three-dimensional radial and square-integrable initial
data is established. For this, we prove a bilinear estimate for free solutions
and extend it to perturbations of bounded quadratic variation. This result is
shown to be sharp by proving the unboundedness of a third order derivative of
the flow map in the super-critical range.
",0,0,1,0,0,0
1827,Lightweight Multilingual Software Analysis,"  Developer preferences, language capabilities and the persistence of older
languages contribute to the trend that large software codebases are often
multilingual, that is, written in more than one computer language. While
developers can leverage monolingual software development tools to build
software components, companies are faced with the problem of managing the
resultant large, multilingual codebases to address issues with security,
efficiency, and quality metrics. The key challenge is to address the opaque
nature of the language interoperability interface: one language calling
procedures in a second (which may call a third, or even back to the first),
resulting in a potentially tangled, inefficient and insecure codebase. An
architecture is proposed for lightweight static analysis of large multilingual
codebases: the MLSA architecture. Its modular and table-oriented structure
addresses the open-ended nature of multiple languages and language
interoperability APIs. We focus here as an application on the construction of
call-graphs that capture both inter-language and intra-language calls. The
algorithms for extracting multilingual call-graphs from codebases are
presented, and several examples of multilingual software engineering analysis
are discussed. The state of the implementation and testing of MLSA is
presented, and the implications for future work are discussed.
",1,0,0,0,0,0
1828,Room-temperature 1.54 $?¬$m photoluminescence of Er:O$_x$ centers at extremely low concentration in silicon,"  The demand for single photon sources at $\lambda~=~1.54~\mu$m, which follows
from the consistent development of quantum networks based on commercial optical
fibers, makes Er:O$_x$ centers in Si still a viable resource thanks to the
optical transition of $Er^{3+}~:~^4I_{13/2}~\rightarrow~^4I_{15/2}$. Yet, to
date, the implementation of such system remains hindered by its extremely low
emission rate. In this Letter, we explore the room-temperature
photoluminescence (PL) at the telecomm wavelength of very low implantation
doses of $Er:O_x$ in $Si$. The emitted photons, excited by a $\lambda~=~792~nm$
laser in both large areas and confined dots of diameter down to $5~\mu$m, are
collected by an inverted confocal microscope. The lower-bound number of
detectable emission centers within our diffraction-limited illumination spot is
estimated to be down to about 10$^4$, corresponding to an emission rate per
individual ion of about $4~\times~10^{3}$ photons/s.
",0,1,0,0,0,0
1829,Sparse Algorithm for Robust LSSVM in Primal Space,"  As enjoying the closed form solution, least squares support vector machine
(LSSVM) has been widely used for classification and regression problems having
the comparable performance with other types of SVMs. However, LSSVM has two
drawbacks: sensitive to outliers and lacking sparseness. Robust LSSVM (R-LSSVM)
overcomes the first partly via nonconvex truncated loss function, but the
current algorithms for R-LSSVM with the dense solution are faced with the
second drawback and are inefficient for training large-scale problems. In this
paper, we interpret the robustness of R-LSSVM from a re-weighted viewpoint and
give a primal R-LSSVM by the representer theorem. The new model may have sparse
solution if the corresponding kernel matrix has low rank. Then approximating
the kernel matrix by a low-rank matrix and smoothing the loss function by
entropy penalty function, we propose a convergent sparse R-LSSVM (SR-LSSVM)
algorithm to achieve the sparse solution of primal R-LSSVM, which overcomes two
drawbacks of LSSVM simultaneously. The proposed algorithm has lower complexity
than the existing algorithms and is very efficient for training large-scale
problems. Many experimental results illustrate that SR-LSSVM can achieve better
or comparable performance with less training time than related algorithms,
especially for training large scale problems.
",1,0,0,1,0,0
1830,Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement Learning,"  We consider the networked multi-agent reinforcement learning (MARL) problem
in a fully decentralized setting, where agents learn to coordinate to achieve
the joint success. This problem is widely encountered in many areas including
traffic control, distributed control, and smart grids. We assume that the
reward function for each agent can be different and observed only locally by
the agent itself. Furthermore, each agent is located at a node of a
communication network and can exchanges information only with its neighbors.
Using softmax temporal consistency and a decentralized optimization method, we
obtain a principled and data-efficient iterative algorithm. In the first step
of each iteration, an agent computes its local policy and value gradients and
then updates only policy parameters. In the second step, the agent propagates
to its neighbors the messages based on its value function and then updates its
own value function. Hence we name the algorithm value propagation. We prove a
non-asymptotic convergence rate 1/T with the nonlinear function approximation.
To the best of our knowledge, it is the first MARL algorithm with convergence
guarantee in the control, off-policy and non-linear function approximation
setting. We empirically demonstrate the effectiveness of our approach in
experiments.
",1,0,0,1,0,0
1831,"Collect at Once, Use Effectively: Making Non-interactive Locally Private Learning Possible","  Non-interactive Local Differential Privacy (LDP) requires data analysts to
collect data from users through noisy channel at once. In this paper, we extend
the frontiers of Non-interactive LDP learning and estimation from several
aspects. For learning with smooth generalized linear losses, we propose an
approximate stochastic gradient oracle estimated from non-interactive LDP
channel, using Chebyshev expansion. Combined with inexact gradient methods, we
obtain an efficient algorithm with quasi-polynomial sample complexity bound.
For the high-dimensional world, we discover that under $\ell_2$-norm assumption
on data points, high-dimensional sparse linear regression and mean estimation
can be achieved with logarithmic dependence on dimension, using random
projection and approximate recovery. We also extend our methods to Kernel Ridge
Regression. Our work is the first one that makes learning and estimation
possible for a broad range of learning tasks under non-interactive LDP model.
",1,0,0,0,0,0
1832,Learning Independent Causal Mechanisms,"  Statistical learning relies upon data sampled from a distribution, and we
usually do not care what actually generated it in the first place. From the
point of view of causal modeling, the structure of each distribution is induced
by physical mechanisms that give rise to dependences between observables.
Mechanisms, however, can be meaningful autonomous modules of generative models
that make sense beyond a particular entailed data distribution, lending
themselves to transfer between problems. We develop an algorithm to recover a
set of independent (inverse) mechanisms from a set of transformed data points.
The approach is unsupervised and based on a set of experts that compete for
data generated by the mechanisms, driving specialization. We analyze the
proposed method in a series of experiments on image data. Each expert learns to
map a subset of the transformed data back to a reference distribution. The
learned mechanisms generalize to novel domains. We discuss implications for
transfer learning and links to recent trends in generative modeling.
",1,0,0,1,0,0
1833,"A Bayesian Model for False Information Belief Impact, Optimal Design, and Fake News Containment","  This work is a technical approach to modeling false information nature,
design, belief impact and containment in multi-agent networks. We present a
Bayesian mathematical model for source information and viewer's belief, and how
the former impacts the latter in a media (network) of broadcasters and viewers.
Given the proposed model, we study how a particular information (true or false)
can be optimally designed into a report, so that on average it conveys the most
amount of the original intended information to the viewers of the network.
Consequently, the model allows us to study susceptibility of a particular group
of viewers to false information, as a function of statistical metrics of the
their prior beliefs (e.g. bias, hesitation, open-mindedness, credibility
assessment etc.). In addition, based on the same model we can study false
information ""containment"" strategies imposed by network administrators.
Specifically, we study a credibility assessment strategy, where every
disseminated report must be within a certain distance of the truth. We study
the trade-off between false and true information-belief convergence using this
scheme which leads to ways for optimally deciding how truth sensitive an
information dissemination network should operate.
",1,0,0,0,0,0
1834,Topological dynamics of gyroscopic and Floquet lattices from Newton's laws,"  Despite intense interest in realizing topological phases across a variety of
electronic, photonic and mechanical platforms, the detailed microscopic origin
of topological behavior often remains elusive. To bridge this conceptual gap,
we show how hallmarks of topological modes - boundary localization and
chirality - emerge from Newton's laws in mechanical topological systems. We
first construct a gyroscopic lattice with analytically solvable edge modes, and
show how the Lorentz and spring restoring forces conspire to support very
robust ""dangling bond"" boundary modes. The chirality and locality of these
modes intuitively emerges from microscopic balancing of restoring forces and
cyclotron tendencies. Next, we introduce the highlight of this work, a very
experimentally realistic mechanical non-equilibrium (Floquet) Chern lattice
driven by AC electromagnets. Through appropriate synchronization of the AC
driving protocol, the Floquet lattice is ""pushed around"" by a rotating
potential analogous to an object washed ashore by water waves. Besides hosting
""dangling bond"" chiral modes analogous to the gyroscopic boundary modes, our
Floquet Chern lattice also supports peculiar half-period chiral modes with no
static analog. With key parameters controlled electronically, our setup has the
advantage of being dynamically tunable for applications involving arbitrary
Floquet modulations. The physical intuition gleaned from our two prototypical
topological systems are applicable not just to arbitrarily complicated
mechanical systems, but also photonic and electrical topological setups.
",0,1,1,0,0,0
1835,Stability of axisymmetric chiral skyrmions,"  We examine topological solitons in a minimal variational model for a chiral
magnet, so-called chiral skyrmions. In the regime of large background fields,
we prove linear stability of axisymmetric chiral skyrmions under arbitrary
perturbations in the energy space, a long-standing open question in physics
literature. Moreover, we show strict local minimality of axisymmetric chiral
skyrmions and nearby existence of moving soliton solution for the
Landau-Lifshitz-Gilbert equation driven by a small spin transfer torque.
",0,0,1,0,0,0
1836,Efficiency versus instability in plasma accelerators,"  Plasma wake-field acceleration is one of the main technologies being
developed for future high-energy colliders. Potentially, it can create a
cost-effective path to the highest possible energies for e+e- or
{\gamma}-{\gamma} colliders and produce a profound effect on the developments
for high-energy physics. Acceleration in a blowout regime, where all plasma
electrons are swept away from the axis, is presently considered to be the
primary choice for beam acceleration. In this paper, we derive a universal
efficiency-instability relation, between the power efficiency and the key
instability parameter of the trailing bunch for beam acceleration in the
blowout regime. We also show that the suppression of instability in the
trailing bunch can be achieved through BNS damping by the introduction of a
beam energy variation along the bunch. Unfortunately, in the high efficiency
regime, the required energy variation is quite high, and is not presently
compatible with collider-quality beams. We would like to stress that the
development of the instability imposes a fundamental limitation on the
acceleration efficiency, and it is unclear how it could be overcome for
high-luminosity linear colliders. With minor modifications, the considered
limitation on the power efficiency is applicable to other types of
acceleration.
",0,1,0,0,0,0
1837,Resistivity bound for hydrodynamic bad metals,"  We obtain a rigorous upper bound on the resistivity $\rho$ of an electron
fluid whose electronic mean free path is short compared to the scale of spatial
inhomogeneities. When such a hydrodynamic electron fluid supports a non-thermal
diffusion process -- such as an imbalance mode between different bands -- we
show that the resistivity bound becomes $\rho \lesssim A \, \Gamma$. The
coefficient $A$ is independent of temperature and inhomogeneity lengthscale,
and $\Gamma$ is a microscopic momentum-preserving scattering rate. In this way
we obtain a unified and novel mechanism -- without umklapp -- for $\rho \sim
T^2$ in a Fermi liquid and the crossover to $\rho \sim T$ in quantum critical
regimes. This behavior is widely observed in transition metal oxides, organic
metals, pnictides and heavy fermion compounds and has presented a longstanding
challenge to transport theory. Our hydrodynamic bound allows phonon
contributions to diffusion constants, including thermal diffusion, to directly
affect the electrical resistivity.
",0,1,0,0,0,0
1838,Minimal Exploration in Structured Stochastic Bandits,"  This paper introduces and addresses a wide class of stochastic bandit
problems where the function mapping the arm to the corresponding reward
exhibits some known structural properties. Most existing structures (e.g.
linear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our
framework. We derive an asymptotic instance-specific regret lower bound for
these problems, and develop OSSB, an algorithm whose regret matches this
fundamental limit. OSSB is not based on the classical principle of ""optimism in
the face of uncertainty"" or on Thompson sampling, and rather aims at matching
the minimal exploration rates of sub-optimal arms as characterized in the
derivation of the regret lower bound. We illustrate the efficiency of OSSB
using numerical experiments in the case of the linear bandit problem and show
that OSSB outperforms existing algorithms, including Thompson sampling.
",1,0,0,1,0,0
1839,Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial Domains,"  While modern day web applications aim to create impact at the civilization
level, they have become vulnerable to adversarial activity, where the next
cyber-attack can take any shape and can originate from anywhere. The increasing
scale and sophistication of attacks, has prompted the need for a data driven
solution, with machine learning forming the core of many cybersecurity systems.
Machine learning was not designed with security in mind, and the essential
assumption of stationarity, requiring that the training and testing data follow
similar distributions, is violated in an adversarial domain. In this paper, an
adversary's view point of a classification based system, is presented. Based on
a formal adversarial model, the Seed-Explore-Exploit framework is presented,
for simulating the generation of data driven and reverse engineering attacks on
classifiers. Experimental evaluation, on 10 real world datasets and using the
Google Cloud Prediction Platform, demonstrates the innate vulnerability of
classifiers and the ease with which evasion can be carried out, without any
explicit information about the classifier type, the training data or the
application domain. The proposed framework, algorithms and empirical
evaluation, serve as a white hat analysis of the vulnerabilities, and aim to
foster the development of secure machine learning frameworks.
",1,0,0,1,0,0
1840,On Optimistic versus Randomized Exploration in Reinforcement Learning,"  We discuss the relative merits of optimistic and randomized approaches to
exploration in reinforcement learning. Optimistic approaches presented in the
literature apply an optimistic boost to the value estimate at each state-action
pair and select actions that are greedy with respect to the resulting
optimistic value function. Randomized approaches sample from among
statistically plausible value functions and select actions that are greedy with
respect to the random sample. Prior computational experience suggests that
randomized approaches can lead to far more statistically efficient learning. We
present two simple analytic examples that elucidate why this is the case. In
principle, there should be optimistic approaches that fare well relative to
randomized approaches, but that would require intractable computation.
Optimistic approaches that have been proposed in the literature sacrifice
statistical efficiency for the sake of computational efficiency. Randomized
approaches, on the other hand, may enable simultaneous statistical and
computational efficiency.
",1,0,0,1,0,0
1841,Fast Monte-Carlo Localization on Aerial Vehicles using Approximate Continuous Belief Representations,"  Size, weight, and power constrained platforms impose constraints on
computational resources that introduce unique challenges in implementing
localization algorithms. We present a framework to perform fast localization on
such platforms enabled by the compressive capabilities of Gaussian Mixture
Model representations of point cloud data. Given raw structural data from a
depth sensor and pitch and roll estimates from an on-board attitude reference
system, a multi-hypothesis particle filter localizes the vehicle by exploiting
the likelihood of the data originating from the mixture model. We demonstrate
analysis of this likelihood in the vicinity of the ground truth pose and detail
its utilization in a particle filter-based vehicle localization strategy, and
later present results of real-time implementations on a desktop system and an
off-the-shelf embedded platform that outperform localization results from
running a state-of-the-art algorithm on the same environment.
",1,0,0,0,0,0
1842,Generalized two-field $?ñ$-attractor models from geometrically finite hyperbolic surfaces,"  We consider four-dimensional gravity coupled to a non-linear sigma model
whose scalar manifold is a non-compact geometrically finite surface $\Sigma$
endowed with a Riemannian metric of constant negative curvature. When the
space-time is an FLRW universe, such theories produce a very wide
generalization of two-field $\alpha$-attractor models, being parameterized by a
positive constant $\alpha$, by the choice of a finitely-generated surface group
$\Gamma\subset \mathrm{PSL}(2,\mathbb{R})$ (which is isomorphic with the
fundamental group of $\Sigma$) and by the choice of a scalar potential defined
on $\Sigma$. The traditional two-field $\alpha$-attractor models arise when
$\Gamma$ is the trivial group, in which case $\Sigma$ is the Poincar?? disk.
We give a general prescription for the study of such models through
uniformization in the so-called ""non-elementary"" case and discuss some of their
qualitative features in the gradient flow approximation, which we relate to
Morse theory. We also discuss some aspects of the SRST approximation in these
models, showing that it is generally not well-suited for studying dynamics near
cusp ends. When $\Sigma$ is non-compact and the scalar potential is
""well-behaved"" at the ends, we show that, in the {\em naive} local one-field
truncation, our generalized models have the same universal behavior as ordinary
one-field $\alpha$-attractors if inflation happens near any of the ends of
$\Sigma$ where the extended potential has a local maximum, for trajectories
which are well approximated by non-canonically parameterized geodesics near the
ends, we also discuss spiral trajectories near the ends.
",0,1,1,0,0,0
1843,The Geodetic Hull Number is Hard for Chordal Graphs,"  We show the hardness of the geodetic hull number for chordal graphs.
",1,0,0,0,0,0
1844,"$\overline{M}_{1,n}$ is usually not uniruled in characteristic $p$","  Using etale cohomology, we define a birational invariant for varieties in
characteristic $p$ that serves as an obstruction to uniruledness - a variant on
an obstruction to unirationality due to Ekedahl. We apply this to
$\overline{M}_{1,n}$ and show that $\overline{M}_{1,n}$ is not uniruled in
characteristic $p$ as long as $n \geq p \geq 11$. To do this, we use Deligne's
description of the etale cohomology of $\overline{M}_{1,n}$ and apply the
theory of congruences between modular forms.
",0,0,1,0,0,0
1845,Active Community Detection: A Maximum Likelihood Approach,"  We propose novel semi-supervised and active learning algorithms for the
problem of community detection on networks. The algorithms are based on
optimizing the likelihood function of the community assignments given a graph
and an estimate of the statistical model that generated it. The optimization
framework is inspired by prior work on the unsupervised community detection
problem in Stochastic Block Models (SBM) using Semi-Definite Programming (SDP).
In this paper we provide the next steps in the evolution of learning
communities in this context which involves a constrained semi-definite
programming algorithm, and a newly presented active learning algorithm. The
active learner intelligently queries nodes that are expected to maximize the
change in the model likelihood. Experimental results show that this active
learning algorithm outperforms the random-selection semi-supervised version of
the same algorithm as well as other state-of-the-art active learning
algorithms. Our algorithms significantly improved performance is demonstrated
on both real-world and SBM-generated networks even when the SBM has a signal to
noise ratio (SNR) below the known unsupervised detectability threshold.
",1,0,0,1,0,0
1846,Continuum Limit of Posteriors in Graph Bayesian Inverse Problems,"  We consider the problem of recovering a function input of a differential
equation formulated on an unknown domain $M$. We assume to have access to a
discrete domain $M_n=\{x_1, \dots, x_n\} \subset M$, and to noisy measurements
of the output solution at $p\le n$ of those points. We introduce a graph-based
Bayesian inverse problem, and show that the graph-posterior measures over
functions in $M_n$ converge, in the large $n$ limit, to a posterior over
functions in $M$ that solves a Bayesian inverse problem with known domain.
The proofs rely on the variational formulation of the Bayesian update, and on
a new topology for the study of convergence of measures over functions on point
clouds to a measure over functions on the continuum. Our framework, techniques,
and results may serve to lay the foundations of robust uncertainty
quantification of graph-based tasks in machine learning. The ideas are
presented in the concrete setting of recovering the initial condition of the
heat equation on an unknown manifold.
",0,0,1,1,0,0
1847,Automatic Conflict Detection in Police Body-Worn Audio,"  Automatic conflict detection has grown in relevance with the advent of
body-worn technology, but existing metrics such as turn-taking and overlap are
poor indicators of conflict in police-public interactions. Moreover, standard
techniques to compute them fall short when applied to such diversified and
noisy contexts. We develop a pipeline catered to this task combining adaptive
noise removal, non-speech filtering and new measures of conflict based on the
repetition and intensity of phrases in speech. We demonstrate the effectiveness
of our approach on body-worn audio data collected by the Los Angeles Police
Department.
",1,0,0,1,0,0
1848,The cobordism hypothesis,"  Assuming a conjecture about factorization homology with adjoints, we prove
the cobordism hypothesis, after Baez-Dolan, Costello, Hopkins-Lurie, and Lurie.
",0,0,1,0,0,0
1849,LAMOST telescope reveals that Neptunian cousins of hot Jupiters are mostly single offspring of stars that are rich in heavy elements,"  We discover a population of short-period, Neptune-size planets sharing key
similarities with hot Jupiters: both populations are preferentially hosted by
metal-rich stars, and both are preferentially found in Kepler systems with
single transiting planets. We use accurate LAMOST DR4 stellar parameters for
main-sequence stars to study the distributions of short-period 1d < P < 10d
Kepler planets as a function of host star metallicity. The radius distribution
of planets around metal-rich stars is more ""puffed up"" as compared to that
around metal-poor hosts. In two period-radius regimes, planets preferentially
reside around metal-rich stars, while there are hardly any planets around
metal-poor stars. One is the well-known hot Jupiters, and the other is a
population of Neptune-size planets (2 R_Earth <~ R_p <~ 6 R_Earth), dubbed as
""Hoptunes"". Also like hot Jupiters, Hoptunes occur more frequently in systems
with single transiting planets though the fraction of Hoptunes occurring in
multiples is larger than that of hot Jupiters. About 1% of solar-type stars
host ""Hoptunes"", and the frequencies of Hoptunes and hot Jupiters increase with
consistent trends as a function of [Fe/H]. In the planet radius distribution,
hot Jupiters and Hoptunes are separated by a ""valley"" at approximately Saturn
size (in the range of 6 R_Earth <~ R_p <~ 10 R_Earth), and this ""hot-Saturn
valley"" represents approximately an order-of-magnitude decrease in planet
frequency compared to hot Jupiters and Hoptunes. The empirical ""kinship""
between Hoptunes and hot Jupiters suggests likely common processes (migration
and/or formation) responsible for their existence.
",0,1,0,0,0,0
1850,A Latent Variable Model for Two-Dimensional Canonical Correlation Analysis and its Variational Inference,"  Describing the dimension reduction (DR) techniques by means of probabilistic
models has recently been given special attention. Probabilistic models, in
addition to a better interpretability of the DR methods, provide a framework
for further extensions of such algorithms. One of the new approaches to the
probabilistic DR methods is to preserving the internal structure of data. It is
meant that it is not necessary that the data first be converted from the matrix
or tensor format to the vector format in the process of dimensionality
reduction. In this paper, a latent variable model for matrix-variate data for
canonical correlation analysis (CCA) is proposed. Since in general there is not
any analytical maximum likelihood solution for this model, we present two
approaches for learning the parameters. The proposed methods are evaluated
using the synthetic data in terms of convergence and quality of mappings. Also,
real data set is employed for assessing the proposed methods with several
probabilistic and none-probabilistic CCA based approaches. The results confirm
the superiority of the proposed methods with respect to the competing
algorithms. Moreover, this model can be considered as a framework for further
extensions.
",1,0,0,1,0,0
1851,Model enumeration in propositional circumscription via unsatisfiable core analysis,"  Many practical problems are characterized by a preference relation over
admissible solutions, where preferred solutions are minimal in some sense. For
example, a preferred diagnosis usually comprises a minimal set of reasons that
is sufficient to cause the observed anomaly. Alternatively, a minimal
correction subset comprises a minimal set of reasons whose deletion is
sufficient to eliminate the observed anomaly. Circumscription formalizes such
preference relations by associating propositional theories with minimal models.
The resulting enumeration problem is addressed here by means of a new algorithm
taking advantage of unsatisfiable core analysis. Empirical evidence of the
efficiency of the algorithm is given by comparing the performance of the
resulting solver, CIRCUMSCRIPTINO, with HCLASP, CAMUS MCS, LBX and MCSLS on the
enumeration of minimal models for problems originating from practical
applications.
This paper is under consideration for acceptance in TPLP.
",1,0,0,0,0,0
1852,Structured Neural Summarization,"  Summarization of long sequences into a concise statement is a core problem in
natural language processing, requiring non-trivial understanding of the input.
Based on the promising results of graph neural networks on highly structured
data, we develop a framework to extend existing sequence encoders with a graph
component that can reason about long-distance relationships in weakly
structured data such as text. In an extensive evaluation, we show that the
resulting hybrid sequence-graph models outperform both pure sequence models as
well as pure graph models on a range of summarization tasks.
",1,0,0,0,0,0
1853,Variations on a Visserian Theme,"  A first order theory T is said to be ""tight"" if for any two deductively
closed extensions U and V of T (both of which are formulated in the language of
T), U and V are bi-interpretable iff U = V. By a theorem of Visser, PA (Peano
Arithmetic) is tight. Here we show that Z_2 (second order arithmetic), ZF
(Zermelo-Fraenkel set theory), and KM (Kelley-Morse theory of classes) are also
tight theories.
",0,0,1,0,0,0
1854,"Galerkin Least-Squares Stabilization in Ice Sheet Modeling - Accuracy, Robustness, and Comparison to other Techniques","  We investigate the accuracy and robustness of one of the most common methods
used in glaciology for the discretization of the $\mathfrak{p}$-Stokes
equations: equal order finite elements with Galerkin Least-Squares (GLS)
stabilization. Furthermore we compare the results to other stabilized methods.
We find that the vertical velocity component is more sensitive to the choice of
GLS stabilization parameter than horizontal velocity. Additionally, the
accuracy of the vertical velocity component is especially important since
errors in this component can cause ice surface instabilities and propagate into
future ice volume predictions. If the element cell size is set to the minimum
edge length and the stabilization parameter is allowed to vary non-linearly
with viscosity, the GLS stabilization parameter found in literature is a good
choice on simple domains. However, near ice margins the standard parameter
choice may result in significant oscillations in the vertical component of the
surface velocity. For these cases, other stabilization techniques, such as the
interior penalty method, result in better accuracy and are less sensitive to
the choice of the stabilization parameter. During this work we also discovered
that the manufactured solutions often used to evaluate errors in glaciology are
not reliable due to high artificial surface forces at singularities. We perform
our numerical experiments in both FEniCS and Elmer/Ice.
",0,1,0,0,0,0
1855,Improved Query Reformulation for Concept Location using CodeRank and Document Structures,"  During software maintenance, developers usually deal with a significant
number of software change requests. As a part of this, they often formulate an
initial query from the request texts, and then attempt to map the concepts
discussed in the request to relevant source code locations in the software
system (a.k.a., concept location). Unfortunately, studies suggest that they
often perform poorly in choosing the right search terms for a change task. In
this paper, we propose a novel technique --ACER-- that takes an initial query,
identifies appropriate search terms from the source code using a novel term
weight --CodeRank, and then suggests effective reformulation to the initial
query by exploiting the source document structures, query quality analysis and
machine learning. Experiments with 1,675 baseline queries from eight subject
systems report that our technique can improve 71% of the baseline queries which
is highly promising. Comparison with five closely related existing techniques
in query reformulation not only validates our empirical findings but also
demonstrates the superiority of our technique.
",1,0,0,0,0,0
1856,High-performance parallel computing in the classroom using the public goods game as an example,"  The use of computers in statistical physics is common because the sheer
number of equations that describe the behavior of an entire system particle by
particle often makes it impossible to solve them exactly. Monte Carlo methods
form a particularly important class of numerical methods for solving problems
in statistical physics. Although these methods are simple in principle, their
proper use requires a good command of statistical mechanics, as well as
considerable computational resources. The aim of this paper is to demonstrate
how the usage of widely accessible graphics cards on personal computers can
elevate the computing power in Monte Carlo simulations by orders of magnitude,
thus allowing live classroom demonstration of phenomena that would otherwise be
out of reach. As an example, we use the public goods game on a square lattice
where two strategies compete for common resources in a social dilemma
situation. We show that the second-order phase transition to an absorbing phase
in the system belongs to the directed percolation universality class, and we
compare the time needed to arrive at this result by means of the main processor
and by means of a suitable graphics card. Parallel computing on graphics
processing units has been developed actively during the last decade, to the
point where today the learning curve for entry is anything but steep for those
familiar with programming. The subject is thus ripe for inclusion in graduate
and advanced undergraduate curricula, and we hope that this paper will
facilitate this process in the realm of physics education. To that end, we
provide a documented source code for an easy reproduction of presented results
and for further development of Monte Carlo simulations of similar systems.
",0,1,0,0,0,0
1857,Coupled spin-charge dynamics in helical Fermi liquids beyond the random phase approximation,"  We consider a helical system of fermions with a generic spin (or pseudospin)
orbit coupling. Using the equation of motion approach for the single-particle
distribution functions, and a mean-field decoupling of the higher order
distribution functions, we find a closed form for the charge and spin density
fluctuations in terms of the charge and spin density linear response functions.
Approximating the nonlocal exchange term with a Hubbard-like local-field
factor, we obtain coupled spin and charge density response matrix beyond the
random phase approximation, whose poles give the dispersion of four collective
spin-charge modes. We apply our generic technique to the well-explored
two-dimensional system with Rashba spin-orbit coupling and illustrate how it
gives results for the collective modes, Drude weight, and spin-Hall
conductivity which are in very good agreement with the results obtained from
other more sophisticated approaches.
",0,1,0,0,0,0
1858,Correlation decay in fermionic lattice systems with power-law interactions at non-zero temperature,"  We study correlations in fermionic lattice systems with long-range
interactions in thermal equilibrium. We prove a bound on the correlation decay
between anti-commuting operators and generalize a long-range Lieb-Robinson type
bound. Our results show that in these systems of spatial dimension $D$ with,
not necessarily translation invariant, two-site interactions decaying
algebraically with the distance with an exponent $\alpha \geq 2\,D$,
correlations between such operators decay at least algebraically with an
exponent arbitrarily close to $\alpha$ at any non-zero temperature. Our bound
is asymptotically tight, which we demonstrate by a high temperature expansion
and by numerically analyzing density-density correlations in the 1D quadratic
(free, exactly solvable) Kitaev chain with long-range pairing.
",0,1,0,0,0,0
1859,Integrated Microsimulation Framework for Dynamic Pedestrian Movement Estimation in Mobility Hub,"  We present an integrated microsimulation framework to estimate the pedestrian
movement over time and space with limited data on directional counts. Using the
activity-based approach, simulation can compute the overall demand and
trajectory of each agent, which are in accordance with the available partial
observations and are in response to the initial and evolving supply conditions
and schedules. This simulation contains a chain of processes including:
activities generation, decision point choices, and assignment. They are
considered in an iteratively updating loop so that the simulation can
dynamically correct its estimates of demand. A Markov chain is constructed for
this loop. These considerations transform the problem into a convergence
problem. A Metropolitan Hasting algorithm is then adapted to identify the
optimal solution. This framework can be used to fill the lack of data or to
model the reactions of demand to exogenous changes in the scenario. Finally, we
present a case study on Montreal Central Station, on which we tested the
developed framework and calibrated the models. We then applied it to a possible
future scenario for the same station.
",0,1,1,0,0,0
1860,Dimensionality Reduction for Stationary Time Series via Stochastic Nonconvex Optimization,"  Stochastic optimization naturally arises in machine learning. Efficient
algorithms with provable guarantees, however, are still largely missing, when
the objective function is nonconvex and the data points are dependent. This
paper studies this fundamental challenge through a streaming PCA problem for
stationary time series data. Specifically, our goal is to estimate the
principle component of time series data with respect to the covariance matrix
of the stationary distribution. Computationally, we propose a variant of Oja's
algorithm combined with downsampling to control the bias of the stochastic
gradient caused by the data dependency. Theoretically, we quantify the
uncertainty of our proposed stochastic algorithm based on diffusion
approximations. This allows us to prove the asymptotic rate of convergence and
further implies near optimal asymptotic sample complexity. Numerical
experiments are provided to support our analysis.
",0,0,0,1,0,0
1861,Efficient tracking of a growing number of experts,"  We consider a variation on the problem of prediction with expert advice,
where new forecasters that were unknown until then may appear at each round. As
often in prediction with expert advice, designing an algorithm that achieves
near-optimal regret guarantees is straightforward, using aggregation of
experts. However, when the comparison class is sufficiently rich, for instance
when the best expert and the set of experts itself changes over time, such
strategies naively require to maintain a prohibitive number of weights
(typically exponential with the time horizon). By contrast, designing
strategies that both achieve a near-optimal regret and maintain a reasonable
number of weights is highly non-trivial. We consider three increasingly
challenging objectives (simple regret, shifting regret and sparse shifting
regret) that extend existing notions defined for a fixed expert ensemble; in
each case, we design strategies that achieve tight regret bounds, adaptive to
the parameters of the comparison class, while being computationally
inexpensive. Moreover, our algorithms are anytime, agnostic to the number of
incoming experts and completely parameter-free. Such remarkable results are
made possible thanks to two simple but highly effective recipes: first the
""abstention trick"" that comes from the specialist framework and enables to
handle the least challenging notions of regret, but is limited when addressing
more sophisticated objectives. Second, the ""muting trick"" that we introduce to
give more flexibility. We show how to combine these two tricks in order to
handle the most challenging class of comparison strategies.
",1,0,0,1,0,0
1862,Toeplitz Inverse Covariance-Based Clustering of Multivariate Time Series Data,"  Subsequence clustering of multivariate time series is a useful tool for
discovering repeated patterns in temporal data. Once these patterns have been
discovered, seemingly complicated datasets can be interpreted as a temporal
sequence of only a small number of states, or clusters. For example, raw sensor
data from a fitness-tracking application can be expressed as a timeline of a
select few actions (i.e., walking, sitting, running). However, discovering
these patterns is challenging because it requires simultaneous segmentation and
clustering of the time series. Furthermore, interpreting the resulting clusters
is difficult, especially when the data is high-dimensional. Here we propose a
new method of model-based clustering, which we call Toeplitz Inverse
Covariance-based Clustering (TICC). Each cluster in the TICC method is defined
by a correlation network, or Markov random field (MRF), characterizing the
interdependencies between different observations in a typical subsequence of
that cluster. Based on this graphical representation, TICC simultaneously
segments and clusters the time series data. We solve the TICC problem through
alternating minimization, using a variation of the expectation maximization
(EM) algorithm. We derive closed-form solutions to efficiently solve the two
resulting subproblems in a scalable way, through dynamic programming and the
alternating direction method of multipliers (ADMM), respectively. We validate
our approach by comparing TICC to several state-of-the-art baselines in a
series of synthetic experiments, and we then demonstrate on an automobile
sensor dataset how TICC can be used to learn interpretable clusters in
real-world scenarios.
",1,0,1,0,0,0
1863,The ellipse law: Kirchhoff meets dislocations,"  In this paper we consider a nonlocal energy $I_\alpha$ whose kernel is
obtained by adding to the Coulomb potential an anisotropic term weighted by a
parameter $\alpha\in \R$. The case $\alpha=0$ corresponds to purely logarithmic
interactions, minimised by the celebrated circle law for a quadratic
confinement; $\alpha=1$ corresponds to the energy of interacting dislocations,
minimised by the semi-circle law. We show that for $\alpha\in (0,1)$ the
minimiser can be computed explicitly and is the normalised characteristic
function of the domain enclosed by an \emph{ellipse}. To prove our result we
borrow techniques from fluid dynamics, in particular those related to
Kirchhoff's celebrated result that domains enclosed by ellipses are rotating
vortex patches, called \emph{Kirchhoff ellipses}. Therefore we show a
surprising connection between vortices and dislocations.
",0,0,1,0,0,0
1864,SAML-QC: a Stochastic Assessment and Machine Learning based QC technique for Industrial Printing,"  Recently, the advancement in industrial automation and high-speed printing
has raised numerous challenges related to the printing quality inspection of
final products. This paper proposes a machine vision based technique to assess
the printing quality of text on industrial objects. The assessment is based on
three quality defects such as text misalignment, varying printing shades, and
misprinted text. The proposed scheme performs the quality inspection through
stochastic assessment technique based on the second-order statistics of
printing. First: the text-containing area on printed product is identified
through image processing techniques. Second: the alignment testing of the
identified text-containing area is performed. Third: optical character
recognition is performed to divide the text into different small boxes and only
the intensity value of each text-containing box is taken as a random variable
and second-order statistics are estimated to determine the varying printing
defects in the text under one, two and three sigma thresholds. Fourth: the
K-Nearest Neighbors based supervised machine learning is performed to provide
the stochastic process for misprinted text detection. Finally, the technique is
deployed on an industrial image for the printing quality assessment with
varying values of n and m. The results have shown that the proposed SAML-QC
technique can perform real-time automated inspection for industrial printing.
",1,0,0,0,0,0
1865,Probing the gravitational redshift with an Earth-orbiting satellite,"  We present an approach to testing the gravitational redshift effect using the
RadioAstron satellite. The experiment is based on a modification of the Gravity
Probe A scheme of nonrelativistic Doppler compensation and benefits from the
highly eccentric orbit and ultra-stable atomic hydrogen maser frequency
standard of the RadioAstron satellite. Using the presented techniques we expect
to reach an accuracy of the gravitational redshift test of order $10^{-5}$, a
magnitude better than that of Gravity Probe A. Data processing is ongoing, our
preliminary results agree with the validity of the Einstein Equivalence
Principle.
",0,1,0,0,0,0
1866,A stencil scaling approach for accelerating matrix-free finite element implementations,"  We present a novel approach to fast on-the-fly low order finite element
assembly for scalar elliptic partial differential equations of Darcy type with
variable coefficients optimized for matrix-free implementations. Our approach
introduces a new operator that is obtained by appropriately scaling the
reference stiffness matrix from the constant coefficient case. Assuming
sufficient regularity, an a priori analysis shows that solutions obtained by
this approach are unique and have asymptotically optimal order convergence in
the $H^1$- and the $L^2$-norm on hierarchical hybrid grids. For the
pre-asymptotic regime, we present a local modification that guarantees uniform
ellipticity of the operator. Cost considerations show that our novel approach
requires roughly one third of the floating-point operations compared to a
classical finite element assembly scheme employing nodal integration. Our
theoretical considerations are illustrated by numerical tests that confirm the
expectations with respect to accuracy and run-time. A large scale application
with more than a hundred billion ($1.6\cdot10^{11}$) degrees of freedom
executed on 14,310 compute cores demonstrates the efficiency of the new scaling
approach.
",1,0,0,0,0,0
1867,Cram??r-Rao Lower Bounds for Positioning with Large Intelligent Surfaces,"  We consider the potential for positioning with a system where antenna arrays
are deployed as a large intelligent surface (LIS). We derive
Fisher-informations and Cram??r-Rao lower bounds (CRLB) in closed-form for
terminals along the central perpendicular line (CPL) of the LIS for all three
Cartesian dimensions. For terminals at positions other than the CPL,
closed-form expressions for the Fisher-informations and CRLBs seem out of
reach, and we alternatively provide approximations (in closed-form) which are
shown to be very accurate. We also show that under mild conditions, the CRLBs
in general decrease quadratically in the surface-area for both the $x$ and $y$
dimensions. For the $z$-dimension (distance from the LIS), the CRLB decreases
linearly in the surface-area when terminals are along the CPL. However, when
terminals move away from the CPL, the CRLB is dramatically increased and then
also decreases quadratically in the surface-area. We also extensively discuss
the impact of different deployments (centralized and distributed) of the LIS.
",1,0,0,0,0,0
1868,Asymptotic behaviour methods for the Heat Equation. Convergence to the Gaussian,"  In this expository work we discuss the asymptotic behaviour of the solutions
of the classical heat equation posed in the whole Euclidean space.
After an introductory review of the main facts on the existence and
properties of solutions, we proceed with the proofs of convergence to the
Gaussian fundamental solution, a result that holds for all integrable
solutions, and represents in the PDE setting the Central Limit Theorem of
probability. We present several methods of proof: first, the scaling method.
Then several versions of the representation method. This is followed by the
functional analysis approach that leads to the famous related equations,
Fokker-Planck and Ornstein-Uhlenbeck. The analysis of this connection is also
given in rather complete form here. Finally, we present the Boltzmann entropy
method, coming from kinetic equations.
The different methods are interesting because of the possible extension to
prove the asymptotic behaviour or stabilization analysis for more general
equations, linear or nonlinear. It all depends a lot on the particular
features, and only one or some of the methods work in each case.Other settings
of the Heat Equation are briefly discussed in Section 9 and a longer mention of
results for different equations is done in Section 10.
",0,0,1,0,0,0
1869,Magnetization dynamics of weakly interacting sub-100 nm square artificial spin ices,"  Artificial Spin Ice (ASI), consisting of a two dimensional array of nanoscale
magnetic elements, provides a fascinating opportunity to observe the physics of
out of equilibrium systems. Initial studies concentrated on the static, frozen
state, whilst more recent studies have accessed the out-of-equilibrium dynamic,
fluctuating state. This opens up exciting possibilities such as the observation
of systems exploring their energy landscape through monopole quasiparticle
creation, potentially leading to ASI magnetricity, and to directly observe
unconventional phase transitions. In this work we have measured and analysed
the magnetic relaxation of thermally active ASI systems by means of SQUID
magnetometry. We have investigated the effect of the interaction strength on
the magnetization dynamics at different temperatures in the range where the
nanomagnets are thermally active and have observed that they follow an
Arrhenius-type N??el-Brown behaviour. An unexpected negative correlation of
the average blocking temperature with the interaction strength is also
observed, which is supported by Monte Carlo simulations. The magnetization
relaxation measurements show faster relaxation for more strongly coupled
nanoelements with similar dimensions. The analysis of the stretching exponents
obtained from the measurements suggest 1-D chain-like magnetization dynamics.
This indicates that the nature of the interactions between nanoelements lowers
the dimensionality of the ASI from 2-D to 1-D. Finally, we present a way to
quantify the effective interaction energy of a square ASI system, and compare
it to the interaction energy calculated from a simple dipole model and also to
the magnetostatic energy computed with micromagnetic simulations.
",0,1,0,0,0,0
1870,Filtering Tweets for Social Unrest,"  Since the events of the Arab Spring, there has been increased interest in
using social media to anticipate social unrest. While efforts have been made
toward automated unrest prediction, we focus on filtering the vast volume of
tweets to identify tweets relevant to unrest, which can be provided to
downstream users for further analysis. We train a supervised classifier that is
able to label Arabic language tweets as relevant to unrest with high
reliability. We examine the relationship between training data size and
performance and investigate ways to optimize the model building process while
minimizing cost. We also explore how confidence thresholds can be set to
achieve desired levels of performance.
",1,0,0,1,0,0
1871,Structured Connectivity Augmentation,"  We initiate the algorithmic study of the following ""structured augmentation""
question: is it possible to increase the connectivity of a given graph G by
superposing it with another given graph H? More precisely, graph F is the
superposition of G and H with respect to injective mapping \phi: V(H)->V(G) if
every edge uv of F is either an edge of G, or \phi^{-1}(u)\phi^{-1}(v) is an
edge of H. We consider the following optimization problem. Given graphs G,H,
and a weight function \omega assigning non-negative weights to pairs of
vertices of V(G), the task is to find \varphi of minimum weight
\omega(\phi)=\sum_{xy\in E(H)}\omega(\phi(x)\varphi(y)) such that the edge
connectivity of the superposition F of G and H with respect to \phi is higher
than the edge connectivity of G. Our main result is the following ""dichotomy""
complexity classification. We say that a class of graphs C has bounded
vertex-cover number, if there is a constant t depending on C only such that the
vertex-cover number of every graph from C does not exceed t. We show that for
every class of graphs C with bounded vertex-cover number, the problems of
superposing into a connected graph F and to 2-edge connected graph F, are
solvable in polynomial time when H\in C. On the other hand, for any hereditary
class C with unbounded vertex-cover number, both problems are NP-hard when H\in
C. For the unweighted variants of structured augmentation problems, i.e. the
problems where the task is to identify whether there is a superposition of
graphs of required connectivity, we provide necessary and sufficient
combinatorial conditions on the existence of such superpositions. These
conditions imply polynomial time algorithms solving the unweighted variants of
the problems.
",1,0,0,0,0,0
1872,Transition probability of Brownian motion in the octant and its application to default modeling,"  We derive a semi-analytic formula for the transition probability of
three-dimensional Brownian motion in the positive octant with absorption at the
boundaries. Separation of variables in spherical coordinates leads to an
eigenvalue problem for the resulting boundary value problem in the two angular
components. The main theoretical result is a solution to the original problem
expressed as an expansion into special functions and an eigenvalue which has to
be chosen to allow a matching of the boundary condition. We discuss and test
several computational methods to solve a finite-dimensional approximation to
this nonlinear eigenvalue problem. Finally, we apply our results to the
computation of default probabilities and credit valuation adjustments in a
structural credit model with mutual liabilities.
",0,0,0,0,0,1
1873,Block-Sparse Recurrent Neural Networks,"  Recurrent Neural Networks (RNNs) are used in state-of-the-art models in
domains such as speech recognition, machine translation, and language
modelling. Sparsity is a technique to reduce compute and memory requirements of
deep learning models. Sparse RNNs are easier to deploy on devices and high-end
server processors. Even though sparse operations need less compute and memory
relative to their dense counterparts, the speed-up observed by using sparse
operations is less than expected on different hardware platforms. In order to
address this issue, we investigate two different approaches to induce block
sparsity in RNNs: pruning blocks of weights in a layer and using group lasso
regularization to create blocks of weights with zeros. Using these techniques,
we demonstrate that we can create block-sparse RNNs with sparsity ranging from
80% to 90% with small loss in accuracy. This allows us to reduce the model size
by roughly 10x. Additionally, we can prune a larger dense network to recover
this loss in accuracy while maintaining high block sparsity and reducing the
overall parameter count. Our technique works with a variety of block sizes up
to 32x32. Block-sparse RNNs eliminate overheads related to data storage and
irregular memory accesses while increasing hardware efficiency compared to
unstructured sparsity.
",1,0,0,1,0,0
1874,Equitable neighbour-sum-distinguishing edge and total colourings,"  With any (not necessarily proper) edge $k$-colouring
$\gamma:E(G)\longrightarrow\{1,\dots,k\}$ of a graph $G$,one can associate a
vertex colouring $\sigma\_{\gamma}$ given by $\sigma\_{\gamma}(v)=\sum\_{e\ni
v}\gamma(e)$.A neighbour-sum-distinguishing edge $k$-colouring is an edge
colouring whose associated vertex colouring is proper.The
neighbour-sum-distinguishing index of a graph $G$ is then the smallest $k$ for
which $G$ admitsa neighbour-sum-distinguishing edge $k$-colouring.These notions
naturally extends to total colourings of graphs that assign colours to both
vertices and edges.We study in this paper equitable
neighbour-sum-distinguishing edge colourings andtotal colourings, that is
colourings $\gamma$ for whichthe number of elements in any two colour classes
of $\gamma$ differ by at most one.We determine the equitable
neighbour-sum-distinguishing indexof complete graphs, complete bipartite graphs
and forests,and the equitable neighbour-sum-distinguishing total chromatic
numberof complete graphs and bipartite graphs.
",1,0,1,0,0,0
1875,An Oracle Property of The Nadaraya-Watson Kernel Estimator for High Dimensional Nonparametric Regression,"  The celebrated Nadaraya-Watson kernel estimator is among the most studied
method for nonparametric regression. A classical result is that its rate of
convergence depends on the number of covariates and deteriorates quickly as the
dimension grows, which underscores the ""curse of dimensionality"" and has
limited its use in high dimensional settings. In this article, we show that
when the true regression function is single or multi-index, the effects of the
curse of dimensionality may be mitigated for the Nadaraya-Watson kernel
estimator. Specifically, we prove that with $K$-fold cross-validation, the
Nadaraya-Watson kernel estimator indexed by a positive semidefinite bandwidth
matrix has an oracle property that its rate of convergence depends on the
number of indices of the regression function rather than the number of
covariates. Intuitively, this oracle property is a consequence of allowing the
bandwidths to diverge to infinity as opposed to restricting them all to
converge to zero at certain rates as done in previous theoretical studies. Our
result provides a theoretical perspective for the use of kernel estimation in
high dimensional nonparametric regression and other applications such as metric
learning when a low rank structure is anticipated. Numerical illustrations are
given through simulations and real data examples.
",0,0,1,1,0,0
1876,Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe,"  We consider the problem of bandit optimization, inspired by stochastic
optimization and online learning problems with bandit feedback. In this
problem, the objective is to minimize a global loss function of all the
actions, not necessarily a cumulative loss. This framework allows us to study a
very general class of problems, with applications in statistics, machine
learning, and other fields. To solve this problem, we analyze the
Upper-Confidence Frank-Wolfe algorithm, inspired by techniques for bandits and
convex optimization. We give theoretical guarantees for the performance of this
algorithm over various classes of functions, and discuss the optimality of
these results.
",0,0,1,1,0,0
1877,Highly sensitive atomic based MW interferometry,"  We theoretically study a scheme to develop an atomic based MW interferometry
using the Rydberg states in Rb. Unlike the traditional MW interferometry, this
scheme is not based upon the electrical circuits, hence the sensitivity of the
phase and the amplitude/strength of the MW field is not limited by the Nyquist
thermal noise. Further this system has great advantage due to its very high
bandwidth, ranging from radio frequency (RF), micro wave (MW) to terahertz
regime. In addition, this is \textbf{orders of magnitude} more sensitive to
field strength as compared to the prior demonstrations on the MW electrometry
using the Rydberg atomic states. However previously studied atomic systems are
only sensitive to the field strength but not to the phase and hence this scheme
provides a great opportunity to characterize the MW completely including the
propagation direction and the wavefront. This study opens up a new dimension in
the Radar technology such as in synthetic aperture radar interferometry. The MW
interferometry is based upon a six-level loopy ladder system involving the
Rydberg states in which two sub-systems interfere constructively or
destructively depending upon the phase between the MW electric fields closing
the loop.
",0,1,0,0,0,0
1878,The Wisdom of a Kalman Crowd,"  The Kalman Filter has been called one of the greatest inventions in
statistics during the 20th century. Its purpose is to measure the state of a
system by processing the noisy data received from different electronic sensors.
In comparison, a useful resource for managers in their effort to make the right
decisions is the wisdom of crowds. This phenomenon allows managers to combine
judgments by different employees to get estimates that are often more accurate
and reliable than estimates, which managers produce alone. Since harnessing the
collective intelligence of employees, and filtering signals from multiple noisy
sensors appear related, we looked at the possibility of using the Kalman Filter
on estimates by people. Our predictions suggest, and our findings based on the
Survey of Professional Forecasters reveal, that the Kalman Filter can help
managers solve their decision-making problems by giving them stronger signals
before they choose. Indeed, when used on a subset of forecasters identified by
the Contribution Weighted Model, the Kalman Filter beat that rule clearly,
across all the forecasting horizons in the survey.
",0,0,0,0,0,1
1879,Noisy independent component analysis of auto-correlated components,"  We present a new method for the separation of superimposed, independent,
auto-correlated components from noisy multi-channel measurement. The presented
method simultaneously reconstructs and separates the components, taking all
channels into account and thereby increases the effective signal-to-noise ratio
considerably, allowing separations even in the high noise regime.
Characteristics of the measurement instruments can be included, allowing for
application in complex measurement situations. Independent posterior samples
can be provided, permitting error estimates on all desired quantities. Using
the concept of information field theory, the algorithm is not restricted to any
dimensionality of the underlying space or discretization scheme thereof.
",0,1,0,1,0,0
1880,Ages and structural and dynamical parameters of two globular clusters in the M81 group,"  GC-1 and GC-2 are two globular clusters (GCs) in the remote halo of M81 and
M82 in the M81 group discovered by Jang et al. using the {\it Hubble Space
Telescope} ({\it HST}) images. These two GCs were observed as part of the
Beijing--Arizona--Taiwan--Connecticut (BATC) Multicolor Sky Survey, using 14
intermediate-band filters covering a wavelength range of 4000--10000 \AA. We
accurately determine these two clusters' ages and masses by comparing their
spectral energy distributions (from 2267 to 20000~{\AA}, comprising photometric
data in the near-ultraviolet of the {\it Galaxy Evolution Explorer}, 14 BATC
intermediate-band, and Two Micron All Sky Survey near-infrared $JHK_{\rm s}$
filters) with theoretical stellar population-synthesis models, resulting in
ages of $15.50\pm3.20$ for GC-1 and $15.10\pm2.70$ Gyr for GC-2. The masses of
GC-1 and GC-2 obtained here are $1.77-2.04\times 10^6$ and $5.20-7.11\times
10^6 \rm~M_\odot$, respectively. In addition, the deep observations with the
Advanced Camera for Surveys and Wide Field Camera 3 on the {\it HST} are used
to provide the surface brightness profiles of GC-1 and GC-2. The structural and
dynamical parameters are derived from fitting the profiles to three different
models; in particular, the internal velocity dispersions of GC-1 and GC-2 are
derived, which can be compared with ones obtained based on spectral
observations in the future. For the first time, in this paper, the $r_h$ versus
$M_V$ diagram shows that GC-2 is an ultra-compact dwarf in the M81 group.
",0,1,0,0,0,0
1881,Bayesian Renewables Scenario Generation via Deep Generative Networks,"  We present a method to generate renewable scenarios using Bayesian
probabilities by implementing the Bayesian generative adversarial
network~(Bayesian GAN), which is a variant of generative adversarial networks
based on two interconnected deep neural networks. By using a Bayesian
formulation, generators can be constructed and trained to produce scenarios
that capture different salient modes in the data, allowing for better diversity
and more accurate representation of the underlying physical process. Compared
to conventional statistical models that are often hard to scale or sample from,
this method is model-free and can generate samples extremely efficiently. For
validation, we use wind and solar times-series data from NREL integration data
sets to train the Bayesian GAN. We demonstrate that proposed method is able to
generate clusters of wind scenarios with different variance and mean value, and
is able to distinguish and generate wind and solar scenarios simultaneously
even if the historical data are intentionally mixed.
",0,0,0,1,0,0
1882,"Graphons: A Nonparametric Method to Model, Estimate, and Design Algorithms for Massive Networks","  Many social and economic systems are naturally represented as networks, from
off-line and on-line social networks, to bipartite networks, like Netflix and
Amazon, between consumers and products. Graphons, developed as limits of
graphs, form a natural, nonparametric method to describe and estimate large
networks like Facebook and LinkedIn. Here we describe the development of the
theory of graphons, for both dense and sparse networks, over the last decade.
We also review theorems showing that we can consistently estimate graphons from
massive networks in a wide variety of models. Finally, we show how to use
graphons to estimate missing links in a sparse network, which has applications
from estimating social and information networks in development economics, to
rigorously and efficiently doing collaborative filtering with applications to
movie recommendations in Netflix and product suggestions in Amazon.
",1,1,0,0,0,0
1883,Hopf Parametric Adjoint Objects through a 2-adjunction of the type Adj-Mnd,"  In this article Hopf parametric adjunctions are defined and analysed within
the context of the 2-adjunction of the type $\mathbf{Adj}$-$\mathbf{Mnd}$. In
order to do so, the definition of adjoint objects in the 2-category of
adjunctions and in the 2-category of monads for $Cat$ are revised and
characterized. This article finalises with the application of the obtained
results on current categorical characterization of Hopf Monads.
",0,0,1,0,0,0
1884,Krylov Subspace Recycling for Fast Iterative Least-Squares in Machine Learning,"  Solving symmetric positive definite linear problems is a fundamental
computational task in machine learning. The exact solution, famously, is
cubicly expensive in the size of the matrix. To alleviate this problem, several
linear-time approximations, such as spectral and inducing-point methods, have
been suggested and are now in wide use. These are low-rank approximations that
choose the low-rank space a priori and do not refine it over time. While this
allows linear cost in the data-set size, it also causes a finite, uncorrected
approximation error. Authors from numerical linear algebra have explored ways
to iteratively refine such low-rank approximations, at a cost of a small number
of matrix-vector multiplications. This idea is particularly interesting in the
many situations in machine learning where one has to solve a sequence of
related symmetric positive definite linear problems. From the machine learning
perspective, such deflation methods can be interpreted as transfer learning of
a low-rank approximation across a time-series of numerical tasks. We study the
use of such methods for our field. Our empirical results show that, on
regression and classification problems of intermediate size, this approach can
interpolate between low computational cost and numerical precision.
",1,0,0,1,0,0
1885,Towards a Physical Oracle for the Partition Problem using Analogue Computing,"  Despite remarkable achievements in its practical tractability, the notorious
class of NP-complete problems has been escaping all attempts to find a
worst-case polynomial time-bound solution algorithms for any of them. The vast
majority of work relies on Turing machines or equivalent models, all of which
relate to digital computing. This raises the question of whether a computer
that is (partly) non-digital could offer a new door towards an efficient
solution. And indeed, the partition problem, which is another NP-complete
sibling of the famous Boolean satisfiability problem SAT, might be open to
efficient solutions using analogue computing. We investigate this hypothesis
here, providing experimental evidence that Partition, and in turn also SAT, may
become tractable on a combined digital and analogue computing machine. This
work provides mostly theoretical and based on simulations, and as such does not
exhibit a polynomial time algorithm to solve NP-complete problems. Instead, it
is intended as a pointer to new directions of research on special-purpose
computing architectures that may help handling the class NP efficiently.
",1,0,0,0,0,0
1886,Bayesian Methods in Cosmology,"  These notes aim at presenting an overview of Bayesian statistics, the
underlying concepts and application methodology that will be useful to
astronomers seeking to analyse and interpret a wide variety of data about the
Universe. The level starts from elementary notions, without assuming any
previous knowledge of statistical methods, and then progresses to more
advanced, research-level topics. After an introduction to the importance of
statistical inference for the physical sciences, elementary notions of
probability theory and inference are introduced and explained. Bayesian methods
are then presented, starting from the meaning of Bayes Theorem and its use as
inferential engine, including a discussion on priors and posterior
distributions. Numerical methods for generating samples from arbitrary
posteriors (including Markov Chain Monte Carlo and Nested Sampling) are then
covered. The last section deals with the topic of Bayesian model selection and
how it is used to assess the performance of models, and contrasts it with the
classical p-value approach. A series of exercises of various levels of
difficulty are designed to further the understanding of the theoretical
material, including fully worked out solutions for most of them.
",0,1,0,1,0,0
1887,Information Extraction in Illicit Domains,"  Extracting useful entities and attribute values from illicit domains such as
human trafficking is a challenging problem with the potential for widespread
social impact. Such domains employ atypical language models, have `long tails'
and suffer from the problem of concept drift. In this paper, we propose a
lightweight, feature-agnostic Information Extraction (IE) paradigm specifically
designed for such domains. Our approach uses raw, unlabeled text from an
initial corpus, and a few (12-120) seed annotations per domain-specific
attribute, to learn robust IE models for unobserved pages and websites.
Empirically, we demonstrate that our approach can outperform feature-centric
Conditional Random Field baselines by over 18\% F-Measure on five annotated
sets of real-world human trafficking datasets in both low-supervision and
high-supervision settings. We also show that our approach is demonstrably
robust to concept drift, and can be efficiently bootstrapped even in a serial
computing environment.
",1,0,0,0,0,0
1888,A Tutorial on Kernel Density Estimation and Recent Advances,"  This tutorial provides a gentle introduction to kernel density estimation
(KDE) and recent advances regarding confidence bands and geometric/topological
features. We begin with a discussion of basic properties of KDE: the
convergence rate under various metrics, density derivative estimation, and
bandwidth selection. Then, we introduce common approaches to the construction
of confidence intervals/bands, and we discuss how to handle bias. Next, we talk
about recent advances in the inference of geometric and topological features of
a density function using KDE. Finally, we illustrate how one can use KDE to
estimate a cumulative distribution function and a receiver operating
characteristic curve. We provide R implementations related to this tutorial at
the end.
",0,0,0,1,0,0
1889,Optimizing expected word error rate via sampling for speech recognition,"  State-level minimum Bayes risk (sMBR) training has become the de facto
standard for sequence-level training of speech recognition acoustic models. It
has an elegant formulation using the expectation semiring, and gives large
improvements in word error rate (WER) over models trained solely using
cross-entropy (CE) or connectionist temporal classification (CTC). sMBR
training optimizes the expected number of frames at which the reference and
hypothesized acoustic states differ. It may be preferable to optimize the
expected WER, but WER does not interact well with the expectation semiring, and
previous approaches based on computing expected WER exactly involve expanding
the lattices used during training. In this paper we show how to perform
optimization of the expected WER by sampling paths from the lattices used
during conventional sMBR training. The gradient of the expected WER is itself
an expectation, and so may be approximated using Monte Carlo sampling. We show
experimentally that optimizing WER during acoustic model training gives 5%
relative improvement in WER over a well-tuned sMBR baseline on a 2-channel
query recognition task (Google Home).
",1,0,0,1,0,0
1890,Real-Time Illegal Parking Detection System Based on Deep Learning,"  The increasing illegal parking has become more and more serious. Nowadays the
methods of detecting illegally parked vehicles are based on background
segmentation. However, this method is weakly robust and sensitive to
environment. Benefitting from deep learning, this paper proposes a novel
illegal vehicle parking detection system. Illegal vehicles captured by camera
are firstly located and classified by the famous Single Shot MultiBox Detector
(SSD) algorithm. To improve the performance, we propose to optimize SSD by
adjusting the aspect ratio of default box to accommodate with our dataset
better. After that, a tracking and analysis of movement is adopted to judge the
illegal vehicles in the region of interest (ROI). Experiments show that the
system can achieve a 99% accuracy and real-time (25FPS) detection with strong
robustness in complex environments.
",1,0,0,1,0,0
1891,On a representation of fractional Brownian motion and the limit distributions of statistics arising in cusp statistical models,"  We discuss some extensions of results from the recent paper by Chernoyarov et
al. (Ann. Inst. Stat. Math., October 2016) concerning limit distributions of
Bayesian and maximum likelihood estimators in the model ""signal plus white
noise"" with irregular cusp-type signals. Using a new representation of
fractional Brownian motion (fBm) in terms of cusp functions we show that as the
noise intensity tends to zero, the limit distributions are expressed in terms
of fBm for the full range of asymmetric cusp-type signals correspondingly with
the Hurst parameter H, 0<H<1. Simulation results for the densities and
variances of the limit distributions of Bayesian and maximum likelihood
estimators are also provided.
",0,0,1,1,0,0
1892,Stochastic Canonical Correlation Analysis,"  We tightly analyze the sample complexity of CCA, provide a learning algorithm
that achieves optimal statistical performance in time linear in the required
number of samples (up to log factors), as well as a streaming algorithm with
similar guarantees.
",1,0,0,1,0,0
1893,Segmentation of Instances by Hashing,"  We propose a novel approach to address the Simultaneous Detection and
Segmentation problem. Using hierarchical structures we use an efficient and
accurate procedure that exploits the hierarchy feature information using
Locality Sensitive Hashing. We build on recent work that utilizes convolutional
neural networks to detect bounding boxes in an image and then use the top
similar hierarchical region that best fits each bounding box after hashing, we
call this approach CZ Segmentation. We then refine our final segmentation
results by automatic hierarchy pruning. CZ Segmentation introduces a train-free
alternative to Hypercolumns. We conduct extensive experiments on PASCAL VOC
2012 segmentation dataset, showing that CZ gives competitive state-of-the-art
object segmentations.
",1,0,0,0,0,0
1894,Grafting for Combinatorial Boolean Model using Frequent Itemset Mining,"  This paper introduces the combinatorial Boolean model (CBM), which is defined
as the class of linear combinations of conjunctions of Boolean attributes. This
paper addresses the issue of learning CBM from labeled data. CBM is of high
knowledge interpretability but na??ve learning of it requires exponentially
large computation time with respect to data dimension and sample size. To
overcome this computational difficulty, we propose an algorithm GRAB (GRAfting
for Boolean datasets), which efficiently learns CBM within the
$L_1$-regularized loss minimization framework. The key idea of GRAB is to
reduce the loss minimization problem to the weighted frequent itemset mining,
in which frequent patterns are efficiently computable. We employ benchmark
datasets to empirically demonstrate that GRAB is effective in terms of
computational efficiency, prediction accuracy and knowledge discovery.
",1,0,0,1,0,0
1895,Rapid Assessment of Damaged Homes in the Florida Keys after Hurricane Irma,"  On September 10, 2017, Hurricane Irma made landfall in the Florida Keys and
caused significant damage. Informed by hydrodynamic storm surge and wave
modeling and post-storm satellite imagery, a rapid damage survey was soon
conducted for 1600+ residential buildings in Big Pine Key and Marathon. Damage
categorizations and statistical analysis reveal distinct factors governing
damage at these two locations. The distance from the coast is significant for
the damage in Big Pine Key, as severely damaged buildings were located near
narrow waterways connected to the ocean. Building type and size are critical in
Marathon, highlighted by the near-complete destruction of trailer communities
there. These observations raise issues of affordability and equity that need
consideration in damage recovery and rebuilding for resilience.
",0,0,0,1,0,0
1896,Status maximization as a source of fairness in a networked dictator game,"  Human behavioural patterns exhibit selfish or competitive, as well as
selfless or altruistic tendencies, both of which have demonstrable effects on
human social and economic activity. In behavioural economics, such effects have
traditionally been illustrated experimentally via simple games like the
dictator and ultimatum games. Experiments with these games suggest that, beyond
rational economic thinking, human decision-making processes are influenced by
social preferences, such as an inclination to fairness. In this study we
suggest that the apparent gap between competitive and altruistic human
tendencies can be bridged by assuming that people are primarily maximising
their status, i.e., a utility function different from simple profit
maximisation. To this end we analyse a simple agent-based model, where
individuals play the repeated dictator game in a social network they can
modify. As model parameters we consider the living costs and the rate at which
agents forget infractions by others. We find that individual strategies used in
the game vary greatly, from selfish to selfless, and that both of the above
parameters determine when individuals form complex and cohesive social
networks.
",1,0,0,0,0,1
1897,On Dziobek Special Central Configurations,"  We study the special central configurations of the curved N-body problem in
S^3. We show that there are special central configurations formed by N masses
for any N >2. We then extend the concept of special central configurations to
S^n, n>0, and study one interesting class of special central configurations in
S^n, the Dziobek special central configurations. We obtain a criterion for them
and reduce it to two sets of equations. Then we apply these equations to
special central configurations of 3 bodies on S^1, 4 bodies on S^2, and 5
bodies in S^3.
",0,0,1,0,0,0
1898,Laser Interferometer Space Antenna,"  Following the selection of The Gravitational Universe by ESA, and the
successful flight of LISA Pathfinder, the LISA Consortium now proposes a 4 year
mission in response to ESA's call for missions for L3. The observatory will be
based on three arms with six active laser links, between three identical
spacecraft in a triangular formation separated by 2.5 million km.
LISA is an all-sky monitor and will offer a wide view of a dynamic cosmos
using Gravitational Waves as new and unique messengers to unveil The
Gravitational Universe. It provides the closest ever view of the infant
Universe at TeV energy scales, has known sources in the form of verification
binaries in the Milky Way, and can probe the entire Universe, from its smallest
scales near the horizons of black holes, all the way to cosmological scales.
The LISA mission will scan the entire sky as it follows behind the Earth in its
orbit, obtaining both polarisations of the Gravitational Waves simultaneously,
and will measure source parameters with astrophysically relevant sensitivity in
a band from below $10^{-4}\,$Hz to above $10^{-1}\,$Hz.
",0,1,0,0,0,0
1899,Learning from a lot: Empirical Bayes in high-dimensional prediction settings,"  Empirical Bayes is a versatile approach to `learn from a lot' in two ways:
first, from a large number of variables and second, from a potentially large
amount of prior information, e.g. stored in public repositories. We review
applications of a variety of empirical Bayes methods to several well-known
model-based prediction methods including penalized regression, linear
discriminant analysis, and Bayesian models with sparse or dense priors. We
discuss `formal' empirical Bayes methods which maximize the marginal
likelihood, but also more informal approaches based on other data summaries. We
contrast empirical Bayes to cross-validation and full Bayes, and discuss hybrid
approaches. To study the relation between the quality of an empirical Bayes
estimator and $p$, the number of variables, we consider a simple empirical
Bayes estimator in a linear model setting.
We argue that empirical Bayes is particularly useful when the prior contains
multiple parameters which model a priori information on variables, termed
`co-data'. In particular, we present two novel examples that allow for co-data.
First, a Bayesian spike-and-slab setting that facilitates inclusion of multiple
co-data sources and types; second, a hybrid empirical Bayes-full Bayes ridge
regression approach for estimation of the posterior predictive interval.
",0,0,0,1,0,0
1900,Dissipativity Theory for Accelerating Stochastic Variance Reduction: A Unified Analysis of SVRG and Katyusha Using Semidefinite Programs,"  Techniques for reducing the variance of gradient estimates used in stochastic
programming algorithms for convex finite-sum problems have received a great
deal of attention in recent years. By leveraging dissipativity theory from
control, we provide a new perspective on two important variance-reduction
algorithms: SVRG and its direct accelerated variant Katyusha. Our perspective
provides a physically intuitive understanding of the behavior of SVRG-like
methods via a principle of energy conservation. The tools discussed here allow
us to automate the convergence analysis of SVRG-like methods by capturing their
essential properties in small semidefinite programs amenable to standard
analysis and computational techniques. Our approach recovers existing
convergence results for SVRG and Katyusha and generalizes the theory to
alternative parameter choices. We also discuss how our approach complements the
linear coupling technique. Our combination of perspectives leads to a better
understanding of accelerated variance-reduced stochastic methods for finite-sum
problems.
",0,0,0,1,0,0
1901,Runout transition and clustering instability observed in binary-mixture avalanche deposits,"  Binary mixtures of dry grains avalanching down a slope are experimentally
studied in order to determine the interaction among coarse and fine grains and
their effect on the deposit morphology. The distance travelled by the massive
front of the avalanche over the horizontal plane of deposition area is measured
as a function of mass content of fine particles in the mixture, grain-size
ratio, and flume tilt. A sudden transition of the runout is detected at a
critical content of fine particles, with a dependence on the grain-size ratio
and flume tilt. This transition is explained as two simultaneous avalanches in
different flowing regimes (a viscous-like one and an inertial one) competing
against each other and provoking a full segregation and a split-off of the
deposit into two well-defined, separated deposits. The formation of the distal
deposit, in turn, depends on a critical amount of coarse particles. This allows
the condensation of the pure coarse deposit around a small, initial seed
cluster, which grows rapidly by braking and capturing subsequent colliding
coarse particles. For different grain-size ratios and keeping a constant total
mass, the change in the amount of fines needed for the transition to occur is
found to be always less than 7%. For avalanches with a total mass of 4 kg we
find that, most of the time, the runout of a binary avalanche is larger than
the runout of monodisperse avalanches of corresponding constituent particles,
due to lubrication on the coarse-dominated side or to drag by inertial
particles on the fine-dominated side.
",0,1,0,0,0,0
1902,A Simple Convex Layers Algorithm,"  Given a set of $n$ points $P$ in the plane, the first layer $L_1$ of $P$ is
formed by the points that appear on $P$'s convex hull. In general, a point
belongs to layer $L_i$, if it lies on the convex hull of the set $P \setminus
\bigcup_{j<i}\{L_j\}$. The \emph{convex layers problem} is to compute the
convex layers $L_i$. Existing algorithms for this problem either do not achieve
the optimal $\mathcal{O}\left(n\log n\right)$ runtime and linear space, or are
overly complex and difficult to apply in practice. We propose a new algorithm
that is both optimal and simple. The simplicity is achieved by independently
computing four sets of monotone convex chains in $\mathcal{O}\left(n\log
n\right)$ time and linear space. These are then merged in
$\mathcal{O}\left(n\log n\right)$ time.
",1,0,0,0,0,0
1903,Entire Solution in an Ignition Nonlocal Dispersal Equation: Asymmetric Kernel,"  This paper mainly focus on the front-like entire solution of a classical
nonlocal dispersal equation with ignition nonlinearity. Especially, the
dispersal kernel function $J$ may not be symmetric here. The asymmetry of $J$
has a great influence on the profile of the traveling waves and the sign of the
wave speeds, which further makes the properties of the entire solution more
diverse. We first investigate the asymptotic behavior of the traveling wave
solutions since it plays an essential role in obtaining the front-like entire
solution. Due to the impact of $f'(0)=0$, we can no longer use the common
method which mainly depending on Ikehara theorem and bilateral Laplace
transform to study the asymptotic rates of the nondecreasing traveling wave and
the nonincreasing one tending to 0, respectively, thus we adopt another method
to investigate them. Afterwards, we establish a new entire solution and obtain
its qualitative properties by constructing proper supersolution and subsolution
and by classifying the sign and size of the wave speeds.
",0,0,1,0,0,0
1904,Fundamental solutions for Schrodinger operators with general inverse square potentials,"  In this paper, we classify the fundamental solutions for a class of
Schrodinger operators.
",0,0,1,0,0,0
1905,Making up for the deficit in a marathon run,"  To predict the final result of an athlete in a marathon run thoroughly is the
eternal desire of each trainer. Usually, the achieved result is weaker than the
predicted one due to the objective (e.g., environmental conditions) as well as
subjective factors (e.g., athlete's malaise). Therefore, making up for the
deficit between predicted and achieved results is the main ingredient of the
analysis performed by trainers after the competition. In the analysis, they
search for parts of a marathon course where the athlete lost time. This paper
proposes an automatic making up for the deficit by using a Differential
Evolution algorithm. In this case study, the results that were obtained by a
wearable sports-watch by an athlete in a real marathon are analyzed. The first
experiments with Differential Evolution show the possibility of using this
method in the future.
",1,0,0,0,0,0
1906,An Efficient Load Balancing Method for Tree Algorithms,"  Nowadays, multiprocessing is mainstream with exponentially increasing number
of processors. Load balancing is, therefore, a critical operation for the
efficient execution of parallel algorithms. In this paper we consider the
fundamental class of tree-based algorithms that are notoriously irregular, and
hard to load-balance with existing static techniques. We propose a hybrid load
balancing method using the utility of statistical random sampling in estimating
the tree depth and node count distributions to uniformly partition an input
tree. To conduct an initial performance study, we implemented the method on an
Intel Xeon Phi accelerator system. We considered the tree traversal operation
on both regular and irregular unbalanced trees manifested by Fibonacci and
unbalanced (biased) randomly generated trees, respectively. The results show
scalable performance for up to the 60 physical processors of the accelerator,
as well as an extrapolated 128 processors case.
",1,0,0,0,0,0
1907,Dynamics of the spin-1/2 Heisenberg chain initialized in a domain-wall state,"  We study the dynamics of an isotropic spin-1/2 Heisenberg chain starting in a
domain-wall initial condition, where the spins are initially up on the left
half-line and down on the right half-line. We focus on the long-time behavior
of the magnetization profile. We perform extensive time-dependent
density-matrix renormalization group simulations (up to t=350) and find that
the data are compatible with a diffusive behavior. Subleading corrections decay
slowly blurring the emergence of the diffusive behavior. We also compare our
results with two alternative scenarios: superdiffusive behavior and enhanced
diffusion with a logarithmic correction. We finally discuss the evolution of
the entanglement entropy.
",0,1,0,0,0,0
1908,Multi-Erasure Locally Recoverable Codes Over Small Fields For Flash Memory Array,"  Erasure codes play an important role in storage systems to prevent data loss.
In this work, we study a class of erasure codes called Multi-Erasure Locally
Recoverable Codes (ME-LRCs) for flash memory array. Compared to previous
related works, we focus on the construction of ME-LRCs over small fields. We
first develop upper and lower bounds on the minimum distance of ME-LRCs. These
bounds explicitly take the field size into account. Our main contribution is to
propose a general construction of ME-LRCs based on generalized tensor product
codes, and study their erasure-correcting property. A decoding algorithm
tailored for erasure recovery is given. We then prove that our construction
yields optimal ME-LRCs with a wide range of code parameters. Finally, we
present several families of ME-LRCs over different fields.
",1,0,0,0,0,0
1909,NSML: A Machine Learning Platform That Enables You to Focus on Your Models,"  Machine learning libraries such as TensorFlow and PyTorch simplify model
implementation. However, researchers are still required to perform a
non-trivial amount of manual tasks such as GPU allocation, training status
tracking, and comparison of models with different hyperparameter settings. We
propose a system to handle these tasks and help researchers focus on models. We
present the requirements of the system based on a collection of discussions
from an online study group comprising 25k members. These include automatic GPU
allocation, learning status visualization, handling model parameter snapshots
as well as hyperparameter modification during learning, and comparison of
performance metrics between models via a leaderboard. We describe the system
architecture that fulfills these requirements and present a proof-of-concept
implementation, NAVER Smart Machine Learning (NSML). We test the system and
confirm substantial efficiency improvements for model development.
",1,0,0,0,0,0
1910,High order local absorbing boundary conditions for acoustic waves in terms of farfield expansions,"  We devise a new high order local absorbing boundary condition (ABC) for
radiating problems and scattering of time-harmonic acoustic waves from
obstacles of arbitrary shape. By introducing an artificial boundary $S$
enclosing the scatterer, the original unbounded domain $\Omega$ is decomposed
into a bounded computational domain $\Omega^{-}$ and an exterior unbounded
domain $\Omega^{+}$. Then, we define interface conditions at the artificial
boundary $S$, from truncated versions of the well-known Wilcox and Karp
farfield expansion representations of the exact solution in the exterior region
$\Omega^{+}$. As a result, we obtain a new local absorbing boundary condition
(ABC) for a bounded problem on $\Omega^{-}$, which effectively accounts for the
outgoing behavior of the scattered field. Contrary to the low order absorbing
conditions previously defined, the order of the error induced by this ABC can
easily match the order of the numerical method in $\Omega^{-}$. We accomplish
this by simply adding as many terms as needed to the truncated farfield
expansions of Wilcox or Karp. The convergence of these expansions guarantees
that the order of approximation of the new ABC can be increased arbitrarily
without having to enlarge the radius of the artificial boundary. We include
numerical results in two and three dimensions which demonstrate the improved
accuracy and simplicity of this new formulation when compared to other
absorbing boundary conditions.
",0,1,1,0,0,0
1911,Simple Necessary Conditions for the Existence of a Hamiltonian Path with Applications to Cactus Graphs,"  We describe some necessary conditions for the existence of a Hamiltonian path
in any graph (in other words, for a graph to be traceable). These conditions
result in a linear time algorithm to decide the Hamiltonian path problem for
cactus graphs. We apply this algorithm to several molecular databases to report
the numbers of graphs that are traceable cactus graphs.
",1,0,0,0,0,0
1912,Bootstrapping Exchangeable Random Graphs,"  We introduce two new bootstraps for exchangeable random graphs. One, the
""empirical graphon"", is based purely on resampling, while the other, the
""histogram stochastic block model"", is a model-based ""sieve"" bootstrap. We show
that both of them accurately approximate the sampling distributions of motif
densities, i.e., of the normalized counts of the number of times fixed
subgraphs appear in the network. These densities characterize the distribution
of (infinite) exchangeable networks. Our bootstraps therefore give, for the
first time, a valid quantification of uncertainty in inferences about
fundamental network statistics, and so of parameters identifiable from them.
",0,0,0,1,0,0
1913,A question proposed by K. Mahler on exceptional sets of transcendental functions with integer coefficients: solution of a Mahler's problem,"  In this paper, we shall prove that any subset of $\overline{\mathbb Q}\cap
B(0,1)$, which is closed under complex conjugation and which contains the
element $0$, is the exceptional set of uncountably many transcendental
functions, analytic in the unit ball, with integer coefficients. This solves a
strong version of an old question proposed by K. Mahler (1976).
",0,0,1,0,0,0
1914,Implementation of the Bin Hierarchy Method for restoring a smooth function from a sampled histogram,"  We present $\texttt{BHM}$, a tool for restoring a smooth function from a
sampled histogram using the bin hierarchy method. The theoretical background of
the method is presented in [arXiv:1707.07625]. The code automatically generates
a smooth polynomial spline with the minimal acceptable number of knots from the
input data. It works universally for any sufficiently regular shaped
distribution and any level of data quality, requiring almost no external
parameter specification. It is particularly useful for large-scale numerical
data analysis. This paper explains the details of the implementation and the
use of the program.
",0,1,0,1,0,0
1915,The Discrete Stochastic Galerkin Method for Hyperbolic Equations with Non-smooth and Random Coefficients,"  We develop a general polynomial chaos (gPC) based stochastic Galerkin (SG)
for hyperbolic equations with random and singular coefficients. Due to the
singu- lar nature of the solution, the standard gPC-SG methods may suffer from
a poor or even non convergence. Taking advantage of the fact that the discrete
solution, by the central type finite difference or finite volume approximations
in space and time for example, is smoother, we first discretize the equation by
a smooth finite difference or finite volume scheme, and then use the gPC-SG
approximation to the discrete system. The jump condition at the interface is
treated using the immersed upwind methods introduced in [8, 12]. This yields a
method that converges with the spectral accuracy for finite mesh size and time
step. We use a linear hyperbolic equation with discontinuous and random
coefficient, and the Liouville equation with discontinuous and random
potential, to illustrate our idea, with both one and second order spatial
discretizations. Spectral convergence is established for the first equation,
and numerical examples for both equations show the desired accu- racy of the
method.
",0,0,1,0,0,0
1916,"Seasonal evolution of $\mathrm{C_2N_2}$, $\mathrm{C_3H_4}$, and $\mathrm{C_4H_2}$ abundances in Titan's lower stratosphere","  We study the seasonal evolution of Titan's lower stratosphere (around
15~mbar) in order to better understand the atmospheric dynamics and chemistry
in this part of the atmosphere. We analysed Cassini/CIRS far-IR observations
from 2006 to 2016 in order to measure the seasonal variations of three
photochemical by-products: $\mathrm{C_4H_2}$, $\mathrm{C_3H_4}$, and
$\mathrm{C_2N_2}$. We show that the abundances of these three gases have
evolved significantly at northern and southern high latitudes since 2006. We
measure a sudden and steep increase of the volume mixing ratios of
$\mathrm{C_4H_2}$, $\mathrm{C_3H_4}$, and $\mathrm{C_2N_2}$ at the south pole
from 2012 to 2013, whereas the abundances of these gases remained approximately
constant at the north pole over the same period. At northern mid-latitudes,
$\mathrm{C_2N_2}$ and $\mathrm{C_4H_2}$ abundances decrease after 2012 while
$\mathrm{C_3H_4}$ abundances stay constant. The comparison of these volume
mixing ratio variations with the predictions of photochemical and dynamical
models provides constraints on the seasonal evolution of atmospheric
circulation and chemical processes at play.
",0,1,0,0,0,0
1917,"Systems, Actors and Agents: Operation in a multicomponent environment","  Multi-agent approach has become popular in computer science and technology.
However, the conventional models of multi-agent and multicomponent systems
implicitly or explicitly assume existence of absolute time or even do not
include time in the set of defining parameters. At the same time, it is proved
theoretically and validated experimentally that there are different times and
time scales in a variety of real systems - physical, chemical, biological,
social, informational, etc. Thus, the goal of this work is construction of a
multi-agent multicomponent system models with concurrency of processes and
diversity of actions. To achieve this goal, a mathematical system actor model
is elaborated and its properties are studied.
",1,0,0,0,0,0
1918,An invitation to model theory and C*-algebras,"  We present an introductory survey to first order logic for metric structures
and its applications to C*-algebras.
",0,0,1,0,0,0
1919,Random Close Packing and the Hard Sphere Percus-Yevick Theory,"  The Percus-Yevick theory for monodisperse hard spheres gives very good
results for the pressure and structure factor of the system in a whole range of
densities that lie within the liquid phase. However, the equation seems to lead
to a very unacceptable result beyond that region. Namely, the Percus-Yevick
theory predicts a smooth behavior of the pressure that diverges only when the
volume fraction $\eta$ approaches unity. Thus, within the theory there seems to
be no indication for the termination of the liquid phase and the transition to
a solid or to a glass. In the present article we study the Percus-Yevick hard
sphere pair distribution function, $g_2(r)$, for various spatial dimensions. We
find that beyond a certain critical volume fraction $\eta_c$, the pair
distribution function, $g_2(r)$, which should be positive definite, becomes
negative at some distances. We also present an intriguing observation that the
critical $\eta_c$ values we find are consistent with volume fractions where
onsets of random close packing (or maximally random jammed states) are reported
in the literature for various dimensions. That observation is supported by an
intuitive argument. This work may have important implications for other systems
for which a Percus-Yevick theory exists.
",0,1,0,0,0,0
1920,The Painlev?? property of $\mathbb{C}P^{N-1}$ sigma models,"  We test the $\mathbb{C}P^{N-1}$ sigma models for the Painlev?? property.
While the construction of finite action solutions ensures their meromorphicity,
the general case requires testing. The test is performed for the equations in
the homogeneous variables, with their first component normalised to one. No
constraints are imposed on the dimensionality of the model or the values of the
initial exponents. This makes the test nontrivial, as the number of equations
and dependent variables are indefinite. A $\mathbb{C}P^{N-1}$ system proves to
have a $(4N-5)$-parameter family of solutions whose movable singularities are
only poles, while the order of the investigated system is $4N-4$. The remaining
degree of freedom, connected with an extra negative resonance, may correspond
to a branching movable essential singularity. An example of such a solution is
provided.
",0,1,0,0,0,0
1921,A comment on Stein's unbiased risk estimate for reduced rank estimators,"  In the framework of matrix valued observables with low rank means, Stein's
unbiased risk estimate (SURE) can be useful for risk estimation and for tuning
the amount of shrinkage towards low rank matrices. This was demonstrated by
Cand??s et al. (2013) for singular value soft thresholding, which is a
Lipschitz continuous estimator. SURE provides an unbiased risk estimate for an
estimator whenever the differentiability requirements for Stein's lemma are
satisfied. Lipschitz continuity of the estimator is sufficient, but it is
emphasized that differentiability Lebesgue almost everywhere isn't. The reduced
rank estimator, which gives the best approximation of the observation with a
fixed rank, is an example of a discontinuous estimator for which Stein's lemma
actually applies. This was observed by Mukherjee et al. (2015), but the proof
was incomplete. This brief note gives a sufficient condition for Stein's lemma
to hold for estimators with discontinuities, which is then shown to be
fulfilled for a class of spectral function estimators including the reduced
rank estimator. Singular value hard thresholding does, however, not satisfy the
condition, and Stein's lemma does not apply to this estimator.
",0,0,1,1,0,0
1922,An empirical study on evaluation metrics of generative adversarial networks,"  Evaluating generative adversarial networks (GANs) is inherently challenging.
In this paper, we revisit several representative sample-based evaluation
metrics for GANs, and address the problem of how to evaluate the evaluation
metrics. We start with a few necessary conditions for metrics to produce
meaningful scores, such as distinguishing real from generated samples,
identifying mode dropping and mode collapsing, and detecting overfitting. With
a series of carefully designed experiments, we comprehensively investigate
existing sample-based metrics and identify their strengths and limitations in
practical settings. Based on these results, we observe that kernel Maximum Mean
Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test seem to
satisfy most of the desirable properties, provided that the distances between
samples are computed in a suitable feature space. Our experiments also unveil
interesting properties about the behavior of several popular GAN models, such
as whether they are memorizing training samples, and how far they are from
learning the target distribution.
",0,0,0,1,0,0
1923,Analytical and simulation studies of pedestrian flow at a crossing with random update rule,"  The intersecting pedestrian flow on the 2D lattice with random update rule is
studied. Each pedestrian has three moving directions without the back step.
Under periodic boundary conditions, an intermediate phase has been found at
which some pedestrians could move along the border of jamming stripes. We have
performed mean field analysis for the moving and intermediate phase
respectively. The analytical results agree with the simulation results well.
The empty site moves along the interface of jamming stripes when the system
only has one empty site. The average movement of empty site in one Monte Carlo
step (MCS) has been analyzed through the master equation. Under open boundary
conditions, the system exhibits moving and jamming phases. The critical
injection probability $\alpha_c$ shows nontrivially against the forward moving
probability $q$. The analytical results of average velocity, the density and
the flow rate against the injection probability in the moving phase also agree
with simulation results well.
",0,1,0,0,0,0
1924,Static and Fluctuating Magnetic Moments in the Ferroelectric Metal LiOsO$_3$,"  LiOsO$_3$ is the first example of a new class of material called a
ferroelectric metal. We performed zero-field and longitudinal-field $\mu$SR,
along with a combination of electronic structure and dipole field calculations,
to determine the magnetic ground state of LiOsO$_3$. We find that the sample
contains both static Li nuclear moments and dynamic Os electronic moments.
Below $\approx 0.7\,$K, the fluctuations of the Os moments slow down, though
remain dynamic down to 0.08$\,$K. We expect this could result in a frozen-out,
disordered ground state at even lower temperatures.
",0,1,0,0,0,0
1925,Compiling Deep Learning Models for Custom Hardware Accelerators,"  Convolutional neural networks (CNNs) are the core of most state-of-the-art
deep learning algorithms specialized for object detection and classification.
CNNs are both computationally complex and embarrassingly parallel. Two
properties that leave room for potential software and hardware optimizations
for embedded systems. Given a programmable hardware accelerator with a CNN
oriented custom instructions set, the compiler's task is to exploit the
hardware's full potential, while abiding with the hardware constraints and
maintaining generality to run different CNN models with varying workload
properties. Snowflake is an efficient and scalable hardware accelerator
implemented on programmable logic devices. It implements a control pipeline for
a custom instruction set. The goal of this paper is to present Snowflake's
compiler that generates machine level instructions from Torch7 model
description files. The main software design points explored in this work are:
model structure parsing, CNN workload breakdown, loop rearrangement for memory
bandwidth optimizations and memory access balancing. The performance achieved
by compiler generated instructions matches against hand optimized code for
convolution layers. Generated instructions also efficiently execute AlexNet and
ResNet18 inference on Snowflake. Snowflake with $256$ processing units was
synthesized on Xilinx's Zynq XC7Z045 FPGA. At $250$ MHz, AlexNet achieved in
$93.6$ frames/s and $1.2$ GB/s of off-chip memory bandwidth, and $21.4$
frames/s and $2.2$ GB/s for ResNet18. Total on-chip power is $5$ W.
",1,0,0,0,0,0
1926,Ranking with Adaptive Neighbors,"  Retrieving the most similar objects in a large-scale database for a given
query is a fundamental building block in many application domains, ranging from
web searches, visual, cross media, and document retrievals. State-of-the-art
approaches have mainly focused on capturing the underlying geometry of the data
manifolds. Graph-based approaches, in particular, define various diffusion
processes on weighted data graphs. Despite success, these approaches rely on
fixed-weight graphs, making ranking sensitive to the input affinity matrix. In
this study, we propose a new ranking algorithm that simultaneously learns the
data affinity matrix and the ranking scores. The proposed optimization
formulation assigns adaptive neighbors to each point in the data based on the
local connectivity, and the smoothness constraint assigns similar ranking
scores to similar data points. We develop a novel and efficient algorithm to
solve the optimization problem. Evaluations using synthetic and real datasets
suggest that the proposed algorithm can outperform the existing methods.
",0,0,0,1,0,0
1927,Identities and central polynomials of real graded division algebras,"  Let $A$ be a finite dimensional real algebra with a division grading by a
finite abelian group $G$. In this paper we provide finite basis for the
$T_G$-ideal of graded identities and for the $T_G$-space of graded central
polynomials for $A$.
",0,0,1,0,0,0
1928,"VB-Courant algebroids, E-Courant algebroids and generalized geometry","  In this paper, we first discuss the relation between VB-Courant algebroids
and E-Courant algebroids and construct some examples of E-Courant algebroids.
Then we introduce the notion of a generalized complex structure on an E-Courant
algebroid, unifying the usual generalized complex structures on
even-dimensional manifolds and generalized contact structures on
odd-dimensional manifolds. Moreover, we study generalized complex structures on
an omni-Lie algebroid in detail. In particular, we show that generalized
complex structures on an omni-Lie algebra $\gl(V)\oplus V$ correspond to
complex Lie algebra structures on V.
",0,0,1,0,0,0
1929,Majorana bound states in hybrid 2D Josephson junctions with ferromagnetic insulators,"  We consider a Josephson junction consisting of superconductor/ferromagnetic
insulator (S/FI) bilayers as electrodes which proximizes a nearby 2D electron
gas. By starting from a generic Josephson hybrid planar setup we present an
exhaustive analysis of the the interplay between the superconducting and
magnetic proximity effects and the conditions under which the structure
undergoes transitions to a non-trivial topological phase. We address the 2D
bound state problem using a general transfer matrix approach that reduces the
problem to an effective 1D Hamiltonian. This allows for straightforward study
of topological properties in different symmetry classes. As an example we
consider a narrow channel coupled with multiple ferromagnetic superconducting
fingers, and discuss how the Majorana bound states can be spatially controlled
by tuning the superconducting phases. Following our approach we also show the
energy spectrum, the free energy and finally the multiterminal Josephson
current of the setup.
",0,1,0,0,0,0
1930,Two-Dimensional Large Gap Topological Insulators with Large Rashba Spin-Orbit Coupling in Group-IV films,"  Rashba spin orbit coupling in topological insulators has attracted much
interest due to its exotic properties closely related to spintronic devices.
The coexistence of nontrivial topology and giant Rashba splitting, however, has
rare been observed in two-dimensional films, limiting severely its potential
applications at room temperature. Here, we propose a series of inversion
asymmetric group IV films, ABZ2, whose stability are confirmed by phonon
spectrum calculations. The analyses of electronic structures reveal that they
are intrinsic 2D TIs with a bulk gap as large as 0.74 eV, except for GeSiF2,
SnSiCl2, GeSiCl2 and GeSiBr2 monolayers which can transform from normal to
topological phases under appropriate tensile strains. Another prominent
intriguing feature is the giant Rashba spin splitting with a magnitude reaching
0.15 eV, the largest value reported in 2D films. These results present a
platform to explore 2D TIs for room temperature device applications.
",0,1,0,0,0,0
1931,Topological boundary invariants for Floquet systems and quantum walks,"  A Floquet systems is a periodically driven quantum system. It can be
described by a Floquet operator. If this unitary operator has a gap in the
spectrum, then one can define associated topological bulk invariants which can
either only depend on the bands of the Floquet operator or also on the time as
a variable. It is shown how a K-theoretic result combined with the
bulk-boundary correspondence leads to edge invariants for the half-space
Floquet operators. These results also apply to topological quantum walks.
",0,1,0,0,0,0
1932,Assumption-Based Approaches to Reasoning with Priorities,"  This paper maps out the relation between different approaches for handling
preferences in argumentation with strict rules and defeasible assumptions by
offering translations between them. The systems we compare are: non-prioritized
defeats i.e. attacks, preference-based defeats, and preference-based defeats
extended with reverse defeat.
",1,0,0,0,0,0
1933,On the Impact of Transposition Errors in Diffusion-Based Channels,"  In this work, we consider diffusion-based molecular communication with and
without drift between two static nano-machines. We employ type-based
information encoding, releasing a single molecule per information bit. At the
receiver, we consider an asynchronous detection algorithm which exploits the
arrival order of the molecules. In such systems, transposition errors
fundamentally undermine reliability and capacity. Thus, in this work we study
the impact of transpositions on the system performance. Towards this, we
present an analytical expression for the exact bit error probability (BEP)
caused by transpositions and derive computationally tractable approximations of
the BEP for diffusion-based channels with and without drift. Based on these
results, we analyze the BEP when background is not negligible and derive the
optimal bit interval that minimizes the BEP. Simulation results confirm the
theoretical results and show the error and goodput performance for different
parameters such as block size or noise generation rate.
",1,0,1,0,0,0
1934,Uniform $L^p$-improving for weighted averages on curves,"  We define variable parameter analogues of the affine arclength measure on
curves and prove near-optimal $L^p$-improving estimates for associated
multilinear generalized Radon transforms. Some of our results are new even in
the convolution case.
",0,0,1,0,0,0
1935,Finite Sample Differentially Private Confidence Intervals,"  We study the problem of estimating finite sample confidence intervals of the
mean of a normal population under the constraint of differential privacy. We
consider both the known and unknown variance cases and construct differentially
private algorithms to estimate confidence intervals. Crucially, our algorithms
guarantee a finite sample coverage, as opposed to an asymptotic coverage.
Unlike most previous differentially private algorithms, we do not require the
domain of the samples to be bounded. We also prove lower bounds on the expected
size of any differentially private confidence set showing that our the
parameters are optimal up to polylogarithmic factors.
",1,0,1,1,0,0
1936,xSDK Foundations: Toward an Extreme-scale Scientific Software Development Kit,"  Extreme-scale computational science increasingly demands multiscale and
multiphysics formulations. Combining software developed by independent groups
is imperative: no single team has resources for all predictive science and
decision support capabilities. Scientific libraries provide high-quality,
reusable software components for constructing applications with improved
robustness and portability. However, without coordination, many libraries
cannot be easily composed. Namespace collisions, inconsistent arguments, lack
of third-party software versioning, and additional difficulties make
composition costly.
The Extreme-scale Scientific Software Development Kit (xSDK) defines
community policies to improve code quality and compatibility across
independently developed packages (hypre, PETSc, SuperLU, Trilinos, and
Alquimia) and provides a foundation for addressing broader issues in software
interoperability, performance portability, and sustainability. The xSDK
provides turnkey installation of member software and seamless combination of
aggregate capabilities, and it marks first steps toward extreme-scale
scientific software ecosystems from which future applications can be composed
rapidly with assured quality and scalability.
",1,0,0,0,0,0
1937,Muon Spin Rotation Analysis of the Internal Magnetic Field of Heavy Fermion System Uranium Beryllium-13,"  Uranium beryllium-13 is a heavy fermion system whose anomalous behavior may
be explained by its poorly understood internal magnetic structure. Here,
uranium beryllium-13's magnetic distribution is probed via muon spin
spectroscopy ($\mu$SR)-a process where positive muons localize at magnetically
unique sites in the crystal lattice and precess at characteristic Larmor
frequencies, providing measurements of the internal field. Muon spin
experiments using the transverse-field technique conducted at varying
temperatures and external magnetic field strengths are analyzed via statistical
methods on ROOT. Two precession frequencies are observed at low temperatures
with an amplitude ratio in the Fourier transform of 2:1, enabling muon stopping
sites to be traced at the geometric centers of the edges of the crystal
lattice. Characteristic strong and weak magnetic sites are deduced,
additionally verified by mathematical relationships. Results can readily be
applied to other heavy fermion systems, and recent identification of quantum
critical points in a host of heavy fermion compounds show a promising future
for the application of these systems in quantum technology. Note that this
paper is an analysis of data, and all experiments mentioned here are conducted
by a third party.
",0,1,0,0,0,0
1938,The infinity-Fucik spectrum,"  In this article we study the behavior as $p \nearrow+\infty$ of the Fucik
spectrum for $p$-Laplace operator with zero Dirichlet boundary conditions in a
bounded domain $\Omega\subset \mathbb{R}^n$. We characterize the limit
equation, and we provide a description of the limit spectrum. Furthermore, we
show some explicit computations of the spectrum for certain configurations of
the domain.
",0,0,1,0,0,0
1939,Unitary Groups as Stabilizers of Orbits,"  We show that a finite unitary group which has orbits spanning the whole space
is necessarily the setwise stabilizer of a certain orbit.
",0,0,1,0,0,0
1940,m-TSNE: A Framework for Visualizing High-Dimensional Multivariate Time Series,"  Multivariate time series (MTS) have become increasingly common in healthcare
domains where human vital signs and laboratory results are collected for
predictive diagnosis. Recently, there have been increasing efforts to visualize
healthcare MTS data based on star charts or parallel coordinates. However, such
techniques might not be ideal for visualizing a large MTS dataset, since it is
difficult to obtain insights or interpretations due to the inherent high
dimensionality of MTS. In this paper, we propose 'm-TSNE': a simple and novel
framework to visualize high-dimensional MTS data by projecting them into a
low-dimensional (2-D or 3-D) space while capturing the underlying data
properties. Our framework is easy to use and provides interpretable insights
for healthcare professionals to understand MTS data. We evaluate our
visualization framework on two real-world datasets and demonstrate that the
results of our m-TSNE show patterns that are easy to understand while the other
methods' visualization may have limitations in interpretability.
",1,0,0,1,0,0
1941,On stabilization of solutions of nonlinear parabolic equations with a gradient term,"  For parabolic equations of the form $$ \frac{\partial u}{\partial t} -
\sum_{i,j=1}^n a_{ij} (x, u) \frac{\partial^2 u}{\partial x_i \partial x_j} + f
(x, u, D u) = 0 \quad \mbox{in } {\mathbb R}_+^{n+1}, $$ where ${\mathbb
R}_+^{n+1} = {\mathbb R}^n \times (0, \infty)$, $n \ge 1$, $D = (\partial /
\partial x_1, \ldots, \partial / \partial x_n)$ is the gradient operator, and
$f$ is some function, we obtain conditions guaranteeing that every solution
tends to zero as $t \to \infty$.
",0,0,1,0,0,0
1942,Deep metric learning for multi-labelled radiographs,"  Many radiological studies can reveal the presence of several co-existing
abnormalities, each one represented by a distinct visual pattern. In this
article we address the problem of learning a distance metric for plain
radiographs that captures a notion of ""radiological similarity"": two chest
radiographs are considered to be similar if they share similar abnormalities.
Deep convolutional neural networks (DCNs) are used to learn a low-dimensional
embedding for the radiographs that is equipped with the desired metric. Two
loss functions are proposed to deal with multi-labelled images and potentially
noisy labels. We report on a large-scale study involving over 745,000 chest
radiographs whose labels were automatically extracted from free-text
radiological reports through a natural language processing system. Using 4,500
validated exams, we demonstrate that the methodology performs satisfactorily on
clustering and image retrieval tasks. Remarkably, the learned metric separates
normal exams from those having radiological abnormalities.
",0,0,0,1,0,0
1943,Algebras of generalized dihedral type,"  We provide a complete classification of all algebras of generalised dihedral
type, which are natural generalizations of algebras which occurred in the study
of blocks with dihedral defect groups. This gives a description by quivers and
relations coming from surface triangulations.
",0,0,1,0,0,0
1944,Resonant particle production during inflation: a full analytical study,"  We revisit the study of the phenomenology associated to a burst of particle
production of a field whose mass is controlled by the inflaton field and
vanishes at one given instance during inflation. This generates a bump in the
correlators of the primordial scalar curvature. We provide a unified formalism
to compute various effects that have been obtained in the literature and
confirm that the dominant effects are due to the rescattering of the produced
particles on the inflaton condensate. We improve over existing results (based
on numerical fits) by providing exact analytic expressions for the shape and
height of the bump, both in the power spectrum and the equilateral bispectrum.
We then study the regime of validity of the perturbative computations of this
signature. Finally, we extend these computations to the case of a burst of
particle production in a sector coupled only gravitationally to the inflaton.
",0,1,0,0,0,0
1945,Average whenever you meet: Opportunistic protocols for community detection,"  Consider the following asynchronous, opportunistic communication model over a
graph $G$: in each round, one edge is activated uniformly and independently at
random and (only) its two endpoints can exchange messages and perform local
computations. Under this model, we study the following random process: The
first time a vertex is an endpoint of an active edge, it chooses a random
number, say $\pm 1$ with probability $1/2$; then, in each round, the two
endpoints of the currently active edge update their values to their average. We
show that, if $G$ exhibits a two-community structure (for example, two
expanders connected by a sparse cut), the values held by the nodes will
collectively reflect the underlying community structure over a suitable phase
of the above process, allowing efficient and effective recovery in important
cases.
In more detail, we first provide a first-moment analysis showing that, for a
large class of almost-regular clustered graphs that includes the stochastic
block model, the expected values held by all but a negligible fraction of the
nodes eventually reflect the underlying cut signal. We prove this property
emerges after a mixing period of length $\mathcal O(n\log n)$. We further
provide a second-moment analysis for a more restricted class of regular
clustered graphs that includes the regular stochastic block model. For this
case, we are able to show that most nodes can efficiently and locally identify
their community of reference over a suitable time window. This results in the
first opportunistic protocols that approximately recover community structure
using only polylogarithmic work per node. Even for the above class of regular
graphs, our second moment analysis requires new concentration bounds on the
product of certain random matrices that are technically challenging and
possibly of independent interest.
",1,0,0,0,0,0
1946,A polynomial-time approximation algorithm for all-terminal network reliability,"  We give a fully polynomial-time randomized approximation scheme (FPRAS) for
the all-terminal network reliability problem, which is to determine the
probability that, in a undirected graph, assuming each edge fails
independently, the remaining graph is still connected. Our main contribution is
to confirm a conjecture by Gorodezky and Pak (Random Struct. Algorithms, 2014),
that the expected running time of the ""cluster-popping"" algorithm in
bi-directed graphs is bounded by a polynomial in the size of the input.
",1,0,0,0,0,0
1947,Provably Accurate Double-Sparse Coding,"  Sparse coding is a crucial subroutine in algorithms for various signal
processing, deep learning, and other machine learning applications. The central
goal is to learn an overcomplete dictionary that can sparsely represent a given
input dataset. However, a key challenge is that storage, transmission, and
processing of the learned dictionary can be untenably high if the data
dimension is high. In this paper, we consider the double-sparsity model
introduced by Rubinstein et al. (2010b) where the dictionary itself is the
product of a fixed, known basis and a data-adaptive sparse component. First, we
introduce a simple algorithm for double-sparse coding that can be amenable to
efficient implementation via neural architectures. Second, we theoretically
analyze its performance and demonstrate asymptotic sample complexity and
running time benefits over existing (provable) approaches for sparse coding. To
our knowledge, our work introduces the first computationally efficient
algorithm for double-sparse coding that enjoys rigorous statistical guarantees.
Finally, we support our analysis via several numerical experiments on simulated
data, confirming that our method can indeed be useful in problem sizes
encountered in practical applications.
",1,0,0,1,0,0
1948,Asymptotic theory for maximum likelihood estimates in reduced-rank multivariate generalised linear models,"  Reduced-rank regression is a dimensionality reduction method with many
applications. The asymptotic theory for reduced rank estimators of parameter
matrices in multivariate linear models has been studied extensively. In
contrast, few theoretical results are available for reduced-rank multivariate
generalised linear models. We develop M-estimation theory for concave criterion
functions that are maximised over parameters spaces that are neither convex nor
closed. These results are used to derive the consistency and asymptotic
distribution of maximum likelihood estimators in reduced-rank multivariate
generalised linear models, when the response and predictor vectors have a joint
distribution. We illustrate our results in a real data classification problem
with binary covariates.
",0,0,1,1,0,0
1949,Sequential two-fold Pearson chi-squared test and tails of the Bessel process distributions,"  We find asymptotic formulas for error probabilities of two-fold Pearson
goodness-of-fit test as functions of two critical levels. These results may be
reformulated in terms of tails of two-dimensional distributions of the Bessel
process. Necessary properties of the Infeld function are obtained.
",0,0,1,1,0,0
1950,Tracking Gaze and Visual Focus of Attention of People Involved in Social Interaction,"  The visual focus of attention (VFOA) has been recognized as a prominent
conversational cue. We are interested in estimating and tracking the VFOAs
associated with multi-party social interactions. We note that in this type of
situations the participants either look at each other or at an object of
interest; therefore their eyes are not always visible. Consequently both gaze
and VFOA estimation cannot be based on eye detection and tracking. We propose a
method that exploits the correlation between eye gaze and head movements. Both
VFOA and gaze are modeled as latent variables in a Bayesian switching
state-space model. The proposed formulation leads to a tractable learning
procedure and to an efficient algorithm that simultaneously tracks gaze and
visual focus. The method is tested and benchmarked using two publicly available
datasets that contain typical multi-party human-robot and human-human
interactions.
",1,0,0,0,0,0
1951,Fully-Dynamic and Kinetic Conflict-Free Coloring of Intervals with Respect to Points,"  We introduce the fully-dynamic conflict-free coloring problem for a set $S$
of intervals in $\mathbb{R}^1$ with respect to points, where the goal is to
maintain a conflict-free coloring for$S$ under insertions and deletions. A
coloring is conflict-free if for each point $p$ contained in some interval, $p$
is contained in an interval whose color is not shared with any other interval
containing $p$. We investigate trade-offs between the number of colors used and
the number of intervals that are recolored upon insertion or deletion of an
interval. Our results include:
- a lower bound on the number of recolorings as a function of the number of
colors, which implies that with $O(1)$ recolorings per update the worst-case
number of colors is $\Omega(\log n/\log\log n)$, and that any strategy using
$O(1/\varepsilon)$ colors needs $\Omega(\varepsilon n^{\varepsilon})$
recolorings;
- a coloring strategy that uses $O(\log n)$ colors at the cost of $O(\log n)$
recolorings, and another strategy that uses $O(1/\varepsilon)$ colors at the
cost of $O(n^{\varepsilon}/\varepsilon)$ recolorings;
- stronger upper and lower bounds for special cases.
We also consider the kinetic setting where the intervals move continuously
(but there are no insertions or deletions); here we show how to maintain a
coloring with only four colors at the cost of three recolorings per event and
show this is tight.
",1,0,0,0,0,0
1952,Magnetic properties in ultra-thin 3d transition metal alloys II: Experimental verification of quantitative theories of damping and spin-pumping,"  A systematic experimental study of Gilbert damping is performed via
ferromagnetic resonance for the disordered crystalline binary 3d transition
metal alloys Ni-Co, Ni-Fe and Co-Fe over the full range of alloy compositions.
After accounting for inhomogeneous linewidth broadening, the damping shows
clear evidence of both interfacial damping enhancement (by spin pumping) and
radiative damping. We quantify these two extrinsic contributions and thereby
determine the intrinsic damping. The comparison of the intrinsic damping to
multiple theoretical calculations yields good qualitative and quantitative
agreement in most cases. Furthermore, the values of the damping obtained in
this study are in good agreement with a wide range of published experimental
and theoretical values. Additionally, we find a compositional dependence of the
spin mixing conductance.
",0,1,0,0,0,0
1953,Detection of Anomalies in Large Scale Accounting Data using Deep Autoencoder Networks,"  Learning to detect fraud in large-scale accounting data is one of the
long-standing challenges in financial statement audits or fraud investigations.
Nowadays, the majority of applied techniques refer to handcrafted rules derived
from known fraud scenarios. While fairly successful, these rules exhibit the
drawback that they often fail to generalize beyond known fraud scenarios and
fraudsters gradually find ways to circumvent them. To overcome this
disadvantage and inspired by the recent success of deep learning we propose the
application of deep autoencoder neural networks to detect anomalous journal
entries. We demonstrate that the trained network's reconstruction error
obtainable for a journal entry and regularized by the entry's individual
attribute probabilities can be interpreted as a highly adaptive anomaly
assessment. Experiments on two real-world datasets of journal entries, show the
effectiveness of the approach resulting in high f1-scores of 32.93 (dataset A)
and 16.95 (dataset B) and less false positive alerts compared to state of the
art baseline methods. Initial feedback received by chartered accountants and
fraud examiners underpinned the quality of the approach in capturing highly
relevant accounting anomalies.
",1,0,0,0,0,0
1954,Alternate Estimation of a Classifier and the Class-Prior from Positive and Unlabeled Data,"  We consider a problem of learning a binary classifier only from positive data
and unlabeled data (PU learning) and estimating the class-prior in unlabeled
data under the case-control scenario. Most of the recent methods of PU learning
require an estimate of the class-prior probability in unlabeled data, and it is
estimated in advance with another method. However, such a two-step approach
which first estimates the class prior and then trains a classifier may not be
the optimal approach since the estimation error of the class-prior is not taken
into account when a classifier is trained. In this paper, we propose a novel
unified approach to estimating the class-prior and training a classifier
alternately. Our proposed method is simple to implement and computationally
efficient. Through experiments, we demonstrate the practical usefulness of the
proposed method.
",0,0,0,1,0,0
1955,Oscillating dipole with fractional quantum source in Aharonov-Bohm electrodynamics,"  We show, in the case of a special dipolar source, that electromagnetic fields
in fractional quantum mechanics have an unexpected space dependence:
propagating fields may have non-transverse components, and the distinction
between near-field zone and wave zone is blurred. We employ an extension of
Maxwell theory, Aharonov-Bohm electrodynamics, which is compatible with
currents $j^\nu$ conserved globally but not locally, we have derived in another
work the field equation $\partial_\mu F^{\mu \nu}=j^\nu+i^\nu$, where $i^\nu$
is a non-local function of $j^\nu$, called ""secondary current"". Y.\ Wei has
recently proved that the probability current in fractional quantum mechanics is
in general not locally conserved. We compute this current for a Gaussian wave
packet with fractional parameter $a=3/2$ and find that in a suitable limit it
can be approximated by our simplified dipolar source. Currents which are not
locally conserved may be present also in other quantum systems whose wave
functions satisfy non-local equations. The combined electromagnetic effects of
such sources and their secondary currents are very interesting both
theoretically and for potential applications.
",0,1,0,0,0,0
1956,Randomized Linear Programming Solves the Discounted Markov Decision Problem In Nearly-Linear (Sometimes Sublinear) Running Time,"  We propose a novel randomized linear programming algorithm for approximating
the optimal policy of the discounted Markov decision problem. By leveraging the
value-policy duality and binary-tree data structures, the algorithm adaptively
samples state-action-state transitions and makes exponentiated primal-dual
updates. We show that it finds an $\epsilon$-optimal policy using nearly-linear
run time in the worst case. When the Markov decision process is ergodic and
specified in some special data formats, the algorithm finds an
$\epsilon$-optimal policy using run time linear in the total number of
state-action pairs, which is sublinear in the input size. These results provide
a new venue and complexity benchmarks for solving stochastic dynamic programs.
",1,0,1,0,0,0
1957,SCAV'18: Report of the 2nd International Workshop on Safe Control of Autonomous Vehicles,"  This report summarizes the discussions, open issues, take-away messages, and
conclusions of the 2nd SCAV workshop.
",1,0,0,0,0,0
1958,Inference For High-Dimensional Split-Plot-Designs: A Unified Approach for Small to Large Numbers of Factor Levels,"  Statisticians increasingly face the problem to reconsider the adaptability of
classical inference techniques. In particular, divers types of high-dimensional
data structures are observed in various research areas; disclosing the
boundaries of conventional multivariate data analysis. Such situations occur,
e.g., frequently in life sciences whenever it is easier or cheaper to
repeatedly generate a large number $d$ of observations per subject than
recruiting many, say $N$, subjects. In this paper we discuss inference
procedures for such situations in general heteroscedastic split-plot designs
with $a$ independent groups of repeated measurements. These will, e.g., be able
to answer questions about the occurrence of certain time, group and
interactions effects or about particular profiles.
The test procedures are based on standardized quadratic forms involving
suitably symmetrized U-statistics-type estimators which are robust against an
increasing number of dimensions $d$ and/or groups $a$. We then discuss its
limit distributions in a general asymptotic framework and additionally propose
improved small sample approximations. Finally its small sample performance is
investigated in simulations and the applicability is illustrated by a real data
analysis.
",0,0,1,1,0,0
1959,Rate Optimal Binary Linear Locally Repairable Codes with Small Availability,"  A locally repairable code with availability has the property that every code
symbol can be recovered from multiple, disjoint subsets of other symbols of
small size. In particular, a code symbol is said to have $(r,t)$-availability
if it can be recovered from $t$ disjoint subsets, each of size at most $r$. A
code with availability is said to be 'rate-optimal', if its rate is maximum
among the class of codes with given locality, availability, and alphabet size.
This paper focuses on rate-optimal binary, linear codes with small
availability, and makes four contributions. First, it establishes tight upper
bounds on the rate of binary linear codes with $(r,2)$ and $(2,3)$
availability. Second, it establishes a uniqueness result for binary
rate-optimal codes, showing that for certain classes of binary linear codes
with $(r,2)$ and $(2,3)$-availability, any rate optimal code must be a direct
sum of shorter rate optimal codes. Third, it presents novel upper bounds on the
rates of binary linear codes with $(2,t)$ and $(r,3)$-availability. In
particular, the main contribution here is a new method for bounding the number
of cosets of the dual of a code with availability, using its covering
properties. Finally, it presents a class of locally repairable linear codes
associated with convex polyhedra, focusing on the codes associated with the
Platonic solids. It demonstrates that these codes are locally repairable with
$t = 2$, and that the codes associated with (geometric) dual polyhedra are
(coding theoretic) duals of each other.
",1,0,1,0,0,0
1960,A general renormalization procedure on the one-dimensional lattice and decay of correlations,"  We present a general form of Renormalization operator $\mathcal{R}$ acting on
potentials $V:\{0,1\}^\mathbb{N} \to \mathbb{R}$. We exhibit the analytical
expression of the fixed point potential $V$ for such operator $\mathcal{R}$.
This potential can be expressed in a naturally way in terms of a certain
integral over the Hausdorff probability on a Cantor type set on the interval
$[0,1]$. This result generalizes a previous one by A. Baraviera, R. Leplaideur
and A. Lopes where the fixed point potential $V$ was of Hofbauer type.
For the potentials of Hofbauer type (a well known case of phase transition)
the decay is like $n^{-\gamma}$, $\gamma>0$.
Among other things we present the estimation of the decay of correlation of
the equilibrium probability associated to the fixed potential $V$ of our
general renormalization procedure. In some cases we get polynomial decay like
$n^{-\gamma}$, $\gamma>0$, and in others a decay faster than $n \,e^{ -\,
\sqrt{n}}$, when $n \to \infty$.
The potentials $g$ we consider here are elements of the so called family of
Walters potentials on $\{0,1\}^\mathbb{N} $ which generalizes the potentials
considered initially by F. Hofbauer. For these potentials some explicit
expressions for the eigenfunctions are known.
In a final section we also show that given any choice $d_n \to 0$ of real
numbers varying with $n \in \mathbb{N}$ there exist a potential $g$ on the
class defined by Walters which has a invariant probability with such numbers as
the coefficients of correlation (for a certain explicit observable function).
",0,1,1,0,0,0
1961,Duality Spectral Sequences for Weierstrass Fibrations and Applications,"  We study duality spectral sequences for Weierstrass fibrations. Using these
spectral sequences, we show that on a K-trivial Weierstrass threefold over a
K-numerically trivial surface, any line bundle of nonzero fiber degree is taken
by a Fourier-Mukai transform to a slope stable locally free sheaf.
",0,0,1,0,0,0
1962,Occupation times for the finite buffer fluid queue with phase-type ON-times,"  In this short communication we study a fluid queue with a finite buffer. The
performance measure we are interested in is the occupation time over a finite
time period, i.e., the fraction of time the workload process is below some
fixed target level. We construct an alternating sequence of sojourn times
$D_1,U_1,...$ where the pairs $(D_i,U_i)_{i\in\mathbb{N}}$ are i.i.d. random
vectors. We use this sequence to determine the distribution function of the
occupation time in terms of its double transform.
",0,0,1,0,0,0
1963,Affine forward variance models,"  We introduce the class of affine forward variance (AFV) models of which both
the conventional Heston model and the rough Heston model are special cases. We
show that AFV models can be characterized by the affine form of their cumulant
generating function, which can be obtained as solution of a convolution Riccati
equation. We further introduce the class of affine forward order flow intensity
(AFI) models, which are structurally similar to AFV models, but driven by jump
processes, and which include Hawkes-type models. We show that the cumulant
generating function of an AFI model satisfies a generalized convolution Riccati
equation and that a high-frequency limit of AFI models converges in
distribution to the AFV model.
",0,0,0,0,0,1
1964,The cohomology of free loop spaces of homogeneous spaces,"  The free loops space $\Lambda X$ of a space $X$ has become an important
object of study particularly in the case when $X$ is a manifold.The study of
free loop spaces is motivated in particular by two main examples. The first is
their relation to geometrically distinct periodic geodesics on a manifold,
originally studied by Gromoll and Meyer in $1969$. More recently the study of
string topology and in particular the Chas-Sullivan loop product has been an
active area of research.
A complete flag manifold is the quotient of a Lie group by its maximal torus
and is one of the nicer examples of a homogeneous space. Both the cohomology
and Chas-Sullivan product structure are understood for spaces $S^n$,
$\mathbb{C}P^n$ and most simple Lie groups. Hence studying the topology of the
free loops space on homogeneous space is a natural next step.
In the thesis we compute the differentials in the integral Leray-Serre
spectral sequence associated to the free loops space fibrations in the cases of
$SU(n+1)/T^n$ and $Sp(n)/T^n$. Study in detail the structure of the third page
of the spectral sequence in the case of $SU(n)$ and give the module structure
of $H^*(\Lambda(SU(3)/T^2);\mathbb{Z})$ and
$H^*(\Lambda(Sp(2)/T^2);\mathbb{Z})$.
",0,0,1,0,0,0
1965,Measurement of the muon-induced neutron seasonal modulation with LVD,"  Cosmic ray muons with the average energy of 280 GeV and neutrons produced by
muons are detected with the Large Volume Detector at LNGS. We present an
analysis of the seasonal variation of the neutron flux on the basis of the data
obtained during 15 years. The measurement of the seasonal variation of the
specific number of neutrons generated by muons allows to obtaine the variation
magnitude of of the average energy of the muon flux at the depth of the LVD
location. The source of the seasonal variation of the total neutron flux is a
change of the intensity and the average energy of the muon flux.
",0,1,0,0,0,0
1966,Trail-Mediated Self-Interaction,"  A number of microorganisms leave persistent trails while moving along
surfaces. For single-cell organisms, the trail-mediated self-interaction will
influence its dynamics. It has been discussed recently [Kranz \textit{et al.}
Phys. Rev. Lett. \textbf{117}, 8101 (2016)] that the self-interaction may
localize the organism above a critical coupling $\chi_c$ to the trail. Here we
will derive a generalized active particle model capturing the key features of
the self-interaction and analyze its behavior for smaller couplings $\chi <
\chi_c$. We find that fluctuations in propulsion speed shift the localization
transition to stronger couplings.
",0,0,0,0,1,0
1967,A sub-super solution method for a class of nonlocal problems involving the p(x)-Laplacian operator and applications,"  In the present paper we study the existence of solutions for some nonlocal
problems involving the p(x)-Laplacian operator. The approach is based on a new
sub-supersolution method
",0,0,1,0,0,0
1968,Summability properties of Gabor expansions,"  We show that there exist complete and minimal systems of time-frequency
shifts of Gaussians in $L^2(\mathbb{R})$ which are not strong Markushevich
basis (do not admit the spectral synthesis). In particular, it implies that
there is no linear summation method for general Gaussian Gabor expansions. On
the other hand we prove that the spectral synthesis for such Gabor systems
holds up to one dimensional defect.
",0,0,1,0,0,0
1969,A Las Vegas algorithm to solve the elliptic curve discrete logarithm problem,"  In this paper, we describe a new Las Vegas algorithm to solve the elliptic
curve discrete logarithm problem. The algorithm depends on a property of the
group of rational points of an elliptic curve and is thus not a generic
algorithm. The algorithm that we describe has some similarities with the most
powerful index-calculus algorithm for the discrete logarithm problem over a
finite field.
",1,0,1,0,0,0
1970,Spectral sequences via examples,"  These are lecture notes for a short course about spectral sequences that was
held at M?­laga, October 18--20 (2016), during the ""Fifth Young Spanish
Topologists Meeting"". The approach was to illustrate the basic notions via
fully computed examples arising from Algebraic Topology and Group Theory.
",0,0,1,0,0,0
1971,Coherent scattering from semi-infinite non-Hermitian potentials,"  When two identical (coherent) beams are injected at a semi-infinite
non-Hermitian medium from left and right, we show that both reflection
$(r_L,r_R)$ and transmission $(t_L,t_R)$ amplitudes are non-reciprocal. In a
parametric domain, there exists Spectral Singularity (SS) at a real energy
$E=E_*$ and the determinant of the time-reversed two port S-matrix i.e.,
$|\det(S)|=|t_L t_R-r_L r_R|$ vanishes sharply at $E=E_*$ displaying the
phenomenon of Coherent Perfect Absorption (CPA). In the complimentary
parametric domain, the potential becomes either left or right reflectionless at
$E=E_z$. But we rule out the existence of Invisibility despite $r_R(E_i)=0$ and
$t_R(E_i)=1$ in these new models. We present two simple exactly solvable models
where the expressions for $E_*$, $E_z$, $E_i$ and the parametric conditions on
the potential have been obtained in explicit and simple forms. Earlier, the
novel phenomena of SS and CPA have been found to occur only in the scattering
complex potentials which are spatially localized (vanish asymptotically) and
having $t_L=t_R$.
",0,1,0,0,0,0
1972,A Higher Structure Identity Principle,"  We prove a Structure Identity Principle for theories defined on types of
$h$-level 3 by defining a general notion of saturation for a large class of
structures definable in the Univalent Foundations.
",1,0,1,0,0,0
1973,Two-Player Games for Efficient Non-Convex Constrained Optimization,"  In recent years, constrained optimization has become increasingly relevant to
the machine learning community, with applications including Neyman-Pearson
classification, robust optimization, and fair machine learning. A natural
approach to constrained optimization is to optimize the Lagrangian, but this is
not guaranteed to work in the non-convex setting, and, if using a first-order
method, cannot cope with non-differentiable constraints (e.g. constraints on
rates or proportions).
The Lagrangian can be interpreted as a two-player game played between a
player who seeks to optimize over the model parameters, and a player who wishes
to maximize over the Lagrange multipliers. We propose a non-zero-sum variant of
the Lagrangian formulation that can cope with non-differentiable--even
discontinuous--constraints, which we call the ""proxy-Lagrangian"". The first
player minimizes external regret in terms of easy-to-optimize ""proxy
constraints"", while the second player enforces the original constraints by
minimizing swap regret.
For this new formulation, as for the Lagrangian in the non-convex setting,
the result is a stochastic classifier. For both the proxy-Lagrangian and
Lagrangian formulations, however, we prove that this classifier, instead of
having unbounded size, can be taken to be a distribution over no more than m+1
models (where m is the number of constraints). This is a significant
improvement in practical terms.
",0,0,0,1,0,0
1974,On thin local sets of the Gaussian free field,"  We study how small a local set of the continuum Gaussian free field (GFF) in
dimension $d$ has to be to ensure that this set is thin, which loosely speaking
means that it captures no GFF mass on itself, in other words, that the field
restricted to it is zero. We provide a criterion on the size of the local set
for this to happen, and on the other hand, we show that this criterion is sharp
by constructing small local sets that are not thin.
",0,0,1,0,0,0
1975,A Note on Prediction Markets,"  In a prediction market, individuals can sequentially place bets on the
outcome of a future event. This leaves a trail of personal probabilities for
the event, each being conditional on the current individual's private
background knowledge and on the previously announced probabilities of other
individuals, which give partial information about their private knowledge. By
means of theory and examples, we revisit some results in this area. In
particular, we consider the case of two individuals, who start with the same
overall probability distribution but different private information, and then
take turns in updating their probabilities. We note convergence of the
announced probabilities to a limiting value, which may or may not be the same
as that based on pooling their private information.
",0,0,1,1,0,0
1976,Dihedral angle prediction using generative adversarial networks,"  Several dihedral angles prediction methods were developed for protein
structure prediction and their other applications. However, distribution of
predicted angles would not be similar to that of real angles. To address this
we employed generative adversarial networks (GAN). Generative adversarial
networks are composed of two adversarially trained networks: a discriminator
and a generator. A discriminator distinguishes samples from a dataset and
generated samples while a generator generates realistic samples. Although the
discriminator of GANs is trained to estimate density, GAN model is intractable.
On the other hand, noise-contrastive estimation (NCE) was introduced to
estimate a normalization constant of an unnormalized statistical model and thus
the density function. In this thesis, we introduce noise-contrastive estimation
generative adversarial networks (NCE-GAN) which enables explicit density
estimation of a GAN model. And a new loss for the generator is proposed. We
also propose residue-wise variants of auxiliary classifier GAN (AC-GAN) and
Semi-supervised GAN to handle sequence information in a window. In our
experiment, the conditional generative adversarial network (C-GAN), AC-GAN and
Semi-supervised GAN were compared. And experiments done with improved
conditions were invested. We identified a phenomenon of AC-GAN that
distribution of its predicted angles is composed of unusual clusters. The
distribution of the predicted angles of Semi-supervised GAN was most similar to
the Ramachandran plot. We found that adding the output of the NCE as an
additional input of the discriminator is helpful to stabilize the training of
the GANs and to capture the detailed structures. Adding regression loss and
using predicted angles by regression loss only model could improve the
conditional generation performance of the C-GAN and AC-GAN.
",0,0,0,1,1,0
1977,A recurrence relation for the odd order moments of the Fabius function,"  A simple recurrence relation for the even order moments of the Fabius
function is proven. Also, a very similar formula for the odd order moments in
terms of the even order moments is proved. The matrices corresponding to these
formulas (and their inverses) are multiplied so as to obtain a matrix that
correspond to a recurrence relation for the odd order moments in terms of
themselves. The theorem at the end gives a closed-form for the coefficients.
",0,0,1,0,0,0
1978,Performance of Range Separated Hybrids: Study within BECKE88 family and Semilocal Exchange Hole based Range Separated Hybrid,"  A long range corrected range separated hybrid functional is developed based
on the density matrix expansion (DME) based semilocal exchange hole with
Lee-Yang-Parr (LYP) correlation. An extensive study involving the proposed
range separated hybrid for thermodynamic as well as properties related to the
fractional occupation number is compared with different BECKE88 family
semilocal, hybrid and range separated hybrids. It has been observed that using
Kohn-Sham kinetic energy dependent exchange hole several properties related to
the fractional occupation number can be improved without hindering the
thermochemical accuracy. The newly constructed range separated hybrid
accurately describe the hydrogen and non-hydrogen reaction barrier heights. The
present range separated functional has been constructed using full semilocal
meta-GGA type exchange hole having exact properties related to exchange hole
therefore, it has a strong physical basis.
",0,1,0,0,0,0
1979,Manifold Mixup: Learning Better Representations by Interpolating Hidden States,"  Deep networks often perform well on the data distribution on which they are
trained, yet give incorrect (and often very confident) answers when evaluated
on points from off of the training distribution. This is exemplified by the
adversarial examples phenomenon but can also be seen in terms of model
generalization and domain shift. Ideally, a model would assign lower confidence
to points unlike those from the training distribution. We propose a regularizer
which addresses this issue by training with interpolated hidden states and
encouraging the classifier to be less confident at these points. Because the
hidden states are learned, this has an important effect of encouraging the
hidden states for a class to be concentrated in such a way so that
interpolations within the same class or between two different classes do not
intersect with the real data points from other classes. This has a major
advantage in that it avoids the underfitting which can result from
interpolating in the input space. We prove that the exact condition for this
problem of underfitting to be avoided by Manifold Mixup is that the
dimensionality of the hidden states exceeds the number of classes, which is
often the case in practice. Additionally, this concentration can be seen as
making the features in earlier layers more discriminative. We show that despite
requiring no significant additional computation, Manifold Mixup achieves large
improvements over strong baselines in supervised learning, robustness to
single-step adversarial attacks, semi-supervised learning, and Negative
Log-Likelihood on held out samples.
",0,0,0,1,0,0
1980,Small Resolution Proofs for QBF using Dependency Treewidth,"  In spite of the close connection between the evaluation of quantified Boolean
formulas (QBF) and propositional satisfiability (SAT), tools and techniques
which exploit structural properties of SAT instances are known to fail for QBF.
This is especially true for the structural parameter treewidth, which has
allowed the design of successful algorithms for SAT but cannot be
straightforwardly applied to QBF since it does not take into account the
interdependencies between quantified variables.
In this work we introduce and develop dependency treewidth, a new structural
parameter based on treewidth which allows the efficient solution of QBF
instances. Dependency treewidth pushes the frontiers of tractability for QBF by
overcoming the limitations of previously introduced variants of treewidth for
QBF. We augment our results by developing algorithms for computing the
decompositions that are required to use the parameter.
",1,0,0,0,0,0
1981,Theoretical Analysis of Generalized Sagnac Effect in the Standard Synchronization,"  The Sagnac effect has been shown in inertial frames as well as rotating
frames. We solve the problem of the generalized Sagnac effect in the standard
synchronization of clocks. The speed of a light beam that traverses an optical
fiber loop is measured with respect to the proper time of the light detector,
and is shown to be other than the constant c, though it appears to be c if
measured by the time standard-synchronized. The fiber loop, which can have an
arbitrary shape, is described by an infinite number of straight lines such that
it can be handled by the general framework of Mansouri and Sexl (MS). For a
complete analysis of the Sagnac effect, the motion of the laboratory should be
taken into account. The MS framework is introduced to deal with its motion
relative to a preferred reference frame. Though the one-way speed of light is
other than c, its two-way speed is shown to be c with respect to the proper
time. The theoretical analysis of the generalized Sagnac effect corresponds to
the experimental results, and shows the usefulness of the standard
synchronization. The introduction of the standard synchrony can make
mathematical manipulation easy and can allow us to deal with relative motions
between inertial frames without information on their velocities relative to the
preferred frame.
",0,1,0,0,0,0
1982,"Lattice thermal expansion and anisotropic displacements in urea, bromomalonic aldehyde, pentachloropyridine and naphthalene","  Anisotropic displacement parameters (ADPs) are commonly used in
crystallography, chemistry and related fields to describe and quantify thermal
motion of atoms. Within the very recent years, these ADPs have become
predictable by lattice dynamics in combination with first-principles theory.
Here, we study four very different molecular crystals, namely urea,
bromomalonic aldehyde, pentachloropyridine, and naphthalene, by
first-principles theory to assess the quality of ADPs calculated in the
quasi-harmonic approximation. In addition, we predict both thermal expansion
and thermal motion within the quasi-harmonic approximation and compare the
predictions with experimental data. Very reliable ADPs are calculated within
the quasi-harmonic approximation for all four cases up to at least 200 K, and
they turn out to be in better agreement with experiment than the harmonic ones.
In one particular case, ADPs can even reliably be predicted up to room
temperature. Our results also hint at the importance of normal-mode
anharmonicity in the calculation of ADPs.
",0,1,0,0,0,0
1983,Learning Heuristic Search via Imitation,"  Robotic motion planning problems are typically solved by constructing a
search tree of valid maneuvers from a start to a goal configuration. Limited
onboard computation and real-time planning constraints impose a limit on how
large this search tree can grow. Heuristics play a crucial role in such
situations by guiding the search towards potentially good directions and
consequently minimizing search effort. Moreover, it must infer such directions
in an efficient manner using only the information uncovered by the search up
until that time. However, state of the art methods do not address the problem
of computing a heuristic that explicitly minimizes search effort. In this
paper, we do so by training a heuristic policy that maps the partial
information from the search to decide which node of the search tree to expand.
Unfortunately, naively training such policies leads to slow convergence and
poor local minima. We present SaIL, an efficient algorithm that trains
heuristic policies by imitating ""clairvoyant oracles"" - oracles that have full
information about the world and demonstrate decisions that minimize search
effort. We leverage the fact that such oracles can be efficiently computed
using dynamic programming and derive performance guarantees for the learnt
heuristic. We validate the approach on a spectrum of environments which show
that SaIL consistently outperforms state of the art algorithms. Our approach
paves the way forward for learning heuristics that demonstrate an anytime
nature - finding feasible solutions quickly and incrementally refining it over
time.
",1,0,0,0,0,0
1984,Hook removal operators on the odd Young graph,"  In this article we consider hook removal operators on odd partitions, i.e.,
partitions labelling odd-degree irreducible characters of finite symmetric
groups. In particular we complete the discussion, started by Isaacs, Navarro,
Olsson and Tiep in 2016, concerning the commutativity of such operators.
",0,0,1,0,0,0
1985,Modal operators and toric ideals,"  In the present paper we consider modal propositional logic and look for the
constraints that are imposed to the propositions of the special type $\Box a$
by the structure of the relevant finite Kripke frame. We translate the usual
language of modal propositional logic in terms of notions of commutative
algebra, namely polynomial rings, ideals, and bases of ideals. We use
extensively the perspective obtained in previous works in Algebraic Statistics.
We prove that the constraints on $\Box a$ can be derived through a binomial
ideal containing a toric ideal and we give sufficient conditions under which
the toric ideal fully describes the constraints.
",0,0,1,0,0,0
1986,Metadynamics for Training Neural Network Model Chemistries: a Competitive Assessment,"  Neural network (NN) model chemistries (MCs) promise to facilitate the
accurate exploration of chemical space and simulation of large reactive
systems. One important path to improving these models is to add layers of
physical detail, especially long-range forces. At short range, however, these
models are data driven and data limited. Little is systematically known about
how data should be sampled, and `test data' chosen randomly from some sampling
techniques can provide poor information about generality. If the sampling
method is narrow `test error' can appear encouragingly tiny while the model
fails catastrophically elsewhere. In this manuscript we competitively evaluate
two common sampling methods: molecular dynamics (MD), normal-mode sampling
(NMS) and one uncommon alternative, Metadynamics (MetaMD), for preparing
training geometries. We show that MD is an inefficient sampling method in the
sense that additional samples do not improve generality. We also show MetaMD is
easily implemented in any NNMC software package with cost that scales linearly
with the number of atoms in a sample molecule. MetaMD is a black-box way to
ensure samples always reach out to new regions of chemical space, while
remaining relevant to chemistry near $k_bT$. It is one cheap tool to address
the issue of generalization.
",0,1,0,1,0,0
1987,ServeNet: A Deep Neural Network for Web Service Classification,"  Automated service classification plays a crucial role in service management
such as service discovery, selection, and composition. In recent years, machine
learning techniques have been used for service classification. However, they
can only predict around 10 to 20 service categories due to the quality of
feature engineering and the imbalance problem of service dataset. In this
paper, we present a deep neural network ServeNet with a novel dataset splitting
algorithm to deal with these issues. ServeNet can automatically abstract
low-level representation to high-level features, and then predict service
classification based on the service datasets produced by the proposed splitting
algorithm. To demonstrate the effectiveness of our approach, we conducted a
comprehensive experimental study on 10,000 real-world services in 50
categories. The result shows that ServeNet can achieve higher accuracy than
other machine learning methods.
",0,0,0,1,0,0
1988,Photoinduced Hund excitons in the breakdown of a two-orbital Mott insulator,"  We study the photoinduced breakdown of a two-orbital Mott insulator and
resulting metallic state. Using time-dependent density matrix renormalization
group, we scrutinize the real-time dynamics of the half-filled two-orbital
Hubbard model interacting with a resonant radiation field pulse. The breakdown,
caused by production of doublon-holon pairs, is enhanced by Hund's exchange,
which dynamically activates large orbital fluctuations. The melting of the Mott
insulator is accompanied by a high to low spin transition with a concomitant
reduction of antiferromagnetic spin fluctuations. Most notably, the overall
time response is driven by the photogeneration of excitons with orbital
character that are stabilized by Hund's coupling. These unconventional ""Hund
excitons"" correspond to bound spin-singlet orbital-triplet doublon-holon pairs.
We study exciton properties such as bandwidth, binding potential, and size
within a semiclassical approach. The photometallic state results from a
coexistence of Hund excitons and doublon-holon plasma.
",0,1,0,0,0,0
1989,Using Synthetic Data to Train Neural Networks is Model-Based Reasoning,"  We draw a formal connection between using synthetic training data to optimize
neural network parameters and approximate, Bayesian, model-based reasoning. In
particular, training a neural network using synthetic data can be viewed as
learning a proposal distribution generator for approximate inference in the
synthetic-data generative model. We demonstrate this connection in a
recognition task where we develop a novel Captcha-breaking architecture and
train it using synthetic data, demonstrating both state-of-the-art performance
and a way of computing task-specific posterior uncertainty. Using a neural
network trained this way, we also demonstrate successful breaking of real-world
Captchas currently used by Facebook and Wikipedia. Reasoning from these
empirical results and drawing connections with Bayesian modeling, we discuss
the robustness of synthetic data results and suggest important considerations
for ensuring good neural network generalization when training with synthetic
data.
",1,0,0,1,0,0
1990,Multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory,"  Learning and memory are intertwined in our brain and their relationship is at
the core of several recent neural network models. In particular, the
Attention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning
network with an emphasis on biological plausibility of memory dynamics and
learning. We find that the AuGMEnT network does not solve some hierarchical
tasks, where higher-level stimuli have to be maintained over a long time, while
lower-level stimuli need to be remembered and forgotten over a shorter
timescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky
or short-timescale and non-leaky or long-timescale units in memory, that allow
to exchange lower-level information while maintaining higher-level one, thus
solving both hierarchical and distractor tasks.
",1,0,0,1,0,0
1991,Dynamical structure of entangled polymers simulated under shear flow,"  The non-linear response of entangled polymers to shear flow is complicated.
Its current understanding is framed mainly as a rheological description in
terms of the complex viscosity. However, the full picture requires an
assessment of the dynamical structure of individual polymer chains which give
rise to the macroscopic observables. Here we shed new light on this problem,
using a computer simulation based on a blob model, extended to describe shear
flow in polymer melts and semi-dilute solutions. We examine the diffusion and
the intermediate scattering spectra during a steady shear flow. The relaxation
dynamics are found to speed up along the flow direction, but slow down along
the shear gradient direction. The third axis, vorticity, shows a slowdown at
the short scale of a tube, but reaches a net speedup at the large scale of the
chain radius of gyration.
",0,1,0,0,0,0
1992,Coherent modulation up to 100 GBd 16QAM using silicon-organic hybrid (SOH) devices,"  We demonstrate the generation of higher-order modulation formats using
silicon-based inphase/quadrature (IQ) modulators at symbol rates of up to 100
GBd. Our devices exploit the advantages of silicon-organic hybrid (SOH)
integration, which combines silicon-on-insulator waveguides with highly
efficient organic electro-optic (EO) cladding materials to enable small drive
voltages and sub-millimeter device lengths. In our experiments, we use an SOH
IQ modulator with a {\pi}-voltage of 1.6 V to generate 100 GBd 16QAM signals.
This is the first time that the 100 GBd mark is reached with an IQ modulator
realized on a semiconductor substrate, leading to a single-polarization line
rate of 400 Gbit/s. The peak-to-peak drive voltages amount to 1.5 Vpp,
corresponding to an electrical energy dissipation in the modulator of only 25
fJ/bit.
",0,1,0,0,0,0
1993,Current induced magnetization switching in PtCoCr structures with enhanced perpendicular magnetic anisotropy and spin-orbit torques,"  Magnetic trilayers having large perpendicular magnetic anisotropy (PMA) and
high spin-orbit torques (SOTs) efficiency are the key to fabricate nonvolatile
magnetic memory and logic devices. In this work, PMA and SOTs are
systematically studied in Pt/Co/Cr stacks as a function of Cr thickness. An
enhanced perpendicular anisotropy field around 10189 Oe is obtained and is
related to the interface between Co and Cr layers. In addition, an effective
spin Hall angle up to 0.19 is observed due to the improved antidamping-like
torque by employing dissimilar metals Pt and Cr with opposite signs of spin
Hall angles on opposite sides of Co layer. Finally, we observed a nearly linear
dependence between spin Hall angle and longitudinal resistivity from their
temperature dependent properties, suggesting that the spin Hall effect may
arise from extrinsic skew scattering mechanism. Our results indicate that 3d
transition metal Cr with a large negative spin Hall angle could be used to
engineer the interfaces of trilayers to enhance PMA and SOTs.
",0,1,0,0,0,0
1994,Grain boundary diffusion in severely deformed Al-based alloy,"  Grain boundary diffusion in severely deformed Al-based AA5024 alloy is
investigated. Different states are prepared by combination of equal channel
angular processing and heat treatments, with the radioisotope $^{57}$Co being
employed as a sensitive probe of a given grain boundary state. Its diffusion
rates near room temperature (320~K) are utilized to quantify the effects of
severe plastic deformation and a presumed formation of a previously reported
deformation-modified state of grain boundaries, solute segregation at the
interfaces, increased dislocation content after deformation and of the
precipitation behavior on the transport phenomena along grain boundaries. The
dominant effect of nano-sized Al$_3$Sc-based precipitates is evaluated using
density functional theory and the Eshelby model for the determination of
elastic stresses around the precipitates.
",0,1,0,0,0,0
1995,Quantum Black Holes and Atomic Nuclei are Hollow,"  The quantum Schrodinger-Newton equation is solved for a self-gravitating Bose
gas at zero temperature. It is derived that the density is non-uniform and a
central hollow cavity exists. The radial distribution of the particle momentum
is uniform. It is shown that a quantum black hole can be formed only above a
certain critical mass. The temperature effect is accounted for via the
Schrodinger-Poisson-Boltzmann equation, where low and high temperature
solutions are obtained. The theoretical analysis is extended to a strong
interacting gas via the Schrodinger-Yukawa equation, showing that the atomic
nuclei are also hollow. Hollow self-gravitating Fermi gases are described by
the Thomas-Fermi equation.
",0,1,0,0,0,0
1996,Learning Non-Discriminatory Predictors,"  We consider learning a predictor which is non-discriminatory with respect to
a ""protected attribute"" according to the notion of ""equalized odds"" proposed by
Hardt et al. [2016]. We study the problem of learning such a non-discriminatory
predictor from a finite training set, both statistically and computationally.
We show that a post-hoc correction approach, as suggested by Hardt et al, can
be highly suboptimal, present a nearly-optimal statistical procedure, argue
that the associated computational problem is intractable, and suggest a second
moment relaxation of the non-discrimination definition for which learning is
tractable.
",1,0,0,0,0,0
1997,Time-Resolved High Spectral Resolution Observation of 2MASSW J0746425+200032AB,"  Many brown dwarfs exhibit photometric variability at levels from tenths to
tens of percents. The photometric variability is related to magnetic activity
or patchy cloud coverage, characteristic of brown dwarfs near the L-T
transition. Time-resolved spectral monitoring of brown dwarfs provides
diagnostics of cloud distribution and condensate properties. However, current
time-resolved spectral studies of brown dwarfs are limited to low spectral
resolution (R$\sim$100) with the exception of the study of Luhman 16 AB at
resolution of 100,000 using the VLT$+$CRIRES. This work yielded the first map
of brown dwarf surface inhomogeneity, highlighting the importance and unique
contribution of high spectral resolution observations. Here, we report on the
time-resolved high spectral resolution observations of a nearby brown dwarf
binary, 2MASSW J0746425+200032AB. We find no coherent spectral variability that
is modulated with rotation. Based on simulations we conclude that the coverage
of a single spot on 2MASSW J0746425+200032AB is smaller than 1\% or 6.25\% if
spot contrast is 50\% or 80\% of its surrounding flux, respectively. Future
high spectral resolution observations aided by adaptive optics systems can put
tighter constraints on the spectral variability of 2MASSW J0746425+200032AB and
other nearby brown dwarfs.
",0,1,0,0,0,0
1998,"Pinned, locked, pushed, and pulled traveling waves in structured environments","  Traveling fronts describe the transition between two alternative states in a
great number of physical and biological systems. Examples include the spread of
beneficial mutations, chemical reactions, and the invasions by foreign species.
In homogeneous environments, the alternative states are separated by a smooth
front moving at a constant velocity. This simple picture can break down in
structured environments such as tissues, patchy landscapes, and microfluidic
devices. Habitat fragmentation can pin the front at a particular location or
lock invasion velocities into specific values. Locked velocities are not
sensitive to moderate changes in dispersal or growth and are determined by the
spatial and temporal periodicity of the environment. The synchronization with
the environment results in discontinuous fronts that propagate as periodic
pulses. We characterize the transition from continuous to locked invasions and
show that it is controlled by positive density-dependence in dispersal or
growth. We also demonstrate that velocity locking is robust to demographic and
environmental fluctuations and examine stochastic dynamics and evolution in
locked invasions.
",0,0,0,0,1,0
1999,Two-pixel polarimetric camera by compressive sensing,"  We propose an original concept of compressive sensing (CS) polarimetric
imaging based on a digital micro-mirror (DMD) array and two single-pixel
detectors. The polarimetric sensitivity of the proposed setup is due to an
experimental imperfection of reflecting mirrors which is exploited here to form
an original reconstruction problem, including a CS problem and a source
separation task. We show that a two-step approach tackling each problem
successively is outperformed by a dedicated combined reconstruction method,
which is explicited in this article and preferably implemented through a
reweighted FISTA algorithm. The combined reconstruction approach is then
further improved by including physical constraints specific to the polarimetric
imaging context considered, which are implemented in an original constrained
GFB algorithm. Numerical simulations demonstrate the efficiency of the 2-pixel
CS polarimetric imaging setup to retrieve polarimetric contrast data with
significant compression rate and good reconstruction quality. The influence of
experimental imperfections of the DMD are also analyzed through numerical
simulations, and 2D polarimetric imaging reconstruction results are finally
presented.
",1,0,0,0,0,0
2000,A Stochastic Programming Approach for Electric Vehicle Charging Network Design,"  Advantages of electric vehicles (EV) include reduction of greenhouse gas and
other emissions, energy security, and fuel economy. The societal benefits of
large-scale adoption of EVs cannot be realized without adequate deployment of
publicly accessible charging stations. We propose a two-stage stochastic
programming model to determine the optimal network of charging stations for a
community considering uncertainties in arrival and dwell time of vehicles,
battery state of charge of arriving vehicles, walkable range and charging
preferences of drivers, demand during weekdays and weekends, and rate of
adoption of EVs within a community. We conducted studies using sample average
approximation (SAA) method which asymptotically converges to an optimal
solution for a two-stage stochastic problem, however it is computationally
expensive for large-scale instances. Therefore, we developed a heuristic to
produce near to optimal solutions quickly for our data instances. We conducted
computational experiments using various publicly available data sources, and
benefits of the solutions are evaluated both quantitatively and qualitatively
for a given community.
",1,0,1,0,0,0
2001,Autonomy in the interactive music system VIVO,"  Interactive Music Systems (IMS) have introduced a new world of music-making
modalities. But can we really say that they create music, as in true autonomous
creation? Here we discuss Video Interactive VST Orchestra (VIVO), an IMS that
considers extra-musical information by adopting a simple salience based model
of user-system interaction when simulating intentionality in automatic music
generation. Key features of the theoretical framework, a brief overview of
pilot research, and a case study providing validation of the model are
presented. This research demonstrates that a meaningful user/system interplay
is established in what we define as reflexive multidominance.
",1,0,0,0,0,0
2002,Information and estimation in Fokker-Planck channels,"  We study the relationship between information- and estimation-theoretic
quantities in time-evolving systems. We focus on the Fokker-Planck channel
defined by a general stochastic differential equation, and show that the time
derivatives of entropy, KL divergence, and mutual information are characterized
by estimation-theoretic quantities involving an appropriate generalization of
the Fisher information. Our results vastly extend De Bruijn's identity and the
classical I-MMSE relation.
",1,0,1,1,0,0
2003,"The role of industry, occupation, and location specific knowledge in the survival of new firms","  How do regions acquire the knowledge they need to diversify their economic
activities? How does the migration of workers among firms and industries
contribute to the diffusion of that knowledge? Here we measure the industry,
occupation, and location-specific knowledge carried by workers from one
establishment to the next using a dataset summarizing the individual work
history for an entire country. We study pioneer firms--firms operating in an
industry that was not present in a region--because the success of pioneers is
the basic unit of regional economic diversification. We find that the growth
and survival of pioneers increase significantly when their first hires are
workers with experience in a related industry, and with work experience in the
same location, but not with past experience in a related occupation. We compare
these results with new firms that are not pioneers and find that
industry-specific knowledge is significantly more important for pioneer than
non-pioneer firms. To address endogeneity we use Bartik instruments, which
leverage national fluctuations in the demand for an activity as shocks for
local labor supply. The instrumental variable estimates support the finding
that industry-related knowledge is a predictor of the survival and growth of
pioneer firms. These findings expand our understanding of the micro-mechanisms
underlying regional economic diversification events.
",0,0,0,0,0,1
2004,Bayes-Optimal Entropy Pursuit for Active Choice-Based Preference Learning,"  We analyze the problem of learning a single user's preferences in an active
learning setting, sequentially and adaptively querying the user over a finite
time horizon. Learning is conducted via choice-based queries, where the user
selects her preferred option among a small subset of offered alternatives.
These queries have been shown to be a robust and efficient way to learn an
individual's preferences. We take a parametric approach and model the user's
preferences through a linear classifier, using a Bayesian prior to encode our
current knowledge of this classifier. The rate at which we learn depends on the
alternatives offered at every time epoch. Under certain noise assumptions, we
show that the Bayes-optimal policy for maximally reducing entropy of the
posterior distribution of this linear classifier is a greedy policy, and that
this policy achieves a linear lower bound when alternatives can be constructed
from the continuum. Further, we analyze a different metric called
misclassification error, proving that the performance of the optimal policy
that minimizes misclassification error is bounded below by a linear function of
differential entropy. Lastly, we numerically compare the greedy entropy
reduction policy with a knowledge gradient policy under a number of scenarios,
examining their performance under both differential entropy and
misclassification error.
",1,0,0,1,0,0
2005,Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes,"  It is widely observed that deep learning models with learned parameters
generalize well, even with much more model parameters than the number of
training samples. We systematically investigate the underlying reasons why deep
neural networks often generalize well, and reveal the difference between the
minima (with the same training error) that generalize well and those they
don't. We show that it is the characteristics the landscape of the loss
function that explains the good generalization capability. For the landscape of
loss function for deep networks, the volume of basin of attraction of good
minima dominates over that of poor minima, which guarantees optimization
methods with random initialization to converge to good minima. We theoretically
justify our findings through analyzing 2-layer neural networks; and show that
the low-complexity solutions have a small norm of Hessian matrix with respect
to model parameters. For deeper networks, extensive numerical evidence helps to
support our arguments.
",1,0,0,1,0,0
2006,Benchmarking Decoupled Neural Interfaces with Synthetic Gradients,"  Artifical Neural Networks are a particular class of learning systems modeled
after biological neural functions with an interesting penchant for Hebbian
learning, that is ""neurons that wire together, fire together"". However, unlike
their natural counterparts, artificial neural networks have a close and
stringent coupling between the modules of neurons in the network. This coupling
or locking imposes upon the network a strict and inflexible structure that
prevent layers in the network from updating their weights until a full
feed-forward and backward pass has occurred. Such a constraint though may have
sufficed for a while, is now no longer feasible in the era of very-large-scale
machine learning, coupled with the increased desire for parallelization of the
learning process across multiple computing infrastructures. To solve this
problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are
introduced as a viable alternative to the backpropagation algorithm. This paper
performs a speed benchmark to compare the speed and accuracy capabilities of
SG-DNI as opposed to a standard neural interface using multilayer perceptron
MLP. SG-DNI shows good promise, in that it not only captures the learning
problem, it is also over 3-fold faster due to it asynchronous learning
capabilities.
",1,0,0,1,0,0
2007,Macquarie University at BioASQ 5b -- Query-based Summarisation Techniques for Selecting the Ideal Answers,"  Macquarie University's contribution to the BioASQ challenge (Task 5b Phase B)
focused on the use of query-based extractive summarisation techniques for the
generation of the ideal answers. Four runs were submitted, with approaches
ranging from a trivial system that selected the first $n$ snippets, to the use
of deep learning approaches under a regression framework. Our experiments and
the ROUGE results of the five test batches of BioASQ indicate surprisingly good
results for the trivial approach. Overall, most of our runs on the first three
test batches achieved the best ROUGE-SU4 results in the challenge.
",1,0,0,0,0,0
2008,Network Topology Modulation for Energy and Data Transmission in Internet of Magneto-Inductive Things,"  Internet-of-things (IoT) architectures connecting a massive number of
heterogeneous devices need energy efficient, low hardware complexity, low cost,
simple and secure mechanisms to realize communication among devices. One of the
emerging schemes is to realize simultaneous wireless information and power
transfer (SWIPT) in an energy harvesting network. Radio frequency (RF)
solutions require special hardware and modulation methods for RF to direct
current (DC) conversion and optimized operation to achieve SWIPT which are
currently in an immature phase. On the other hand, magneto-inductive (MI)
communication transceivers are intrinsically energy harvesting with potential
for SWIPT in an efficient manner. In this article, novel modulation and
demodulation mechanisms are presented in a combined framework with
multiple-access channel (MAC) communication and wireless power transmission.
The network topology of power transmitting active coils in a transceiver
composed of a grid of coils is changed as a novel method to transmit
information. Practical demodulation schemes are formulated and numerically
simulated for two-user MAC topology of small size coils. The transceivers are
suitable to attach to everyday objects to realize reliable local area network
(LAN) communication performances with tens of meters communication ranges. The
designed scheme is promising for future IoT applications requiring SWIPT with
energy efficient, low cost, low power and low hardware complexity solutions.
",1,0,1,0,0,0
2009,Core structure of two-dimensional Fermi gas vortices in the BEC-BCS crossover region,"  We report $T=0$ diffusion Monte Carlo results for the ground-state and vortex
excitation of unpolarized spin-1/2 fermions in a two-dimensional disk. We
investigate how vortex core structure properties behave over the BEC-BCS
crossover. We calculate the vortex excitation energy, density profiles, and
vortex core properties related to the current. We find a density suppression at
the vortex core on the BCS side of the crossover, and a depleted core on the
BEC limit. Size-effect dependencies in the disk geometry were carefully
studied.
",0,1,0,0,0,0
2010,One can hear the Euler characteristic of a simplicial complex,"  We prove that that the number p of positive eigenvalues of the connection
Laplacian L of a finite abstract simplicial complex G matches the number b of
even dimensional simplices in G and that the number n of negative eigenvalues
matches the number f of odd-dimensional simplices in G. The Euler
characteristic X(G) of G therefore can be spectrally described as X(G)=p-n.
This is in contrast to the more classical Hodge Laplacian H which acts on the
same Hilbert space, where X(G) is not yet known to be accessible from the
spectrum of H. Given an ordering of G coming from a build-up as a CW complex,
every simplex x in G is now associated to a unique eigenvector of L and the
correspondence is computable. The Euler characteristic is now not only the
potential energy summing over all g(x,y) with g=L^{-1} but also agrees with a
logarithmic energy tr(log(i L)) 2/(i pi) of the spectrum of L. We also give
here examples of L-isospectral but non-isomorphic abstract finite simplicial
complexes. One example shows that we can not hear the cohomology of the
complex.
",1,0,1,0,0,0
2011,A new complete Calabi-Yau metric on $\mathbb{C}^3$,"  Motivated by the study of collapsing Calabi-Yau threefolds with a Lefschetz
K3 fibration, we construct a complete Calabi-Yau metric on $\mathbb{C}^3$ with
maximal volume growth, which in the appropriate scale is expected to model the
collapsing metric near the nodal point. This new Calabi-Yau metric has singular
tangent cone at infinity, and its Riemannian geometry has certain non-standard
features near the singularity of the tangent cone $\mathbb{C}^2/\mathbb{Z}_2
\times \mathbb{C}$, which are more typical of adiabatic limit problems. The
proof uses an existence result in H-J. Hein's PhD thesis to perturb an
asymptotic approximate solution into an actual solution, and the main
difficulty lies in correcting the slowly decaying error terms.
",0,0,1,0,0,0
2012,Weighted density fields as improved probes of modified gravity models,"  When it comes to searches for extensions to general relativity, large efforts
are being dedicated to accurate predictions for the power spectrum of density
perturbations. While this observable is known to be sensitive to the
gravitational theory, its efficiency as a diagnostic for gravity is
significantly reduced when Solar System constraints are strictly adhered to. We
show that this problem can be overcome by studying weigthed density fields. We
propose a transformation of the density field for which the impact of modified
gravity on the power spectrum can be increased by more than a factor of three.
The signal is not only amplified, but the modified gravity features are shifted
to larger scales which are less affected by baryonic physics. Furthermore, the
overall signal-to-noise increases, which in principle makes identifying
signatures of modified gravity with future galaxy surveys more feasible. While
our analysis is focused on modified gravity, the technique can be applied to
other problems in cosmology, such as the detection of neutrinos, the effects of
baryons or baryon acoustic oscillations.
",0,1,0,0,0,0
2013,Nonlinear control for an uncertain electromagnetic actuator,"  This paper presents the design of a nonlinear control law for a typical
electromagnetic actuator system. Electromagnetic actuators are widely
implemented in industrial applications, and especially as linear positioning
system. In this work, we aim at taking into account a magnetic phenomenon that
is usually neglected: flux fringing. This issue is addressed with an uncertain
modeling approach. The proposed control law consists of two steps, a
backstepping control regulates the mechanical part and a sliding mode approach
controls the coil current and the magnetic force implicitly. An illustrative
example shows the effectiveness of the presented approach.
",1,0,0,0,0,0
